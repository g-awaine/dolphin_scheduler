[WI-0][TI-0] - [INFO] 2024-04-23 12:01:02.873 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7443820224719101 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:01:03.892 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7527173913043478 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:02:04.324 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7917888563049853 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:02:05.373 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7479892761394102 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1144] - [INFO] 2024-04-23 12:02:17.611 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1144, processInstanceId=489, status=9, startTime=1713840953087, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/80/489/1144.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_80/489/1144, endTime=1713841331355, processId=3026, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713844636735)
[WI-0][TI-1144] - [INFO] 2024-04-23 12:02:17.616 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1144, processInstanceId=489, status=9, startTime=1713840953087, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/80/489/1144.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_80/489/1144, endTime=1713841331355, processId=3026, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713844937611)
[WI-0][TI-0] - [INFO] 2024-04-23 12:02:58.569 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1195, taskName=search for intake JSON files, firstSubmitTime=1713844978557, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=84, appIds=null, processInstanceId=496, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240423'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1195'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340113777664'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240423120258'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='496'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240422'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.571 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake JSON files to wait queue success
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.572 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.573 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.573 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.573 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.573 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713844978573
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.574 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 496_1195
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.574 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1195,
  "taskName" : "search for intake JSON files",
  "firstSubmitTime" : 1713844978557,
  "startTime" : 1713844978573,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240423/13201021801792/84/496/1195.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 84,
  "processInstanceId" : 496,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set raw_file_dir to null\\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1195"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340113777664"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423120258"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "496"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240422"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "496_1195",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.575 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.576 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.576 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.583 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.589 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.591 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/496/1195 check successfully
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.591 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.591 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.591 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.591 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.591 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"",
  "resourceList" : [ ]
}
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.591 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.591 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.591 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.591 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.591 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.593 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.593 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.593 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/reddit/processing -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set raw_file_dir to null
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
fi

echo "#{setValue(raw_file_dir=${raw_file_dir})}"
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.593 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.593 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/496/1195/496_1195.sh
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:58.618 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 5596
[WI-0][TI-0] - [INFO] 2024-04-23 12:02:58.618 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1195] - [INFO] 2024-04-23 12:02:58.676 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1195, processInstanceId=496, startTime=1713844978573, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/84/496/1195.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1195] - [INFO] 2024-04-23 12:02:58.679 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1195, processInstanceId=496, startTime=1713844978573, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/84/496/1195.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1713844978675)
[WI-0][TI-1195] - [INFO] 2024-04-23 12:02:58.679 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1195, processInstanceId=496, startTime=1713844978573, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1195] - [INFO] 2024-04-23 12:02:58.686 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1195, processInstanceId=496, startTime=1713844978573, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1713844978675)
[WI-0][TI-1195] - [INFO] 2024-04-23 12:02:58.689 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1195, success=true)
[WI-0][TI-1195] - [INFO] 2024-04-23 12:02:58.694 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1195)
[WI-0][TI-1195] - [INFO] 2024-04-23 12:02:58.700 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1195, success=true)
[WI-0][TI-1195] - [INFO] 2024-04-23 12:02:58.706 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1195)
[WI-0][TI-0] - [INFO] 2024-04-23 12:02:59.626 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	#{setValue(raw_file_dir=/local_storage/reddit/processing/386-20240422.json)}
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:59.627 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/496/1195, processId:5596 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:59.629 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:59.629 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:59.629 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:59.630 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:59.632 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:59.632 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:59.632 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/496/1195
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:59.632 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/496/1195
[WI-496][TI-1195] - [INFO] 2024-04-23 12:02:59.633 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1195] - [INFO] 2024-04-23 12:02:59.692 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1195, processInstanceId=496, status=7, startTime=1713844978573, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/84/496/1195.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/496/1195, endTime=1713844979629, processId=5596, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1195] - [INFO] 2024-04-23 12:02:59.703 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1195, processInstanceId=496, status=7, startTime=1713844978573, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/84/496/1195.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/496/1195, endTime=1713844979629, processId=5596, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713844979692)
[WI-0][TI-1195] - [INFO] 2024-04-23 12:02:59.704 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1195, success=true)
[WI-0][TI-1195] - [INFO] 2024-04-23 12:02:59.707 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1195, success=true)
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:00.814 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1197, taskName=move to processing, firstSubmitTime=1713844980807, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=84, appIds=null, processInstanceId=496, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to processing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240423'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1197'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340390094208'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240423120300'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='496'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/386-20240422.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240422'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.816 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to processing to wait queue success
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.817 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.818 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.818 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.819 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.819 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713844980819
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.819 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 496_1197
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.819 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1197,
  "taskName" : "move to processing",
  "firstSubmitTime" : 1713844980807,
  "startTime" : 1713844980819,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240423/13201021801792/84/496/1197.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 84,
  "processInstanceId" : 496,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the reddit processing folder\\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\\nfilename=$(basename \\\"${raw_file_dir}\\\")\\nif ! grep -q \\\"${REDDIT_HOME}/processing\\\" <<< \\\"${raw_file_dir}\\\"; then\\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\\nelse\\n    echo JSON is already in processing\\nfi\\n\\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\\necho \\\"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to processing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1197"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340390094208"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423120300"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "496"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/386-20240422.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240422"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "496_1197",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/386-20240422.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.820 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.820 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.820 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.847 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.848 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.849 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/496/1197 check successfully
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.849 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.849 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.850 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.850 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.850 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"",
  "resourceList" : [ ]
}
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.850 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.851 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}] successfully
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.851 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.851 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.851 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.852 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.853 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.853 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the reddit processing folder
# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error
filename=$(basename "/local_storage/reddit/processing/386-20240422.json")
if ! grep -q "/local_storage/reddit/processing" <<< "/local_storage/reddit/processing/386-20240422.json"; then
    mv /local_storage/reddit/processing/386-20240422.json /local_storage/reddit/processing
else
    echo JSON is already in processing
fi

# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing
echo "#{setValue(raw_file_dir=/local_storage/reddit/processing/${filename})}"
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.854 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.854 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/496/1197/496_1197.sh
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:00.858 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 5610
[WI-0][TI-1197] - [INFO] 2024-04-23 12:03:01.702 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1197, success=true)
[WI-0][TI-1197] - [INFO] 2024-04-23 12:03:01.705 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1197, processInstanceId=496, startTime=1713844980819, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1197] - [INFO] 2024-04-23 12:03:01.707 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1197, processInstanceId=496, startTime=1713844980819, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1713844981705)
[WI-0][TI-1197] - [INFO] 2024-04-23 12:03:01.709 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1197)
[WI-0][TI-1197] - [INFO] 2024-04-23 12:03:01.714 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1197)
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:01.860 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	JSON is already in processing
	#{setValue(raw_file_dir=/local_storage/reddit/processing/386-20240422.json)}
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:01.865 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/496/1197, processId:5610 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:01.868 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:01.868 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:01.869 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:01.870 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:01.872 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:01.872 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:01.872 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/496/1197
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:01.873 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/496/1197
[WI-496][TI-1197] - [INFO] 2024-04-23 12:03:01.873 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1197] - [INFO] 2024-04-23 12:03:02.699 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1197, success=true)
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:02.734 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1198, taskName=spark preprocessing, firstSubmitTime=1713844982728, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=84, appIds=null, processInstanceId=496, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# $SPARK_HOME/bin/spark-submit \\\n#     --master spark://spark-master:7077 \\\n#     --conf spark.driver.cores=2 \\\n#     --conf spark.driver.memory=2G \\\n#     --conf spark.executor.instances=1 \\\n#     --conf spark.executor.cores=2 \\\n#     --conf spark.executor.memory=2G \\\n#     --files ${raw_file_dir} \\\n#     --name reddit_preprocessing spark_reddit_preprocessing.py\n\n$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --files ${raw_file_dir} \\\n    --name reddit_preprocessing spark_reddit_preprocessing.py\n","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='spark preprocessing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240423'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1198'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13210058002752'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240423120302'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='496'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/386-20240422.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240422'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.735 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: spark preprocessing to wait queue success
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.736 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.739 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.739 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.739 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.739 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713844982739
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.739 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 496_1198
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.739 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1198,
  "taskName" : "spark preprocessing",
  "firstSubmitTime" : 1713844982728,
  "startTime" : 1713844982739,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240423/13201021801792/84/496/1198.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 84,
  "processInstanceId" : 496,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# $SPARK_HOME/bin/spark-submit \\\\\\n#     --master spark://spark-master:7077 \\\\\\n#     --conf spark.driver.cores=2 \\\\\\n#     --conf spark.driver.memory=2G \\\\\\n#     --conf spark.executor.instances=1 \\\\\\n#     --conf spark.executor.cores=2 \\\\\\n#     --conf spark.executor.memory=2G \\\\\\n#     --files ${raw_file_dir} \\\\\\n#     --name reddit_preprocessing spark_reddit_preprocessing.py\\n\\n$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --files ${raw_file_dir} \\\\\\n    --name reddit_preprocessing spark_reddit_preprocessing.py\\n\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "spark preprocessing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1198"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13210058002752"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423120302"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "496"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/386-20240422.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240422"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "496_1198",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/386-20240422.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.740 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.740 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.740 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.744 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.745 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.745 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/496/1198 check successfully
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.745 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.747 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py, resourceRelativePath=spark_reddit_preprocessing.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/496/1198/spark_reddit_preprocessing.py)})
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.747 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.747 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.747 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# $SPARK_HOME/bin/spark-submit \\\n#     --master spark://spark-master:7077 \\\n#     --conf spark.driver.cores=2 \\\n#     --conf spark.driver.memory=2G \\\n#     --conf spark.executor.instances=1 \\\n#     --conf spark.executor.cores=2 \\\n#     --conf spark.executor.memory=2G \\\n#     --files ${raw_file_dir} \\\n#     --name reddit_preprocessing spark_reddit_preprocessing.py\n\n$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --files ${raw_file_dir} \\\n    --name reddit_preprocessing spark_reddit_preprocessing.py\n",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py",
    "res" : null
  } ]
}
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.747 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.747 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}] successfully
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.748 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.748 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.748 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.748 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.748 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.748 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
# $SPARK_HOME/bin/spark-submit \
#     --master spark://spark-master:7077 \
#     --conf spark.driver.cores=2 \
#     --conf spark.driver.memory=2G \
#     --conf spark.executor.instances=1 \
#     --conf spark.executor.cores=2 \
#     --conf spark.executor.memory=2G \
#     --files /local_storage/reddit/processing/386-20240422.json \
#     --name reddit_preprocessing spark_reddit_preprocessing.py

$SPARK_HOME/bin/spark-submit \
    --master local \
    --files /local_storage/reddit/processing/386-20240422.json \
    --name reddit_preprocessing spark_reddit_preprocessing.py

[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.748 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.748 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/496/1198/496_1198.sh
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:02.766 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 5622
[WI-0][TI-1198] - [INFO] 2024-04-23 12:03:03.712 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1198, success=true)
[WI-0][TI-1198] - [INFO] 2024-04-23 12:03:03.714 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1198, processInstanceId=496, startTime=1713844982739, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/84/496/1198.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1198] - [INFO] 2024-04-23 12:03:03.717 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1198, processInstanceId=496, startTime=1713844982739, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/84/496/1198.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1713844983714)
[WI-0][TI-1198] - [INFO] 2024-04-23 12:03:03.719 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1198)
[WI-0][TI-1198] - [INFO] 2024-04-23 12:03:03.724 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1198, success=true)
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:03.768 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:05.603 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.92 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:05.777 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/23 12:03:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:06.619 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.853658536585366 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:07.627 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7781065088757396 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:07.783 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/23 12:03:06 INFO SparkContext: Running Spark version 3.5.1
	24/04/23 12:03:06 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/04/23 12:03:06 INFO SparkContext: Java version 1.8.0_402
	24/04/23 12:03:06 INFO ResourceUtils: ==============================================================
	24/04/23 12:03:06 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/04/23 12:03:06 INFO ResourceUtils: ==============================================================
	24/04/23 12:03:06 INFO SparkContext: Submitted application: reddit_preprocessing
	24/04/23 12:03:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/04/23 12:03:07 INFO ResourceProfile: Limiting resource is cpu
	24/04/23 12:03:07 INFO ResourceProfileManager: Added ResourceProfile id: 0
	24/04/23 12:03:07 INFO SecurityManager: Changing view acls to: default
	24/04/23 12:03:07 INFO SecurityManager: Changing modify acls to: default
	24/04/23 12:03:07 INFO SecurityManager: Changing view acls groups to: 
	24/04/23 12:03:07 INFO SecurityManager: Changing modify acls groups to: 
	24/04/23 12:03:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
	24/04/23 12:03:07 INFO Utils: Successfully started service 'sparkDriver' on port 39695.
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:08.629 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9047619047619048 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:08.790 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/23 12:03:07 INFO SparkEnv: Registering MapOutputTracker
	24/04/23 12:03:07 INFO SparkEnv: Registering BlockManagerMaster
	24/04/23 12:03:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/04/23 12:03:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/04/23 12:03:07 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/04/23 12:03:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2518b641-23e1-4762-acc5-3d64eff51912
	24/04/23 12:03:07 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
	24/04/23 12:03:07 INFO SparkEnv: Registering OutputCommitCoordinator
	24/04/23 12:03:08 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/04/23 12:03:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/04/23 12:03:08 INFO SparkContext: Added file file:///local_storage/reddit/processing/386-20240422.json at file:///local_storage/reddit/processing/386-20240422.json with timestamp 1713844986885
	24/04/23 12:03:08 INFO Utils: Copying /local_storage/reddit/processing/386-20240422.json to /tmp/spark-aece77ef-f402-4afe-b347-a84ec7c333cd/userFiles-1c803c73-ba41-4298-8e5c-eeb769aa7a02/386-20240422.json
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:09.630 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.879746835443038 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:09.802 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/23 12:03:08 INFO Executor: Starting executor ID driver on host c0e814e3f0bd
	24/04/23 12:03:08 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/04/23 12:03:08 INFO Executor: Java version 1.8.0_402
	24/04/23 12:03:09 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/04/23 12:03:09 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@4fcd1ee1 for default.
	24/04/23 12:03:09 INFO Executor: Fetching file:///local_storage/reddit/processing/386-20240422.json with timestamp 1713844986885
	24/04/23 12:03:09 INFO Utils: /local_storage/reddit/processing/386-20240422.json has been previously copied to /tmp/spark-aece77ef-f402-4afe-b347-a84ec7c333cd/userFiles-1c803c73-ba41-4298-8e5c-eeb769aa7a02/386-20240422.json
	24/04/23 12:03:09 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33471.
	24/04/23 12:03:09 INFO NettyBlockTransferService: Server created on c0e814e3f0bd:33471
	24/04/23 12:03:09 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/04/23 12:03:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, c0e814e3f0bd, 33471, None)
	24/04/23 12:03:09 INFO BlockManagerMasterEndpoint: Registering block manager c0e814e3f0bd:33471 with 366.3 MiB RAM, BlockManagerId(driver, c0e814e3f0bd, 33471, None)
	24/04/23 12:03:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, c0e814e3f0bd, 33471, None)
	24/04/23 12:03:09 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, c0e814e3f0bd, 33471, None)
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:10.632 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9301075268817205 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:10.804 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	
	
	 /tmp/spark-aece77ef-f402-4afe-b347-a84ec7c333cd/userFiles-1c803c73-ba41-4298-8e5c-eeb769aa7a02/386-20240422.json 
	
	
	
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:11.638 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8721590909090908 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:11.807 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/23 12:03:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 345.6 KiB, free 366.0 MiB)
	24/04/23 12:03:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 365.9 MiB)
	24/04/23 12:03:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on c0e814e3f0bd:33471 (size: 32.7 KiB, free: 366.3 MiB)
	24/04/23 12:03:11 INFO SparkContext: Created broadcast 0 from wholeTextFiles at NativeMethodAccessorImpl.java:0
	24/04/23 12:03:11 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/04/23 12:03:11 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/496/1198/spark-warehouse'.
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:12.639 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7966101694915254 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:15.662 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7011494252873564 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:17.692 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.847457627118644 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:17.822 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/23 12:03:17 INFO CodeGenerator: Code generated in 498.252859 ms
	24/04/23 12:03:17 INFO FileInputFormat: Total input files to process : 1
	24/04/23 12:03:17 INFO FileInputFormat: Total input files to process : 1
	24/04/23 12:03:17 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
	24/04/23 12:03:17 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/23 12:03:17 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
	24/04/23 12:03:17 INFO DAGScheduler: Parents of final stage: List()
	24/04/23 12:03:17 INFO DAGScheduler: Missing parents: List()
	24/04/23 12:03:17 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:18.748 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9149560117302052 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:18.831 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/23 12:03:18 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 23.3 KiB, free 365.9 MiB)
	24/04/23 12:03:18 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 11.4 KiB, free 365.9 MiB)
	24/04/23 12:03:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on c0e814e3f0bd:33471 (size: 11.4 KiB, free: 366.3 MiB)
	24/04/23 12:03:18 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
	24/04/23 12:03:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/23 12:03:18 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
	24/04/23 12:03:18 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (c0e814e3f0bd, executor driver, partition 0, PROCESS_LOCAL, 8011 bytes) 
	24/04/23 12:03:18 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:19.751 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8042168674698795 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:19.843 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/23 12:03:18 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-aece77ef-f402-4afe-b347-a84ec7c333cd/userFiles-1c803c73-ba41-4298-8e5c-eeb769aa7a02/386-20240422.json:0+71860
	24/04/23 12:03:19 INFO CodeGenerator: Code generated in 36.313819 ms
	24/04/23 12:03:19 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
	org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
	
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator.isEmpty(Iterator.scala:387)
		at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
		at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)
		at scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)
		at scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)
		at scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)
		at scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:20.753 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7522123893805309 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:03:20.846 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/23 12:03:19 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (c0e814e3f0bd executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
	
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator.isEmpty(Iterator.scala:387)
		at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
		at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)
		at scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)
		at scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)
		at scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)
		at scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	
	24/04/23 12:03:19 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
	24/04/23 12:03:19 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
	24/04/23 12:03:19 INFO TaskSchedulerImpl: Cancelling stage 0
	24/04/23 12:03:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (c0e814e3f0bd executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
	
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator.isEmpty(Iterator.scala:387)
		at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
		at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)
		at scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)
		at scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)
		at scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)
		at scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	
	Driver stacktrace:
	24/04/23 12:03:19 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) failed in 2.089 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (c0e814e3f0bd executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
	
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator.isEmpty(Iterator.scala:387)
		at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
		at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)
		at scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)
		at scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)
		at scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)
		at scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	
	Driver stacktrace:
	24/04/23 12:03:19 INFO DAGScheduler: Job 0 failed: json at NativeMethodAccessorImpl.java:0, took 2.275694 s
	24/04/23 12:03:20 INFO BlockManagerInfo: Removed broadcast_1_piece0 on c0e814e3f0bd:33471 in memory (size: 11.4 KiB, free: 366.3 MiB)
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/496/1198/spark_reddit_preprocessing.py", line 50, in <module>
	    raw_df = spark.read.json(raw_df)
	             ^^^^^^^^^^^^^^^^^^^^^^^
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 440, in json
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
	py4j.protocol.Py4JJavaError: An error occurred while calling o30.json.
	: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (c0e814e3f0bd executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
	
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator.isEmpty(Iterator.scala:387)
		at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
		at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)
		at scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)
		at scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)
		at scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)
		at scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	
	Driver stacktrace:
		at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
		at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
		at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
		at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
		at scala.Option.foreach(Option.scala:407)
		at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
		at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.infer(JsonInferSchema.scala:120)
		at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$5(JsonDataSource.scala:109)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:109)
		at org.apache.spark.sql.DataFrameReader.$anonfun$json$4(DataFrameReader.scala:416)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:416)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:391)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:377)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:282)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
	
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator.isEmpty(Iterator.scala:387)
		at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
		at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)
		at scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)
		at scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)
		at scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)
		at scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		... 1 more
	
	24/04/23 12:03:20 INFO SparkContext: Invoking stop() from shutdown hook
	24/04/23 12:03:20 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/04/23 12:03:20 INFO SparkUI: Stopped Spark web UI at http://c0e814e3f0bd:4040
	24/04/23 12:03:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/04/23 12:03:20 INFO MemoryStore: MemoryStore cleared
	24/04/23 12:03:20 INFO BlockManager: BlockManager stopped
	24/04/23 12:03:20 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/04/23 12:03:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/04/23 12:03:20 INFO SparkContext: Successfully stopped SparkContext
	24/04/23 12:03:20 INFO ShutdownHookManager: Shutdown hook called
	24/04/23 12:03:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-aece77ef-f402-4afe-b347-a84ec7c333cd
	24/04/23 12:03:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-985ae05d-9ac8-44e3-a6ac-97469b55d02a
	24/04/23 12:03:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-aece77ef-f402-4afe-b347-a84ec7c333cd/pyspark-98865241-7f31-4dd0-825f-a08a976c8313
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:20.852 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/496/1198, processId:5622 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:20.856 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:20.856 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:20.864 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:20.864 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:20.867 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:20.867 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:20.867 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/496/1198
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:20.868 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/496/1198
[WI-496][TI-1198] - [INFO] 2024-04-23 12:03:20.868 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1198] - [INFO] 2024-04-23 12:03:21.737 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1198, success=true)
[WI-0][TI-1123] - [INFO] 2024-04-23 12:04:55.460 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1123, processInstanceId=487, status=9, startTime=1713838208504, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1123.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1123, endTime=1713838483436, processId=1677, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713844795109)
[WI-0][TI-1123] - [INFO] 2024-04-23 12:04:55.468 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1123, processInstanceId=487, status=9, startTime=1713838208504, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1123.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1123, endTime=1713838483436, processId=1677, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713845095460)
[WI-0][TI-1111] - [INFO] 2024-04-23 12:04:57.470 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1111, processInstanceId=487, status=9, startTime=1713837256404, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1111.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1111, endTime=1713838170352, processId=1017, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713844797117)
[WI-0][TI-1111] - [INFO] 2024-04-23 12:04:57.472 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1111, processInstanceId=487, status=9, startTime=1713837256404, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1111.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1111, endTime=1713838170352, processId=1017, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713845097470)
[WI-0][TI-0] - [INFO] 2024-04-23 12:05:34.711 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7217391304347827 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:05:35.726 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8379888268156425 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:05:36.732 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7340720221606648 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:05:46.810 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1199, taskName=search for intake JSON files, firstSubmitTime=1713845146804, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=84, appIds=null, processInstanceId=497, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240423'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1199'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340113777664'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240423120546'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='497'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240422'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.811 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake JSON files to wait queue success
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.812 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.814 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.814 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.818 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.820 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713845146820
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.820 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 497_1199
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.821 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1199,
  "taskName" : "search for intake JSON files",
  "firstSubmitTime" : 1713845146804,
  "startTime" : 1713845146820,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240423/13201021801792/84/497/1199.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 84,
  "processInstanceId" : 497,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set raw_file_dir to null\\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1199"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340113777664"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423120546"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "497"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240422"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "497_1199",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.821 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.821 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.821 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.837 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.838 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.839 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/497/1199 check successfully
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.839 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.839 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.839 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.839 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.839 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"",
  "resourceList" : [ ]
}
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.840 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.840 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.840 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.840 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.840 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.841 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.842 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.842 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/reddit/processing -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set raw_file_dir to null
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
fi

echo "#{setValue(raw_file_dir=${raw_file_dir})}"
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.842 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.842 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/497/1199/497_1199.sh
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:46.856 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 5781
[WI-0][TI-1199] - [INFO] 2024-04-23 12:05:47.589 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1199, processInstanceId=497, startTime=1713845146820, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/84/497/1199.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1199] - [INFO] 2024-04-23 12:05:47.593 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1199, processInstanceId=497, startTime=1713845146820, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/84/497/1199.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1713845147588)
[WI-0][TI-1199] - [INFO] 2024-04-23 12:05:47.594 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1199, processInstanceId=497, startTime=1713845146820, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1199] - [INFO] 2024-04-23 12:05:47.597 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1199, processInstanceId=497, startTime=1713845146820, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1713845147588)
[WI-0][TI-1199] - [INFO] 2024-04-23 12:05:47.597 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1199, success=true)
[WI-0][TI-1199] - [INFO] 2024-04-23 12:05:47.607 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1199)
[WI-0][TI-1199] - [INFO] 2024-04-23 12:05:47.614 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1199, success=true)
[WI-0][TI-1199] - [INFO] 2024-04-23 12:05:47.622 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1199)
[WI-0][TI-0] - [INFO] 2024-04-23 12:05:47.939 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	#{setValue(raw_file_dir=/local_storage/reddit/processing/386-20240422.json)}
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:47.952 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/497/1199, processId:5781 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:47.953 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:47.954 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:47.954 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:47.954 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:47.959 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:47.959 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:47.959 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/497/1199
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:47.960 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/497/1199
[WI-497][TI-1199] - [INFO] 2024-04-23 12:05:47.960 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1199] - [INFO] 2024-04-23 12:05:48.599 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1199, processInstanceId=497, status=7, startTime=1713845146820, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/84/497/1199.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/497/1199, endTime=1713845147954, processId=5781, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1199] - [INFO] 2024-04-23 12:05:48.611 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1199, processInstanceId=497, status=7, startTime=1713845146820, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/84/497/1199.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/497/1199, endTime=1713845147954, processId=5781, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713845148598)
[WI-0][TI-1199] - [INFO] 2024-04-23 12:05:48.616 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1199, success=true)
[WI-0][TI-1199] - [INFO] 2024-04-23 12:05:48.619 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1199, success=true)
[WI-0][TI-0] - [INFO] 2024-04-23 12:05:49.668 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1201, taskName=move to processing, firstSubmitTime=1713845149659, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=84, appIds=null, processInstanceId=497, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to processing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240423'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1201'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340390094208'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240423120549'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='497'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/386-20240422.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240422'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.669 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to processing to wait queue success
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.670 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.671 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.672 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.672 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.672 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713845149672
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.672 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 497_1201
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.672 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1201,
  "taskName" : "move to processing",
  "firstSubmitTime" : 1713845149659,
  "startTime" : 1713845149672,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240423/13201021801792/84/497/1201.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 84,
  "processInstanceId" : 497,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the reddit processing folder\\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\\nfilename=$(basename \\\"${raw_file_dir}\\\")\\nif ! grep -q \\\"${REDDIT_HOME}/processing\\\" <<< \\\"${raw_file_dir}\\\"; then\\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\\nelse\\n    echo JSON is already in processing\\nfi\\n\\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\\necho \\\"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to processing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1201"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340390094208"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423120549"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "497"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/386-20240422.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240422"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "497_1201",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/386-20240422.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.673 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.673 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.673 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.711 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.719 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.720 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/497/1201 check successfully
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.721 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.721 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.723 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.723 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.723 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"",
  "resourceList" : [ ]
}
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.723 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.723 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}] successfully
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.723 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.723 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.724 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.724 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.728 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.733 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the reddit processing folder
# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error
filename=$(basename "/local_storage/reddit/processing/386-20240422.json")
if ! grep -q "/local_storage/reddit/processing" <<< "/local_storage/reddit/processing/386-20240422.json"; then
    mv /local_storage/reddit/processing/386-20240422.json /local_storage/reddit/processing
else
    echo JSON is already in processing
fi

# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing
echo "#{setValue(raw_file_dir=/local_storage/reddit/processing/${filename})}"
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.734 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.734 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/497/1201/497_1201.sh
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:49.741 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 5801
[WI-0][TI-1201] - [INFO] 2024-04-23 12:05:50.608 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1201, success=true)
[WI-0][TI-1201] - [INFO] 2024-04-23 12:05:50.613 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1201)
[WI-0][TI-0] - [INFO] 2024-04-23 12:05:50.739 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	JSON is already in processing
	#{setValue(raw_file_dir=/local_storage/reddit/processing/386-20240422.json)}
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:50.743 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/497/1201, processId:5801 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:50.743 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:50.743 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:50.743 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:50.744 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:50.747 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:50.747 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:50.747 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/497/1201
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:50.748 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/497/1201
[WI-497][TI-1201] - [INFO] 2024-04-23 12:05:50.748 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1201] - [INFO] 2024-04-23 12:05:51.604 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1201, success=true)
[WI-0][TI-0] - [INFO] 2024-04-23 12:05:51.707 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1202, taskName=spark preprocessing, firstSubmitTime=1713845151701, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=84, appIds=null, processInstanceId=497, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# $SPARK_HOME/bin/spark-submit \\\n#     --master spark://spark-master:7077 \\\n#     --conf spark.driver.cores=2 \\\n#     --conf spark.driver.memory=2G \\\n#     --conf spark.executor.instances=1 \\\n#     --conf spark.executor.cores=2 \\\n#     --conf spark.executor.memory=2G \\\n#     --files ${raw_file_dir} \\\n#     --name reddit_preprocessing spark_reddit_preprocessing.py\n\n$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --files ${raw_file_dir} \\\n    --name reddit_preprocessing spark_reddit_preprocessing.py\n","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='spark preprocessing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240423'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1202'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13210058002752'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240423120551'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='497'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/386-20240422.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240422'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.709 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: spark preprocessing to wait queue success
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.709 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.711 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.711 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.711 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.711 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713845151711
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.711 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 497_1202
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.711 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1202,
  "taskName" : "spark preprocessing",
  "firstSubmitTime" : 1713845151701,
  "startTime" : 1713845151711,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240423/13201021801792/84/497/1202.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 84,
  "processInstanceId" : 497,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# $SPARK_HOME/bin/spark-submit \\\\\\n#     --master spark://spark-master:7077 \\\\\\n#     --conf spark.driver.cores=2 \\\\\\n#     --conf spark.driver.memory=2G \\\\\\n#     --conf spark.executor.instances=1 \\\\\\n#     --conf spark.executor.cores=2 \\\\\\n#     --conf spark.executor.memory=2G \\\\\\n#     --files ${raw_file_dir} \\\\\\n#     --name reddit_preprocessing spark_reddit_preprocessing.py\\n\\n$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --files ${raw_file_dir} \\\\\\n    --name reddit_preprocessing spark_reddit_preprocessing.py\\n\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "spark preprocessing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1202"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13210058002752"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423120551"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "497"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/386-20240422.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240422"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "497_1202",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/386-20240422.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.712 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.712 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.712 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.714 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.714 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.715 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/497/1202 check successfully
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.716 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.720 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py, resourceRelativePath=spark_reddit_preprocessing.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/497/1202/spark_reddit_preprocessing.py)})
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.721 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.721 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.721 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# $SPARK_HOME/bin/spark-submit \\\n#     --master spark://spark-master:7077 \\\n#     --conf spark.driver.cores=2 \\\n#     --conf spark.driver.memory=2G \\\n#     --conf spark.executor.instances=1 \\\n#     --conf spark.executor.cores=2 \\\n#     --conf spark.executor.memory=2G \\\n#     --files ${raw_file_dir} \\\n#     --name reddit_preprocessing spark_reddit_preprocessing.py\n\n$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --files ${raw_file_dir} \\\n    --name reddit_preprocessing spark_reddit_preprocessing.py\n",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py",
    "res" : null
  } ]
}
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.722 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.722 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}] successfully
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.722 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.723 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.723 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.728 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.728 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.728 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
# $SPARK_HOME/bin/spark-submit \
#     --master spark://spark-master:7077 \
#     --conf spark.driver.cores=2 \
#     --conf spark.driver.memory=2G \
#     --conf spark.executor.instances=1 \
#     --conf spark.executor.cores=2 \
#     --conf spark.executor.memory=2G \
#     --files /local_storage/reddit/processing/386-20240422.json \
#     --name reddit_preprocessing spark_reddit_preprocessing.py

$SPARK_HOME/bin/spark-submit \
    --master local \
    --files /local_storage/reddit/processing/386-20240422.json \
    --name reddit_preprocessing spark_reddit_preprocessing.py

[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.728 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.728 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/497/1202/497_1202.sh
[WI-497][TI-1202] - [INFO] 2024-04-23 12:05:51.744 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 5813
[WI-0][TI-1202] - [INFO] 2024-04-23 12:05:52.612 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1202, success=true)
[WI-0][TI-1202] - [INFO] 2024-04-23 12:05:52.624 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1202)
[WI-0][TI-0] - [INFO] 2024-04-23 12:05:52.747 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-23 12:05:52.776 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8787878787878788 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:05:53.791 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9390581717451523 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:05:54.791 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8309455587392549 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:05:56.132 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/23 12:05:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-23 12:05:57.137 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/23 12:05:56 INFO SparkContext: Running Spark version 3.5.1
	24/04/23 12:05:56 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/04/23 12:05:56 INFO SparkContext: Java version 1.8.0_402
	24/04/23 12:05:56 INFO ResourceUtils: ==============================================================
	24/04/23 12:05:56 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/04/23 12:05:56 INFO ResourceUtils: ==============================================================
	24/04/23 12:05:56 INFO SparkContext: Submitted application: reddit_preprocessing
	24/04/23 12:05:56 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/04/23 12:05:56 INFO ResourceProfile: Limiting resource is cpu
	24/04/23 12:05:56 INFO ResourceProfileManager: Added ResourceProfile id: 0
	24/04/23 12:05:56 INFO SecurityManager: Changing view acls to: default
	24/04/23 12:05:56 INFO SecurityManager: Changing modify acls to: default
	24/04/23 12:05:56 INFO SecurityManager: Changing view acls groups to: 
	24/04/23 12:05:56 INFO SecurityManager: Changing modify acls groups to: 
	24/04/23 12:05:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
	24/04/23 12:05:56 INFO Utils: Successfully started service 'sparkDriver' on port 33539.
	24/04/23 12:05:56 INFO SparkEnv: Registering MapOutputTracker
	24/04/23 12:05:57 INFO SparkEnv: Registering BlockManagerMaster
	24/04/23 12:05:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/04/23 12:05:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/04/23 12:05:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/04/23 12:05:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ebd44284-67f0-48b0-a28e-505dd73289a8
	24/04/23 12:05:57 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[WI-0][TI-0] - [INFO] 2024-04-23 12:05:58.139 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/23 12:05:57 INFO SparkEnv: Registering OutputCommitCoordinator
	24/04/23 12:05:57 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/04/23 12:05:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/04/23 12:05:57 INFO SparkContext: Added file file:///local_storage/reddit/processing/386-20240422.json at file:///local_storage/reddit/processing/386-20240422.json with timestamp 1713845156236
	24/04/23 12:05:57 INFO Utils: Copying /local_storage/reddit/processing/386-20240422.json to /tmp/spark-a827e30f-6c58-48eb-ad94-502cf6d93137/userFiles-59bcdb93-f8c1-46df-9ce4-00612b78bf1b/386-20240422.json
	24/04/23 12:05:57 INFO Executor: Starting executor ID driver on host c0e814e3f0bd
	24/04/23 12:05:57 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/04/23 12:05:57 INFO Executor: Java version 1.8.0_402
	24/04/23 12:05:57 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/04/23 12:05:57 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@3fd2ac2b for default.
	24/04/23 12:05:57 INFO Executor: Fetching file:///local_storage/reddit/processing/386-20240422.json with timestamp 1713845156236
	24/04/23 12:05:57 INFO Utils: /local_storage/reddit/processing/386-20240422.json has been previously copied to /tmp/spark-a827e30f-6c58-48eb-ad94-502cf6d93137/userFiles-59bcdb93-f8c1-46df-9ce4-00612b78bf1b/386-20240422.json
	24/04/23 12:05:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45215.
	24/04/23 12:05:57 INFO NettyBlockTransferService: Server created on c0e814e3f0bd:45215
	24/04/23 12:05:57 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/04/23 12:05:57 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, c0e814e3f0bd, 45215, None)
	24/04/23 12:05:57 INFO BlockManagerMasterEndpoint: Registering block manager c0e814e3f0bd:45215 with 366.3 MiB RAM, BlockManagerId(driver, c0e814e3f0bd, 45215, None)
	24/04/23 12:05:57 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, c0e814e3f0bd, 45215, None)
	24/04/23 12:05:57 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, c0e814e3f0bd, 45215, None)
[WI-0][TI-0] - [INFO] 2024-04-23 12:05:59.141 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	
	
	 /tmp/spark-a827e30f-6c58-48eb-ad94-502cf6d93137/userFiles-59bcdb93-f8c1-46df-9ce4-00612b78bf1b/386-20240422.json 
	
	
	
	24/04/23 12:05:58 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/04/23 12:05:58 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/497/1202/spark-warehouse'.
[WI-0][TI-0] - [INFO] 2024-04-23 12:06:00.143 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/23 12:05:59 INFO InMemoryFileIndex: It took 39 ms to list leaf files for 1 paths.
	24/04/23 12:05:59 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[WI-0][TI-0] - [INFO] 2024-04-23 12:06:00.162 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7368421052631579 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:06:03.194 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/23 12:06:02 INFO FileSourceStrategy: Pushed Filters: 
	24/04/23 12:06:02 INFO FileSourceStrategy: Post-Scan Filters: 
	24/04/23 12:06:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
	24/04/23 12:06:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 365.9 MiB)
	24/04/23 12:06:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on c0e814e3f0bd:45215 (size: 34.4 KiB, free: 366.3 MiB)
	24/04/23 12:06:03 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
	24/04/23 12:06:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4266164 bytes, open cost is considered as scanning 4194304 bytes.
[WI-0][TI-0] - [INFO] 2024-04-23 12:06:04.200 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/23 12:06:03 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
	24/04/23 12:06:03 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/23 12:06:03 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
	24/04/23 12:06:03 INFO DAGScheduler: Parents of final stage: List()
	24/04/23 12:06:03 INFO DAGScheduler: Missing parents: List()
	24/04/23 12:06:03 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/23 12:06:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.4 KiB, free 365.9 MiB)
	24/04/23 12:06:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 365.9 MiB)
	24/04/23 12:06:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on c0e814e3f0bd:45215 (size: 7.7 KiB, free: 366.3 MiB)
	24/04/23 12:06:03 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
	24/04/23 12:06:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/23 12:06:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
	24/04/23 12:06:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (c0e814e3f0bd, executor driver, partition 0, PROCESS_LOCAL, 8481 bytes) 
	24/04/23 12:06:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
	24/04/23 12:06:04 INFO FileScanRDD: Reading File path: file:///tmp/spark-a827e30f-6c58-48eb-ad94-502cf6d93137/userFiles-59bcdb93-f8c1-46df-9ce4-00612b78bf1b/386-20240422.json, range: 0-71860, partition values: [empty row]
[WI-0][TI-0] - [INFO] 2024-04-23 12:06:04.200 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7859237536656892 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:06:05.207 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/23 12:06:04 INFO CodeGenerator: Code generated in 314.76102 ms
	24/04/23 12:06:04 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1964 bytes result sent to driver
	24/04/23 12:06:04 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1104 ms on c0e814e3f0bd (executor driver) (1/1)
	24/04/23 12:06:04 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
	24/04/23 12:06:04 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1.365 s
	24/04/23 12:06:04 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/23 12:06:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
	24/04/23 12:06:04 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1.464861 s
	
	
	
	 1 
	
	
	
	24/04/23 12:06:05 INFO BlockManagerInfo: Removed broadcast_1_piece0 on c0e814e3f0bd:45215 in memory (size: 7.7 KiB, free: 366.3 MiB)
[WI-0][TI-0] - [INFO] 2024-04-23 12:06:05.216 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7734806629834254 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:06:06.223 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/497/1202/spark_reddit_preprocessing.py", line 54, in <module>
	    new_df = raw_df.withColumn("type", split(raw_df["name"], "_")[0]) \
	                                             ~~~~~~^^^^^^^^
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 3078, in __getitem__
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
	pyspark.errors.exceptions.captured.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `name` cannot be resolved. Did you mean one of the following? [`_corrupt_record`].
	24/04/23 12:06:05 INFO SparkContext: Invoking stop() from shutdown hook
	24/04/23 12:06:05 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/04/23 12:06:05 INFO SparkUI: Stopped Spark web UI at http://c0e814e3f0bd:4040
	24/04/23 12:06:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/04/23 12:06:05 INFO MemoryStore: MemoryStore cleared
	24/04/23 12:06:05 INFO BlockManager: BlockManager stopped
	24/04/23 12:06:05 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/04/23 12:06:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/04/23 12:06:05 INFO SparkContext: Successfully stopped SparkContext
	24/04/23 12:06:05 INFO ShutdownHookManager: Shutdown hook called
	24/04/23 12:06:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-7950d8a4-9131-46fd-9669-380830490ac0
	24/04/23 12:06:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-a827e30f-6c58-48eb-ad94-502cf6d93137/pyspark-30af208f-cda5-4735-9f29-aa3b52f95858
	24/04/23 12:06:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-a827e30f-6c58-48eb-ad94-502cf6d93137
[WI-497][TI-1202] - [INFO] 2024-04-23 12:06:06.243 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/497/1202, processId:5813 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-497][TI-1202] - [INFO] 2024-04-23 12:06:06.243 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-497][TI-1202] - [INFO] 2024-04-23 12:06:06.243 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-497][TI-1202] - [INFO] 2024-04-23 12:06:06.243 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-497][TI-1202] - [INFO] 2024-04-23 12:06:06.244 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-497][TI-1202] - [INFO] 2024-04-23 12:06:06.249 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-497][TI-1202] - [INFO] 2024-04-23 12:06:06.251 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-497][TI-1202] - [INFO] 2024-04-23 12:06:06.251 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/497/1202
[WI-497][TI-1202] - [INFO] 2024-04-23 12:06:06.252 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_84/497/1202
[WI-497][TI-1202] - [INFO] 2024-04-23 12:06:06.253 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1202] - [INFO] 2024-04-23 12:06:06.643 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1202, success=true)
[WI-0][TI-1144] - [INFO] 2024-04-23 12:07:18.399 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1144, processInstanceId=489, status=9, startTime=1713840953087, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/80/489/1144.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_80/489/1144, endTime=1713841331355, processId=3026, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713844937611)
[WI-0][TI-1144] - [INFO] 2024-04-23 12:07:18.403 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1144, processInstanceId=489, status=9, startTime=1713840953087, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/80/489/1144.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_80/489/1144, endTime=1713841331355, processId=3026, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713845238399)
[WI-0][TI-0] - [INFO] 2024-04-23 12:09:04.840 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7720797720797721 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1123] - [INFO] 2024-04-23 12:09:55.798 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1123, processInstanceId=487, status=9, startTime=1713838208504, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1123.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1123, endTime=1713838483436, processId=1677, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713845095460)
[WI-0][TI-1123] - [INFO] 2024-04-23 12:09:55.801 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1123, processInstanceId=487, status=9, startTime=1713838208504, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1123.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1123, endTime=1713838483436, processId=1677, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713845395798)
[WI-0][TI-1111] - [INFO] 2024-04-23 12:09:57.806 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1111, processInstanceId=487, status=9, startTime=1713837256404, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1111.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1111, endTime=1713838170352, processId=1017, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713845097470)
[WI-0][TI-1111] - [INFO] 2024-04-23 12:09:57.809 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1111, processInstanceId=487, status=9, startTime=1713837256404, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1111.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1111, endTime=1713838170352, processId=1017, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713845397806)
[WI-0][TI-0] - [INFO] 2024-04-23 12:11:40.739 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8022922636103151 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:12:15.854 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7086834733893558 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1144] - [INFO] 2024-04-23 12:12:18.714 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1144, processInstanceId=489, status=9, startTime=1713840953087, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/80/489/1144.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_80/489/1144, endTime=1713841331355, processId=3026, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713845238399)
[WI-0][TI-1144] - [INFO] 2024-04-23 12:12:18.717 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1144, processInstanceId=489, status=9, startTime=1713840953087, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/80/489/1144.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_80/489/1144, endTime=1713841331355, processId=3026, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713845538714)
[WI-0][TI-0] - [INFO] 2024-04-23 12:12:20.902 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7944444444444444 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:12:21.829 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1203, taskName=Extract Reddit Data, firstSubmitTime=1713845541815, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13045665829504, processDefineVersion=12, appIds=null, processInstanceId=498, scheduleTime=0, globalParams=[{"prop":"subreddit","direct":"IN","type":"VARCHAR","value":"singapore"},{"prop":"limit","direct":"IN","type":"VARCHAR","value":"3"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"data","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"import praw\nimport json\n\nclass RedditPostDataExtractor:\n    def __init__(self):\n        self.data = []\n\n    def extract_data(self, hot_posts):\n        if hot_posts:\n            for post in hot_posts:\n                # initialise a dictionary to contain the data of the post\n                post_data = {}\n\n                # use permalink as the primary key\n                permalink = post.permalink\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the poster's name\n                if post.author is not None:\n                    post_data['author'] = post.author.name\n                else:\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\n\n\n                # get the unix time in which the post was created\n                post_data['unix_time'] = post.created_utc\n                post_data['permalink'] = post.permalink\n                post_data['score'] = post.score\n                post_data['subreddit'] = post.subreddit.display_name\n                post_data['post_title'] = post.title\n                post_data['body'] = post.selftext\n\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\n                # t3 represents that it is a post\n                post_data['name'] = post.name\n\n                # store the posts data in the dictionary where the name is used as the key\n                self.data.append(post_data)\n\n                # extract the comments from the post\n                comments = post.comments.list()\n\n                # extract the data in the comments and store them inside the dictionary\n                self.extract_comments_data(comments)\n        \n        return self.data\n\n    def extract_comments_data(self, comments):\n        if comments:\n            for comment in comments:\n                # initialise a dictionary to contain the data of the comment\n                comment_data = {}\n\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the commenter name\n                if comment.author is not None:\n                    comment_data['author'] = comment.author.name\n                else:\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the comment was created\n                comment_data['unix_time'] = comment.created_utc\n                comment_data['permalink'] = comment.permalink\n                comment_data['score'] = comment.score\n                comment_data['subreddit'] = comment.subreddit.display_name\n                comment_data['post_title'] = comment.submission.title\n                comment_data['body'] = comment.body\n\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\n                # t1 represents that it is a post\n                comment_data['name'] = comment.name\n\n                # store the comment's data in the dictionary where the name is used as the key\n                self.data.append(comment_data)\n\n                # extract the comment's replies from the post\n                # replies = comment.replies\n\n                # extract the data in the comments and store them inside the dictionary\n                # self.extract_comments_data(replies)\n        \n        return\n    \ndef get_hot_posts(reddit_instance, subreddit_name, limit):\n    # Define the subreddit\n    subreddit = reddit_instance.subreddit(subreddit_name)\n\n    # Get hot posts from the subreddit\n    hot_posts = subreddit.hot(limit=limit)\n\n    # Filter out posts that are not pinned\n    filtered_posts = [post for post in hot_posts if not post.stickied]\n    return filtered_posts\n\n# initialize Reddit instance\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\n                     user_agent='Prawlingv1')\n\n\n# choose the subreddit to extract data from\nsubreddit = ${subreddit}\nlimit = ${limit}\n\n# get hot posts from the subreddit\nhot_posts = get_hot_posts(reddit, subreddit, limit)\n\n# initialise a reddit data extractor object\ndata_extractor = RedditPostDataExtractor()\n\n# extract data from the hot posts from r/singapore\ndata = data_extractor.extract_data(hot_posts)\ndata","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='Extract Reddit Data'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240423'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1203'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13045662268160'}, subreddit=Property{prop='subreddit', direct=IN, type=VARCHAR, value='singapore'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240423121221'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='498'}, limit=Property{prop='limit', direct=IN, type=VARCHAR, value='3'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240422'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13045665829504'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.841 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: Extract Reddit Data to wait queue success
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.846 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.851 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.851 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.851 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.851 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713845541851
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.851 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 498_1203
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.851 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1203,
  "taskName" : "Extract Reddit Data",
  "firstSubmitTime" : 1713845541815,
  "startTime" : 1713845541851,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240423/13045665829504/12/498/1203.log",
  "processId" : 0,
  "processDefineCode" : 13045665829504,
  "processDefineVersion" : 12,
  "processInstanceId" : 498,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"subreddit\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"singapore\"},{\"prop\":\"limit\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"3\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"data\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"import praw\\nimport json\\n\\nclass RedditPostDataExtractor:\\n    def __init__(self):\\n        self.data = []\\n\\n    def extract_data(self, hot_posts):\\n        if hot_posts:\\n            for post in hot_posts:\\n                # initialise a dictionary to contain the data of the post\\n                post_data = {}\\n\\n                # use permalink as the primary key\\n                permalink = post.permalink\\n\\n                # get the relevant post's data and store it inside post_data\\n\\n                # get the poster's name\\n                if post.author is not None:\\n                    post_data['author'] = post.author.name\\n                else:\\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\\n\\n\\n                # get the unix time in which the post was created\\n                post_data['unix_time'] = post.created_utc\\n                post_data['permalink'] = post.permalink\\n                post_data['score'] = post.score\\n                post_data['subreddit'] = post.subreddit.display_name\\n                post_data['post_title'] = post.title\\n                post_data['body'] = post.selftext\\n\\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\\n                # t3 represents that it is a post\\n                post_data['name'] = post.name\\n\\n                # store the posts data in the dictionary where the name is used as the key\\n                self.data.append(post_data)\\n\\n                # extract the comments from the post\\n                comments = post.comments.list()\\n\\n                # extract the data in the comments and store them inside the dictionary\\n                self.extract_comments_data(comments)\\n        \\n        return self.data\\n\\n    def extract_comments_data(self, comments):\\n        if comments:\\n            for comment in comments:\\n                # initialise a dictionary to contain the data of the comment\\n                comment_data = {}\\n\\n\\n                # get the relevant post's data and store it inside post_data\\n\\n                # get the commenter name\\n                if comment.author is not None:\\n                    comment_data['author'] = comment.author.name\\n                else:\\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\\n\\n                # get the unix time in which the comment was created\\n                comment_data['unix_time'] = comment.created_utc\\n                comment_data['permalink'] = comment.permalink\\n                comment_data['score'] = comment.score\\n                comment_data['subreddit'] = comment.subreddit.display_name\\n                comment_data['post_title'] = comment.submission.title\\n                comment_data['body'] = comment.body\\n\\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\\n                # t1 represents that it is a post\\n                comment_data['name'] = comment.name\\n\\n                # store the comment's data in the dictionary where the name is used as the key\\n                self.data.append(comment_data)\\n\\n                # extract the comment's replies from the post\\n                # replies = comment.replies\\n\\n                # extract the data in the comments and store them inside the dictionary\\n                # self.extract_comments_data(replies)\\n        \\n        return\\n    \\ndef get_hot_posts(reddit_instance, subreddit_name, limit):\\n    # Define the subreddit\\n    subreddit = reddit_instance.subreddit(subreddit_name)\\n\\n    # Get hot posts from the subreddit\\n    hot_posts = subreddit.hot(limit=limit)\\n\\n    # Filter out posts that are not pinned\\n    filtered_posts = [post for post in hot_posts if not post.stickied]\\n    return filtered_posts\\n\\n# initialize Reddit instance\\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\\n                     user_agent='Prawlingv1')\\n\\n\\n# choose the subreddit to extract data from\\nsubreddit = ${subreddit}\\nlimit = ${limit}\\n\\n# get hot posts from the subreddit\\nhot_posts = get_hot_posts(reddit, subreddit, limit)\\n\\n# initialise a reddit data extractor object\\ndata_extractor = RedditPostDataExtractor()\\n\\n# extract data from the hot posts from r/singapore\\ndata = data_extractor.extract_data(hot_posts)\\ndata\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "Extract Reddit Data"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1203"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13045662268160"
    },
    "subreddit" : {
      "prop" : "subreddit",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "singapore"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423121221"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "498"
    },
    "limit" : {
      "prop" : "limit",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240422"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13045665829504"
    }
  },
  "taskAppId" : "498_1203",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.852 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.852 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.852 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.858 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.858 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.859 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_12/498/1203 check successfully
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.859 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.878 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.882 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.884 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.887 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "data",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "import praw\nimport json\n\nclass RedditPostDataExtractor:\n    def __init__(self):\n        self.data = []\n\n    def extract_data(self, hot_posts):\n        if hot_posts:\n            for post in hot_posts:\n                # initialise a dictionary to contain the data of the post\n                post_data = {}\n\n                # use permalink as the primary key\n                permalink = post.permalink\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the poster's name\n                if post.author is not None:\n                    post_data['author'] = post.author.name\n                else:\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\n\n\n                # get the unix time in which the post was created\n                post_data['unix_time'] = post.created_utc\n                post_data['permalink'] = post.permalink\n                post_data['score'] = post.score\n                post_data['subreddit'] = post.subreddit.display_name\n                post_data['post_title'] = post.title\n                post_data['body'] = post.selftext\n\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\n                # t3 represents that it is a post\n                post_data['name'] = post.name\n\n                # store the posts data in the dictionary where the name is used as the key\n                self.data.append(post_data)\n\n                # extract the comments from the post\n                comments = post.comments.list()\n\n                # extract the data in the comments and store them inside the dictionary\n                self.extract_comments_data(comments)\n        \n        return self.data\n\n    def extract_comments_data(self, comments):\n        if comments:\n            for comment in comments:\n                # initialise a dictionary to contain the data of the comment\n                comment_data = {}\n\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the commenter name\n                if comment.author is not None:\n                    comment_data['author'] = comment.author.name\n                else:\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the comment was created\n                comment_data['unix_time'] = comment.created_utc\n                comment_data['permalink'] = comment.permalink\n                comment_data['score'] = comment.score\n                comment_data['subreddit'] = comment.subreddit.display_name\n                comment_data['post_title'] = comment.submission.title\n                comment_data['body'] = comment.body\n\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\n                # t1 represents that it is a post\n                comment_data['name'] = comment.name\n\n                # store the comment's data in the dictionary where the name is used as the key\n                self.data.append(comment_data)\n\n                # extract the comment's replies from the post\n                # replies = comment.replies\n\n                # extract the data in the comments and store them inside the dictionary\n                # self.extract_comments_data(replies)\n        \n        return\n    \ndef get_hot_posts(reddit_instance, subreddit_name, limit):\n    # Define the subreddit\n    subreddit = reddit_instance.subreddit(subreddit_name)\n\n    # Get hot posts from the subreddit\n    hot_posts = subreddit.hot(limit=limit)\n\n    # Filter out posts that are not pinned\n    filtered_posts = [post for post in hot_posts if not post.stickied]\n    return filtered_posts\n\n# initialize Reddit instance\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\n                     user_agent='Prawlingv1')\n\n\n# choose the subreddit to extract data from\nsubreddit = ${subreddit}\nlimit = ${limit}\n\n# get hot posts from the subreddit\nhot_posts = get_hot_posts(reddit, subreddit, limit)\n\n# initialise a reddit data extractor object\ndata_extractor = RedditPostDataExtractor()\n\n# extract data from the hot posts from r/singapore\ndata = data_extractor.extract_data(hot_posts)\ndata",
  "resourceList" : [ ]
}
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.888 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.888 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.888 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.888 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.888 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.888 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import praw
import json

class RedditPostDataExtractor:
    def __init__(self):
        self.data = []

    def extract_data(self, hot_posts):
        if hot_posts:
            for post in hot_posts:
                # initialise a dictionary to contain the data of the post
                post_data = {}

                # use permalink as the primary key
                permalink = post.permalink

                # get the relevant post's data and store it inside post_data

                # get the poster's name
                if post.author is not None:
                    post_data['author'] = post.author.name
                else:
                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'


                # get the unix time in which the post was created
                post_data['unix_time'] = post.created_utc
                post_data['permalink'] = post.permalink
                post_data['score'] = post.score
                post_data['subreddit'] = post.subreddit.display_name
                post_data['post_title'] = post.title
                post_data['body'] = post.selftext

                # get the unique name of the post (name refers to the id of the post prefixed with t3_)
                # t3 represents that it is a post
                post_data['name'] = post.name

                # store the posts data in the dictionary where the name is used as the key
                self.data.append(post_data)

                # extract the comments from the post
                comments = post.comments.list()

                # extract the data in the comments and store them inside the dictionary
                self.extract_comments_data(comments)
        
        return self.data

    def extract_comments_data(self, comments):
        if comments:
            for comment in comments:
                # initialise a dictionary to contain the data of the comment
                comment_data = {}


                # get the relevant post's data and store it inside post_data

                # get the commenter name
                if comment.author is not None:
                    comment_data['author'] = comment.author.name
                else:
                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the comment was created
                comment_data['unix_time'] = comment.created_utc
                comment_data['permalink'] = comment.permalink
                comment_data['score'] = comment.score
                comment_data['subreddit'] = comment.subreddit.display_name
                comment_data['post_title'] = comment.submission.title
                comment_data['body'] = comment.body

                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)
                # t1 represents that it is a post
                comment_data['name'] = comment.name

                # store the comment's data in the dictionary where the name is used as the key
                self.data.append(comment_data)

                # extract the comment's replies from the post
                # replies = comment.replies

                # extract the data in the comments and store them inside the dictionary
                # self.extract_comments_data(replies)
        
        return
    
def get_hot_posts(reddit_instance, subreddit_name, limit):
    # Define the subreddit
    subreddit = reddit_instance.subreddit(subreddit_name)

    # Get hot posts from the subreddit
    hot_posts = subreddit.hot(limit=limit)

    # Filter out posts that are not pinned
    filtered_posts = [post for post in hot_posts if not post.stickied]
    return filtered_posts

# initialize Reddit instance
reddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',
                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',
                     user_agent='Prawlingv1')


# choose the subreddit to extract data from
subreddit = ${subreddit}
limit = ${limit}

# get hot posts from the subreddit
hot_posts = get_hot_posts(reddit, subreddit, limit)

# initialise a reddit data extractor object
data_extractor = RedditPostDataExtractor()

# extract data from the hot posts from r/singapore
data = data_extractor.extract_data(hot_posts)
data
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.889 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_12/498/1203
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.890 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_12/498/1203/py_498_1203.py
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.890 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import praw
import json

class RedditPostDataExtractor:
    def __init__(self):
        self.data = []

    def extract_data(self, hot_posts):
        if hot_posts:
            for post in hot_posts:
                # initialise a dictionary to contain the data of the post
                post_data = {}

                # use permalink as the primary key
                permalink = post.permalink

                # get the relevant post's data and store it inside post_data

                # get the poster's name
                if post.author is not None:
                    post_data['author'] = post.author.name
                else:
                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'


                # get the unix time in which the post was created
                post_data['unix_time'] = post.created_utc
                post_data['permalink'] = post.permalink
                post_data['score'] = post.score
                post_data['subreddit'] = post.subreddit.display_name
                post_data['post_title'] = post.title
                post_data['body'] = post.selftext

                # get the unique name of the post (name refers to the id of the post prefixed with t3_)
                # t3 represents that it is a post
                post_data['name'] = post.name

                # store the posts data in the dictionary where the name is used as the key
                self.data.append(post_data)

                # extract the comments from the post
                comments = post.comments.list()

                # extract the data in the comments and store them inside the dictionary
                self.extract_comments_data(comments)
        
        return self.data

    def extract_comments_data(self, comments):
        if comments:
            for comment in comments:
                # initialise a dictionary to contain the data of the comment
                comment_data = {}


                # get the relevant post's data and store it inside post_data

                # get the commenter name
                if comment.author is not None:
                    comment_data['author'] = comment.author.name
                else:
                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the comment was created
                comment_data['unix_time'] = comment.created_utc
                comment_data['permalink'] = comment.permalink
                comment_data['score'] = comment.score
                comment_data['subreddit'] = comment.subreddit.display_name
                comment_data['post_title'] = comment.submission.title
                comment_data['body'] = comment.body

                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)
                # t1 represents that it is a post
                comment_data['name'] = comment.name

                # store the comment's data in the dictionary where the name is used as the key
                self.data.append(comment_data)

                # extract the comment's replies from the post
                # replies = comment.replies

                # extract the data in the comments and store them inside the dictionary
                # self.extract_comments_data(replies)
        
        return
    
def get_hot_posts(reddit_instance, subreddit_name, limit):
    # Define the subreddit
    subreddit = reddit_instance.subreddit(subreddit_name)

    # Get hot posts from the subreddit
    hot_posts = subreddit.hot(limit=limit)

    # Filter out posts that are not pinned
    filtered_posts = [post for post in hot_posts if not post.stickied]
    return filtered_posts

# initialize Reddit instance
reddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',
                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',
                     user_agent='Prawlingv1')


# choose the subreddit to extract data from
subreddit = singapore
limit = 3

# get hot posts from the subreddit
hot_posts = get_hot_posts(reddit, subreddit, limit)

# initialise a reddit data extractor object
data_extractor = RedditPostDataExtractor()

# extract data from the hot posts from r/singapore
data = data_extractor.extract_data(hot_posts)
data
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.915 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.916 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.916 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_12/498/1203/py_498_1203.py
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.916 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.916 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_12/498/1203/498_1203.sh
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:21.927 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6007
[WI-0][TI-1203] - [INFO] 2024-04-23 12:12:22.676 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1203, success=true)
[WI-0][TI-1203] - [INFO] 2024-04-23 12:12:22.682 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1203)
[WI-0][TI-0] - [INFO] 2024-04-23 12:12:22.935 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_12/498/1203/py_498_1203.py", line 107, in <module>
	    subreddit = singapore
	                ^^^^^^^^^
	NameError: name 'singapore' is not defined
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:22.947 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_12/498/1203, processId:6007 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:22.950 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:22.950 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:22.950 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:22.950 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:22.954 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:22.954 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:22.954 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_12/498/1203
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:22.955 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_12/498/1203
[WI-498][TI-1203] - [INFO] 2024-04-23 12:12:22.955 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1203] - [INFO] 2024-04-23 12:12:23.680 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1203, success=true)
[WI-0][TI-0] - [INFO] 2024-04-23 12:12:46.077 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7037037037037036 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:13:09.188 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7244318181818181 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:13:33.300 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7663817663817665 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-23 12:13:40.092 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1204, taskName=Extract Reddit Data, firstSubmitTime=1713845620084, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13045665829504, processDefineVersion=13, appIds=null, processInstanceId=499, scheduleTime=0, globalParams=[{"prop":"subreddit","direct":"IN","type":"VARCHAR","value":"singapore"},{"prop":"limit","direct":"IN","type":"VARCHAR","value":"3"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"data","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"import praw\nimport json\n\nclass RedditPostDataExtractor:\n    def __init__(self):\n        self.data = []\n\n    def extract_data(self, hot_posts):\n        if hot_posts:\n            for post in hot_posts:\n                # initialise a dictionary to contain the data of the post\n                post_data = {}\n\n                # use permalink as the primary key\n                permalink = post.permalink\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the poster's name\n                if post.author is not None:\n                    post_data['author'] = post.author.name\n                else:\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\n\n\n                # get the unix time in which the post was created\n                post_data['unix_time'] = post.created_utc\n                post_data['permalink'] = post.permalink\n                post_data['score'] = post.score\n                post_data['subreddit'] = post.subreddit.display_name\n                post_data['post_title'] = post.title\n                post_data['body'] = post.selftext\n\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\n                # t3 represents that it is a post\n                post_data['name'] = post.name\n\n                # store the posts data in the dictionary where the name is used as the key\n                self.data.append(post_data)\n\n                # extract the comments from the post\n                comments = post.comments.list()\n\n                # extract the data in the comments and store them inside the dictionary\n                self.extract_comments_data(comments)\n        \n        return self.data\n\n    def extract_comments_data(self, comments):\n        if comments:\n            for comment in comments:\n                # initialise a dictionary to contain the data of the comment\n                comment_data = {}\n\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the commenter name\n                if comment.author is not None:\n                    comment_data['author'] = comment.author.name\n                else:\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the comment was created\n                comment_data['unix_time'] = comment.created_utc\n                comment_data['permalink'] = comment.permalink\n                comment_data['score'] = comment.score\n                comment_data['subreddit'] = comment.subreddit.display_name\n                comment_data['post_title'] = comment.submission.title\n                comment_data['body'] = comment.body\n\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\n                # t1 represents that it is a post\n                comment_data['name'] = comment.name\n\n                # store the comment's data in the dictionary where the name is used as the key\n                self.data.append(comment_data)\n\n                # extract the comment's replies from the post\n                # replies = comment.replies\n\n                # extract the data in the comments and store them inside the dictionary\n                # self.extract_comments_data(replies)\n        \n        return\n    \ndef get_hot_posts(reddit_instance, subreddit_name, limit):\n    # Define the subreddit\n    subreddit = reddit_instance.subreddit(subreddit_name)\n\n    # Get hot posts from the subreddit\n    hot_posts = subreddit.hot(limit=limit)\n\n    # Filter out posts that are not pinned\n    filtered_posts = [post for post in hot_posts if not post.stickied]\n    return filtered_posts\n\n# initialize Reddit instance\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\n                     user_agent='Prawlingv1')\n\n\n# choose the subreddit to extract data from\nsubreddit = '${subreddit}'\nlimit = ${limit}\n\n# get hot posts from the subreddit\nhot_posts = get_hot_posts(reddit, subreddit, limit)\n\n# initialise a reddit data extractor object\ndata_extractor = RedditPostDataExtractor()\n\n# extract data from the hot posts from r/singapore\ndata = data_extractor.extract_data(hot_posts)\ndata","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='Extract Reddit Data'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240423'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1204'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13045662268160'}, subreddit=Property{prop='subreddit', direct=IN, type=VARCHAR, value='singapore'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240423121340'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='499'}, limit=Property{prop='limit', direct=IN, type=VARCHAR, value='3'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240422'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13045665829504'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.093 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: Extract Reddit Data to wait queue success
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.093 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.095 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.096 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.096 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.097 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713845620097
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.097 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 499_1204
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.098 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1204,
  "taskName" : "Extract Reddit Data",
  "firstSubmitTime" : 1713845620084,
  "startTime" : 1713845620097,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240423/13045665829504/13/499/1204.log",
  "processId" : 0,
  "processDefineCode" : 13045665829504,
  "processDefineVersion" : 13,
  "processInstanceId" : 499,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"subreddit\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"singapore\"},{\"prop\":\"limit\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"3\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"data\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"import praw\\nimport json\\n\\nclass RedditPostDataExtractor:\\n    def __init__(self):\\n        self.data = []\\n\\n    def extract_data(self, hot_posts):\\n        if hot_posts:\\n            for post in hot_posts:\\n                # initialise a dictionary to contain the data of the post\\n                post_data = {}\\n\\n                # use permalink as the primary key\\n                permalink = post.permalink\\n\\n                # get the relevant post's data and store it inside post_data\\n\\n                # get the poster's name\\n                if post.author is not None:\\n                    post_data['author'] = post.author.name\\n                else:\\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\\n\\n\\n                # get the unix time in which the post was created\\n                post_data['unix_time'] = post.created_utc\\n                post_data['permalink'] = post.permalink\\n                post_data['score'] = post.score\\n                post_data['subreddit'] = post.subreddit.display_name\\n                post_data['post_title'] = post.title\\n                post_data['body'] = post.selftext\\n\\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\\n                # t3 represents that it is a post\\n                post_data['name'] = post.name\\n\\n                # store the posts data in the dictionary where the name is used as the key\\n                self.data.append(post_data)\\n\\n                # extract the comments from the post\\n                comments = post.comments.list()\\n\\n                # extract the data in the comments and store them inside the dictionary\\n                self.extract_comments_data(comments)\\n        \\n        return self.data\\n\\n    def extract_comments_data(self, comments):\\n        if comments:\\n            for comment in comments:\\n                # initialise a dictionary to contain the data of the comment\\n                comment_data = {}\\n\\n\\n                # get the relevant post's data and store it inside post_data\\n\\n                # get the commenter name\\n                if comment.author is not None:\\n                    comment_data['author'] = comment.author.name\\n                else:\\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\\n\\n                # get the unix time in which the comment was created\\n                comment_data['unix_time'] = comment.created_utc\\n                comment_data['permalink'] = comment.permalink\\n                comment_data['score'] = comment.score\\n                comment_data['subreddit'] = comment.subreddit.display_name\\n                comment_data['post_title'] = comment.submission.title\\n                comment_data['body'] = comment.body\\n\\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\\n                # t1 represents that it is a post\\n                comment_data['name'] = comment.name\\n\\n                # store the comment's data in the dictionary where the name is used as the key\\n                self.data.append(comment_data)\\n\\n                # extract the comment's replies from the post\\n                # replies = comment.replies\\n\\n                # extract the data in the comments and store them inside the dictionary\\n                # self.extract_comments_data(replies)\\n        \\n        return\\n    \\ndef get_hot_posts(reddit_instance, subreddit_name, limit):\\n    # Define the subreddit\\n    subreddit = reddit_instance.subreddit(subreddit_name)\\n\\n    # Get hot posts from the subreddit\\n    hot_posts = subreddit.hot(limit=limit)\\n\\n    # Filter out posts that are not pinned\\n    filtered_posts = [post for post in hot_posts if not post.stickied]\\n    return filtered_posts\\n\\n# initialize Reddit instance\\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\\n                     user_agent='Prawlingv1')\\n\\n\\n# choose the subreddit to extract data from\\nsubreddit = '${subreddit}'\\nlimit = ${limit}\\n\\n# get hot posts from the subreddit\\nhot_posts = get_hot_posts(reddit, subreddit, limit)\\n\\n# initialise a reddit data extractor object\\ndata_extractor = RedditPostDataExtractor()\\n\\n# extract data from the hot posts from r/singapore\\ndata = data_extractor.extract_data(hot_posts)\\ndata\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "Extract Reddit Data"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1204"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13045662268160"
    },
    "subreddit" : {
      "prop" : "subreddit",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "singapore"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423121340"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "499"
    },
    "limit" : {
      "prop" : "limit",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240422"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13045665829504"
    }
  },
  "taskAppId" : "499_1204",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.100 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.100 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.100 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.104 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.105 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.106 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_13/499/1204 check successfully
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.106 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.106 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.106 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.107 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.107 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "data",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "import praw\nimport json\n\nclass RedditPostDataExtractor:\n    def __init__(self):\n        self.data = []\n\n    def extract_data(self, hot_posts):\n        if hot_posts:\n            for post in hot_posts:\n                # initialise a dictionary to contain the data of the post\n                post_data = {}\n\n                # use permalink as the primary key\n                permalink = post.permalink\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the poster's name\n                if post.author is not None:\n                    post_data['author'] = post.author.name\n                else:\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\n\n\n                # get the unix time in which the post was created\n                post_data['unix_time'] = post.created_utc\n                post_data['permalink'] = post.permalink\n                post_data['score'] = post.score\n                post_data['subreddit'] = post.subreddit.display_name\n                post_data['post_title'] = post.title\n                post_data['body'] = post.selftext\n\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\n                # t3 represents that it is a post\n                post_data['name'] = post.name\n\n                # store the posts data in the dictionary where the name is used as the key\n                self.data.append(post_data)\n\n                # extract the comments from the post\n                comments = post.comments.list()\n\n                # extract the data in the comments and store them inside the dictionary\n                self.extract_comments_data(comments)\n        \n        return self.data\n\n    def extract_comments_data(self, comments):\n        if comments:\n            for comment in comments:\n                # initialise a dictionary to contain the data of the comment\n                comment_data = {}\n\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the commenter name\n                if comment.author is not None:\n                    comment_data['author'] = comment.author.name\n                else:\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the comment was created\n                comment_data['unix_time'] = comment.created_utc\n                comment_data['permalink'] = comment.permalink\n                comment_data['score'] = comment.score\n                comment_data['subreddit'] = comment.subreddit.display_name\n                comment_data['post_title'] = comment.submission.title\n                comment_data['body'] = comment.body\n\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\n                # t1 represents that it is a post\n                comment_data['name'] = comment.name\n\n                # store the comment's data in the dictionary where the name is used as the key\n                self.data.append(comment_data)\n\n                # extract the comment's replies from the post\n                # replies = comment.replies\n\n                # extract the data in the comments and store them inside the dictionary\n                # self.extract_comments_data(replies)\n        \n        return\n    \ndef get_hot_posts(reddit_instance, subreddit_name, limit):\n    # Define the subreddit\n    subreddit = reddit_instance.subreddit(subreddit_name)\n\n    # Get hot posts from the subreddit\n    hot_posts = subreddit.hot(limit=limit)\n\n    # Filter out posts that are not pinned\n    filtered_posts = [post for post in hot_posts if not post.stickied]\n    return filtered_posts\n\n# initialize Reddit instance\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\n                     user_agent='Prawlingv1')\n\n\n# choose the subreddit to extract data from\nsubreddit = '${subreddit}'\nlimit = ${limit}\n\n# get hot posts from the subreddit\nhot_posts = get_hot_posts(reddit, subreddit, limit)\n\n# initialise a reddit data extractor object\ndata_extractor = RedditPostDataExtractor()\n\n# extract data from the hot posts from r/singapore\ndata = data_extractor.extract_data(hot_posts)\ndata",
  "resourceList" : [ ]
}
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.108 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.108 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.108 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.108 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.108 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.109 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import praw
import json

class RedditPostDataExtractor:
    def __init__(self):
        self.data = []

    def extract_data(self, hot_posts):
        if hot_posts:
            for post in hot_posts:
                # initialise a dictionary to contain the data of the post
                post_data = {}

                # use permalink as the primary key
                permalink = post.permalink

                # get the relevant post's data and store it inside post_data

                # get the poster's name
                if post.author is not None:
                    post_data['author'] = post.author.name
                else:
                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'


                # get the unix time in which the post was created
                post_data['unix_time'] = post.created_utc
                post_data['permalink'] = post.permalink
                post_data['score'] = post.score
                post_data['subreddit'] = post.subreddit.display_name
                post_data['post_title'] = post.title
                post_data['body'] = post.selftext

                # get the unique name of the post (name refers to the id of the post prefixed with t3_)
                # t3 represents that it is a post
                post_data['name'] = post.name

                # store the posts data in the dictionary where the name is used as the key
                self.data.append(post_data)

                # extract the comments from the post
                comments = post.comments.list()

                # extract the data in the comments and store them inside the dictionary
                self.extract_comments_data(comments)
        
        return self.data

    def extract_comments_data(self, comments):
        if comments:
            for comment in comments:
                # initialise a dictionary to contain the data of the comment
                comment_data = {}


                # get the relevant post's data and store it inside post_data

                # get the commenter name
                if comment.author is not None:
                    comment_data['author'] = comment.author.name
                else:
                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the comment was created
                comment_data['unix_time'] = comment.created_utc
                comment_data['permalink'] = comment.permalink
                comment_data['score'] = comment.score
                comment_data['subreddit'] = comment.subreddit.display_name
                comment_data['post_title'] = comment.submission.title
                comment_data['body'] = comment.body

                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)
                # t1 represents that it is a post
                comment_data['name'] = comment.name

                # store the comment's data in the dictionary where the name is used as the key
                self.data.append(comment_data)

                # extract the comment's replies from the post
                # replies = comment.replies

                # extract the data in the comments and store them inside the dictionary
                # self.extract_comments_data(replies)
        
        return
    
def get_hot_posts(reddit_instance, subreddit_name, limit):
    # Define the subreddit
    subreddit = reddit_instance.subreddit(subreddit_name)

    # Get hot posts from the subreddit
    hot_posts = subreddit.hot(limit=limit)

    # Filter out posts that are not pinned
    filtered_posts = [post for post in hot_posts if not post.stickied]
    return filtered_posts

# initialize Reddit instance
reddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',
                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',
                     user_agent='Prawlingv1')


# choose the subreddit to extract data from
subreddit = '${subreddit}'
limit = ${limit}

# get hot posts from the subreddit
hot_posts = get_hot_posts(reddit, subreddit, limit)

# initialise a reddit data extractor object
data_extractor = RedditPostDataExtractor()

# extract data from the hot posts from r/singapore
data = data_extractor.extract_data(hot_posts)
data
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.110 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_13/499/1204
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.110 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_13/499/1204/py_499_1204.py
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.110 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import praw
import json

class RedditPostDataExtractor:
    def __init__(self):
        self.data = []

    def extract_data(self, hot_posts):
        if hot_posts:
            for post in hot_posts:
                # initialise a dictionary to contain the data of the post
                post_data = {}

                # use permalink as the primary key
                permalink = post.permalink

                # get the relevant post's data and store it inside post_data

                # get the poster's name
                if post.author is not None:
                    post_data['author'] = post.author.name
                else:
                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'


                # get the unix time in which the post was created
                post_data['unix_time'] = post.created_utc
                post_data['permalink'] = post.permalink
                post_data['score'] = post.score
                post_data['subreddit'] = post.subreddit.display_name
                post_data['post_title'] = post.title
                post_data['body'] = post.selftext

                # get the unique name of the post (name refers to the id of the post prefixed with t3_)
                # t3 represents that it is a post
                post_data['name'] = post.name

                # store the posts data in the dictionary where the name is used as the key
                self.data.append(post_data)

                # extract the comments from the post
                comments = post.comments.list()

                # extract the data in the comments and store them inside the dictionary
                self.extract_comments_data(comments)
        
        return self.data

    def extract_comments_data(self, comments):
        if comments:
            for comment in comments:
                # initialise a dictionary to contain the data of the comment
                comment_data = {}


                # get the relevant post's data and store it inside post_data

                # get the commenter name
                if comment.author is not None:
                    comment_data['author'] = comment.author.name
                else:
                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the comment was created
                comment_data['unix_time'] = comment.created_utc
                comment_data['permalink'] = comment.permalink
                comment_data['score'] = comment.score
                comment_data['subreddit'] = comment.subreddit.display_name
                comment_data['post_title'] = comment.submission.title
                comment_data['body'] = comment.body

                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)
                # t1 represents that it is a post
                comment_data['name'] = comment.name

                # store the comment's data in the dictionary where the name is used as the key
                self.data.append(comment_data)

                # extract the comment's replies from the post
                # replies = comment.replies

                # extract the data in the comments and store them inside the dictionary
                # self.extract_comments_data(replies)
        
        return
    
def get_hot_posts(reddit_instance, subreddit_name, limit):
    # Define the subreddit
    subreddit = reddit_instance.subreddit(subreddit_name)

    # Get hot posts from the subreddit
    hot_posts = subreddit.hot(limit=limit)

    # Filter out posts that are not pinned
    filtered_posts = [post for post in hot_posts if not post.stickied]
    return filtered_posts

# initialize Reddit instance
reddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',
                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',
                     user_agent='Prawlingv1')


# choose the subreddit to extract data from
subreddit = 'singapore'
limit = 3

# get hot posts from the subreddit
hot_posts = get_hot_posts(reddit, subreddit, limit)

# initialise a reddit data extractor object
data_extractor = RedditPostDataExtractor()

# extract data from the hot posts from r/singapore
data = data_extractor.extract_data(hot_posts)
data
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.111 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.111 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.111 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_13/499/1204/py_499_1204.py
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.111 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.111 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_13/499/1204/499_1204.sh
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:40.124 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6031
[WI-0][TI-1204] - [INFO] 2024-04-23 12:13:40.850 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1204, success=true)
[WI-0][TI-1204] - [INFO] 2024-04-23 12:13:40.856 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1204)
[WI-0][TI-0] - [INFO] 2024-04-23 12:13:41.132 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:43.138 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_13/499/1204, processId:6031 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:43.139 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:43.139 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:43.140 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:43.140 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:43.146 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:43.147 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:43.147 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_13/499/1204
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:43.148 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_13/499/1204
[WI-499][TI-1204] - [INFO] 2024-04-23 12:13:43.149 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1204] - [INFO] 2024-04-23 12:13:43.856 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1204, success=true)
[WI-0][TI-1123] - [INFO] 2024-04-23 12:14:56.576 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1123, processInstanceId=487, status=9, startTime=1713838208504, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1123.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1123, endTime=1713838483436, processId=1677, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713845395798)
[WI-0][TI-1123] - [INFO] 2024-04-23 12:14:56.581 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1123, processInstanceId=487, status=9, startTime=1713838208504, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1123.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1123, endTime=1713838483436, processId=1677, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713845696576)
[WI-0][TI-1111] - [INFO] 2024-04-23 12:14:58.620 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1111, processInstanceId=487, status=9, startTime=1713837256404, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1111.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1111, endTime=1713838170352, processId=1017, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713845397806)
[WI-0][TI-1111] - [INFO] 2024-04-23 12:14:58.629 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1111, processInstanceId=487, status=9, startTime=1713837256404, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1111.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1111, endTime=1713838170352, processId=1017, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713845698620)
[WI-0][TI-1144] - [INFO] 2024-04-23 12:17:19.298 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1144, processInstanceId=489, status=9, startTime=1713840953087, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/80/489/1144.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_80/489/1144, endTime=1713841331355, processId=3026, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713845538714)
[WI-0][TI-1144] - [INFO] 2024-04-23 12:17:19.302 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1144, processInstanceId=489, status=9, startTime=1713840953087, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/80/489/1144.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_80/489/1144, endTime=1713841331355, processId=3026, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713845839298)
[WI-0][TI-1123] - [INFO] 2024-04-23 12:19:56.836 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1123, processInstanceId=487, status=9, startTime=1713838208504, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1123.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1123, endTime=1713838483436, processId=1677, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713845696576)
[WI-0][TI-1123] - [INFO] 2024-04-23 12:19:56.841 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1123, processInstanceId=487, status=9, startTime=1713838208504, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1123.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1123, endTime=1713838483436, processId=1677, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713845996836)
[WI-0][TI-1111] - [INFO] 2024-04-23 12:19:58.849 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1111, processInstanceId=487, status=9, startTime=1713837256404, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1111.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1111, endTime=1713838170352, processId=1017, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713845698620)
[WI-0][TI-1111] - [INFO] 2024-04-23 12:19:58.854 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1111, processInstanceId=487, status=9, startTime=1713837256404, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1111.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1111, endTime=1713838170352, processId=1017, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713845998849)
[WI-0][TI-1144] - [INFO] 2024-04-23 12:22:19.896 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1144, processInstanceId=489, status=9, startTime=1713840953087, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/80/489/1144.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_80/489/1144, endTime=1713841331355, processId=3026, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713845839298)
[WI-0][TI-1144] - [INFO] 2024-04-23 12:22:19.899 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1144, processInstanceId=489, status=9, startTime=1713840953087, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/80/489/1144.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_80/489/1144, endTime=1713841331355, processId=3026, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713846139896)
[WI-0][TI-1123] - [INFO] 2024-04-23 12:24:57.262 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1123, processInstanceId=487, status=9, startTime=1713838208504, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1123.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1123, endTime=1713838483436, processId=1677, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713845996836)
[WI-0][TI-1123] - [INFO] 2024-04-23 12:24:57.266 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1123, processInstanceId=487, status=9, startTime=1713838208504, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1123.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1123, endTime=1713838483436, processId=1677, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713846297262)
[WI-0][TI-1111] - [INFO] 2024-04-23 12:24:59.284 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1111, processInstanceId=487, status=9, startTime=1713837256404, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1111.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1111, endTime=1713838170352, processId=1017, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713845998849)
[WI-0][TI-1111] - [INFO] 2024-04-23 12:24:59.292 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1111, processInstanceId=487, status=9, startTime=1713837256404, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1111.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1111, endTime=1713838170352, processId=1017, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713846299284)
[WI-0][TI-1144] - [INFO] 2024-04-23 12:27:20.109 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1144, processInstanceId=489, status=9, startTime=1713840953087, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/80/489/1144.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_80/489/1144, endTime=1713841331355, processId=3026, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713846139896)
[WI-0][TI-1144] - [INFO] 2024-04-23 12:27:20.114 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1144, processInstanceId=489, status=9, startTime=1713840953087, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/80/489/1144.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_80/489/1144, endTime=1713841331355, processId=3026, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713846440109)
[WI-0][TI-1123] - [INFO] 2024-04-23 12:29:57.820 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1123, processInstanceId=487, status=9, startTime=1713838208504, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1123.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1123, endTime=1713838483436, processId=1677, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713846297262)
[WI-0][TI-1123] - [INFO] 2024-04-23 12:29:57.823 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1123, processInstanceId=487, status=9, startTime=1713838208504, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1123.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1123, endTime=1713838483436, processId=1677, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713846597820)
[WI-0][TI-1111] - [INFO] 2024-04-23 12:29:59.826 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1111, processInstanceId=487, status=9, startTime=1713837256404, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1111.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1111, endTime=1713838170352, processId=1017, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713846299284)
[WI-0][TI-1111] - [INFO] 2024-04-23 12:29:59.829 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1111, processInstanceId=487, status=9, startTime=1713837256404, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1111.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1111, endTime=1713838170352, processId=1017, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713846599826)
[WI-0][TI-1144] - [INFO] 2024-04-23 12:32:20.195 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1144, processInstanceId=489, status=9, startTime=1713840953087, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/80/489/1144.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_80/489/1144, endTime=1713841331355, processId=3026, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713846440109)
[WI-0][TI-1144] - [INFO] 2024-04-23 12:32:20.199 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1144, processInstanceId=489, status=9, startTime=1713840953087, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/80/489/1144.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_80/489/1144, endTime=1713841331355, processId=3026, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713846740195)
[WI-0][TI-1123] - [INFO] 2024-04-23 12:34:58.535 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1123, processInstanceId=487, status=9, startTime=1713838208504, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1123.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1123, endTime=1713838483436, processId=1677, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713846597820)
[WI-0][TI-1123] - [INFO] 2024-04-23 12:34:58.540 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1123, processInstanceId=487, status=9, startTime=1713838208504, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1123.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1123, endTime=1713838483436, processId=1677, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713846898535)
[WI-0][TI-1111] - [INFO] 2024-04-23 12:35:00.541 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1111, processInstanceId=487, status=9, startTime=1713837256404, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1111.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1111, endTime=1713838170352, processId=1017, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713846599826)
[WI-0][TI-1111] - [INFO] 2024-04-23 12:35:00.544 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1111, processInstanceId=487, status=9, startTime=1713837256404, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1111.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1111, endTime=1713838170352, processId=1017, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713846900541)
[WI-0][TI-1144] - [INFO] 2024-04-23 12:37:20.203 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1144, processInstanceId=489, status=9, startTime=1713840953087, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/80/489/1144.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_80/489/1144, endTime=1713841331355, processId=3026, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713846740195)
[WI-0][TI-1144] - [INFO] 2024-04-23 12:37:20.207 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1144, processInstanceId=489, status=9, startTime=1713840953087, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/80/489/1144.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_80/489/1144, endTime=1713841331355, processId=3026, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713847040203)
[WI-0][TI-1123] - [INFO] 2024-04-23 12:39:59.339 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1123, processInstanceId=487, status=9, startTime=1713838208504, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1123.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1123, endTime=1713838483436, processId=1677, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713846898535)
[WI-0][TI-1123] - [INFO] 2024-04-23 12:39:59.342 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1123, processInstanceId=487, status=9, startTime=1713838208504, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1123.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1123, endTime=1713838483436, processId=1677, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713847199339)
[WI-0][TI-1111] - [INFO] 2024-04-23 12:40:01.366 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1111, processInstanceId=487, status=9, startTime=1713837256404, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1111.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1111, endTime=1713838170352, processId=1017, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713846900541)
[WI-0][TI-1111] - [INFO] 2024-04-23 12:40:01.379 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1111, processInstanceId=487, status=9, startTime=1713837256404, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1111.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1111, endTime=1713838170352, processId=1017, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713847201366)
[WI-0][TI-1144] - [INFO] 2024-04-23 12:42:20.636 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1144, processInstanceId=489, status=9, startTime=1713840953087, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/80/489/1144.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_80/489/1144, endTime=1713841331355, processId=3026, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713847040203)
[WI-0][TI-1144] - [INFO] 2024-04-23 12:42:20.641 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1144, processInstanceId=489, status=9, startTime=1713840953087, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/80/489/1144.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_80/489/1144, endTime=1713841331355, processId=3026, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713847340636)
[WI-0][TI-1123] - [INFO] 2024-04-23 12:44:59.506 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1123, processInstanceId=487, status=9, startTime=1713838208504, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1123.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1123, endTime=1713838483436, processId=1677, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713847199339)
[WI-0][TI-1123] - [INFO] 2024-04-23 12:44:59.514 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1123, processInstanceId=487, status=9, startTime=1713838208504, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1123.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1123, endTime=1713838483436, processId=1677, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713847499506)
[WI-0][TI-1111] - [INFO] 2024-04-23 12:45:01.517 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1111, processInstanceId=487, status=9, startTime=1713837256404, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1111.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1111, endTime=1713838170352, processId=1017, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713847201366)
[WI-0][TI-1111] - [INFO] 2024-04-23 12:45:01.525 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1111, processInstanceId=487, status=9, startTime=1713837256404, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1111.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1111, endTime=1713838170352, processId=1017, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713847501517)
[WI-0][TI-1144] - [INFO] 2024-04-23 12:47:20.877 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1144, processInstanceId=489, status=9, startTime=1713840953087, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/80/489/1144.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_80/489/1144, endTime=1713841331355, processId=3026, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713847340636)
[WI-0][TI-1144] - [INFO] 2024-04-23 12:47:20.884 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1144, processInstanceId=489, status=9, startTime=1713840953087, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/80/489/1144.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_80/489/1144, endTime=1713841331355, processId=3026, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713847640877)
[WI-0][TI-1123] - [INFO] 2024-04-23 12:49:59.723 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1123, processInstanceId=487, status=9, startTime=1713838208504, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1123.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1123, endTime=1713838483436, processId=1677, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713847499506)
[WI-0][TI-1123] - [INFO] 2024-04-23 12:49:59.729 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1123, processInstanceId=487, status=9, startTime=1713838208504, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1123.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1123, endTime=1713838483436, processId=1677, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713847799723)
[WI-0][TI-1111] - [INFO] 2024-04-23 12:50:01.730 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1111, processInstanceId=487, status=9, startTime=1713837256404, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1111.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1111, endTime=1713838170352, processId=1017, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713847501517)
[WI-0][TI-1111] - [INFO] 2024-04-23 12:50:01.734 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1111, processInstanceId=487, status=9, startTime=1713837256404, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1111.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1111, endTime=1713838170352, processId=1017, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713847801730)
[WI-0][TI-1144] - [INFO] 2024-04-23 12:52:21.577 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1144, processInstanceId=489, status=9, startTime=1713840953087, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/80/489/1144.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_80/489/1144, endTime=1713841331355, processId=3026, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713847640877)
[WI-0][TI-1144] - [INFO] 2024-04-23 12:52:21.592 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1144, processInstanceId=489, status=9, startTime=1713840953087, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/80/489/1144.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_80/489/1144, endTime=1713841331355, processId=3026, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713847941577)
[WI-0][TI-1123] - [INFO] 2024-04-23 12:54:59.751 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1123, processInstanceId=487, status=9, startTime=1713838208504, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1123.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1123, endTime=1713838483436, processId=1677, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713847799723)
[WI-0][TI-1123] - [INFO] 2024-04-23 12:54:59.755 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1123, processInstanceId=487, status=9, startTime=1713838208504, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1123.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1123, endTime=1713838483436, processId=1677, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713848099751)
[WI-0][TI-1111] - [INFO] 2024-04-23 12:55:01.760 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1111, processInstanceId=487, status=9, startTime=1713837256404, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1111.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1111, endTime=1713838170352, processId=1017, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713847801730)
[WI-0][TI-1111] - [INFO] 2024-04-23 12:55:01.774 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1111, processInstanceId=487, status=9, startTime=1713837256404, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/78/487/1111.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_78/487/1111, endTime=1713838170352, processId=1017, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713848101760)
[WI-0][TI-1144] - [INFO] 2024-04-23 12:57:22.551 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1144, processInstanceId=489, status=9, startTime=1713840953087, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/80/489/1144.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_80/489/1144, endTime=1713841331355, processId=3026, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713847941577)
[WI-0][TI-1144] - [INFO] 2024-04-23 12:57:22.554 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1144, processInstanceId=489, status=9, startTime=1713840953087, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240423/13201021801792/80/489/1144.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_80/489/1144, endTime=1713841331355, processId=3026, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/386-20240422.json"}], eventCreateTime=0, eventSendTime=1713848242551)
