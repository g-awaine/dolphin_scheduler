[WI-0][TI-0] - [WARN] 2024-04-25 03:49:44.833 +0800 o.s.c.k.c.p.AbstractKubernetesProfileEnvironmentPostProcessor:[258] - Not running inside kubernetes. Skipping 'kubernetes' profile activation.
[WI-0][TI-0] - [WARN] 2024-04-25 03:49:50.539 +0800 o.s.c.k.c.p.AbstractKubernetesProfileEnvironmentPostProcessor:[258] - Not running inside kubernetes. Skipping 'kubernetes' profile activation.
[WI-0][TI-0] - [INFO] 2024-04-25 03:49:50.564 +0800 o.a.d.s.w.WorkerServer:[634] - No active profile set, falling back to 1 default profile: "default"
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:01.241 +0800 o.s.c.c.s.GenericScope:[283] - BeanFactory id=7e7bf7c6-2cb3-34d2-a0d5-a56161c67d0e
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:02.923 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'org.springframework.cloud.commons.config.CommonsConfigAutoConfiguration' of type [org.springframework.cloud.commons.config.CommonsConfigAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:02.927 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'org.springframework.cloud.client.loadbalancer.LoadBalancerDefaultMappingsProviderAutoConfiguration' of type [org.springframework.cloud.client.loadbalancer.LoadBalancerDefaultMappingsProviderAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:02.948 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'loadBalancerClientsDefaultsMappingsProvider' of type [org.springframework.cloud.client.loadbalancer.LoadBalancerDefaultMappingsProviderAutoConfiguration$$Lambda$392/1342373353] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:02.958 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'defaultsBindHandlerAdvisor' of type [org.springframework.cloud.commons.config.DefaultsBindHandlerAdvisor] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:03.092 +0800 o.a.d.c.u.NetUtils:[312] - Get all NetworkInterfaces: [name:eth0 (eth0), name:lo (lo)]
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:03.268 +0800 o.a.d.s.w.c.WorkerConfig:[98] - 
****************************Worker Configuration**************************************
  listen-port -> 1234
  exec-threads -> 100
  max-heartbeat-interval -> PT10S
  host-weight -> 100
  tenantConfig -> TenantConfig(autoCreateTenantEnabled=true, distributedTenantEnabled=false, defaultTenantEnabled=false)
  server-load-protection -> WorkerServerLoadProtection(enabled=true, maxCpuUsagePercentageThresholds=0.7, maxJVMMemoryUsagePercentageThresholds=0.7, maxSystemMemoryUsagePercentageThresholds=0.7, maxDiskUsagePercentageThresholds=0.7)
  registry-disconnect-strategy -> ConnectStrategyProperties(strategy=WAITING, maxWaitingTime=PT1M40S)
  task-execute-threads-full-policy: REJECT
  address -> 172.18.1.1:1234
  registry-path: /nodes/worker/172.18.1.1:1234
****************************Worker Configuration**************************************
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:03.268 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'workerConfig' of type [org.apache.dolphinscheduler.server.worker.config.WorkerConfig$$EnhancerBySpringCGLIB$$20716c19] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:03.976 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'io.kubernetes.client.spring.extended.manifests.config.KubernetesManifestsAutoConfiguration' of type [io.kubernetes.client.spring.extended.manifests.config.KubernetesManifestsAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:04.037 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'kubernetes.manifests-io.kubernetes.client.spring.extended.manifests.config.KubernetesManifestsProperties' of type [io.kubernetes.client.spring.extended.manifests.config.KubernetesManifestsProperties] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:04.048 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'io.kubernetes.client.spring.extended.controller.config.KubernetesInformerAutoConfiguration' of type [io.kubernetes.client.spring.extended.controller.config.KubernetesInformerAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:04.159 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'defaultApiClient' of type [io.kubernetes.client.openapi.ApiClient] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:04.844 +0800 o.e.j.u.log:[170] - Logging initialized @34791ms to org.eclipse.jetty.util.log.Slf4jLog
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:06.057 +0800 o.s.b.w.e.j.JettyServletWebServerFactory:[166] - Server initialized with port: 1235
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:06.061 +0800 o.e.j.s.Server:[375] - jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 1.8.0_402-b06
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:06.575 +0800 o.e.j.s.h.C.application:[2368] - Initializing Spring embedded WebApplicationContext
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:06.607 +0800 o.s.b.w.s.c.ServletWebServerApplicationContext:[292] - Root WebApplicationContext: initialization completed in 15865 ms
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:09.304 +0800 o.e.j.s.session:[334] - DefaultSessionIdManager workerName=node0
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:09.304 +0800 o.e.j.s.session:[339] - No SessionScavenger set, using defaults
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:09.307 +0800 o.e.j.s.session:[132] - node0 Scavenging every 660000ms
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:09.457 +0800 o.e.j.s.h.ContextHandler:[921] - Started o.s.b.w.e.j.JettyEmbeddedWebAppContext@68f776ee{application,/,[file:///tmp/jetty-docbase.1235.6453475844963952425/],AVAILABLE}
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:09.458 +0800 o.e.j.s.Server:[415] - Started @39405ms
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.190 +0800 o.a.c.f.i.CuratorFrameworkImpl:[338] - Starting
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.246 +0800 o.a.z.ZooKeeper:[98] - Client environment:zookeeper.version=3.8.0-5a02a05eddb59aee6ac762f7ea82e92a68eb9c0f, built on 2022-02-25 08:49 UTC
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.247 +0800 o.a.z.ZooKeeper:[98] - Client environment:host.name=4691b849b0ea
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.260 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.version=1.8.0_402
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.260 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.vendor=Temurin
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.260 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.home=/opt/java/openjdk
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.261 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.class.path=/opt/dolphinscheduler/conf:/opt/dolphinscheduler/libs/kerb-client-1.0.1.jar:/opt/dolphinscheduler/libs/spring-cloud-starter-kubernetes-client-config-2.1.3.jar:/opt/dolphinscheduler/libs/oauth2-oidc-sdk-9.35.jar:/opt/dolphinscheduler/libs/jsqlparser-4.4.jar:/opt/dolphinscheduler/libs/reactive-streams-1.0.4.jar:/opt/dolphinscheduler/libs/kubernetes-model-extensions-5.10.2.jar:/opt/dolphinscheduler/libs/zookeeper-3.8.0.jar:/opt/dolphinscheduler/libs/kubernetes-model-apps-5.10.2.jar:/opt/dolphinscheduler/libs/grpc-api-1.41.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-pigeon-3.2.1.jar:/opt/dolphinscheduler/libs/client-java-api-13.0.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-obs-3.2.1.jar:/opt/dolphinscheduler/libs/spring-web-5.3.22.jar:/opt/dolphinscheduler/libs/aws-java-sdk-emr-1.12.300.jar:/opt/dolphinscheduler/libs/spring-context-5.3.22.jar:/opt/dolphinscheduler/libs/gapic-google-cloud-storage-v2-2.18.0-alpha.jar:/opt/dolphinscheduler/libs/spring-cloud-kubernetes-client-config-2.1.3.jar:/opt/dolphinscheduler/libs/jetty-io-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/annotations-13.0.jar:/opt/dolphinscheduler/libs/jpam-1.1.jar:/opt/dolphinscheduler/libs/joda-time-2.10.13.jar:/opt/dolphinscheduler/libs/spring-cloud-kubernetes-client-autoconfig-2.1.3.jar:/opt/dolphinscheduler/libs/kerb-identity-1.0.1.jar:/opt/dolphinscheduler/libs/vertica-jdbc-12.0.4-0.jar:/opt/dolphinscheduler/libs/grpc-googleapis-1.52.1.jar:/opt/dolphinscheduler/libs/aliyun-java-sdk-kms-2.11.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-dvc-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-hana-3.2.1.jar:/opt/dolphinscheduler/libs/kubernetes-httpclient-okhttp-6.0.0.jar:/opt/dolphinscheduler/libs/jline-2.12.jar:/opt/dolphinscheduler/libs/sshd-core-2.8.0.jar:/opt/dolphinscheduler/libs/jna-platform-5.10.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-sqlserver-3.2.1.jar:/opt/dolphinscheduler/libs/commons-beanutils-1.9.4.jar:/opt/dolphinscheduler/libs/grpc-services-1.41.0.jar:/opt/dolphinscheduler/libs/jmespath-java-1.12.300.jar:/opt/dolphinscheduler/libs/curator-client-5.3.0.jar:/opt/dolphinscheduler/libs/proto-google-common-protos-2.0.1.jar:/opt/dolphinscheduler/libs/mybatis-3.5.10.jar:/opt/dolphinscheduler/libs/jackson-annotations-2.13.4.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-all-3.2.1.jar:/opt/dolphinscheduler/libs/jakarta.xml.bind-api-2.3.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-jupyter-3.2.1.jar:/opt/dolphinscheduler/libs/mybatis-plus-extension-3.5.2.jar:/opt/dolphinscheduler/libs/j2objc-annotations-1.3.jar:/opt/dolphinscheduler/libs/Java-WebSocket-1.5.1.jar:/opt/dolphinscheduler/libs/azure-storage-common-12.20.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-sagemaker-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-databend-3.2.1.jar:/opt/dolphinscheduler/libs/google-auth-library-oauth2-http-1.15.0.jar:/opt/dolphinscheduler/libs/jetty-security-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-python-3.2.1.jar:/opt/dolphinscheduler/libs/snappy-java-1.1.10.1.jar:/opt/dolphinscheduler/libs/kubernetes-model-batch-5.10.2.jar:/opt/dolphinscheduler/libs/netty-transport-native-epoll-4.1.53.Final-linux-x86_64.jar:/opt/dolphinscheduler/libs/jackson-core-asl-1.9.13.jar:/opt/dolphinscheduler/libs/gson-fire-1.8.5.jar:/opt/dolphinscheduler/libs/clickhouse-jdbc-0.4.6.jar:/opt/dolphinscheduler/libs/grpc-netty-shaded-1.41.0.jar:/opt/dolphinscheduler/libs/caffeine-2.9.3.jar:/opt/dolphinscheduler/libs/aliyun-java-sdk-core-4.5.10.jar:/opt/dolphinscheduler/libs/jackson-module-jaxb-annotations-2.13.3.jar:/opt/dolphinscheduler/libs/jetty-http-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/jersey-servlet-1.19.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-dinky-3.2.1.jar:/opt/dolphinscheduler/libs/simpleclient_tracer_common-0.15.0.jar:/opt/dolphinscheduler/libs/netty-handler-4.1.53.Final.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-datafactory-3.2.1.jar:/opt/dolphinscheduler/libs/json-path-2.7.0.jar:/opt/dolphinscheduler/libs/mybatis-plus-annotation-3.5.2.jar:/opt/dolphinscheduler/libs/azure-resourcemanager-datafactory-1.0.0-beta.19.jar:/opt/dolphinscheduler/libs/commons-codec-1.11.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-master-3.2.1.jar:/opt/dolphinscheduler/libs/spring-aop-5.3.22.jar:/opt/dolphinscheduler/libs/kubernetes-model-core-5.10.2.jar:/opt/dolphinscheduler/libs/kubernetes-model-networking-5.10.2.jar:/opt/dolphinscheduler/libs/kotlin-stdlib-jdk7-1.6.21.jar:/opt/dolphinscheduler/libs/kerby-xdr-1.0.1.jar:/opt/dolphinscheduler/libs/aliyun-java-sdk-ram-3.1.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-k8s-3.2.1.jar:/opt/dolphinscheduler/libs/guava-31.1-jre.jar:/opt/dolphinscheduler/libs/netty-codec-dns-4.1.53.Final.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-kubeflow-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-azure-sql-3.2.1.jar:/opt/dolphinscheduler/libs/HikariCP-4.0.3.jar:/opt/dolphinscheduler/libs/hive-metastore-2.3.9.jar:/opt/dolphinscheduler/libs/msal4j-1.13.3.jar:/opt/dolphinscheduler/libs/jackson-core-2.13.4.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-hive-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-mr-3.2.1.jar:/opt/dolphinscheduler/libs/kubernetes-model-node-5.10.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-common-3.2.1.jar:/opt/dolphinscheduler/libs/jose4j-0.7.8.jar:/opt/dolphinscheduler/libs/jul-to-slf4j-1.7.36.jar:/opt/dolphinscheduler/libs/azure-identity-1.7.1.jar:/opt/dolphinscheduler/libs/spring-boot-autoconfigure-2.7.3.jar:/opt/dolphinscheduler/libs/commons-math3-3.1.1.jar:/opt/dolphinscheduler/libs/jackson-jaxrs-json-provider-2.13.3.jar:/opt/dolphinscheduler/libs/perfmark-api-0.23.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-doris-3.2.1.jar:/opt/dolphinscheduler/libs/httpclient-4.5.13.jar:/opt/dolphinscheduler/libs/hive-service-2.3.9.jar:/opt/dolphinscheduler/libs/kerby-util-1.0.1.jar:/opt/dolphinscheduler/libs/commons-collections-3.2.2.jar:/opt/dolphinscheduler/libs/hadoop-yarn-client-3.2.4.jar:/opt/dolphinscheduler/libs/commons-text-1.8.jar:/opt/dolphinscheduler/libs/dolphinscheduler-worker-3.2.1.jar:/opt/dolphinscheduler/libs/hadoop-yarn-common-3.2.4.jar:/opt/dolphinscheduler/libs/zeppelin-client-0.10.1.jar:/opt/dolphinscheduler/libs/azure-core-management-1.10.1.jar:/opt/dolphinscheduler/libs/hadoop-mapreduce-client-jobclient-3.2.4.jar:/opt/dolphinscheduler/libs/jta-1.1.jar:/opt/dolphinscheduler/libs/annotations-4.1.1.4.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-alert-3.2.1.jar:/opt/dolphinscheduler/libs/curator-framework-5.3.0.jar:/opt/dolphinscheduler/libs/hive-jdbc-2.3.9.jar:/opt/dolphinscheduler/libs/kyuubi-hive-jdbc-shaded-1.7.0.jar:/opt/dolphinscheduler/libs/client-java-proto-13.0.2.jar:/opt/dolphinscheduler/libs/checker-qual-3.19.0.jar:/opt/dolphinscheduler/libs/jetty-servlet-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/google-cloud-core-grpc-2.10.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-shell-3.2.1.jar:/opt/dolphinscheduler/libs/spring-security-rsa-1.0.10.RELEASE.jar:/opt/dolphinscheduler/libs/avro-1.7.7.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-postgresql-3.2.1.jar:/opt/dolphinscheduler/libs/netty-nio-client-2.17.282.jar:/opt/dolphinscheduler/libs/spring-webmvc-5.3.22.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-mysql-3.2.1.jar:/opt/dolphinscheduler/libs/opentracing-util-0.33.0.jar:/opt/dolphinscheduler/libs/auto-value-1.10.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-all-3.2.1.jar:/opt/dolphinscheduler/libs/jetcd-core-0.5.11.jar:/opt/dolphinscheduler/libs/grpc-context-1.41.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-datasync-3.2.1.jar:/opt/dolphinscheduler/libs/jackson-dataformat-cbor-2.13.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-gcs-3.2.1.jar:/opt/dolphinscheduler/libs/client-java-extended-13.0.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-redshift-3.2.1.jar:/opt/dolphinscheduler/libs/opencsv-2.3.jar:/opt/dolphinscheduler/libs/jcip-annotations-1.0-1.jar:/opt/dolphinscheduler/libs/accessors-smart-2.4.8.jar:/opt/dolphinscheduler/libs/hadoop-client-3.2.4.jar:/opt/dolphinscheduler/libs/commons-collections4-4.3.jar:/opt/dolphinscheduler/libs/metrics-core-4.2.11.jar:/opt/dolphinscheduler/libs/netty-resolver-4.1.53.Final.jar:/opt/dolphinscheduler/libs/parquet-hadoop-bundle-1.8.1.jar:/opt/dolphinscheduler/libs/bucket4j-core-6.2.0.jar:/opt/dolphinscheduler/libs/spring-cloud-starter-3.1.3.jar:/opt/dolphinscheduler/libs/mssql-jdbc-11.2.1.jre8.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/dolphinscheduler/libs/kubernetes-model-coordination-5.10.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-linkis-3.2.1.jar:/opt/dolphinscheduler/libs/commons-net-3.6.jar:/opt/dolphinscheduler/libs/netty-transport-native-kqueue-4.1.53.Final-osx-x86_64.jar:/opt/dolphinscheduler/libs/dolphinscheduler-meter-3.2.1.jar:/opt/dolphinscheduler/libs/kubernetes-client-api-6.0.0.jar:/opt/dolphinscheduler/libs/kubernetes-model-certificates-5.10.2.jar:/opt/dolphinscheduler/libs/opencensus-api-0.31.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-mlflow-3.2.1.jar:/opt/dolphinscheduler/libs/kotlin-stdlib-jdk8-1.6.21.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-jdbc-3.2.1.jar:/opt/dolphinscheduler/libs/audience-annotations-0.12.0.jar:/opt/dolphinscheduler/libs/simpleclient_httpserver-0.15.0.jar:/opt/dolphinscheduler/libs/jetty-xml-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/kerby-asn1-1.0.1.jar:/opt/dolphinscheduler/libs/lang-tag-1.6.jar:/opt/dolphinscheduler/libs/api-common-2.6.0.jar:/opt/dolphinscheduler/libs/HdrHistogram-2.1.12.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-abs-3.2.1.jar:/opt/dolphinscheduler/libs/failsafe-2.4.4.jar:/opt/dolphinscheduler/libs/hbase-noop-htrace-4.1.1.jar:/opt/dolphinscheduler/libs/jamon-runtime-2.3.1.jar:/opt/dolphinscheduler/libs/jetty-util-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/threetenbp-1.6.5.jar:/opt/dolphinscheduler/libs/jakarta.activation-api-1.2.2.jar:/opt/dolphinscheduler/libs/kubernetes-model-common-5.10.2.jar:/opt/dolphinscheduler/libs/okhttp-4.9.3.jar:/opt/dolphinscheduler/libs/profiles-2.17.282.jar:/opt/dolphinscheduler/libs/hadoop-auth-3.2.4.jar:/opt/dolphinscheduler/libs/grpc-netty-1.41.0.jar:/opt/dolphinscheduler/libs/httpcore-nio-4.4.15.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-seatunnel-3.2.1.jar:/opt/dolphinscheduler/libs/aws-java-sdk-sagemaker-1.12.300.jar:/opt/dolphinscheduler/libs/oshi-core-6.1.1.jar:/opt/dolphinscheduler/libs/bonecp-0.8.0.RELEASE.jar:/opt/dolphinscheduler/libs/jackson-module-parameter-names-2.13.3.jar:/opt/dolphinscheduler/libs/bcutil-jdk15on-1.69.jar:/opt/dolphinscheduler/libs/aws-json-protocol-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-base-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-all-3.2.1.jar:/opt/dolphinscheduler/libs/derby-10.14.2.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-snowflake-3.2.1.jar:/opt/dolphinscheduler/libs/netty-codec-http2-4.1.53.Final.jar:/opt/dolphinscheduler/libs/jetty-continuation-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/jackson-mapper-asl-1.9.13.jar:/opt/dolphinscheduler/libs/datanucleus-core-4.1.17.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/dolphinscheduler/libs/failureaccess-1.0.1.jar:/opt/dolphinscheduler/libs/error_prone_annotations-2.5.1.jar:/opt/dolphinscheduler/libs/kerb-admin-1.0.1.jar:/opt/dolphinscheduler/libs/token-provider-1.0.1.jar:/opt/dolphinscheduler/libs/reactor-netty-core-1.0.22.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-all-3.2.1.jar:/opt/dolphinscheduler/libs/jakarta.servlet-api-4.0.4.jar:/opt/dolphinscheduler/libs/http-client-spi-2.17.282.jar:/opt/dolphinscheduler/libs/regions-2.17.282.jar:/opt/dolphinscheduler/libs/logback-core-1.2.11.jar:/opt/dolphinscheduler/libs/json-1.8.jar:/opt/dolphinscheduler/libs/gax-grpc-2.23.0.jar:/opt/dolphinscheduler/libs/google-http-client-1.42.3.jar:/opt/dolphinscheduler/libs/hive-serde-2.3.9.jar:/opt/dolphinscheduler/libs/jettison-1.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-http-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-zookeeper-3.2.1.jar:/opt/dolphinscheduler/libs/spring-security-crypto-5.7.3.jar:/opt/dolphinscheduler/libs/auth-2.17.282.jar:/opt/dolphinscheduler/libs/proto-google-cloud-storage-v2-2.18.0-alpha.jar:/opt/dolphinscheduler/libs/jetty-server-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/javax.annotation-api-1.3.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-starrocks-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-dataquality-3.2.1.jar:/opt/dolphinscheduler/libs/jcl-over-slf4j-1.7.36.jar:/opt/dolphinscheduler/libs/commons-lang3-3.12.0.jar:/opt/dolphinscheduler/libs/tomcat-embed-el-9.0.65.jar:/opt/dolphinscheduler/libs/opentracing-noop-0.33.0.jar:/opt/dolphinscheduler/libs/client-java-13.0.2.jar:/opt/dolphinscheduler/libs/spring-beans-5.3.22.jar:/opt/dolphinscheduler/libs/snakeyaml-1.33.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-ssh-3.2.1.jar:/opt/dolphinscheduler/libs/commons-pool-1.6.jar:/opt/dolphinscheduler/libs/javax.servlet-api-3.1.0.jar:/opt/dolphinscheduler/libs/hadoop-common-3.2.4.jar:/opt/dolphinscheduler/libs/kubernetes-model-apiextensions-5.10.2.jar:/opt/dolphinscheduler/libs/spring-boot-2.7.3.jar:/opt/dolphinscheduler/libs/bcprov-ext-jdk15on-1.69.jar:/opt/dolphinscheduler/libs/spring-boot-starter-2.7.3.jar:/opt/dolphinscheduler/libs/paranamer-2.3.jar:/opt/dolphinscheduler/libs/httpmime-4.5.13.jar:/opt/dolphinscheduler/libs/reactor-core-3.4.22.jar:/opt/dolphinscheduler/libs/azure-core-1.36.0.jar:/opt/dolphinscheduler/libs/bcpkix-jdk15on-1.69.jar:/opt/dolphinscheduler/libs/kubernetes-model-scheduling-5.10.2.jar:/opt/dolphinscheduler/libs/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/dolphinscheduler/libs/websocket-client-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/unirest-java-3.7.04-standalone.jar:/opt/dolphinscheduler/libs/hadoop-mapreduce-client-core-3.2.4.jar:/opt/dolphinscheduler/libs/google-auth-library-credentials-1.15.0.jar:/opt/dolphinscheduler/libs/azure-storage-internal-avro-12.6.0.jar:/opt/dolphinscheduler/libs/jackson-datatype-jdk8-2.13.3.jar:/opt/dolphinscheduler/libs/spring-expression-5.3.22.jar:/opt/dolphinscheduler/libs/aspectjweaver-1.9.7.jar:/opt/dolphinscheduler/libs/websocket-common-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/client-java-api-fluent-13.0.2.jar:/opt/dolphinscheduler/libs/mybatis-plus-3.5.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-athena-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-oss-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-openmldb-3.2.1.jar:/opt/dolphinscheduler/libs/curator-recipes-5.3.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-api-3.2.1.jar:/opt/dolphinscheduler/libs/netty-all-4.1.53.Final.jar:/opt/dolphinscheduler/libs/netty-resolver-dns-4.1.53.Final.jar:/opt/dolphinscheduler/libs/kubernetes-model-autoscaling-5.10.2.jar:/opt/dolphinscheduler/libs/kerb-util-1.0.1.jar:/opt/dolphinscheduler/libs/grpc-alts-1.41.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-presto-3.2.1.jar:/opt/dolphinscheduler/libs/kerb-simplekdc-1.0.1.jar:/opt/dolphinscheduler/libs/protobuf-java-util-3.17.2.jar:/opt/dolphinscheduler/libs/azure-core-http-netty-1.13.0.jar:/opt/dolphinscheduler/libs/transaction-api-1.1.jar:/opt/dolphinscheduler/libs/datanucleus-api-jdo-4.2.4.jar:/opt/dolphinscheduler/libs/zeppelin-common-0.10.1.jar:/opt/dolphinscheduler/libs/zt-zip-1.15.jar:/opt/dolphinscheduler/libs/animal-sniffer-annotations-1.19.jar:/opt/dolphinscheduler/libs/google-http-client-jackson2-1.42.3.jar:/opt/dolphinscheduler/libs/netty-buffer-4.1.53.Final.jar:/opt/dolphinscheduler/libs/jsr305-3.0.0.jar:/opt/dolphinscheduler/libs/netty-transport-classes-epoll-4.1.79.Final.jar:/opt/dolphinscheduler/libs/spring-boot-actuator-autoconfigure-2.7.3.jar:/opt/dolphinscheduler/libs/simpleclient-0.15.0.jar:/opt/dolphinscheduler/libs/aliyun-sdk-oss-3.15.1.jar:/opt/dolphinscheduler/libs/json-utils-2.17.282.jar:/opt/dolphinscheduler/libs/tephra-api-0.6.0.jar:/opt/dolphinscheduler/libs/spring-jdbc-5.3.22.jar:/opt/dolphinscheduler/libs/grpc-xds-1.41.0.jar:/opt/dolphinscheduler/libs/logback-classic-1.2.11.jar:/opt/dolphinscheduler/libs/google-cloud-storage-2.18.0.jar:/opt/dolphinscheduler/libs/micrometer-registry-prometheus-1.9.3.jar:/opt/dolphinscheduler/libs/grpc-core-1.41.0.jar:/opt/dolphinscheduler/libs/aws-java-sdk-kms-1.12.300.jar:/opt/dolphinscheduler/libs/kubernetes-model-rbac-5.10.2.jar:/opt/dolphinscheduler/libs/ini4j-0.5.4.jar:/opt/dolphinscheduler/libs/spring-boot-starter-web-2.7.3.jar:/opt/dolphinscheduler/libs/spring-cloud-commons-3.1.3.jar:/opt/dolphinscheduler/libs/netty-codec-http-4.1.53.Final.jar:/opt/dolphinscheduler/libs/jetty-client-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/jetcd-common-0.5.11.jar:/opt/dolphinscheduler/libs/metrics-spi-2.17.282.jar:/opt/dolphinscheduler/libs/utils-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-emr-3.2.1.jar:/opt/dolphinscheduler/libs/protocol-core-2.17.282.jar:/opt/dolphinscheduler/libs/micrometer-core-1.9.3.jar:/opt/dolphinscheduler/libs/netty-transport-native-unix-common-4.1.53.Final.jar:/opt/dolphinscheduler/libs/zjsonpatch-0.4.11.jar:/opt/dolphinscheduler/libs/annotations-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-sql-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-common-3.2.1.jar:/opt/dolphinscheduler/libs/apache-client-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-oceanbase-3.2.1.jar:/opt/dolphinscheduler/libs/jdo-api-3.0.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-datax-3.2.1.jar:/opt/dolphinscheduler/libs/postgresql-42.4.1.jar:/opt/dolphinscheduler/libs/jetty-util-ajax-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/gax-2.23.0.jar:/opt/dolphinscheduler/libs/grpc-stub-1.41.0.jar:/opt/dolphinscheduler/libs/libfb303-0.9.3.jar:/opt/dolphinscheduler/libs/spring-core-5.3.22.jar:/opt/dolphinscheduler/libs/jaxb-api-2.3.1.jar:/opt/dolphinscheduler/libs/hive-service-rpc-2.3.9.jar:/opt/dolphinscheduler/libs/kubernetes-model-flowcontrol-5.10.2.jar:/opt/dolphinscheduler/libs/commons-logging-1.1.1.jar:/opt/dolphinscheduler/libs/mybatis-spring-2.0.7.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-oracle-3.2.1.jar:/opt/dolphinscheduler/libs/opencensus-proto-0.2.0.jar:/opt/dolphinscheduler/libs/grpc-protobuf-1.41.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-clickhouse-3.2.1.jar:/opt/dolphinscheduler/libs/spring-cloud-starter-bootstrap-3.1.3.jar:/opt/dolphinscheduler/libs/kubernetes-model-admissionregistration-5.10.2.jar:/opt/dolphinscheduler/libs/grpc-google-cloud-storage-v2-2.18.0-alpha.jar:/opt/dolphinscheduler/libs/esdk-obs-java-bundle-3.23.3.jar:/opt/dolphinscheduler/libs/dnsjava-2.1.7.jar:/opt/dolphinscheduler/libs/asm-9.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-spark-3.2.1.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/dolphinscheduler/libs/re2j-1.6.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-vertica-3.2.1.jar:/opt/dolphinscheduler/libs/json-smart-2.4.8.jar:/opt/dolphinscheduler/libs/reactor-netty-http-1.0.22.jar:/opt/dolphinscheduler/libs/google-api-services-storage-v1-rev20220705-2.0.0.jar:/opt/dolphinscheduler/libs/kubernetes-client-6.0.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-kyuubi-3.2.1.jar:/opt/dolphinscheduler/libs/auto-value-annotations-1.10.1.jar:/opt/dolphinscheduler/libs/commons-cli-1.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-data-quality-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-chunjun-3.2.1.jar:/opt/dolphinscheduler/libs/kerb-server-1.0.1.jar:/opt/dolphinscheduler/libs/content-type-2.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-remoteshell-3.2.1.jar:/opt/dolphinscheduler/libs/opencensus-contrib-http-util-0.31.1.jar:/opt/dolphinscheduler/libs/druid-1.2.20.jar:/opt/dolphinscheduler/libs/javolution-5.5.1.jar:/opt/dolphinscheduler/libs/protobuf-java-3.17.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-db2-3.2.1.jar:/opt/dolphinscheduler/libs/aws-java-sdk-core-1.12.300.jar:/opt/dolphinscheduler/libs/spring-boot-starter-logging-2.7.3.jar:/opt/dolphinscheduler/libs/kerby-pkix-1.0.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-procedure-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-etcd-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-sqoop-3.2.1.jar:/opt/dolphinscheduler/libs/zookeeper-jute-3.8.0.jar:/opt/dolphinscheduler/libs/netty-resolver-dns-native-macos-4.1.53.Final-osx-x86_64.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-sagemaker-3.2.1.jar:/opt/dolphinscheduler/libs/spring-boot-configuration-processor-2.6.1.jar:/opt/dolphinscheduler/libs/hive-common-2.3.9.jar:/opt/dolphinscheduler/libs/kerb-common-1.0.1.jar:/opt/dolphinscheduler/libs/stax-api-1.0.1.jar:/opt/dolphinscheduler/libs/kubernetes-model-storageclass-5.10.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-s3-3.2.1.jar:/opt/dolphinscheduler/libs/spring-boot-starter-jetty-2.7.3.jar:/opt/dolphinscheduler/libs/okio-2.8.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-dms-3.2.1.jar:/opt/dolphinscheduler/libs/simpleclient_common-0.15.0.jar:/opt/dolphinscheduler/libs/sdk-core-2.17.282.jar:/opt/dolphinscheduler/libs/javax.activation-api-1.2.0.jar:/opt/dolphinscheduler/libs/netty-handler-proxy-4.1.53.Final.jar:/opt/dolphinscheduler/libs/kerby-config-1.0.1.jar:/opt/dolphinscheduler/libs/netty-tcnative-classes-2.0.53.Final.jar:/opt/dolphinscheduler/libs/jackson-jaxrs-base-2.13.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-trino-3.2.1.jar:/opt/dolphinscheduler/libs/presto-jdbc-0.238.1.jar:/opt/dolphinscheduler/libs/websocket-api-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-flink-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-api-3.2.1.jar:/opt/dolphinscheduler/libs/kubernetes-model-metrics-5.10.2.jar:/opt/dolphinscheduler/libs/datasync-2.17.282.jar:/opt/dolphinscheduler/libs/hadoop-yarn-api-3.2.4.jar:/opt/dolphinscheduler/libs/google-http-client-apache-v2-1.42.3.jar:/opt/dolphinscheduler/libs/hadoop-annotations-3.2.4.jar:/opt/dolphinscheduler/libs/google-oauth-client-1.34.1.jar:/opt/dolphinscheduler/libs/sshd-common-2.8.0.jar:/opt/dolphinscheduler/libs/LatencyUtils-2.0.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-spark-3.2.1.jar:/opt/dolphinscheduler/libs/slf4j-api-1.7.36.jar:/opt/dolphinscheduler/libs/snowflake-jdbc-3.13.29.jar:/opt/dolphinscheduler/libs/jackson-datatype-jsr310-2.13.3.jar:/opt/dolphinscheduler/libs/trino-jdbc-402.jar:/opt/dolphinscheduler/libs/logging-interceptor-4.9.3.jar:/opt/dolphinscheduler/libs/simpleclient_tracer_otel_agent-0.15.0.jar:/opt/dolphinscheduler/libs/janino-3.0.16.jar:/opt/dolphinscheduler/libs/jackson-dataformat-yaml-2.13.3.jar:/opt/dolphinscheduler/libs/ion-java-1.0.2.jar:/opt/dolphinscheduler/libs/commons-dbcp-1.4.jar:/opt/dolphinscheduler/libs/jsp-api-2.1.jar:/opt/dolphinscheduler/libs/google-http-client-gson-1.42.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-hivecli-3.2.1.jar:/opt/dolphinscheduler/libs/spring-boot-actuator-2.7.3.jar:/opt/dolphinscheduler/libs/google-cloud-core-http-2.10.0.jar:/opt/dolphinscheduler/libs/DmJdbcDriver18-8.1.2.79.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-java-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-dameng-3.2.1.jar:/opt/dolphinscheduler/libs/sshd-sftp-2.8.0.jar:/opt/dolphinscheduler/libs/kerb-crypto-1.0.1.jar:/opt/dolphinscheduler/libs/conscrypt-openjdk-uber-2.5.2.jar:/opt/dolphinscheduler/libs/httpcore-4.4.15.jar:/opt/dolphinscheduler/libs/lz4-java-1.4.0.jar:/opt/dolphinscheduler/libs/guava-retrying-2.0.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-flink-stream-3.2.1.jar:/opt/dolphinscheduler/libs/spring-tx-5.3.22.jar:/opt/dolphinscheduler/libs/grpc-auth-1.41.0.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/dolphinscheduler/libs/databend-jdbc-0.0.7.jar:/opt/dolphinscheduler/libs/bcprov-jdk15on-1.69.jar:/opt/dolphinscheduler/libs/commons-lang-2.6.jar:/opt/dolphinscheduler/libs/kubernetes-model-policy-5.10.2.jar:/opt/dolphinscheduler/libs/commons-io-2.11.0.jar:/opt/dolphinscheduler/libs/grpc-grpclb-1.41.0.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/dolphinscheduler/libs/opentracing-api-0.33.0.jar:/opt/dolphinscheduler/libs/sshd-scp-2.8.0.jar:/opt/dolphinscheduler/libs/reload4j-1.2.18.3.jar:/opt/dolphinscheduler/libs/netty-tcnative-2.0.48.Final.jar:/opt/dolphinscheduler/libs/jdom2-2.0.6.1.jar:/opt/dolphinscheduler/libs/spring-boot-starter-json-2.7.3.jar:/opt/dolphinscheduler/libs/netty-transport-native-epoll-4.1.53.Final.jar:/opt/dolphinscheduler/libs/google-cloud-core-2.10.0.jar:/opt/dolphinscheduler/libs/javax.jdo-3.2.0-m3.jar:/opt/dolphinscheduler/libs/gax-httpjson-0.108.0.jar:/opt/dolphinscheduler/libs/mybatis-plus-core-3.5.2.jar:/opt/dolphinscheduler/libs/auto-service-annotations-1.0.1.jar:/opt/dolphinscheduler/libs/nimbus-jose-jwt-9.22.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-zeppelin-3.2.1.jar:/opt/dolphinscheduler/libs/commons-compress-1.21.jar:/opt/dolphinscheduler/libs/hadoop-hdfs-client-3.2.4.jar:/opt/dolphinscheduler/libs/msal4j-persistence-extension-1.1.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-pytorch-3.2.1.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/dolphinscheduler/libs/jetty-servlets-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/woodstox-core-6.4.0.jar:/opt/dolphinscheduler/libs/spring-cloud-kubernetes-commons-2.1.3.jar:/opt/dolphinscheduler/libs/grpc-protobuf-lite-1.41.0.jar:/opt/dolphinscheduler/libs/jetty-webapp-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/eventstream-1.0.1.jar:/opt/dolphinscheduler/libs/commons-compiler-3.1.7.jar:/opt/dolphinscheduler/libs/netty-transport-4.1.53.Final.jar:/opt/dolphinscheduler/libs/spring-cloud-context-3.1.3.jar:/opt/dolphinscheduler/libs/aws-java-sdk-dms-1.12.300.jar:/opt/dolphinscheduler/libs/hive-storage-api-2.4.0.jar:/opt/dolphinscheduler/libs/client-java-spring-integration-13.0.2.jar:/opt/dolphinscheduler/libs/spring-boot-starter-actuator-2.7.3.jar:/opt/dolphinscheduler/libs/third-party-jackson-core-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-hdfs-3.2.1.jar:/opt/dolphinscheduler/libs/netty-codec-4.1.53.Final.jar:/opt/dolphinscheduler/libs/google-http-client-appengine-1.42.3.jar:/opt/dolphinscheduler/libs/commons-configuration2-2.1.1.jar:/opt/dolphinscheduler/libs/datanucleus-rdbms-4.1.19.jar:/opt/dolphinscheduler/libs/swagger-annotations-1.6.2.jar:/opt/dolphinscheduler/libs/simpleclient_tracer_otel-0.15.0.jar:/opt/dolphinscheduler/libs/kotlin-stdlib-common-1.6.21.jar:/opt/dolphinscheduler/libs/aws-core-2.17.282.jar:/opt/dolphinscheduler/libs/kerb-core-1.0.1.jar:/opt/dolphinscheduler/libs/httpasyncclient-4.1.5.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-worker-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-spi-3.2.1.jar:/opt/dolphinscheduler/libs/okhttp-2.7.5.jar:/opt/dolphinscheduler/libs/google-api-client-2.2.0.jar:/opt/dolphinscheduler/libs/kotlin-stdlib-1.6.21.jar:/opt/dolphinscheduler/libs/jakarta.websocket-api-1.1.2.jar:/opt/dolphinscheduler/libs/libthrift-0.9.3.jar:/opt/dolphinscheduler/libs/spring-boot-starter-aop-2.7.3.jar:/opt/dolphinscheduler/libs/netty-common-4.1.53.Final.jar:/opt/dolphinscheduler/libs/log4j-1.2-api-2.17.2.jar:/opt/dolphinscheduler/libs/jackson-databind-2.13.4.jar:/opt/dolphinscheduler/libs/jackson-dataformat-xml-2.13.3.jar:/opt/dolphinscheduler/libs/kubernetes-model-events-5.10.2.jar:/opt/dolphinscheduler/libs/gson-2.9.1.jar:/opt/dolphinscheduler/libs/proto-google-iam-v1-1.9.0.jar:/opt/dolphinscheduler/libs/netty-codec-socks-4.1.53.Final.jar:/opt/dolphinscheduler/libs/zjsonpatch-0.3.0.jar:/opt/dolphinscheduler/libs/hadoop-mapreduce-client-common-3.2.4.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-api-3.2.1.jar:/opt/dolphinscheduler/libs/azure-storage-blob-12.21.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-zeppelin-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-api-3.2.1.jar:/opt/dolphinscheduler/libs/aws-java-sdk-s3-1.12.300.jar:/opt/dolphinscheduler/libs/stax2-api-4.2.1.jar:/opt/dolphinscheduler/libs/jakarta.annotation-api-1.3.5.jar:/opt/dolphinscheduler/libs/kubernetes-model-discovery-5.10.2.jar:/opt/dolphinscheduler/libs/jna-5.10.0.jar:/opt/dolphinscheduler/libs/spring-jcl-5.3.22.jar
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.261 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.261 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.io.tmpdir=/tmp
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.261 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.compiler=<NA>
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.264 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.name=Linux
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.264 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.arch=amd64
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.265 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.version=6.5.0-28-generic
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.265 +0800 o.a.z.ZooKeeper:[98] - Client environment:user.name=root
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.265 +0800 o.a.z.ZooKeeper:[98] - Client environment:user.home=/root
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.265 +0800 o.a.z.ZooKeeper:[98] - Client environment:user.dir=/opt/dolphinscheduler
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.265 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.memory.free=3194MB
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.265 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.memory.max=3840MB
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.265 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.memory.total=3840MB
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.281 +0800 o.a.z.ZooKeeper:[637] - Initiating client connection, connectString=dolphinscheduler-zookeeper:2181 sessionTimeout=30000 watcher=org.apache.curator.ConnectionState@1de30c31
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.341 +0800 o.a.z.c.X509Util:[77] - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.358 +0800 o.a.z.ClientCnxnSocket:[239] - jute.maxbuffer value is 1048575 Bytes
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.386 +0800 o.a.z.ClientCnxn:[1732] - zookeeper.request.timeout value is 0. feature enabled=false
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.428 +0800 o.a.c.f.i.CuratorFrameworkImpl:[386] - Default schema
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.449 +0800 o.a.z.ClientCnxn:[1171] - Opening socket connection to server dolphinscheduler-zookeeper/172.18.0.3:2181.
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.458 +0800 o.a.z.ClientCnxn:[1173] - SASL config status: Will not attempt to authenticate using SASL (unknown error)
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.472 +0800 o.a.z.ClientCnxn:[1005] - Socket connection established, initiating session, client: /172.18.1.1:54962, server: dolphinscheduler-zookeeper/172.18.0.3:2181
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.514 +0800 o.a.z.ClientCnxn:[1444] - Session establishment complete on server dolphinscheduler-zookeeper/172.18.0.3:2181, session id = 0x10000268b0c0000, negotiated timeout = 30000
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.529 +0800 o.a.c.f.s.ConnectionStateManager:[252] - State change: CONNECTED
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.692 +0800 o.a.c.f.i.EnsembleTracker:[201] - New config event received: {}
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:10.740 +0800 o.a.c.f.i.EnsembleTracker:[201] - New config event received: {}
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:11.532 +0800 o.a.d.s.w.r.WorkerRpcServer:[41] - WorkerRpcServer starting...
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.007 +0800 o.a.d.e.b.NettyRemotingServer:[112] - WorkerRpcServer bind success at port: 1234
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.007 +0800 o.a.d.s.w.r.WorkerRpcServer:[43] - WorkerRpcServer started...
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.113 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: JAVA - JavaTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.117 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: JAVA - JavaTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.117 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: JUPYTER - JupyterTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.120 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: JUPYTER - JupyterTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.121 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SPARK - SparkTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.125 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SPARK - SparkTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.126 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: FLINK_STREAM - FlinkStreamTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.129 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: FLINK_STREAM - FlinkStreamTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.129 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: PYTHON - PythonTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.130 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: PYTHON - PythonTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.130 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DATASYNC - DatasyncTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.131 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DATASYNC - DatasyncTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.131 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DATA_FACTORY - DatafactoryTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.132 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DATA_FACTORY - DatafactoryTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.132 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: CHUNJUN - ChunJunTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.133 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: CHUNJUN - ChunJunTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.134 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: REMOTESHELL - RemoteShellTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.135 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: REMOTESHELL - RemoteShellTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.135 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: PIGEON - PigeonTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.137 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: PIGEON - PigeonTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.137 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SHELL - ShellTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.138 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SHELL - ShellTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.138 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: PROCEDURE - ProcedureTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.140 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: PROCEDURE - ProcedureTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.140 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: MR - MapReduceTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.141 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: MR - MapReduceTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.141 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SQOOP - SqoopTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.142 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SQOOP - SqoopTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.143 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: PYTORCH - PytorchTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.144 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: PYTORCH - PytorchTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.145 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: K8S - K8sTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.146 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: K8S - K8sTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.146 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SEATUNNEL - SeatunnelTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.149 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SEATUNNEL - SeatunnelTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.149 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SAGEMAKER - SagemakerTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.150 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SAGEMAKER - SagemakerTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.151 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: HTTP - HttpTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.152 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: HTTP - HttpTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.153 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: EMR - EmrTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.155 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: EMR - EmrTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.156 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DMS - DmsTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.157 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DMS - DmsTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.158 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DATA_QUALITY - DataQualityTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.159 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DATA_QUALITY - DataQualityTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.159 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: KUBEFLOW - KubeflowTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.160 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: KUBEFLOW - KubeflowTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.160 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SQL - SqlTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.162 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SQL - SqlTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.163 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DVC - DvcTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.164 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DVC - DvcTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.164 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DATAX - DataxTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.166 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DATAX - DataxTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.166 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: ZEPPELIN - ZeppelinTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.168 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: ZEPPELIN - ZeppelinTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.168 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DINKY - DinkyTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.170 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DINKY - DinkyTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.170 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: MLFLOW - MlflowTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.171 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: MLFLOW - MlflowTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.171 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: OPENMLDB - OpenmldbTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.173 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: OPENMLDB - OpenmldbTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.173 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: LINKIS - LinkisTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.174 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: LINKIS - LinkisTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.174 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: FLINK - FlinkTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.175 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: FLINK - FlinkTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.175 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: HIVECLI - HiveCliTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.177 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: HIVECLI - HiveCliTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.282 +0800 o.a.d.c.u.JSONUtils:[72] - init timezone: sun.util.calendar.ZoneInfo[id="Asia/Shanghai",offset=28800000,dstSavings=0,useDaylight=false,transitions=31,lastRule=null]
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:12.559 +0800 o.a.d.s.w.r.WorkerRegistryClient:[104] - Worker node: 172.18.1.1:1234 registry to ZK /nodes/worker/172.18.1.1:1234 successfully
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:13.634 +0800 o.a.d.c.m.BaseHeartBeatTask:[48] - Starting WorkerHeartBeatTask...
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:13.639 +0800 o.a.d.c.m.BaseHeartBeatTask:[50] - Started WorkerHeartBeatTask, heartBeatInterval: 10000...
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:13.640 +0800 o.a.d.s.w.r.WorkerRegistryClient:[114] - Worker node: 172.18.1.1:1234 registry finished
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:13.641 +0800 o.a.d.s.w.m.MessageRetryRunner:[69] - Message retry runner staring
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:13.643 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.7482014388489209 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:13.688 +0800 o.a.d.s.w.m.MessageRetryRunner:[72] - Injected message sender: TaskInstanceExecutionFinishEventSender
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:13.688 +0800 o.a.d.s.w.m.MessageRetryRunner:[72] - Injected message sender: TaskInstanceExecutionInfoUpdateEventSender
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:13.690 +0800 o.a.d.s.w.m.MessageRetryRunner:[72] - Injected message sender: TaskInstanceExecutionRunningEventSender
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:13.691 +0800 o.a.d.s.w.m.MessageRetryRunner:[75] - Message retry runner started
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:14.704 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.297979797979798 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:15.716 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.3285024154589373 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:16.718 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.9042553191489362 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:17.773 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.6206146926536733 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:18.965 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.696969696969697 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:20.030 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.921875 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:21.077 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.6025641025641026 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:22.159 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.993103448275862 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:23.252 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.7058823529411764 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:23.510 +0800 o.s.b.a.e.w.EndpointLinksResolver:[58] - Exposing 3 endpoint(s) beneath base path '/actuator'
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:23.902 +0800 o.e.j.s.h.C.application:[2368] - Initializing Spring DispatcherServlet 'dispatcherServlet'
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:23.903 +0800 o.s.w.s.DispatcherServlet:[525] - Initializing Servlet 'dispatcherServlet'
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:23.905 +0800 o.s.w.s.DispatcherServlet:[547] - Completed initialization in 1 ms
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:23.926 +0800 o.e.j.s.AbstractConnector:[333] - Started ServerConnector@d28c214{HTTP/1.1, (http/1.1)}{0.0.0.0:1235}
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:23.928 +0800 o.s.b.w.e.j.JettyWebServer:[172] - Jetty started on port(s) 1235 (http/1.1) with context path '/'
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:23.954 +0800 o.a.d.s.w.WorkerServer:[61] - Started WorkerServer in 47.361 seconds (JVM running for 53.901)
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:24.255 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.6880267364138333 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:25.708 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9920817678665212 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:26.710 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8714285714285714 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:27.766 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8900523560209423 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:29.909 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8347826086956521 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:30.929 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7301587301587301 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:31.935 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.1697247706422018 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:32.943 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9197324414715718 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:35.002 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.979933110367893 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:36.026 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8687150837988826 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:37.028 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9357541899441342 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:38.129 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9646643109540636 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:39.173 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9337349397590361 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:40.176 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9819494584837545 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:41.183 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9635627530364372 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:42.185 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9876543209876544 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:43.187 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9506726457399104 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:44.191 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:45.193 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.977346278317152 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:46.227 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9956523361695775 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:47.229 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9591078066914498 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:48.312 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9778481012658227 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:49.341 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0056497175141244 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:50.344 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9590674103802462 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:51.347 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9915730337078652 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:52.352 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9809264305177112 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:53.354 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9786666666666667 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:54.357 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9548022598870056 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:55.359 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9638888888888888 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:56.378 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9673202614379085 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:57.384 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9405099150141644 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:50:58.392 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.781767955801105 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:51:02.459 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.956268221574344 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:51:03.476 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8728323699421966 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:51:04.481 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:51:05.485 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.911504424778761 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:51:06.487 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8433333333333333 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:51:07.490 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9210526315789473 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:51:08.493 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7213114754098361 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:51:24.737 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7381615598885793 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:51:27.795 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8085551948051948 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:51:28.832 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.71109658361783 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:51:31.870 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8005698005698005 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:51:32.886 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9267605633802817 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:51:33.903 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7250000000000001 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:51:34.906 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.704225352112676 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:51:39.941 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7780979827089338 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:51:40.958 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7374301675977655 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:51:42.970 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8243626062322946 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:51:59.058 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8583815028901733 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:52:00.082 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8770491803278688 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:52:01.089 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8240000000000001 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:53:03.669 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.754874651810585 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:53:04.688 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.75 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:53:13.726 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7245508982035929 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:53:18.772 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7535014005602241 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:53:19.825 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7017543859649122 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:53:32.879 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7694524495677234 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:53:33.919 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8310626702997276 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:54:09.164 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7746478873239437 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:54:19.221 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7478991596638656 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:54:47.448 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7677053824362606 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:55:00.517 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8390092879256966 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:56:04.865 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8835227272727273 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:56:11.969 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8179271708683473 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:56:22.014 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7280701754385965 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:56:27.059 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7416666666666667 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:56:45.161 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7629179331306991 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:00.256 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8005698005698005 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:03.295 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7586206896551724 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:05.338 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8187210275560134 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:07.397 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8470254957507082 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:08.151 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1530, taskName=spark test, firstSubmitTime=1713988626977, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13383445003744, processDefineVersion=2, appIds=null, processInstanceId=600, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13001194483488, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master spark://spark-master:7077 \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    spark-test.py\n","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark-test.py","res":null}]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='spark test'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13001194483488'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='600'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1530'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='spark_test'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13383432925920'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13383445003744'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425035707'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.319 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: spark test to wait queue success
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.350 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.355 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.357 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.357 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.357 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713988628357
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.358 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 600_1530
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.381 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1530,
  "taskName" : "spark test",
  "firstSubmitTime" : 1713988626977,
  "startTime" : 1713988628357,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13383445003744/2/600/1530.log",
  "processId" : 0,
  "processDefineCode" : 13383445003744,
  "processDefineVersion" : 2,
  "processInstanceId" : 600,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13001194483488,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master spark://spark-master:7077 \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name reddit_preprocessing \\\\\\n    spark-test.py\\n\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark-test.py\",\"res\":null}]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "spark test"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13001194483488"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "600"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1530"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "spark_test"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13383432925920"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13383445003744"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425035707"
    }
  },
  "taskAppId" : "600_1530",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.401 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.401 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.401 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:08.417 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.1312056737588652 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.552 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.584 +0800 o.a.d.c.u.OSUtils:[231] - create linux os user: default
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.585 +0800 o.a.d.c.u.OSUtils:[233] - execute cmd: sudo useradd -g root
 default
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.776 +0800 o.a.d.c.u.OSUtils:[190] - create user default success
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.776 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.782 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1530 check successfully
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.784 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.822 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark-test.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark-test.py, resourceRelativePath=spark-test.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1530/spark-test.py)})
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.829 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.831 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.835 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master spark://spark-master:7077 \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    spark-test.py\n",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark-test.py",
    "res" : null
  } ]
}
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.835 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.836 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.838 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.838 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.838 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.856 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.857 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.857 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master spark://spark-master:7077 \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name reddit_preprocessing \
    spark-test.py

[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.857 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.859 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1530/600_1530.sh
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:08.864 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 167
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:09.421 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8118811881188119 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1530] - [INFO] 2024-04-25 03:57:09.517 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1530, success=true)
[WI-0][TI-1530] - [INFO] 2024-04-25 03:57:09.557 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1530)
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:09.868 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	/tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1530/600_1530.sh: line 6: /bin/spark-submit: No such file or directory
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:09.882 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1530, processId:167 ,exitStatusCode:127 ,processWaitForStatus:true ,processExitValue:127
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:09.883 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:09.884 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:09.884 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:09.885 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:09.900 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:09.901 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:09.901 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1530
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:09.945 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1530
[WI-600][TI-1530] - [INFO] 2024-04-25 03:57:09.950 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:10.422 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7448680351906158 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1530] - [INFO] 2024-04-25 03:57:10.478 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1530, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:10.622 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1531, taskName=spark test, firstSubmitTime=1713988630531, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13383445003744, processDefineVersion=2, appIds=null, processInstanceId=600, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13001194483488, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master spark://spark-master:7077 \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    spark-test.py\n","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark-test.py","res":null}]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='spark test'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13001194483488'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='600'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1531'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='spark_test'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13383432925920'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13383445003744'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425035710'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.652 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: spark test to wait queue success
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.653 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.655 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.656 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.656 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.656 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713988630656
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.657 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 600_1531
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.658 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1531,
  "taskName" : "spark test",
  "firstSubmitTime" : 1713988630531,
  "startTime" : 1713988630656,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13383445003744/2/600/1531.log",
  "processId" : 0,
  "processDefineCode" : 13383445003744,
  "processDefineVersion" : 2,
  "processInstanceId" : 600,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13001194483488,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master spark://spark-master:7077 \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name reddit_preprocessing \\\\\\n    spark-test.py\\n\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark-test.py\",\"res\":null}]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "spark test"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13001194483488"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "600"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1531"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "spark_test"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13383432925920"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13383445003744"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425035710"
    }
  },
  "taskAppId" : "600_1531",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.660 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.660 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.660 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.676 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.684 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.687 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1531 check successfully
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.688 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.719 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark-test.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark-test.py, resourceRelativePath=spark-test.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1531/spark-test.py)})
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.721 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.721 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.722 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master spark://spark-master:7077 \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    spark-test.py\n",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark-test.py",
    "res" : null
  } ]
}
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.722 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.724 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.724 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.725 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.725 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.726 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.726 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.727 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master spark://spark-master:7077 \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name reddit_preprocessing \
    spark-test.py

[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.727 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.728 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1531/600_1531.sh
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:10.739 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 182
[WI-0][TI-1531] - [INFO] 2024-04-25 03:57:11.462 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1531, success=true)
[WI-0][TI-1531] - [INFO] 2024-04-25 03:57:11.475 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1531)
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:11.740 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	/tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1531/600_1531.sh: line 6: /bin/spark-submit: No such file or directory
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:11.743 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1531, processId:182 ,exitStatusCode:127 ,processWaitForStatus:true ,processExitValue:127
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:11.743 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:11.743 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:11.744 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:11.745 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:11.749 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:11.749 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:11.749 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1531
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:11.750 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1531
[WI-600][TI-1531] - [INFO] 2024-04-25 03:57:11.752 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1531] - [INFO] 2024-04-25 03:57:12.462 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1531, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:12.562 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1532, taskName=spark test, firstSubmitTime=1713988632531, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13383445003744, processDefineVersion=2, appIds=null, processInstanceId=600, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13001194483488, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master spark://spark-master:7077 \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    spark-test.py\n","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark-test.py","res":null}]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='spark test'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13001194483488'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='600'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1532'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='spark_test'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13383432925920'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13383445003744'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425035712'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.564 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: spark test to wait queue success
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.567 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.570 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.573 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.573 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.574 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713988632574
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.576 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 600_1532
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.577 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1532,
  "taskName" : "spark test",
  "firstSubmitTime" : 1713988632531,
  "startTime" : 1713988632574,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13383445003744/2/600/1532.log",
  "processId" : 0,
  "processDefineCode" : 13383445003744,
  "processDefineVersion" : 2,
  "processInstanceId" : 600,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13001194483488,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master spark://spark-master:7077 \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name reddit_preprocessing \\\\\\n    spark-test.py\\n\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark-test.py\",\"res\":null}]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "spark test"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13001194483488"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "600"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1532"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "spark_test"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13383432925920"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13383445003744"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425035712"
    }
  },
  "taskAppId" : "600_1532",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.578 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.579 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.579 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.599 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.600 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.601 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1532 check successfully
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.602 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.607 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark-test.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark-test.py, resourceRelativePath=spark-test.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1532/spark-test.py)})
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.615 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.619 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.620 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master spark://spark-master:7077 \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    spark-test.py\n",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark-test.py",
    "res" : null
  } ]
}
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.620 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.621 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.621 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.621 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.622 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.623 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.623 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.623 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master spark://spark-master:7077 \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name reddit_preprocessing \
    spark-test.py

[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.623 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.624 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1532/600_1532.sh
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:12.629 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 196
[WI-0][TI-1532] - [INFO] 2024-04-25 03:57:13.476 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1532, success=true)
[WI-0][TI-1532] - [INFO] 2024-04-25 03:57:13.490 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1532)
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:13.631 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	/tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1532/600_1532.sh: line 6: /bin/spark-submit: No such file or directory
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:13.633 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1532, processId:196 ,exitStatusCode:127 ,processWaitForStatus:true ,processExitValue:127
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:13.633 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:13.634 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:13.635 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:13.636 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:13.641 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:13.641 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:13.641 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1532
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:13.642 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1532
[WI-600][TI-1532] - [INFO] 2024-04-25 03:57:13.643 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1532] - [INFO] 2024-04-25 03:57:14.483 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1532, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:14.612 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1533, taskName=spark test, firstSubmitTime=1713988634592, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13383445003744, processDefineVersion=2, appIds=null, processInstanceId=600, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13001194483488, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master spark://spark-master:7077 \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    spark-test.py\n","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark-test.py","res":null}]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='spark test'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13001194483488'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='600'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1533'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='spark_test'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13383432925920'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13383445003744'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425035714'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.614 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: spark test to wait queue success
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.614 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.637 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.638 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.638 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.638 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713988634638
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.639 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 600_1533
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.640 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1533,
  "taskName" : "spark test",
  "firstSubmitTime" : 1713988634592,
  "startTime" : 1713988634638,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13383445003744/2/600/1533.log",
  "processId" : 0,
  "processDefineCode" : 13383445003744,
  "processDefineVersion" : 2,
  "processInstanceId" : 600,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13001194483488,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master spark://spark-master:7077 \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name reddit_preprocessing \\\\\\n    spark-test.py\\n\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark-test.py\",\"res\":null}]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "spark test"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13001194483488"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "600"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1533"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "spark_test"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13383432925920"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13383445003744"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425035714"
    }
  },
  "taskAppId" : "600_1533",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.640 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.640 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.640 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.692 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.694 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.695 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1533 check successfully
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.695 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.699 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark-test.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark-test.py, resourceRelativePath=spark-test.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1533/spark-test.py)})
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.701 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.702 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.703 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master spark://spark-master:7077 \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    spark-test.py\n",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark-test.py",
    "res" : null
  } ]
}
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.704 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.705 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.705 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.706 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.706 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.709 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.709 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.710 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master spark://spark-master:7077 \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name reddit_preprocessing \
    spark-test.py

[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.710 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.711 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1533/600_1533.sh
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:14.723 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 207
[WI-0][TI-1533] - [INFO] 2024-04-25 03:57:15.488 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1533, success=true)
[WI-0][TI-1533] - [INFO] 2024-04-25 03:57:15.497 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1533)
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:15.724 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	/tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1533/600_1533.sh: line 6: /bin/spark-submit: No such file or directory
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:15.729 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1533, processId:207 ,exitStatusCode:127 ,processWaitForStatus:true ,processExitValue:127
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:15.731 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:15.732 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:15.732 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:15.732 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:15.737 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:15.738 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:15.738 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1533
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:15.739 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_2/600/1533
[WI-600][TI-1533] - [INFO] 2024-04-25 03:57:15.740 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1533] - [INFO] 2024-04-25 03:57:16.492 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1533, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:20.463 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7463976945244957 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:21.512 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7961432506887053 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:23.529 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7237237237237237 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:28.612 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7294117647058824 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:36.689 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7126436781609196 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:39.718 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7744807121661721 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:46.784 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7859237536656891 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:49.813 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8062678062678063 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:55.365 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1534, taskName=spark test, firstSubmitTime=1713988675325, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13383445003744, processDefineVersion=3, appIds=null, processInstanceId=601, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13001194483488, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master spark://spark-master:7077 \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    spark-test.py\n","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark-test.py","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='spark test'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13001194483488'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='601'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1534'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='spark_test'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13383432925920'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13383445003744'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425035755'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.380 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: spark test to wait queue success
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.385 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.387 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.387 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.387 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.387 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713988675387
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.387 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 601_1534
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.388 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1534,
  "taskName" : "spark test",
  "firstSubmitTime" : 1713988675325,
  "startTime" : 1713988675387,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13383445003744/3/601/1534.log",
  "processId" : 0,
  "processDefineCode" : 13383445003744,
  "processDefineVersion" : 3,
  "processInstanceId" : 601,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13001194483488,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master spark://spark-master:7077 \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name reddit_preprocessing \\\\\\n    spark-test.py\\n\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark-test.py\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "spark test"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13001194483488"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "601"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1534"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "spark_test"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13383432925920"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13383445003744"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425035755"
    }
  },
  "taskAppId" : "601_1534",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.388 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.388 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.389 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.403 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.406 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.408 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_3/601/1534 check successfully
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.409 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.411 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark-test.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark-test.py, resourceRelativePath=spark-test.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_3/601/1534/spark-test.py)})
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.412 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.413 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.413 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master spark://spark-master:7077 \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    spark-test.py\n",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark-test.py",
    "res" : null
  } ]
}
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.414 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.414 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.414 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.414 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.414 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.415 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.415 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.416 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master spark://spark-master:7077 \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name reddit_preprocessing \
    spark-test.py

[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.416 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.416 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_3/601/1534/601_1534.sh
[WI-601][TI-1534] - [INFO] 2024-04-25 03:57:55.453 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 226
[WI-0][TI-1534] - [INFO] 2024-04-25 03:57:55.677 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1534, success=true)
[WI-0][TI-1534] - [INFO] 2024-04-25 03:57:55.759 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1534)
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:55.944 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8476190476190476 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:56.464 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 03:57:58.018 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.784741144414169 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:58:00.112 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8575581395348837 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:58:01.147 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.829971181556196 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:58:04.570 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 03:58:04 INFO SparkContext: Running Spark version 3.5.1
	24/04/25 03:58:04 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/04/25 03:58:04 INFO SparkContext: Java version 1.8.0_402
	24/04/25 03:58:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	24/04/25 03:58:04 INFO ResourceUtils: ==============================================================
	24/04/25 03:58:04 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/04/25 03:58:04 INFO ResourceUtils: ==============================================================
	24/04/25 03:58:04 INFO SparkContext: Submitted application: My App
	24/04/25 03:58:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/04/25 03:58:04 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
	24/04/25 03:58:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
[WI-0][TI-0] - [INFO] 2024-04-25 03:58:05.589 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 03:58:04 INFO SecurityManager: Changing view acls to: default
	24/04/25 03:58:04 INFO SecurityManager: Changing modify acls to: default
	24/04/25 03:58:04 INFO SecurityManager: Changing view acls groups to: 
	24/04/25 03:58:04 INFO SecurityManager: Changing modify acls groups to: 
	24/04/25 03:58:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
[WI-0][TI-0] - [INFO] 2024-04-25 03:58:06.184 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7590027700831025 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:58:06.590 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 03:58:06 INFO Utils: Successfully started service 'sparkDriver' on port 34691.
	24/04/25 03:58:06 INFO SparkEnv: Registering MapOutputTracker
	24/04/25 03:58:06 INFO SparkEnv: Registering BlockManagerMaster
	24/04/25 03:58:06 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/04/25 03:58:06 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/04/25 03:58:06 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[WI-0][TI-0] - [INFO] 2024-04-25 03:58:07.596 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 03:58:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-79887cbe-ab2b-41db-9d48-079f125e2884
	24/04/25 03:58:06 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
	24/04/25 03:58:06 INFO SparkEnv: Registering OutputCommitCoordinator
	24/04/25 03:58:07 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/04/25 03:58:07 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/04/25 03:58:07 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[WI-0][TI-0] - [INFO] 2024-04-25 03:58:08.598 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 03:58:07 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.4:7077 after 51 ms (0 ms spent in bootstraps)
	24/04/25 03:58:07 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master spark-master:7077
	org.apache.spark.SparkException: Exception thrown in awaitResult: 
		at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
		at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
		at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
		at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
		at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
		at org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:108)
		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
		at java.util.concurrent.FutureTask.run(FutureTask.java:266)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: java.lang.RuntimeException: java.io.InvalidClassException: org.apache.spark.rpc.netty.RpcEndpointVerifier$CheckExistence; local class incompatible: stream classdesc serialVersionUID = 5378738997755484868, local class serialVersionUID = 7789290765573734431
		at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:597)
		at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2051)
		at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1898)
		at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2224)
		at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)
		at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)
		at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)
		at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)
		at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:123)
		at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
		at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
		at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:646)
		at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:697)
		at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:682)
		at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:163)
		at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:109)
		at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
		at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
		at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
	
		at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:209)
		at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)
		at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
		at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		... 1 more
[WI-0][TI-0] - [INFO] 2024-04-25 03:58:19.310 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7747747747747749 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:58:27.632 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 03:58:27 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
	24/04/25 03:58:27 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master spark-master:7077
	org.apache.spark.SparkException: Exception thrown in awaitResult: 
		at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
		at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
		at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
		at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
		at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
		at org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:108)
		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
		at java.util.concurrent.FutureTask.run(FutureTask.java:266)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: java.lang.RuntimeException: java.io.InvalidClassException: org.apache.spark.rpc.netty.RpcEndpointVerifier$CheckExistence; local class incompatible: stream classdesc serialVersionUID = 5378738997755484868, local class serialVersionUID = 7789290765573734431
		at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:597)
		at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2051)
		at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1898)
		at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2224)
		at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)
		at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)
		at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)
		at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)
		at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:123)
		at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
		at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
		at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:646)
		at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:697)
		at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:682)
		at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:163)
		at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:109)
		at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
		at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
		at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
	
		at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:209)
		at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)
		at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
		at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		... 1 more
[WI-0][TI-0] - [INFO] 2024-04-25 03:58:33.404 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7304347826086955 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:58:35.473 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7234636871508379 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:58:41.507 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7083333333333334 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:58:47.805 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 03:58:47 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
	24/04/25 03:58:47 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master spark-master:7077
	org.apache.spark.SparkException: Exception thrown in awaitResult: 
		at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
		at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
		at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
		at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
		at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
		at org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:108)
		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
		at java.util.concurrent.FutureTask.run(FutureTask.java:266)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: java.lang.RuntimeException: java.io.InvalidClassException: org.apache.spark.rpc.netty.RpcEndpointVerifier$CheckExistence; local class incompatible: stream classdesc serialVersionUID = 5378738997755484868, local class serialVersionUID = 7789290765573734431
		at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:597)
		at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2051)
		at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1898)
		at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2224)
		at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)
		at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)
		at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)
		at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)
		at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:123)
		at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
		at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
		at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:646)
		at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:697)
		at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:682)
		at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:163)
		at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:109)
		at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
		at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
		at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
	
		at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:209)
		at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)
		at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
		at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		... 1 more
[WI-0][TI-0] - [INFO] 2024-04-25 03:59:07.788 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7839335180055401 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:59:07.891 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 03:59:07 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.
	24/04/25 03:59:07 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.
	24/04/25 03:59:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37303.
	24/04/25 03:59:07 INFO NettyBlockTransferService: Server created on 4691b849b0ea:37303
	24/04/25 03:59:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/04/25 03:59:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4691b849b0ea, 37303, None)
	24/04/25 03:59:07 INFO BlockManagerMasterEndpoint: Registering block manager 4691b849b0ea:37303 with 366.3 MiB RAM, BlockManagerId(driver, 4691b849b0ea, 37303, None)
	24/04/25 03:59:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4691b849b0ea, 37303, None)
	24/04/25 03:59:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4691b849b0ea, 37303, None)
[WI-0][TI-0] - [INFO] 2024-04-25 03:59:08.808 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9111842105263157 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:59:08.893 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 03:59:08 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
	24/04/25 03:59:08 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/04/25 03:59:08 INFO SparkUI: Stopped Spark web UI at http://4691b849b0ea:4040
	24/04/25 03:59:08 INFO StandaloneSchedulerBackend: Shutting down all executors
	24/04/25 03:59:08 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
	24/04/25 03:59:08 WARN StandaloneAppClient$ClientEndpoint: Drop UnregisterApplication(null) because has not yet connected to master
[WI-0][TI-0] - [INFO] 2024-04-25 03:59:09.900 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 03:59:08 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/04/25 03:59:09 INFO MemoryStore: MemoryStore cleared
	24/04/25 03:59:09 INFO BlockManager: BlockManager stopped
	24/04/25 03:59:09 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/04/25 03:59:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/04/25 03:59:09 INFO SparkContext: Successfully stopped SparkContext
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_3/601/1534/spark-test.py", line 8, in <module>
	    .getOrCreate()
	     ^^^^^^^^^^^^^
	  File "/opt/spark-3.5.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/session.py", line 500, in getOrCreate
	  File "/opt/spark-3.5.1-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/session.py", line 589, in __init__
	  File "/opt/spark-3.5.1-bin-hadoop3-scala2.13/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1587, in __call__
	  File "/opt/spark-3.5.1-bin-hadoop3-scala2.13/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
	py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.
	: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
	This stopped SparkContext was created at:
	
	org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	py4j.Gateway.invoke(Gateway.java:238)
	py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	java.lang.Thread.run(Thread.java:750)
	
	The currently active SparkContext was created at:
	
	(No active SparkContext.)
	         
		at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)
		at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:113)
		at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:106)
		at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
		at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
		at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
		at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:238)
		at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
		at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	
	24/04/25 03:59:09 INFO ShutdownHookManager: Shutdown hook called
	24/04/25 03:59:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-07f12aa0-49b4-4b36-980a-0b7c6dc9ef6e
	24/04/25 03:59:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-7e1bb4ea-174c-457f-87e8-baab3e9b74b7/pyspark-29e45a2d-0168-4583-862d-320c9a759a0e
	24/04/25 03:59:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-7e1bb4ea-174c-457f-87e8-baab3e9b74b7
[WI-601][TI-1534] - [INFO] 2024-04-25 03:59:09.902 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_3/601/1534, processId:226 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-601][TI-1534] - [INFO] 2024-04-25 03:59:09.903 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-601][TI-1534] - [INFO] 2024-04-25 03:59:09.904 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-601][TI-1534] - [INFO] 2024-04-25 03:59:09.904 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-601][TI-1534] - [INFO] 2024-04-25 03:59:09.906 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-601][TI-1534] - [INFO] 2024-04-25 03:59:09.910 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-601][TI-1534] - [INFO] 2024-04-25 03:59:09.910 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-601][TI-1534] - [INFO] 2024-04-25 03:59:09.910 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_3/601/1534
[WI-601][TI-1534] - [INFO] 2024-04-25 03:59:09.911 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_3/601/1534
[WI-601][TI-1534] - [INFO] 2024-04-25 03:59:09.911 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1534] - [INFO] 2024-04-25 03:59:10.219 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1534, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 03:59:10.249 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1535, taskName=spark test, firstSubmitTime=1713988750234, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13383445003744, processDefineVersion=3, appIds=null, processInstanceId=601, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13001194483488, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master spark://spark-master:7077 \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    spark-test.py\n","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark-test.py","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='spark test'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13001194483488'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='601'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1535'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='spark_test'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13383432925920'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13383445003744'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425035910'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.251 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: spark test to wait queue success
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.251 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.252 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.252 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.252 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.252 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713988750252
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.253 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 601_1535
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.253 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1535,
  "taskName" : "spark test",
  "firstSubmitTime" : 1713988750234,
  "startTime" : 1713988750252,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13383445003744/3/601/1535.log",
  "processId" : 0,
  "processDefineCode" : 13383445003744,
  "processDefineVersion" : 3,
  "processInstanceId" : 601,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13001194483488,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master spark://spark-master:7077 \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name reddit_preprocessing \\\\\\n    spark-test.py\\n\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark-test.py\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "spark test"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13001194483488"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "601"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1535"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "spark_test"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13383432925920"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13383445003744"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425035910"
    }
  },
  "taskAppId" : "601_1535",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.253 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.253 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.253 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.258 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.259 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.259 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_3/601/1535 check successfully
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.259 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.262 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark-test.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark-test.py, resourceRelativePath=spark-test.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_3/601/1535/spark-test.py)})
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.267 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.267 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.267 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master spark://spark-master:7077 \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    spark-test.py\n",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark-test.py",
    "res" : null
  } ]
}
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.267 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.268 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.268 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.268 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.268 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.268 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.269 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.269 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master spark://spark-master:7077 \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name reddit_preprocessing \
    spark-test.py

[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.272 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.276 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13001194483488/13383445003744_3/601/1535/601_1535.sh
[WI-601][TI-1535] - [INFO] 2024-04-25 03:59:10.281 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 368
[WI-0][TI-0] - [INFO] 2024-04-25 03:59:10.291 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1535] - [INFO] 2024-04-25 03:59:11.231 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1535, success=true)
[WI-0][TI-1535] - [INFO] 2024-04-25 03:59:11.287 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1535)
[WI-0][TI-0] - [INFO] 2024-04-25 03:59:11.842 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8240469208211143 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:59:12.893 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9109441118114936 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:59:13.903 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8255743769470405 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:59:15.308 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 03:59:14 INFO SparkContext: Running Spark version 3.5.1
	24/04/25 03:59:14 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/04/25 03:59:14 INFO SparkContext: Java version 1.8.0_402
	24/04/25 03:59:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	24/04/25 03:59:15 INFO ResourceUtils: ==============================================================
	24/04/25 03:59:15 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/04/25 03:59:15 INFO ResourceUtils: ==============================================================
	24/04/25 03:59:15 INFO SparkContext: Submitted application: My App
	24/04/25 03:59:15 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/04/25 03:59:15 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
	24/04/25 03:59:15 INFO ResourceProfileManager: Added ResourceProfile id: 0
	24/04/25 03:59:15 INFO SecurityManager: Changing view acls to: default
	24/04/25 03:59:15 INFO SecurityManager: Changing modify acls to: default
	24/04/25 03:59:15 INFO SecurityManager: Changing view acls groups to: 
	24/04/25 03:59:15 INFO SecurityManager: Changing modify acls groups to: 
	24/04/25 03:59:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
[WI-0][TI-0] - [INFO] 2024-04-25 03:59:16.311 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 03:59:15 INFO Utils: Successfully started service 'sparkDriver' on port 37021.
	24/04/25 03:59:15 INFO SparkEnv: Registering MapOutputTracker
	24/04/25 03:59:15 INFO SparkEnv: Registering BlockManagerMaster
	24/04/25 03:59:15 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/04/25 03:59:15 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/04/25 03:59:15 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/04/25 03:59:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f347dba1-346d-4ddf-8236-5f08b7360217
	24/04/25 03:59:15 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
	24/04/25 03:59:15 INFO SparkEnv: Registering OutputCommitCoordinator
	24/04/25 03:59:15 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/04/25 03:59:16 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[WI-0][TI-0] - [INFO] 2024-04-25 03:59:17.312 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 03:59:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
	24/04/25 03:59:16 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.4:7077 after 50 ms (1 ms spent in bootstraps)
	24/04/25 03:59:16 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master spark-master:7077
	org.apache.spark.SparkException: Exception thrown in awaitResult: 
		at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
		at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
		at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
		at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
		at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
		at org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:108)
		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
		at java.util.concurrent.FutureTask.run(FutureTask.java:266)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: java.lang.RuntimeException: java.io.InvalidClassException: org.apache.spark.rpc.netty.RpcEndpointVerifier$CheckExistence; local class incompatible: stream classdesc serialVersionUID = 5378738997755484868, local class serialVersionUID = 7789290765573734431
		at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:597)
		at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2051)
		at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1898)
		at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2224)
		at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)
		at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)
		at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)
		at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)
		at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:123)
		at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
		at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
		at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:646)
		at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:697)
		at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:682)
		at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:163)
		at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:109)
		at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
		at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
		at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
	
		at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:209)
		at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)
		at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
		at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		... 1 more
[WI-0][TI-0] - [INFO] 2024-04-25 03:59:21.929 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7567567567567568 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:59:23.002 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9268292682926829 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:59:24.006 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8596491228070176 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:59:28.028 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8072289156626506 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:59:29.099 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7215909090909091 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:59:33.160 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7954464975534381 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:59:36.217 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8179190751445087 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:59:37.260 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7437325905292479 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:59:37.346 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 03:59:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
	24/04/25 03:59:36 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master spark-master:7077
	org.apache.spark.SparkException: Exception thrown in awaitResult: 
		at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
		at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
		at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
		at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
		at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
		at org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:108)
		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
		at java.util.concurrent.FutureTask.run(FutureTask.java:266)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: java.lang.RuntimeException: java.io.InvalidClassException: org.apache.spark.rpc.netty.RpcEndpointVerifier$CheckExistence; local class incompatible: stream classdesc serialVersionUID = 5378738997755484868, local class serialVersionUID = 7789290765573734431
		at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:597)
		at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2051)
		at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1898)
		at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2224)
		at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)
		at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)
		at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)
		at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)
		at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:123)
		at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
		at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
		at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:646)
		at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:697)
		at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:682)
		at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:163)
		at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:109)
		at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
		at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
		at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
	
		at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:209)
		at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)
		at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
		at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		... 1 more
[WI-0][TI-0] - [INFO] 2024-04-25 03:59:51.362 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8171091445427728 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 03:59:57.400 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 03:59:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
	24/04/25 03:59:56 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master spark-master:7077
	org.apache.spark.SparkException: Exception thrown in awaitResult: 
		at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
		at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
		at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
		at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
		at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
		at org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:108)
		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
		at java.util.concurrent.FutureTask.run(FutureTask.java:266)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: java.lang.RuntimeException: java.io.InvalidClassException: org.apache.spark.rpc.netty.RpcEndpointVerifier$CheckExistence; local class incompatible: stream classdesc serialVersionUID = 5378738997755484868, local class serialVersionUID = 7789290765573734431
		at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:597)
		at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2051)
		at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1898)
		at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2224)
		at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)
		at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)
		at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)
		at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)
		at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:123)
		at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
		at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
		at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:646)
		at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:697)
		at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:682)
		at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:163)
		at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:109)
		at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
		at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
		at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
	
		at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:209)
		at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)
		at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
		at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		... 1 more
