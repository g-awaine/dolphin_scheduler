[WI-0][TI-0] - [INFO] 2024-04-25 15:04:25.866 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7462686567164178 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:04:41.004 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7007042253521126 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:04:42.019 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7560975609756098 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:05:05.176 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7181208053691275 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:05:11.384 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.802158273381295 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:08:37.588 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1737, taskName=read kafka, firstSubmitTime=1714028916971, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=6, appIds=null, processInstanceId=651, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"input_topic","direct":"OUT","type":"VARCHAR","value":""},{"prop":"output_topic","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n","resourceList":[]}, environmentConfig=export JAVA_HOME=/opt/java/openjdk, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='read kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='651'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1737'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386664399040'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425150837'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-0][TI-0] - [INFO] 2024-04-25 15:08:37.616 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.896551724137931 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.661 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: read kafka to wait queue success
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.676 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.682 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.682 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.690 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.691 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714028917691
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.692 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 651_1737
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.718 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1737,
  "taskName" : "read kafka",
  "firstSubmitTime" : 1714028916971,
  "startTime" : 1714028917691,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/6/651/1737.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 6,
  "processInstanceId" : 651,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"input_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"},{\"prop\":\"output_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\ntopics=\\\"reddit_post_processed_topic|reddit_post_sa_topic\\\"\\n\\n# parse the topics string into input and output topics\\nIFS='|' read -r input_topic output_topic <<< \\\"$topics\\\"\\n\\n# Print the input and output topics\\necho \\\"Input Topic: $input_topic\\\"\\necho \\\"Output Topic: $output_topic\\\"\\n\\necho \\\"#{setValue(input_topic=${input_topic})}\\\"\\necho \\\"#{setValue(output_topic=${output_topic})}\\\"\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export JAVA_HOME=/opt/java/openjdk",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "read kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "651"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1737"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386664399040"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425150837"
    }
  },
  "taskAppId" : "651_1737",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.748 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.749 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.749 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.851 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.881 +0800 o.a.d.c.u.OSUtils:[231] - create linux os user: default
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.883 +0800 o.a.d.c.u.OSUtils:[233] - execute cmd: sudo useradd -g root
 default
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.951 +0800 o.a.d.c.u.OSUtils:[190] - create user default success
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.953 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.956 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_6/651/1737 check successfully
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.958 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.966 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.980 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.988 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.991 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "input_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  }, {
    "prop" : "output_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n",
  "resourceList" : [ ]
}
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.992 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.992 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.995 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.995 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:37.996 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:38.008 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:38.009 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:38.009 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export JAVA_HOME=/opt/java/openjdk
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

topics="reddit_post_processed_topic|reddit_post_sa_topic"

# parse the topics string into input and output topics
IFS='|' read -r input_topic output_topic <<< "$topics"

# Print the input and output topics
echo "Input Topic: $input_topic"
echo "Output Topic: $output_topic"

echo "#{setValue(input_topic=${input_topic})}"
echo "#{setValue(output_topic=${output_topic})}"

[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:38.009 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:38.011 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_6/651/1737/651_1737.sh
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:38.016 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 1969
[WI-0][TI-1737] - [INFO] 2024-04-25 15:08:38.419 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1737, success=true)
[WI-0][TI-1737] - [INFO] 2024-04-25 15:08:38.492 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1737)
[WI-0][TI-0] - [INFO] 2024-04-25 15:08:38.636 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9190751445086704 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:08:39.019 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Input Topic: reddit_post_processed_topic
	Output Topic: reddit_post_sa_topic
	#{setValue(input_topic=reddit_post_processed_topic)}
	#{setValue(output_topic=reddit_post_sa_topic)}
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:39.021 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_6/651/1737, processId:1969 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:39.022 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:39.023 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:39.023 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:39.029 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:39.060 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:39.061 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:39.061 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_6/651/1737
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:39.076 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_6/651/1737
[WI-651][TI-1737] - [INFO] 2024-04-25 15:08:39.081 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1737] - [INFO] 2024-04-25 15:08:39.361 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1737, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:08:39.486 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1738, taskName=sentiment anaysis, firstSubmitTime=1714028919453, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=6, appIds=null, processInstanceId=651, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1738'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425150839'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='651'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.496 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.552 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.593 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.593 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.593 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.593 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714028919593
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.594 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 651_1738
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.595 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1738,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714028919453,
  "startTime" : 1714028919593,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/6/651/1738.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 6,
  "processInstanceId" : 651,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1738"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425150839"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "651"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "651_1738",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.597 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.597 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.599 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.621 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.623 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.625 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_6/651/1738 check successfully
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.627 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.632 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-0][TI-0] - [INFO] 2024-04-25 15:08:39.640 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8762798943521835 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.649 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.660 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.687 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.689 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.692 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.693 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.694 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.695 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.695 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.697 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_6/651/1738
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.697 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_6/651/1738/py_651_1738.py
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.698 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.732 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.733 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.734 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_6/651/1738/py_651_1738.py
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.735 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.736 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_6/651/1738/651_1738.sh
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:39.747 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 1983
[WI-0][TI-1738] - [INFO] 2024-04-25 15:08:40.373 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1738, success=true)
[WI-0][TI-1738] - [INFO] 2024-04-25 15:08:40.391 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1738)
[WI-0][TI-0] - [INFO] 2024-04-25 15:08:40.648 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9495602836879432 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:08:40.748 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	JAVA_HOME is not set
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_6/651/1738/py_651_1738.py", line 14, in <module>
	    .getOrCreate()
	     ^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py", line 497, in getOrCreate
	    sc = SparkContext.getOrCreate(sparkConf)
	         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 515, in getOrCreate
	    SparkContext(conf=conf or SparkConf())
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 201, in __init__
	    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 436, in _ensure_initialized
	    SparkContext._gateway = gateway or launch_gateway(conf)
	                                       ^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/java_gateway.py", line 107, in launch_gateway
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:40.753 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_6/651/1738, processId:1983 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:40.754 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:40.756 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:40.757 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:40.758 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:40.773 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:40.774 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:40.775 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_6/651/1738
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:40.776 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_6/651/1738
[WI-651][TI-1738] - [INFO] 2024-04-25 15:08:40.776 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1738] - [INFO] 2024-04-25 15:08:41.396 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1738, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:08:41.649 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9456521739130435 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:08:42.654 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9085872576177285 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:08:45.675 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.768595041322314 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:08:58.918 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7235772357723577 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:09:11.009 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7321937321937322 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:09:12.017 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7681159420289855 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:09:28.082 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7853107344632768 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:09:32.166 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1742, taskName=read kafka, firstSubmitTime=1714028972141, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=7, appIds=null, processInstanceId=652, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"input_topic","direct":"OUT","type":"VARCHAR","value":""},{"prop":"output_topic","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n","resourceList":[]}, environmentConfig=export JAVA_HOME=/opt/java/openjdk, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='read kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='652'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1742'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386664399040'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425150932'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.170 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: read kafka to wait queue success
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.173 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.181 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.181 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.181 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.182 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714028972182
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.182 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 652_1742
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.182 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1742,
  "taskName" : "read kafka",
  "firstSubmitTime" : 1714028972141,
  "startTime" : 1714028972182,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/7/652/1742.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 7,
  "processInstanceId" : 652,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"input_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"},{\"prop\":\"output_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\ntopics=\\\"reddit_post_processed_topic|reddit_post_sa_topic\\\"\\n\\n# parse the topics string into input and output topics\\nIFS='|' read -r input_topic output_topic <<< \\\"$topics\\\"\\n\\n# Print the input and output topics\\necho \\\"Input Topic: $input_topic\\\"\\necho \\\"Output Topic: $output_topic\\\"\\n\\necho \\\"#{setValue(input_topic=${input_topic})}\\\"\\necho \\\"#{setValue(output_topic=${output_topic})}\\\"\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export JAVA_HOME=/opt/java/openjdk",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "read kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "652"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1742"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386664399040"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425150932"
    }
  },
  "taskAppId" : "652_1742",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.183 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.183 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.183 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.198 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.199 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.200 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1742 check successfully
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.201 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.201 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.202 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.202 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.203 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "input_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  }, {
    "prop" : "output_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n",
  "resourceList" : [ ]
}
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.203 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.203 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.203 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.203 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.203 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.208 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.209 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.209 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export JAVA_HOME=/opt/java/openjdk
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

topics="reddit_post_processed_topic|reddit_post_sa_topic"

# parse the topics string into input and output topics
IFS='|' read -r input_topic output_topic <<< "$topics"

# Print the input and output topics
echo "Input Topic: $input_topic"
echo "Output Topic: $output_topic"

echo "#{setValue(input_topic=${input_topic})}"
echo "#{setValue(output_topic=${output_topic})}"

[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.209 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.209 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1742/652_1742.sh
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:32.212 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2010
[WI-0][TI-1742] - [INFO] 2024-04-25 15:09:32.459 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1742, success=true)
[WI-0][TI-1742] - [INFO] 2024-04-25 15:09:32.473 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1742)
[WI-0][TI-0] - [INFO] 2024-04-25 15:09:33.217 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Input Topic: reddit_post_processed_topic
	Output Topic: reddit_post_sa_topic
	#{setValue(input_topic=reddit_post_processed_topic)}
	#{setValue(output_topic=reddit_post_sa_topic)}
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:33.237 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1742, processId:2010 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:33.242 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:33.243 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:33.243 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:33.244 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:33.250 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:33.251 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:33.251 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1742
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:33.252 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1742
[WI-652][TI-1742] - [INFO] 2024-04-25 15:09:33.252 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1742] - [INFO] 2024-04-25 15:09:33.456 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1742, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:09:33.574 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1743, taskName=sentiment anaysis, firstSubmitTime=1714028973564, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=7, appIds=null, processInstanceId=652, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1743'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425150933'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='652'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.576 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.577 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.580 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.580 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.580 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.581 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714028973581
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.581 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 652_1743
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.582 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1743,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714028973564,
  "startTime" : 1714028973581,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/7/652/1743.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 7,
  "processInstanceId" : 652,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1743"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425150933"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "652"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "652_1743",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.582 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.583 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.583 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.590 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.591 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.593 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1743 check successfully
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.593 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.595 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.596 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.597 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.597 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.598 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.599 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.600 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.601 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.601 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.602 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.603 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1743
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.604 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1743/py_652_1743.py
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.604 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.605 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.605 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.605 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1743/py_652_1743.py
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.605 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.606 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1743/652_1743.sh
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:33.612 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2020
[WI-0][TI-1743] - [INFO] 2024-04-25 15:09:34.466 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1743, success=true)
[WI-0][TI-1743] - [INFO] 2024-04-25 15:09:34.496 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1743)
[WI-0][TI-0] - [INFO] 2024-04-25 15:09:34.617 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	JAVA_HOME is not set
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1743/py_652_1743.py", line 14, in <module>
	    .getOrCreate()
	     ^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py", line 497, in getOrCreate
	    sc = SparkContext.getOrCreate(sparkConf)
	         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 515, in getOrCreate
	    SparkContext(conf=conf or SparkConf())
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 201, in __init__
	    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 436, in _ensure_initialized
	    SparkContext._gateway = gateway or launch_gateway(conf)
	                                       ^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/java_gateway.py", line 107, in launch_gateway
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:34.626 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1743, processId:2020 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:34.626 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:34.626 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:34.627 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:34.640 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:34.687 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:34.688 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:34.688 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1743
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:34.689 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1743
[WI-652][TI-1743] - [INFO] 2024-04-25 15:09:34.689 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1743] - [INFO] 2024-04-25 15:09:35.472 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1743, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:09:35.572 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1744, taskName=sentiment anaysis, firstSubmitTime=1714028975557, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=7, appIds=null, processInstanceId=652, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1744'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425150935'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='652'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.586 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.584 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.588 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.588 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.589 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.589 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714028975589
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.589 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 652_1744
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.589 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1744,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714028975557,
  "startTime" : 1714028975589,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/7/652/1744.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 7,
  "processInstanceId" : 652,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1744"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425150935"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "652"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "652_1744",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.590 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.590 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.590 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.598 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.598 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.599 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1744 check successfully
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.600 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.600 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.601 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.602 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.602 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.603 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.603 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.603 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.604 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.605 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.605 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.606 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1744
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.606 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1744/py_652_1744.py
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.606 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.607 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.607 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.607 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1744/py_652_1744.py
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.612 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.612 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1744/652_1744.sh
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:35.622 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2039
[WI-0][TI-1744] - [INFO] 2024-04-25 15:09:36.466 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1744, success=true)
[WI-0][TI-1744] - [INFO] 2024-04-25 15:09:36.483 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1744)
[WI-0][TI-0] - [INFO] 2024-04-25 15:09:36.629 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	JAVA_HOME is not set
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1744/py_652_1744.py", line 14, in <module>
	    .getOrCreate()
	     ^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py", line 497, in getOrCreate
	    sc = SparkContext.getOrCreate(sparkConf)
	         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 515, in getOrCreate
	    SparkContext(conf=conf or SparkConf())
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 201, in __init__
	    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 436, in _ensure_initialized
	    SparkContext._gateway = gateway or launch_gateway(conf)
	                                       ^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/java_gateway.py", line 107, in launch_gateway
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:36.648 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1744, processId:2039 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:36.649 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:36.649 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:36.649 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:36.650 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:36.684 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:36.685 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:36.685 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1744
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:36.687 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1744
[WI-652][TI-1744] - [INFO] 2024-04-25 15:09:36.687 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-04-25 15:09:37.141 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8119658119658121 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1744] - [INFO] 2024-04-25 15:09:37.476 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1744, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:09:37.603 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1745, taskName=sentiment anaysis, firstSubmitTime=1714028977560, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=7, appIds=null, processInstanceId=652, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1745'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425150937'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='652'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.606 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.606 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.618 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.618 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.618 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.624 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714028977624
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.624 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 652_1745
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.627 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1745,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714028977560,
  "startTime" : 1714028977624,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/7/652/1745.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 7,
  "processInstanceId" : 652,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1745"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425150937"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "652"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "652_1745",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.633 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.636 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.637 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.669 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.672 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.676 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1745 check successfully
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.676 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.676 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.677 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.677 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.677 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.678 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.678 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.678 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.678 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.678 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.678 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.679 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1745
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.680 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1745/py_652_1745.py
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.680 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.681 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.681 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.681 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1745/py_652_1745.py
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.681 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.681 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1745/652_1745.sh
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:37.712 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2063
[WI-0][TI-0] - [INFO] 2024-04-25 15:09:37.725 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 15:09:38.157 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8455284552845528 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1745] - [INFO] 2024-04-25 15:09:38.482 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1745, success=true)
[WI-0][TI-1745] - [INFO] 2024-04-25 15:09:38.506 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1745)
[WI-0][TI-0] - [INFO] 2024-04-25 15:09:38.727 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	JAVA_HOME is not set
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1745/py_652_1745.py", line 14, in <module>
	    .getOrCreate()
	     ^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py", line 497, in getOrCreate
	    sc = SparkContext.getOrCreate(sparkConf)
	         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 515, in getOrCreate
	    SparkContext(conf=conf or SparkConf())
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 201, in __init__
	    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 436, in _ensure_initialized
	    SparkContext._gateway = gateway or launch_gateway(conf)
	                                       ^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/java_gateway.py", line 107, in launch_gateway
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:38.728 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1745, processId:2063 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:38.729 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:38.729 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:38.729 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:38.730 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:38.749 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:38.757 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:38.757 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1745
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:38.758 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1745
[WI-652][TI-1745] - [INFO] 2024-04-25 15:09:38.759 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1745] - [INFO] 2024-04-25 15:09:39.469 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1745, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:09:39.720 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1746, taskName=sentiment anaysis, firstSubmitTime=1714028979638, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=7, appIds=null, processInstanceId=652, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1746'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425150939'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='652'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.731 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.732 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.734 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.737 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.758 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.778 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714028979778
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.780 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 652_1746
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.780 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1746,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714028979638,
  "startTime" : 1714028979778,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/7/652/1746.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 7,
  "processInstanceId" : 652,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1746"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425150939"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "652"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "652_1746",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.781 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.798 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.798 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.811 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.811 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.812 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1746 check successfully
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.812 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.813 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.814 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.815 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.815 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.816 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.816 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.817 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.818 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.818 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.819 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.819 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1746
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.819 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1746/py_652_1746.py
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.820 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.820 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.824 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.825 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1746/py_652_1746.py
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.832 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.834 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1746/652_1746.sh
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:39.841 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2082
[WI-0][TI-0] - [INFO] 2024-04-25 15:09:40.168 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7507598784194529 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1746] - [INFO] 2024-04-25 15:09:40.491 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1746, success=true)
[WI-0][TI-1746] - [INFO] 2024-04-25 15:09:40.507 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1746)
[WI-0][TI-0] - [INFO] 2024-04-25 15:09:40.842 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	JAVA_HOME is not set
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1746/py_652_1746.py", line 14, in <module>
	    .getOrCreate()
	     ^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py", line 497, in getOrCreate
	    sc = SparkContext.getOrCreate(sparkConf)
	         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 515, in getOrCreate
	    SparkContext(conf=conf or SparkConf())
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 201, in __init__
	    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 436, in _ensure_initialized
	    SparkContext._gateway = gateway or launch_gateway(conf)
	                                       ^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/java_gateway.py", line 107, in launch_gateway
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:40.844 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1746, processId:2082 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:40.845 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:40.845 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:40.845 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:40.845 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:40.849 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:40.851 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:40.851 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1746
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:40.852 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_7/652/1746
[WI-652][TI-1746] - [INFO] 2024-04-25 15:09:40.853 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1746] - [INFO] 2024-04-25 15:09:41.484 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1746, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:09:42.189 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7585227272727273 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:09:43.204 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7473404255319149 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:09:51.224 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7822349570200573 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:10:04.307 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7909604519774011 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:11:18.637 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7449275362318841 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:11:28.696 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9192200557103064 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:11:32.643 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1747, taskName=read kafka, firstSubmitTime=1714029092628, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=8, appIds=null, processInstanceId=653, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"input_topic","direct":"OUT","type":"VARCHAR","value":""},{"prop":"output_topic","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n","resourceList":[]}, environmentConfig=export JAVA_HOME=/opt/java/openjdk, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='read kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='653'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1747'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386664399040'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425151132'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.645 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: read kafka to wait queue success
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.647 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.664 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.666 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.666 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.666 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714029092666
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.668 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 653_1747
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.668 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1747,
  "taskName" : "read kafka",
  "firstSubmitTime" : 1714029092628,
  "startTime" : 1714029092666,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/8/653/1747.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 8,
  "processInstanceId" : 653,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"input_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"},{\"prop\":\"output_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\ntopics=\\\"reddit_post_processed_topic|reddit_post_sa_topic\\\"\\n\\n# parse the topics string into input and output topics\\nIFS='|' read -r input_topic output_topic <<< \\\"$topics\\\"\\n\\n# Print the input and output topics\\necho \\\"Input Topic: $input_topic\\\"\\necho \\\"Output Topic: $output_topic\\\"\\n\\necho \\\"#{setValue(input_topic=${input_topic})}\\\"\\necho \\\"#{setValue(output_topic=${output_topic})}\\\"\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export JAVA_HOME=/opt/java/openjdk",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "read kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "653"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1747"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386664399040"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425151132"
    }
  },
  "taskAppId" : "653_1747",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.669 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.669 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.669 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.700 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.701 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.706 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1747 check successfully
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.710 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.710 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.711 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.711 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-0][TI-1747] - [INFO] 2024-04-25 15:11:32.713 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1747, success=true)
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.720 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "input_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  }, {
    "prop" : "output_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n",
  "resourceList" : [ ]
}
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.722 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.722 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.722 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.722 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.722 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.730 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.733 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.733 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export JAVA_HOME=/opt/java/openjdk
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

topics="reddit_post_processed_topic|reddit_post_sa_topic"

# parse the topics string into input and output topics
IFS='|' read -r input_topic output_topic <<< "$topics"

# Print the input and output topics
echo "Input Topic: $input_topic"
echo "Output Topic: $output_topic"

echo "#{setValue(input_topic=${input_topic})}"
echo "#{setValue(output_topic=${output_topic})}"

[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.733 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.733 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1747/653_1747.sh
[WI-0][TI-0] - [INFO] 2024-04-25 15:11:32.739 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7548209366391184 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:32.764 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2121
[WI-0][TI-1747] - [INFO] 2024-04-25 15:11:33.704 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1747)
[WI-0][TI-0] - [INFO] 2024-04-25 15:11:33.767 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Input Topic: reddit_post_processed_topic
	Output Topic: reddit_post_sa_topic
	#{setValue(input_topic=reddit_post_processed_topic)}
	#{setValue(output_topic=reddit_post_sa_topic)}
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:33.776 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1747, processId:2121 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:33.778 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:33.778 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:33.778 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:33.779 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:33.788 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:33.789 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:33.789 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1747
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:33.790 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1747
[WI-653][TI-1747] - [INFO] 2024-04-25 15:11:33.791 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1747] - [INFO] 2024-04-25 15:11:34.713 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1747, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:11:34.784 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1748, taskName=sentiment anaysis, firstSubmitTime=1714029094762, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=8, appIds=null, processInstanceId=653, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1748'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425151134'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='653'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.789 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.789 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.794 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.794 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.794 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.794 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714029094794
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.795 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 653_1748
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.803 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1748,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714029094762,
  "startTime" : 1714029094794,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/8/653/1748.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 8,
  "processInstanceId" : 653,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1748"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425151134"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "653"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "653_1748",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.804 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.804 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.804 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.807 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.808 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.808 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1748 check successfully
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.813 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.813 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.816 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.816 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.816 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.817 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.817 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.817 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.817 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.817 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.817 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.826 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1748
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.827 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1748/py_653_1748.py
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.827 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.828 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.828 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.828 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1748/py_653_1748.py
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.828 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.829 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1748/653_1748.sh
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:34.855 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2132
[WI-0][TI-0] - [INFO] 2024-04-25 15:11:34.856 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1748] - [INFO] 2024-04-25 15:11:35.729 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1748, success=true)
[WI-0][TI-1748] - [INFO] 2024-04-25 15:11:35.751 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1748)
[WI-0][TI-0] - [INFO] 2024-04-25 15:11:35.799 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8920993833229064 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:11:36.812 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8328611898016998 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:11:38.837 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8291316526610644 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:11:38.886 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-04-25 15:11:39.865 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7012195121951219 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:11:39.892 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 15:11:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-25 15:11:41.908 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9046153846153847 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:11:42.905 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /home/default/nltk_data...
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1748/py_653_1748.py", line 17, in <module>
	    nltk.download('vader_lexicon')
	  File "/usr/local/lib/python3.11/dist-packages/nltk/downloader.py", line 777, in download
	    for msg in self.incr_download(info_or_id, download_dir, force):
	  File "/usr/local/lib/python3.11/dist-packages/nltk/downloader.py", line 642, in incr_download
	    yield from self._download_package(info, download_dir, force)
	  File "/usr/local/lib/python3.11/dist-packages/nltk/downloader.py", line 699, in _download_package
	    os.makedirs(download_dir)
	  File "<frozen os>", line 215, in makedirs
	  File "<frozen os>", line 225, in makedirs
	PermissionError: [Errno 13] Permission denied: '/home/default'
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:42.906 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1748, processId:2132 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:42.907 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:42.908 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:42.908 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:42.908 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:42.911 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:42.911 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:42.912 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1748
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:42.912 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1748
[WI-653][TI-1748] - [INFO] 2024-04-25 15:11:42.912 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-04-25 15:11:42.926 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8067226890756303 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1748] - [INFO] 2024-04-25 15:11:43.742 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1748, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:11:43.927 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8523676880222841 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:11:44.931 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7442528735632183 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:11:47.946 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7424657534246575 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:11:55.112 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7493188010899182 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:11:56.824 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1752, taskName=read kafka, firstSubmitTime=1714029116778, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=8, appIds=null, processInstanceId=653, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"input_topic","direct":"OUT","type":"VARCHAR","value":""},{"prop":"output_topic","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n","resourceList":[]}, environmentConfig=export JAVA_HOME=/opt/java/openjdk, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=OUT, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='read kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=OUT, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1752'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386664399040'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425151156'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='653'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"input_topic","direct":"OUT","type":"VARCHAR","value":"reddit_post_processed_topic"},{"prop":"output_topic","direct":"OUT","type":"VARCHAR","value":"reddit_post_sa_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.834 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: read kafka to wait queue success
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.834 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.840 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.840 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.840 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.841 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714029116841
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.842 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 653_1752
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.857 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1752,
  "taskName" : "read kafka",
  "firstSubmitTime" : 1714029116778,
  "startTime" : 1714029116841,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/8/653/1752.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 8,
  "processInstanceId" : 653,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"input_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"},{\"prop\":\"output_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\ntopics=\\\"reddit_post_processed_topic|reddit_post_sa_topic\\\"\\n\\n# parse the topics string into input and output topics\\nIFS='|' read -r input_topic output_topic <<< \\\"$topics\\\"\\n\\n# Print the input and output topics\\necho \\\"Input Topic: $input_topic\\\"\\necho \\\"Output Topic: $output_topic\\\"\\n\\necho \\\"#{setValue(input_topic=${input_topic})}\\\"\\necho \\\"#{setValue(output_topic=${output_topic})}\\\"\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export JAVA_HOME=/opt/java/openjdk",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "OUT",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "read kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "OUT",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1752"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386664399040"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425151156"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "653"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "653_1752",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"input_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"},{\"prop\":\"output_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.858 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.860 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.861 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.870 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.871 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.871 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1752 check successfully
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.872 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.872 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.874 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.874 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.875 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "input_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  }, {
    "prop" : "output_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n",
  "resourceList" : [ ]
}
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.875 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.876 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"input_topic","direct":"OUT","type":"VARCHAR","value":"reddit_post_processed_topic"},{"prop":"output_topic","direct":"OUT","type":"VARCHAR","value":"reddit_post_sa_topic"}] successfully
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.876 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.876 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.876 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.877 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.878 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.878 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export JAVA_HOME=/opt/java/openjdk
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

topics="reddit_post_processed_topic|reddit_post_sa_topic"

# parse the topics string into input and output topics
IFS='|' read -r input_topic output_topic <<< "$topics"

# Print the input and output topics
echo "Input Topic: $input_topic"
echo "Output Topic: $output_topic"

echo "#{setValue(input_topic=reddit_post_processed_topic)}"
echo "#{setValue(output_topic=reddit_post_sa_topic)}"

[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.878 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.878 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1752/653_1752.sh
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:56.901 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2240
[WI-0][TI-1752] - [INFO] 2024-04-25 15:11:57.776 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1752, success=true)
[WI-0][TI-1752] - [INFO] 2024-04-25 15:11:57.785 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1752)
[WI-0][TI-0] - [INFO] 2024-04-25 15:11:57.902 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Input Topic: reddit_post_processed_topic
	Output Topic: reddit_post_sa_topic
	#{setValue(input_topic=reddit_post_processed_topic)}
	#{setValue(output_topic=reddit_post_sa_topic)}
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:57.908 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1752, processId:2240 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:57.910 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:57.910 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:57.910 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:57.910 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:57.914 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:57.914 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:57.914 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1752
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:57.915 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1752
[WI-653][TI-1752] - [INFO] 2024-04-25 15:11:57.915 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1752] - [INFO] 2024-04-25 15:11:58.782 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1752, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:11:58.848 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1753, taskName=sentiment anaysis, firstSubmitTime=1714029118834, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=8, appIds=null, processInstanceId=653, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1753'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425151158'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='653'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.850 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.850 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.853 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.853 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.853 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.853 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714029118853
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.853 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 653_1753
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.854 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1753,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714029118834,
  "startTime" : 1714029118853,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/8/653/1753.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 8,
  "processInstanceId" : 653,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1753"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425151158"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "653"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "653_1753",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.855 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.855 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.855 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.860 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.861 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.862 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1753 check successfully
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.862 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.862 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.863 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.863 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.863 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.864 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.864 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.864 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.865 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.865 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.865 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.866 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1753
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.866 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1753/py_653_1753.py
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.869 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.870 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.870 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.871 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1753/py_653_1753.py
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.871 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.871 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1753/653_1753.sh
[WI-653][TI-1753] - [INFO] 2024-04-25 15:11:58.875 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2250
[WI-0][TI-1753] - [INFO] 2024-04-25 15:11:59.847 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1753, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:11:59.881 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1753] - [INFO] 2024-04-25 15:11:59.892 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1753)
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:00.138 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7071005917159764 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:01.926 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
	24/04/25 15:12:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:03.937 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /home/default/nltk_data...
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1753/py_653_1753.py", line 17, in <module>
	    nltk.download('vader_lexicon')
	  File "/usr/local/lib/python3.11/dist-packages/nltk/downloader.py", line 777, in download
	    for msg in self.incr_download(info_or_id, download_dir, force):
	  File "/usr/local/lib/python3.11/dist-packages/nltk/downloader.py", line 642, in incr_download
	    yield from self._download_package(info, download_dir, force)
	  File "/usr/local/lib/python3.11/dist-packages/nltk/downloader.py", line 699, in _download_package
	    os.makedirs(download_dir)
	  File "<frozen os>", line 215, in makedirs
	  File "<frozen os>", line 225, in makedirs
	PermissionError: [Errno 13] Permission denied: '/home/default'
[WI-653][TI-1753] - [INFO] 2024-04-25 15:12:03.942 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1753, processId:2250 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-653][TI-1753] - [INFO] 2024-04-25 15:12:03.943 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1753] - [INFO] 2024-04-25 15:12:03.943 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-653][TI-1753] - [INFO] 2024-04-25 15:12:03.943 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1753] - [INFO] 2024-04-25 15:12:03.944 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-653][TI-1753] - [INFO] 2024-04-25 15:12:03.950 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-653][TI-1753] - [INFO] 2024-04-25 15:12:03.951 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-653][TI-1753] - [INFO] 2024-04-25 15:12:03.951 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1753
[WI-653][TI-1753] - [INFO] 2024-04-25 15:12:03.952 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1753
[WI-653][TI-1753] - [INFO] 2024-04-25 15:12:03.952 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1753] - [INFO] 2024-04-25 15:12:04.794 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1753, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:04.931 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1754, taskName=sentiment anaysis, firstSubmitTime=1714029124910, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=8, appIds=null, processInstanceId=653, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1754'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425151204'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='653'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.933 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.933 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.939 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.942 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.942 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.942 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714029124942
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.942 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 653_1754
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.942 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1754,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714029124910,
  "startTime" : 1714029124942,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/8/653/1754.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 8,
  "processInstanceId" : 653,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1754"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425151204"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "653"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "653_1754",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.943 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.943 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.943 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.949 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.951 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.952 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1754 check successfully
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.952 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.952 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.952 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.953 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.954 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.955 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.955 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.955 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.955 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.955 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.955 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.956 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1754
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.956 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1754/py_653_1754.py
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.956 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.957 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.959 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.959 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1754/py_653_1754.py
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.959 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.959 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1754/653_1754.sh
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:04.971 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2351
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:04.971 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1754] - [INFO] 2024-04-25 15:12:05.802 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1754, success=true)
[WI-0][TI-1754] - [INFO] 2024-04-25 15:12:05.815 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1754)
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:07.974 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:08.161 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7425149700598802 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:08.977 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 15:12:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:09.231 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7857142857142857 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:10.240 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.827485380116959 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:10.981 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /home/default/nltk_data...
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1754/py_653_1754.py", line 17, in <module>
	    nltk.download('vader_lexicon')
	  File "/usr/local/lib/python3.11/dist-packages/nltk/downloader.py", line 777, in download
	    for msg in self.incr_download(info_or_id, download_dir, force):
	  File "/usr/local/lib/python3.11/dist-packages/nltk/downloader.py", line 642, in incr_download
	    yield from self._download_package(info, download_dir, force)
	  File "/usr/local/lib/python3.11/dist-packages/nltk/downloader.py", line 699, in _download_package
	    os.makedirs(download_dir)
	  File "<frozen os>", line 215, in makedirs
	  File "<frozen os>", line 225, in makedirs
	PermissionError: [Errno 13] Permission denied: '/home/default'
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:11.242 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8076923076923077 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:11.990 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1754, processId:2351 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:11.992 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:11.993 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:11.993 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:11.993 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:11.998 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:11.999 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:11.999 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1754
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:12.003 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/653/1754
[WI-653][TI-1754] - [INFO] 2024-04-25 15:12:12.005 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1754] - [INFO] 2024-04-25 15:12:12.806 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1754, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:13.253 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7167630057803468 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:20.292 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7023121387283237 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:22.308 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7280966767371602 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:22.993 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1757, taskName=read kafka, firstSubmitTime=1714029142978, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=8, appIds=null, processInstanceId=654, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"input_topic","direct":"OUT","type":"VARCHAR","value":""},{"prop":"output_topic","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n","resourceList":[]}, environmentConfig=export JAVA_HOME=/opt/java/openjdk, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='read kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='654'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1757'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386664399040'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425151222'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:22.994 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: read kafka to wait queue success
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:22.995 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:22.998 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:22.999 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:22.999 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:22.999 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714029142999
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:22.999 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 654_1757
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:22.999 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1757,
  "taskName" : "read kafka",
  "firstSubmitTime" : 1714029142978,
  "startTime" : 1714029142999,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/8/654/1757.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 8,
  "processInstanceId" : 654,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"input_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"},{\"prop\":\"output_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\ntopics=\\\"reddit_post_processed_topic|reddit_post_sa_topic\\\"\\n\\n# parse the topics string into input and output topics\\nIFS='|' read -r input_topic output_topic <<< \\\"$topics\\\"\\n\\n# Print the input and output topics\\necho \\\"Input Topic: $input_topic\\\"\\necho \\\"Output Topic: $output_topic\\\"\\n\\necho \\\"#{setValue(input_topic=${input_topic})}\\\"\\necho \\\"#{setValue(output_topic=${output_topic})}\\\"\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export JAVA_HOME=/opt/java/openjdk",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "read kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "654"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1757"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386664399040"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425151222"
    }
  },
  "taskAppId" : "654_1757",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:23.000 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:23.000 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:23.000 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:23.017 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:23.017 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:23.018 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/654/1757 check successfully
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:23.018 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:23.019 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:23.020 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:23.020 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:23.020 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "input_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  }, {
    "prop" : "output_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n",
  "resourceList" : [ ]
}
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:23.020 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:23.020 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:23.020 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:23.020 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:23.020 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:23.021 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:23.021 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:23.021 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export JAVA_HOME=/opt/java/openjdk
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

topics="reddit_post_processed_topic|reddit_post_sa_topic"

# parse the topics string into input and output topics
IFS='|' read -r input_topic output_topic <<< "$topics"

# Print the input and output topics
echo "Input Topic: $input_topic"
echo "Output Topic: $output_topic"

echo "#{setValue(input_topic=${input_topic})}"
echo "#{setValue(output_topic=${output_topic})}"

[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:23.021 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:23.021 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/654/1757/654_1757.sh
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:23.029 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2458
[WI-0][TI-1757] - [INFO] 2024-04-25 15:12:23.919 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1757, success=true)
[WI-0][TI-1757] - [INFO] 2024-04-25 15:12:23.930 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1757)
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:24.051 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Input Topic: reddit_post_processed_topic
	Output Topic: reddit_post_sa_topic
	#{setValue(input_topic=reddit_post_processed_topic)}
	#{setValue(output_topic=reddit_post_sa_topic)}
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:24.052 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/654/1757, processId:2458 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:24.052 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:24.052 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:24.052 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:24.054 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:24.058 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:24.059 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:24.059 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/654/1757
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:24.060 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/654/1757
[WI-654][TI-1757] - [INFO] 2024-04-25 15:12:24.061 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1757] - [INFO] 2024-04-25 15:12:24.910 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1757, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:25.013 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1758, taskName=sentiment anaysis, firstSubmitTime=1714029144997, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=8, appIds=null, processInstanceId=654, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1758'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425151225'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='654'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.015 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.015 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.016 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.017 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.017 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.017 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714029145017
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.017 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 654_1758
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.017 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1758,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714029144997,
  "startTime" : 1714029145017,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/8/654/1758.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 8,
  "processInstanceId" : 654,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1758"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425151225"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "654"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "654_1758",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.018 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.018 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.018 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.024 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.025 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.030 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/654/1758 check successfully
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.031 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.031 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.031 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.031 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.031 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.032 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.032 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.032 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.032 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.032 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.032 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.033 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/654/1758
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.033 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/654/1758/py_654_1758.py
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.033 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.036 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.036 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.037 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/654/1758/py_654_1758.py
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.037 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.037 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/654/1758/654_1758.sh
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:25.051 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2468
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:25.407 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7254901960784313 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1758] - [INFO] 2024-04-25 15:12:25.920 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1758, success=true)
[WI-0][TI-1758] - [INFO] 2024-04-25 15:12:25.935 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1758)
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:26.064 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:29.077 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
	24/04/25 15:12:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:30.501 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.787292817679558 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:31.079 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /home/default/nltk_data...
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/654/1758/py_654_1758.py", line 17, in <module>
	    nltk.download('vader_lexicon')
	  File "/usr/local/lib/python3.11/dist-packages/nltk/downloader.py", line 777, in download
	    for msg in self.incr_download(info_or_id, download_dir, force):
	  File "/usr/local/lib/python3.11/dist-packages/nltk/downloader.py", line 642, in incr_download
	    yield from self._download_package(info, download_dir, force)
	  File "/usr/local/lib/python3.11/dist-packages/nltk/downloader.py", line 699, in _download_package
	    os.makedirs(download_dir)
	  File "<frozen os>", line 215, in makedirs
	  File "<frozen os>", line 225, in makedirs
	PermissionError: [Errno 13] Permission denied: '/home/default'
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:31.550 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.88283378746594 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:32.569 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7914285714285715 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:33.089 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/654/1758, processId:2468 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:33.090 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:33.091 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:33.091 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:33.091 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:33.094 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:33.095 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:33.095 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/654/1758
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:33.096 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/654/1758
[WI-654][TI-1758] - [INFO] 2024-04-25 15:12:33.096 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:33.571 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7705382436260624 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1758] - [INFO] 2024-04-25 15:12:33.915 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1758, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:34.572 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9171270718232044 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:35.577 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7264957264957265 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:36.581 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.816618911174785 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:44.614 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7492877492877492 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:45.629 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7781065088757396 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:12:46.631 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7560137457044673 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:13:07.707 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7507246376811594 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:13:36.902 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7065527065527065 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:13:38.931 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7166212534059946 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:13:42.977 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.771186440677966 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:13:52.466 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1762, taskName=read kafka, firstSubmitTime=1714029232425, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=8, appIds=null, processInstanceId=655, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"input_topic","direct":"OUT","type":"VARCHAR","value":""},{"prop":"output_topic","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n","resourceList":[]}, environmentConfig=export JAVA_HOME=/opt/java/openjdk, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='read kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='655'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1762'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386664399040'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425151352'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.468 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: read kafka to wait queue success
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.468 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.480 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.481 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.482 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.482 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714029232482
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.482 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 655_1762
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.483 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1762,
  "taskName" : "read kafka",
  "firstSubmitTime" : 1714029232425,
  "startTime" : 1714029232482,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/8/655/1762.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 8,
  "processInstanceId" : 655,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"input_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"},{\"prop\":\"output_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\ntopics=\\\"reddit_post_processed_topic|reddit_post_sa_topic\\\"\\n\\n# parse the topics string into input and output topics\\nIFS='|' read -r input_topic output_topic <<< \\\"$topics\\\"\\n\\n# Print the input and output topics\\necho \\\"Input Topic: $input_topic\\\"\\necho \\\"Output Topic: $output_topic\\\"\\n\\necho \\\"#{setValue(input_topic=${input_topic})}\\\"\\necho \\\"#{setValue(output_topic=${output_topic})}\\\"\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export JAVA_HOME=/opt/java/openjdk",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "read kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "655"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1762"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386664399040"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425151352"
    }
  },
  "taskAppId" : "655_1762",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.502 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.516 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.517 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.520 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.521 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.521 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/655/1762 check successfully
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.522 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.522 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.522 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.522 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.523 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "input_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  }, {
    "prop" : "output_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n",
  "resourceList" : [ ]
}
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.525 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.526 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.526 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.527 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.527 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.527 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.527 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.527 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export JAVA_HOME=/opt/java/openjdk
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

topics="reddit_post_processed_topic|reddit_post_sa_topic"

# parse the topics string into input and output topics
IFS='|' read -r input_topic output_topic <<< "$topics"

# Print the input and output topics
echo "Input Topic: $input_topic"
echo "Output Topic: $output_topic"

echo "#{setValue(input_topic=${input_topic})}"
echo "#{setValue(output_topic=${output_topic})}"

[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.527 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.527 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/655/1762/655_1762.sh
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:52.539 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2589
[WI-0][TI-0] - [INFO] 2024-04-25 15:13:52.541 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 15:13:53.057 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7293447293447294 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1762] - [INFO] 2024-04-25 15:13:53.122 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1762, success=true)
[WI-0][TI-1762] - [INFO] 2024-04-25 15:13:53.132 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1762)
[WI-0][TI-0] - [INFO] 2024-04-25 15:13:53.554 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Input Topic: reddit_post_processed_topic
	Output Topic: reddit_post_sa_topic
	#{setValue(input_topic=reddit_post_processed_topic)}
	#{setValue(output_topic=reddit_post_sa_topic)}
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:53.564 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/655/1762, processId:2589 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:53.576 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:53.576 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:53.576 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:53.577 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:53.584 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:53.584 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:53.584 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/655/1762
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:53.585 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/655/1762
[WI-655][TI-1762] - [INFO] 2024-04-25 15:13:53.585 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1762] - [INFO] 2024-04-25 15:13:54.118 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1762, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:13:54.272 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1763, taskName=sentiment anaysis, firstSubmitTime=1714029234235, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=8, appIds=null, processInstanceId=655, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1763'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425151354'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='655'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.274 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.287 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.293 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.294 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.294 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.294 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714029234294
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.295 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 655_1763
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.303 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1763,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714029234235,
  "startTime" : 1714029234294,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/8/655/1763.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 8,
  "processInstanceId" : 655,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1763"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425151354"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "655"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "655_1763",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.304 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.304 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.304 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.313 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.313 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.314 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/655/1763 check successfully
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.314 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.314 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.315 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.315 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.315 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.316 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.316 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.316 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.316 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.316 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.316 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.317 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/655/1763
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.317 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/655/1763/py_655_1763.py
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.318 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.319 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.319 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.319 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/655/1763/py_655_1763.py
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.320 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.320 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/655/1763/655_1763.sh
[WI-655][TI-1763] - [INFO] 2024-04-25 15:13:54.324 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2600
[WI-0][TI-1763] - [INFO] 2024-04-25 15:13:55.137 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1763, success=true)
[WI-0][TI-1763] - [INFO] 2024-04-25 15:13:55.168 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1763)
[WI-0][TI-0] - [INFO] 2024-04-25 15:13:55.324 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 15:13:56.075 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9209039548022598 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:13:57.114 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8796791443850267 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:13:58.117 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7768115942028986 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:13:58.336 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
	24/04/25 15:13:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-25 15:13:59.126 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8258426966292136 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:14:00.129 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.875 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:14:01.131 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8642857142857143 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:14:01.342 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /home/default/nltk_data...
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/655/1763/py_655_1763.py", line 17, in <module>
	    nltk.download('vader_lexicon')
	  File "/usr/local/lib/python3.11/dist-packages/nltk/downloader.py", line 777, in download
	    for msg in self.incr_download(info_or_id, download_dir, force):
	  File "/usr/local/lib/python3.11/dist-packages/nltk/downloader.py", line 642, in incr_download
	    yield from self._download_package(info, download_dir, force)
	  File "/usr/local/lib/python3.11/dist-packages/nltk/downloader.py", line 699, in _download_package
	    os.makedirs(download_dir)
	  File "<frozen os>", line 215, in makedirs
	  File "<frozen os>", line 225, in makedirs
	PermissionError: [Errno 13] Permission denied: '/home/default'
[WI-0][TI-0] - [INFO] 2024-04-25 15:14:02.133 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8918918918918919 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-655][TI-1763] - [INFO] 2024-04-25 15:14:03.347 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/655/1763, processId:2600 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-655][TI-1763] - [INFO] 2024-04-25 15:14:03.348 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-655][TI-1763] - [INFO] 2024-04-25 15:14:03.348 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-655][TI-1763] - [INFO] 2024-04-25 15:14:03.348 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-655][TI-1763] - [INFO] 2024-04-25 15:14:03.348 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-655][TI-1763] - [INFO] 2024-04-25 15:14:03.356 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-655][TI-1763] - [INFO] 2024-04-25 15:14:03.356 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-655][TI-1763] - [INFO] 2024-04-25 15:14:03.356 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/655/1763
[WI-655][TI-1763] - [INFO] 2024-04-25 15:14:03.357 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_8/655/1763
[WI-655][TI-1763] - [INFO] 2024-04-25 15:14:03.357 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1763] - [INFO] 2024-04-25 15:14:04.163 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1763, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:14:07.146 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7052023121387284 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:27:55.168 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7423822714681441 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:29:27.857 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8128491620111732 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:29:30.878 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7450980392156863 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:32:41.972 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9620253164556962 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:32:42.981 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9842767295597484 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:32:43.983 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9562841530054644 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:32:44.985 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9512893982808023 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:32:45.991 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8983516483516483 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:32:46.992 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7829670329670331 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:32:47.994 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8694444444444445 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:33:38.169 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7888563049853372 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:33:41.222 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7223796033994334 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:33:42.210 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1767, taskName=read kafka, firstSubmitTime=1714030422200, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=9, appIds=null, processInstanceId=656, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"input_topic","direct":"OUT","type":"VARCHAR","value":""},{"prop":"output_topic","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n","resourceList":[]}, environmentConfig=export JAVA_HOME=/opt/java/openjdk, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='read kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='656'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1767'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386664399040'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425153342'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.212 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: read kafka to wait queue success
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.212 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.222 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.222 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.222 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.222 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714030422222
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.223 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 656_1767
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.223 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1767,
  "taskName" : "read kafka",
  "firstSubmitTime" : 1714030422200,
  "startTime" : 1714030422222,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/9/656/1767.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 9,
  "processInstanceId" : 656,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"input_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"},{\"prop\":\"output_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\ntopics=\\\"reddit_post_processed_topic|reddit_post_sa_topic\\\"\\n\\n# parse the topics string into input and output topics\\nIFS='|' read -r input_topic output_topic <<< \\\"$topics\\\"\\n\\n# Print the input and output topics\\necho \\\"Input Topic: $input_topic\\\"\\necho \\\"Output Topic: $output_topic\\\"\\n\\necho \\\"#{setValue(input_topic=${input_topic})}\\\"\\necho \\\"#{setValue(output_topic=${output_topic})}\\\"\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export JAVA_HOME=/opt/java/openjdk",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "read kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "656"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1767"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386664399040"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425153342"
    }
  },
  "taskAppId" : "656_1767",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.224 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.224 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.224 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.227 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.228 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.228 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/656/1767 check successfully
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.228 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.228 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.229 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.229 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.229 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "input_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  }, {
    "prop" : "output_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n",
  "resourceList" : [ ]
}
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.229 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.229 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.229 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.229 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.230 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.231 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.231 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.231 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export JAVA_HOME=/opt/java/openjdk
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

topics="reddit_post_processed_topic|reddit_post_sa_topic"

# parse the topics string into input and output topics
IFS='|' read -r input_topic output_topic <<< "$topics"

# Print the input and output topics
echo "Input Topic: $input_topic"
echo "Output Topic: $output_topic"

echo "#{setValue(input_topic=${input_topic})}"
echo "#{setValue(output_topic=${output_topic})}"

[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.231 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.232 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/656/1767/656_1767.sh
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:42.238 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2938
[WI-0][TI-1767] - [INFO] 2024-04-25 15:33:42.457 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1767, success=true)
[WI-0][TI-1767] - [INFO] 2024-04-25 15:33:42.469 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1767)
[WI-0][TI-0] - [INFO] 2024-04-25 15:33:43.238 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Input Topic: reddit_post_processed_topic
	Output Topic: reddit_post_sa_topic
	#{setValue(input_topic=reddit_post_processed_topic)}
	#{setValue(output_topic=reddit_post_sa_topic)}
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:43.259 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/656/1767, processId:2938 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:43.259 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:43.259 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:43.259 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:43.260 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:43.266 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:43.266 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:43.266 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/656/1767
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:43.269 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/656/1767
[WI-656][TI-1767] - [INFO] 2024-04-25 15:33:43.269 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1767] - [INFO] 2024-04-25 15:33:43.447 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1767, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:33:43.554 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1768, taskName=sentiment anaysis, firstSubmitTime=1714030423530, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=9, appIds=null, processInstanceId=656, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1768'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425153343'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='656'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.564 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.567 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.581 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.581 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.582 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.582 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714030423582
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.582 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 656_1768
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.582 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1768,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714030423530,
  "startTime" : 1714030423582,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/9/656/1768.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 9,
  "processInstanceId" : 656,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1768"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425153343"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "656"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "656_1768",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.583 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.583 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.583 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.590 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.591 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.591 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/656/1768 check successfully
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.591 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.591 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.592 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.592 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.592 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.592 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.592 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.592 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.592 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.592 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.592 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.595 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/656/1768
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.595 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/656/1768/py_656_1768.py
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.595 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.596 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.596 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.596 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/656/1768/py_656_1768.py
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.596 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.596 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/656/1768/656_1768.sh
[WI-656][TI-1768] - [INFO] 2024-04-25 15:33:43.609 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2949
[WI-0][TI-0] - [INFO] 2024-04-25 15:33:43.609 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1768] - [INFO] 2024-04-25 15:33:44.485 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1768, success=true)
[WI-0][TI-1768] - [INFO] 2024-04-25 15:33:44.498 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1768)
[WI-0][TI-0] - [INFO] 2024-04-25 15:33:45.246 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7851002865329513 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:33:46.259 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7201166180758017 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:33:47.260 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7620481927710844 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:33:47.673 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
	24/04/25 15:33:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-25 15:33:48.262 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7991967871485944 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:33:49.264 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8642857142857143 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:33:50.266 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8916967509025271 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:33:50.689 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-04-25 15:33:51.275 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7453874538745386 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:33:53.288 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7861271676300579 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:33:55.395 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8352941176470587 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:33:56.433 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8879551820728291 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:33:57.435 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.742603550295858 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:33:58.437 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7515337423312883 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:33:59.438 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.711864406779661 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:00.440 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8074534161490684 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:01.445 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.727810650887574 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:01.718 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:02.451 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7275449101796407 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:03.733 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 15:34:02 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
	org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
	
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator.isEmpty(Iterator.scala:387)
		at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
		at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)
		at scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)
		at scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)
		at scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)
		at scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	24/04/25 15:34:02 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (15dedf891bed executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
	
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator.isEmpty(Iterator.scala:387)
		at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
		at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)
		at scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)
		at scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)
		at scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)
		at scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	
	24/04/25 15:34:02 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/656/1768/py_656_1768.py", line 43, in <module>
	    df = spark.read.json(rdd)
	         ^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py", line 440, in json
	    return self._df(self._jreader.json(jrdd))
	                    ^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
	    return f(*a, **kw)
	           ^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/py4j/protocol.py", line 326, in get_return_value
	    raise Py4JJavaError(
	py4j.protocol.Py4JJavaError: An error occurred while calling o28.json.
	: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (15dedf891bed executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
	
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator.isEmpty(Iterator.scala:387)
		at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
		at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)
		at scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)
		at scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)
		at scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)
		at scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	
	Driver stacktrace:
		at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
		at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
		at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
		at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
		at scala.Option.foreach(Option.scala:407)
		at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
		at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.infer(JsonInferSchema.scala:120)
		at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$5(JsonDataSource.scala:109)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:109)
		at org.apache.spark.sql.DataFrameReader.$anonfun$json$4(DataFrameReader.scala:416)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:416)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:391)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:377)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:282)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
	
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator.isEmpty(Iterator.scala:387)
		at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
		at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)
		at scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)
		at scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)
		at scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)
		at scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		... 1 more
	
[WI-656][TI-1768] - [INFO] 2024-04-25 15:34:04.737 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/656/1768, processId:2949 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-656][TI-1768] - [INFO] 2024-04-25 15:34:04.739 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-656][TI-1768] - [INFO] 2024-04-25 15:34:04.740 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-656][TI-1768] - [INFO] 2024-04-25 15:34:04.740 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-656][TI-1768] - [INFO] 2024-04-25 15:34:04.741 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-656][TI-1768] - [INFO] 2024-04-25 15:34:04.745 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-656][TI-1768] - [INFO] 2024-04-25 15:34:04.745 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-656][TI-1768] - [INFO] 2024-04-25 15:34:04.745 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/656/1768
[WI-656][TI-1768] - [INFO] 2024-04-25 15:34:04.746 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/656/1768
[WI-656][TI-1768] - [INFO] 2024-04-25 15:34:04.746 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1768] - [INFO] 2024-04-25 15:34:05.666 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1768, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:05.799 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1769, taskName=sentiment anaysis, firstSubmitTime=1714030445784, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=9, appIds=null, processInstanceId=656, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1769'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425153405'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='656'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.800 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.800 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.802 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.802 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.802 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.802 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714030445802
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.802 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 656_1769
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.802 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1769,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714030445784,
  "startTime" : 1714030445802,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/9/656/1769.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 9,
  "processInstanceId" : 656,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1769"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425153405"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "656"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "656_1769",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.808 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.808 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.809 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.817 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.818 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.819 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/656/1769 check successfully
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.819 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.820 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.821 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.821 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.821 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.822 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.822 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.822 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.822 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.832 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.833 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.841 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/656/1769
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.842 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/656/1769/py_656_1769.py
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.842 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.843 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.844 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.844 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/656/1769/py_656_1769.py
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.844 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.844 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/656/1769/656_1769.sh
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:05.857 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3069
[WI-0][TI-1769] - [INFO] 2024-04-25 15:34:06.664 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1769, success=true)
[WI-0][TI-1769] - [INFO] 2024-04-25 15:34:06.675 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1769)
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:06.865 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:08.481 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8166666666666667 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:09.507 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8793103448275862 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:09.875 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
	24/04/25 15:34:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:10.523 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9208211143695015 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:11.556 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9205298013245033 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:12.560 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9271255060728746 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:12.923 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:13.562 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9391304347826086 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:14.566 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8512396694214875 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:15.567 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7085714285714286 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:16.569 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8010899182561309 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:17.572 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.877005347593583 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:18.583 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7785016286644951 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:19.617 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7668711656441718 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:20.628 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7961432506887052 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:21.630 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8076513475874498 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:22.631 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8629908103592314 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:23.037 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:23.634 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8910614525139665 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:24.636 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8240223463687151 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:25.045 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 15:34:24 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
	org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
	
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator.isEmpty(Iterator.scala:387)
		at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
		at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)
		at scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)
		at scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)
		at scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)
		at scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	24/04/25 15:34:24 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (15dedf891bed executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
	
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator.isEmpty(Iterator.scala:387)
		at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
		at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)
		at scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)
		at scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)
		at scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)
		at scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	
	24/04/25 15:34:24 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
	
	[Stage 0:>                                                          (0 + 0) / 1]
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/656/1769/py_656_1769.py", line 43, in <module>
	    df = spark.read.json(rdd)
	         ^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py", line 440, in json
	    return self._df(self._jreader.json(jrdd))
	                    ^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
	    return f(*a, **kw)
	           ^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/py4j/protocol.py", line 326, in get_return_value
	    raise Py4JJavaError(
	py4j.protocol.Py4JJavaError: An error occurred while calling o28.json.
	: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (15dedf891bed executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
	
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator.isEmpty(Iterator.scala:387)
		at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
		at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)
		at scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)
		at scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)
		at scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)
		at scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	
	Driver stacktrace:
		at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
		at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
		at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
		at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
		at scala.Option.foreach(Option.scala:407)
		at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
		at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.infer(JsonInferSchema.scala:120)
		at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$5(JsonDataSource.scala:109)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:109)
		at org.apache.spark.sql.DataFrameReader.$anonfun$json$4(DataFrameReader.scala:416)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:416)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:391)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:377)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:282)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
	
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator.isEmpty(Iterator.scala:387)
		at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
		at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)
		at scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)
		at scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)
		at scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)
		at scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		... 1 more
	
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:26.053 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/656/1769, processId:3069 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:26.055 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:26.055 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:26.055 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:26.055 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:26.059 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:26.059 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:26.059 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/656/1769
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:26.059 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/656/1769
[WI-656][TI-1769] - [INFO] 2024-04-25 15:34:26.060 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1769] - [INFO] 2024-04-25 15:34:26.802 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1769, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:27.655 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8137046016702176 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:34:28.664 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9000985221674876 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:35:06.799 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7787114845938375 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:35:09.820 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7657142857142858 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:35:15.894 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.729050279329609 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:35:26.180 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1772, taskName=read kafka, firstSubmitTime=1714030526161, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=9, appIds=null, processInstanceId=657, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"input_topic","direct":"OUT","type":"VARCHAR","value":""},{"prop":"output_topic","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n","resourceList":[]}, environmentConfig=export JAVA_HOME=/opt/java/openjdk, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='read kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='657'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1772'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386664399040'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425153526'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.187 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: read kafka to wait queue success
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.189 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.203 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.204 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.205 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.205 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714030526205
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.205 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 657_1772
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.205 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1772,
  "taskName" : "read kafka",
  "firstSubmitTime" : 1714030526161,
  "startTime" : 1714030526205,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/9/657/1772.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 9,
  "processInstanceId" : 657,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"input_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"},{\"prop\":\"output_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\ntopics=\\\"reddit_post_processed_topic|reddit_post_sa_topic\\\"\\n\\n# parse the topics string into input and output topics\\nIFS='|' read -r input_topic output_topic <<< \\\"$topics\\\"\\n\\n# Print the input and output topics\\necho \\\"Input Topic: $input_topic\\\"\\necho \\\"Output Topic: $output_topic\\\"\\n\\necho \\\"#{setValue(input_topic=${input_topic})}\\\"\\necho \\\"#{setValue(output_topic=${output_topic})}\\\"\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export JAVA_HOME=/opt/java/openjdk",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "read kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "657"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1772"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386664399040"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425153526"
    }
  },
  "taskAppId" : "657_1772",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.209 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.209 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.209 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.223 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.224 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.225 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/657/1772 check successfully
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.239 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.240 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.240 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.240 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.242 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "input_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  }, {
    "prop" : "output_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n",
  "resourceList" : [ ]
}
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.244 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.245 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.245 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.246 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.246 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.247 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.247 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.247 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export JAVA_HOME=/opt/java/openjdk
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

topics="reddit_post_processed_topic|reddit_post_sa_topic"

# parse the topics string into input and output topics
IFS='|' read -r input_topic output_topic <<< "$topics"

# Print the input and output topics
echo "Input Topic: $input_topic"
echo "Output Topic: $output_topic"

echo "#{setValue(input_topic=${input_topic})}"
echo "#{setValue(output_topic=${output_topic})}"

[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.247 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.247 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/657/1772/657_1772.sh
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:26.255 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3199
[WI-0][TI-1772] - [INFO] 2024-04-25 15:35:26.901 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1772, success=true)
[WI-0][TI-1772] - [INFO] 2024-04-25 15:35:26.928 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1772)
[WI-0][TI-0] - [INFO] 2024-04-25 15:35:27.256 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Input Topic: reddit_post_processed_topic
	Output Topic: reddit_post_sa_topic
	#{setValue(input_topic=reddit_post_processed_topic)}
	#{setValue(output_topic=reddit_post_sa_topic)}
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:27.258 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/657/1772, processId:3199 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:27.259 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:27.259 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:27.259 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:27.259 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:27.262 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:27.262 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:27.263 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/657/1772
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:27.264 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/657/1772
[WI-657][TI-1772] - [INFO] 2024-04-25 15:35:27.264 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1772] - [INFO] 2024-04-25 15:35:27.907 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1772, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:35:28.017 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1773, taskName=sentiment anaysis, firstSubmitTime=1714030528005, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=9, appIds=null, processInstanceId=657, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1773'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425153528'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='657'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.018 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.020 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.025 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.026 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.026 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.026 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714030528026
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.026 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 657_1773
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.028 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1773,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714030528005,
  "startTime" : 1714030528026,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/9/657/1773.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 9,
  "processInstanceId" : 657,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1773"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425153528"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "657"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "657_1773",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.029 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.030 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.030 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.033 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.035 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.036 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/657/1773 check successfully
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.037 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.037 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.037 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.037 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.038 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.038 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.038 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.038 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.038 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.038 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.038 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.039 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/657/1773
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.039 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/657/1773/py_657_1773.py
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.039 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.040 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.040 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.040 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/657/1773/py_657_1773.py
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.040 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.040 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/657/1773/657_1773.sh
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:28.043 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3210
[WI-0][TI-1773] - [INFO] 2024-04-25 15:35:28.904 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1773, success=true)
[WI-0][TI-1773] - [INFO] 2024-04-25 15:35:28.914 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1773)
[WI-0][TI-0] - [INFO] 2024-04-25 15:35:29.043 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 15:35:31.046 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
	24/04/25 15:35:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-25 15:35:33.049 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-04-25 15:35:34.995 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8098360655737704 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:35:36.005 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.821917808219178 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:35:39.182 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:>                                                          (0 + 1) / 1]
	24/04/25 15:35:38 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
	org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
	
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator.isEmpty(Iterator.scala:387)
		at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
		at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)
		at scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)
		at scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)
		at scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)
		at scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	24/04/25 15:35:38 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (15dedf891bed executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
	
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator.isEmpty(Iterator.scala:387)
		at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
		at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)
		at scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)
		at scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)
		at scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)
		at scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	
	24/04/25 15:35:38 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
[WI-0][TI-0] - [INFO] 2024-04-25 15:35:40.188 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/657/1773/py_657_1773.py", line 43, in <module>
	    df = spark.read.json(rdd)
	         ^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py", line 440, in json
	    return self._df(self._jreader.json(jrdd))
	                    ^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
	    return f(*a, **kw)
	           ^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/py4j/protocol.py", line 326, in get_return_value
	    raise Py4JJavaError(
	py4j.protocol.Py4JJavaError: An error occurred while calling o28.json.
	: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (15dedf891bed executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
	
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator.isEmpty(Iterator.scala:387)
		at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
		at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)
		at scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)
		at scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)
		at scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)
		at scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	
	Driver stacktrace:
		at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
		at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
		at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
		at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
		at scala.Option.foreach(Option.scala:407)
		at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
		at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.infer(JsonInferSchema.scala:120)
		at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$5(JsonDataSource.scala:109)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:109)
		at org.apache.spark.sql.DataFrameReader.$anonfun$json$4(DataFrameReader.scala:416)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:416)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:391)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:377)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:282)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
	
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator.isEmpty(Iterator.scala:387)
		at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
		at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)
		at scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)
		at scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)
		at scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)
		at scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		... 1 more
	
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:40.191 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/657/1773, processId:3210 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:40.192 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:40.192 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:40.192 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:40.193 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:40.198 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:40.199 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:40.200 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/657/1773
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:40.201 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/657/1773
[WI-657][TI-1773] - [INFO] 2024-04-25 15:35:40.203 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1773] - [INFO] 2024-04-25 15:35:40.944 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1773, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:19.409 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1777, taskName=read kafka, firstSubmitTime=1714030579401, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=9, appIds=null, processInstanceId=658, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"input_topic","direct":"OUT","type":"VARCHAR","value":""},{"prop":"output_topic","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n","resourceList":[]}, environmentConfig=export JAVA_HOME=/opt/java/openjdk, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='read kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='658'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1777'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386664399040'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425153619'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.412 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: read kafka to wait queue success
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.412 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.415 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.415 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.415 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.415 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714030579415
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.415 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 658_1777
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.415 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1777,
  "taskName" : "read kafka",
  "firstSubmitTime" : 1714030579401,
  "startTime" : 1714030579415,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/9/658/1777.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 9,
  "processInstanceId" : 658,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"input_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"},{\"prop\":\"output_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\ntopics=\\\"reddit_post_processed_topic|reddit_post_sa_topic\\\"\\n\\n# parse the topics string into input and output topics\\nIFS='|' read -r input_topic output_topic <<< \\\"$topics\\\"\\n\\n# Print the input and output topics\\necho \\\"Input Topic: $input_topic\\\"\\necho \\\"Output Topic: $output_topic\\\"\\n\\necho \\\"#{setValue(input_topic=${input_topic})}\\\"\\necho \\\"#{setValue(output_topic=${output_topic})}\\\"\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export JAVA_HOME=/opt/java/openjdk",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "read kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "658"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1777"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386664399040"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425153619"
    }
  },
  "taskAppId" : "658_1777",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.416 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.416 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.417 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.443 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.444 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.447 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/658/1777 check successfully
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.447 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.449 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.449 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.449 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.450 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "input_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  }, {
    "prop" : "output_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n",
  "resourceList" : [ ]
}
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.450 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.450 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.450 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.450 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.450 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.451 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.451 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.451 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export JAVA_HOME=/opt/java/openjdk
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

topics="reddit_post_processed_topic|reddit_post_sa_topic"

# parse the topics string into input and output topics
IFS='|' read -r input_topic output_topic <<< "$topics"

# Print the input and output topics
echo "Input Topic: $input_topic"
echo "Output Topic: $output_topic"

echo "#{setValue(input_topic=${input_topic})}"
echo "#{setValue(output_topic=${output_topic})}"

[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.452 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.453 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/658/1777/658_1777.sh
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:19.467 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3340
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:19.477 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1777] - [INFO] 2024-04-25 15:36:20.003 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1777, success=true)
[WI-0][TI-1777] - [INFO] 2024-04-25 15:36:20.010 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1777)
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:20.480 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Input Topic: reddit_post_processed_topic
	Output Topic: reddit_post_sa_topic
	#{setValue(input_topic=reddit_post_processed_topic)}
	#{setValue(output_topic=reddit_post_sa_topic)}
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:20.481 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/658/1777, processId:3340 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:20.481 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:20.481 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:20.481 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:20.482 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:20.484 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:20.485 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:20.485 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/658/1777
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:20.485 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/658/1777
[WI-658][TI-1777] - [INFO] 2024-04-25 15:36:20.485 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1777] - [INFO] 2024-04-25 15:36:20.998 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1777, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:21.045 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1778, taskName=sentiment anaysis, firstSubmitTime=1714030581038, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=9, appIds=null, processInstanceId=658, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1778'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425153621'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='658'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.047 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.047 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.049 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.049 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.049 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.049 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714030581049
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.049 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 658_1778
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.051 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1778,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714030581038,
  "startTime" : 1714030581049,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/9/658/1778.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 9,
  "processInstanceId" : 658,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1778"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425153621"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "658"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "658_1778",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.055 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.055 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.055 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.063 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.064 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.064 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/658/1778 check successfully
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.064 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.064 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.065 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.065 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.065 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.065 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.066 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.066 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.066 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.066 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.066 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.067 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/658/1778
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.067 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/658/1778/py_658_1778.py
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.067 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.068 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.068 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.068 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/658/1778/py_658_1778.py
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.068 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.068 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/658/1778/658_1778.sh
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:21.083 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3350
[WI-0][TI-1778] - [INFO] 2024-04-25 15:36:22.002 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1778, success=true)
[WI-0][TI-1778] - [INFO] 2024-04-25 15:36:22.014 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1778)
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:22.084 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:23.175 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8397626112759644 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:24.184 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.755844234496124 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:25.093 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
	24/04/25 15:36:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:25.191 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8912280701754386 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:26.195 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8576388888888888 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:27.199 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7933130699088146 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:28.106 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:28.204 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8728157597501828 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:29.218 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.810146738692252 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:30.222 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8674033149171272 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:31.227 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8347578347578348 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:32.233 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9283276450511946 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:33.247 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9096045197740114 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:34.303 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8854166666666666 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:35.305 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8389830508474576 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:36.323 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8588588588588589 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:37.327 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.826219512195122 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:38.167 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:>                                                          (0 + 0) / 1]
	
	[Stage 0:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:38.332 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8907284768211922 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:39.348 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7552447552447553 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:40.194 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:===========================================================(1 + 0) / 1]
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:40.350 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7807017543859649 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:41.358 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7022058823529411 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:42.206 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 1:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:42.361 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7180327868852459 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:43.207 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:43.367 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7103658536585367 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:44.212 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 2:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:44.383 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8801498127340823 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:45.216 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	|         content|             context|           datetime|     id|score|subreddit|type|                 url|           user|negative|neutral|positive|compound|
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	|singapore is bad|21apr2024 orchard...|2024-04-21 22:10:42|1c9itat|  131|singapore|post|https://www.reddi...|premiumplatinum|   0.636|  0.364|     0.0| -0.5423|
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/658/1778/py_658_1778.py", line 77, in <module>
	    .save()
	     ^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py", line 1461, in save
	    self._jwrite.save()
	  File "/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py", line 185, in deco
	    raise converted from None
	pyspark.errors.exceptions.captured.AnalysisException: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.
[WI-0][TI-0] - [INFO] 2024-04-25 15:36:45.384 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8375000000000001 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:46.219 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/658/1778, processId:3350 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:46.221 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:46.221 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:46.221 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:46.222 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:46.225 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:46.226 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:46.226 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/658/1778
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:46.226 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/658/1778
[WI-658][TI-1778] - [INFO] 2024-04-25 15:36:46.227 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1778] - [INFO] 2024-04-25 15:36:47.088 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1778, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:38:45.025 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7138810198300283 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:38:46.038 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8407821229050279 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:38:47.039 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7659574468085106 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:38:49.075 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.786110557438044 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:39:47.632 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1782, taskName=read kafka, firstSubmitTime=1714030787605, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=9, appIds=null, processInstanceId=659, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"input_topic","direct":"OUT","type":"VARCHAR","value":""},{"prop":"output_topic","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n","resourceList":[]}, environmentConfig=export JAVA_HOME=/opt/java/openjdk, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='read kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='659'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1782'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386664399040'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425153947'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.639 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: read kafka to wait queue success
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.639 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.649 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.649 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.649 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.649 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714030787649
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.649 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 659_1782
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.653 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1782,
  "taskName" : "read kafka",
  "firstSubmitTime" : 1714030787605,
  "startTime" : 1714030787649,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/9/659/1782.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 9,
  "processInstanceId" : 659,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"input_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"},{\"prop\":\"output_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\ntopics=\\\"reddit_post_processed_topic|reddit_post_sa_topic\\\"\\n\\n# parse the topics string into input and output topics\\nIFS='|' read -r input_topic output_topic <<< \\\"$topics\\\"\\n\\n# Print the input and output topics\\necho \\\"Input Topic: $input_topic\\\"\\necho \\\"Output Topic: $output_topic\\\"\\n\\necho \\\"#{setValue(input_topic=${input_topic})}\\\"\\necho \\\"#{setValue(output_topic=${output_topic})}\\\"\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export JAVA_HOME=/opt/java/openjdk",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "read kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "659"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1782"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386664399040"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425153947"
    }
  },
  "taskAppId" : "659_1782",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.654 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.654 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.654 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.672 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.672 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.673 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1782 check successfully
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.673 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.673 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.673 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.673 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.674 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "input_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  }, {
    "prop" : "output_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n",
  "resourceList" : [ ]
}
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.674 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.674 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.674 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.674 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.674 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.674 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.677 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.677 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export JAVA_HOME=/opt/java/openjdk
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

topics="reddit_post_processed_topic|reddit_post_sa_topic"

# parse the topics string into input and output topics
IFS='|' read -r input_topic output_topic <<< "$topics"

# Print the input and output topics
echo "Input Topic: $input_topic"
echo "Output Topic: $output_topic"

echo "#{setValue(input_topic=${input_topic})}"
echo "#{setValue(output_topic=${output_topic})}"

[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.678 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.678 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1782/659_1782.sh
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:47.710 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3544
[WI-0][TI-0] - [INFO] 2024-04-25 15:39:47.710 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1782] - [INFO] 2024-04-25 15:39:48.012 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1782, success=true)
[WI-0][TI-1782] - [INFO] 2024-04-25 15:39:48.019 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1782)
[WI-0][TI-0] - [INFO] 2024-04-25 15:39:48.753 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Input Topic: reddit_post_processed_topic
	Output Topic: reddit_post_sa_topic
	#{setValue(input_topic=reddit_post_processed_topic)}
	#{setValue(output_topic=reddit_post_sa_topic)}
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:48.764 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1782, processId:3544 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:48.764 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:48.765 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:48.765 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:48.765 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:48.768 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:48.769 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:48.769 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1782
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:48.770 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1782
[WI-659][TI-1782] - [INFO] 2024-04-25 15:39:48.770 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1782] - [INFO] 2024-04-25 15:39:49.007 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1782, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:39:49.115 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1783, taskName=sentiment anaysis, firstSubmitTime=1714030789106, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=9, appIds=null, processInstanceId=659, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1783'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425153949'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='659'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.120 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.120 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.121 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.122 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.122 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.122 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714030789122
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.122 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 659_1783
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.123 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1783,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714030789106,
  "startTime" : 1714030789122,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/9/659/1783.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 9,
  "processInstanceId" : 659,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1783"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425153949"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "659"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "659_1783",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.123 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.124 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.124 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.126 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.127 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.127 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1783 check successfully
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.127 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.127 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.129 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.130 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.131 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.131 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.131 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.131 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.131 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.131 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.131 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.132 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1783
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.132 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1783/py_659_1783.py
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.132 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.133 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.133 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.133 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1783/py_659_1783.py
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.133 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.133 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1783/659_1783.sh
[WI-659][TI-1783] - [INFO] 2024-04-25 15:39:49.137 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3555
[WI-0][TI-1783] - [INFO] 2024-04-25 15:39:50.011 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1783, success=true)
[WI-0][TI-1783] - [INFO] 2024-04-25 15:39:50.020 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1783)
[WI-0][TI-0] - [INFO] 2024-04-25 15:39:50.137 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 15:39:52.150 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-04-25 15:39:53.155 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 15:39:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-25 15:39:54.157 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-04-25 15:39:56.339 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7878787878787878 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:39:57.356 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7206703910614525 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:00.162 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:01.164 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:03.180 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 1:>                                                          (0 + 1) / 1]
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:04.381 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7338129496402878 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:05.185 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	|         content|             context|           datetime|     id|score|subreddit|type|                 url|           user|negative|neutral|positive|compound|
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	|singapore is bad|21apr2024 orchard...|2024-04-21 22:10:42|1c9itat|  131|singapore|post|https://www.reddi...|premiumplatinum|   0.636|  0.364|     0.0| -0.5423|
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1783/py_659_1783.py", line 77, in <module>
	    .save()
	     ^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py", line 1461, in save
	    self._jwrite.save()
	  File "/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py", line 185, in deco
	    raise converted from None
	pyspark.errors.exceptions.captured.AnalysisException: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.
[WI-659][TI-1783] - [INFO] 2024-04-25 15:40:06.192 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1783, processId:3555 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-659][TI-1783] - [INFO] 2024-04-25 15:40:06.193 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1783] - [INFO] 2024-04-25 15:40:06.194 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-659][TI-1783] - [INFO] 2024-04-25 15:40:06.194 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1783] - [INFO] 2024-04-25 15:40:06.195 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-659][TI-1783] - [INFO] 2024-04-25 15:40:06.199 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-659][TI-1783] - [INFO] 2024-04-25 15:40:06.200 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-659][TI-1783] - [INFO] 2024-04-25 15:40:06.200 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1783
[WI-659][TI-1783] - [INFO] 2024-04-25 15:40:06.201 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1783
[WI-659][TI-1783] - [INFO] 2024-04-25 15:40:06.201 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1783] - [INFO] 2024-04-25 15:40:07.100 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1783, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:07.223 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1784, taskName=sentiment anaysis, firstSubmitTime=1714030807211, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=9, appIds=null, processInstanceId=659, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1784'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425154007'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='659'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.228 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.230 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.233 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.233 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.233 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.234 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714030807234
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.234 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 659_1784
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.234 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1784,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714030807211,
  "startTime" : 1714030807234,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/9/659/1784.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 9,
  "processInstanceId" : 659,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1784"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425154007"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "659"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "659_1784",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.236 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.236 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.236 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.240 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.241 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.241 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1784 check successfully
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.241 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.242 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.243 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.243 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.245 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.246 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.246 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.246 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.246 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.247 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.248 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.249 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1784
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.250 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1784/py_659_1784.py
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.250 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.252 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.252 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.252 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1784/py_659_1784.py
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.252 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.253 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1784/659_1784.sh
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:07.256 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3694
[WI-0][TI-1784] - [INFO] 2024-04-25 15:40:08.113 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1784, success=true)
[WI-0][TI-1784] - [INFO] 2024-04-25 15:40:08.124 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1784)
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:08.257 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:10.268 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
	24/04/25 15:40:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:12.288 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:13.489 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7045454545454546 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:19.313 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:20.330 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:21.602 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 1:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:22.604 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7908496732026143 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:22.605 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:23.609 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	|         content|             context|           datetime|     id|score|subreddit|type|                 url|           user|negative|neutral|positive|compound|
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	|singapore is bad|21apr2024 orchard...|2024-04-21 22:10:42|1c9itat|  131|singapore|post|https://www.reddi...|premiumplatinum|   0.636|  0.364|     0.0| -0.5423|
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1784/py_659_1784.py", line 77, in <module>
	    .save()
	     ^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py", line 1461, in save
	    self._jwrite.save()
	  File "/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py", line 185, in deco
	    raise converted from None
	pyspark.errors.exceptions.captured.AnalysisException: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:24.611 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1784, processId:3694 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:24.611 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:24.611 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:24.611 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:24.612 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:24.619 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:24.619 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:24.619 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1784
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:24.621 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1784
[WI-659][TI-1784] - [INFO] 2024-04-25 15:40:24.621 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1784] - [INFO] 2024-04-25 15:40:25.607 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1784, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:25.673 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1785, taskName=sentiment anaysis, firstSubmitTime=1714030825660, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=9, appIds=null, processInstanceId=659, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1785'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425154025'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='659'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.674 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.675 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.678 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.679 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.679 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.679 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714030825679
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.679 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 659_1785
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.680 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1785,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714030825660,
  "startTime" : 1714030825679,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/9/659/1785.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 9,
  "processInstanceId" : 659,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1785"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425154025"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "659"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "659_1785",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.681 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.681 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.681 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.685 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.686 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.686 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1785 check successfully
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.686 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.687 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.690 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.690 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.694 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.695 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.695 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.695 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.695 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.695 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.695 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.696 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1785
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.696 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1785/py_659_1785.py
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.697 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.698 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.698 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.698 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1785/py_659_1785.py
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.698 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.698 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1785/659_1785.sh
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:25.701 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3838
[WI-0][TI-1785] - [INFO] 2024-04-25 15:40:26.620 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1785, success=true)
[WI-0][TI-1785] - [INFO] 2024-04-25 15:40:26.630 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1785)
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:26.702 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:28.705 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
	24/04/25 15:40:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:30.713 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:37.718 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:38.720 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:39.693 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.717391304347826 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:40.788 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 1:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:41.789 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:41.812 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8333333333333334 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:42.792 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 2:>                                                          (0 + 1) / 1]
	
	                                                                                
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	|         content|             context|           datetime|     id|score|subreddit|type|                 url|           user|negative|neutral|positive|compound|
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	|singapore is bad|21apr2024 orchard...|2024-04-21 22:10:42|1c9itat|  131|singapore|post|https://www.reddi...|premiumplatinum|   0.636|  0.364|     0.0| -0.5423|
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1785/py_659_1785.py", line 77, in <module>
	    .save()
	     ^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py", line 1461, in save
	    self._jwrite.save()
	  File "/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py", line 185, in deco
	    raise converted from None
	pyspark.errors.exceptions.captured.AnalysisException: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:43.800 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1785, processId:3838 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:43.800 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:43.800 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:43.801 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:43.801 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:43.806 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:43.809 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:43.809 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1785
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:43.810 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1785
[WI-659][TI-1785] - [INFO] 2024-04-25 15:40:43.810 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1785] - [INFO] 2024-04-25 15:40:44.656 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1785, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:44.705 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1786, taskName=sentiment anaysis, firstSubmitTime=1714030844687, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=9, appIds=null, processInstanceId=659, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1786'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425154044'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='659'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.707 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.708 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.709 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.710 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.710 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.710 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714030844710
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.710 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 659_1786
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.712 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1786,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714030844687,
  "startTime" : 1714030844710,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/9/659/1786.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 9,
  "processInstanceId" : 659,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1786"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425154044"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "659"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "659_1786",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.713 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.713 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.713 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.721 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.722 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.723 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1786 check successfully
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.723 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.723 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.723 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.724 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.724 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.724 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.725 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.725 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.725 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.725 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.725 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.726 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1786
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.727 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1786/py_659_1786.py
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.727 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.729 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.729 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.729 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1786/py_659_1786.py
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.729 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.729 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1786/659_1786.sh
[WI-659][TI-1786] - [INFO] 2024-04-25 15:40:44.735 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3981
[WI-0][TI-1786] - [INFO] 2024-04-25 15:40:45.664 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1786, success=true)
[WI-0][TI-1786] - [INFO] 2024-04-25 15:40:45.680 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1786)
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:45.750 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:47.768 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
	24/04/25 15:40:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:49.863 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8896321070234113 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:50.779 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:57.785 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:58.795 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:58.958 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8518518518518519 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:40:59.991 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7564469914040115 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:41:00.801 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 1:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-25 15:41:01.804 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 15:41:02.001 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9082278481012658 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:41:02.818 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 2:>                                                          (0 + 1) / 1]
	
	                                                                                
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	|         content|             context|           datetime|     id|score|subreddit|type|                 url|           user|negative|neutral|positive|compound|
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	|singapore is bad|21apr2024 orchard...|2024-04-21 22:10:42|1c9itat|  131|singapore|post|https://www.reddi...|premiumplatinum|   0.636|  0.364|     0.0| -0.5423|
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1786/py_659_1786.py", line 77, in <module>
	    .save()
	     ^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py", line 1461, in save
	    self._jwrite.save()
	  File "/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py", line 185, in deco
	    raise converted from None
	pyspark.errors.exceptions.captured.AnalysisException: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.
[WI-0][TI-0] - [INFO] 2024-04-25 15:41:03.042 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8592814371257486 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:41:04.044 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8016997167138811 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-659][TI-1786] - [INFO] 2024-04-25 15:41:04.820 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1786, processId:3981 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-659][TI-1786] - [INFO] 2024-04-25 15:41:04.821 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1786] - [INFO] 2024-04-25 15:41:04.821 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-659][TI-1786] - [INFO] 2024-04-25 15:41:04.821 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-659][TI-1786] - [INFO] 2024-04-25 15:41:04.822 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-659][TI-1786] - [INFO] 2024-04-25 15:41:04.825 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-659][TI-1786] - [INFO] 2024-04-25 15:41:04.826 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-659][TI-1786] - [INFO] 2024-04-25 15:41:04.827 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1786
[WI-659][TI-1786] - [INFO] 2024-04-25 15:41:04.828 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_9/659/1786
[WI-659][TI-1786] - [INFO] 2024-04-25 15:41:04.829 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1786] - [INFO] 2024-04-25 15:41:05.812 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1786, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:41:11.058 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8587570621468926 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:42:08.285 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7017045454545454 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:42:48.484 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7558823529411764 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:43:06.563 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7587209302325582 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:43:09.537 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1787, taskName=read kafka, firstSubmitTime=1714030989526, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=10, appIds=null, processInstanceId=660, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"input_topic","direct":"OUT","type":"VARCHAR","value":""},{"prop":"output_topic","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n","resourceList":[]}, environmentConfig=export JAVA_HOME=/opt/java/openjdk, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='read kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='660'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1787'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386664399040'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425154309'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.538 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: read kafka to wait queue success
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.539 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.545 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.545 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.545 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.545 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714030989545
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.545 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 660_1787
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.546 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1787,
  "taskName" : "read kafka",
  "firstSubmitTime" : 1714030989526,
  "startTime" : 1714030989545,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1787.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 10,
  "processInstanceId" : 660,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"input_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"},{\"prop\":\"output_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\ntopics=\\\"reddit_post_processed_topic|reddit_post_sa_topic\\\"\\n\\n# parse the topics string into input and output topics\\nIFS='|' read -r input_topic output_topic <<< \\\"$topics\\\"\\n\\n# Print the input and output topics\\necho \\\"Input Topic: $input_topic\\\"\\necho \\\"Output Topic: $output_topic\\\"\\n\\necho \\\"#{setValue(input_topic=${input_topic})}\\\"\\necho \\\"#{setValue(output_topic=${output_topic})}\\\"\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export JAVA_HOME=/opt/java/openjdk",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "read kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "660"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1787"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386664399040"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425154309"
    }
  },
  "taskAppId" : "660_1787",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.547 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.547 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.547 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.553 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.554 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.554 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1787 check successfully
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.555 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.555 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.555 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.556 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.556 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "input_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  }, {
    "prop" : "output_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n",
  "resourceList" : [ ]
}
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.556 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.556 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.556 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.556 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.556 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.556 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.556 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.557 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export JAVA_HOME=/opt/java/openjdk
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

topics="reddit_post_processed_topic|reddit_post_sa_topic"

# parse the topics string into input and output topics
IFS='|' read -r input_topic output_topic <<< "$topics"

# Print the input and output topics
echo "Input Topic: $input_topic"
echo "Output Topic: $output_topic"

echo "#{setValue(input_topic=${input_topic})}"
echo "#{setValue(output_topic=${output_topic})}"

[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.557 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.557 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1787/660_1787.sh
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:09.563 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 4148
[WI-0][TI-1787] - [INFO] 2024-04-25 15:43:10.092 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1787, success=true)
[WI-0][TI-1787] - [INFO] 2024-04-25 15:43:10.137 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1787)
[WI-0][TI-0] - [INFO] 2024-04-25 15:43:10.565 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Input Topic: reddit_post_processed_topic
	Output Topic: reddit_post_sa_topic
	#{setValue(input_topic=reddit_post_processed_topic)}
	#{setValue(output_topic=reddit_post_sa_topic)}
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:10.567 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1787, processId:4148 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:10.568 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:10.568 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:10.568 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:10.569 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:10.576 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:10.577 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:10.577 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1787
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:10.577 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1787
[WI-660][TI-1787] - [INFO] 2024-04-25 15:43:10.578 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-04-25 15:43:10.587 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7551020408163265 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1787] - [INFO] 2024-04-25 15:43:11.073 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1787, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:43:11.185 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1788, taskName=sentiment anaysis, firstSubmitTime=1714030991178, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=10, appIds=null, processInstanceId=660, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1788'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425154311'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='660'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.191 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.202 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.207 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.208 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.208 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.208 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714030991208
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.208 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 660_1788
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.208 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1788,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714030991178,
  "startTime" : 1714030991208,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1788.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 10,
  "processInstanceId" : 660,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1788"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425154311"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "660"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "660_1788",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.209 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.209 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.209 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.213 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.216 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.217 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1788 check successfully
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.217 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.217 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.218 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.218 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.218 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.219 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.219 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.219 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.219 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.219 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.219 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.220 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1788
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.221 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1788/py_660_1788.py
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.221 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.221 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.221 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.221 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1788/py_660_1788.py
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.221 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.222 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1788/660_1788.sh
[WI-660][TI-1788] - [INFO] 2024-04-25 15:43:11.225 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 4159
[WI-0][TI-1788] - [INFO] 2024-04-25 15:43:12.075 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1788, success=true)
[WI-0][TI-1788] - [INFO] 2024-04-25 15:43:12.095 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1788)
[WI-0][TI-0] - [INFO] 2024-04-25 15:43:12.228 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 15:43:14.249 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-ada68eb5-de5b-4f69-8aa1-35b945f15d03;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-04-25 15:43:19.269 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central
[WI-0][TI-0] - [INFO] 2024-04-25 15:43:19.636 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8428571428571429 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:43:20.275 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
[WI-0][TI-0] - [INFO] 2024-04-25 15:43:20.663 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7292817679558011 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:43:21.275 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
[WI-0][TI-0] - [INFO] 2024-04-25 15:43:24.315 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.slf4j#slf4j-api;2.0.7 in central
[WI-0][TI-0] - [INFO] 2024-04-25 15:43:31.331 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[WI-0][TI-0] - [INFO] 2024-04-25 15:43:32.332 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[WI-0][TI-0] - [INFO] 2024-04-25 15:43:39.347 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found commons-logging#commons-logging;1.1.3 in central
[WI-0][TI-0] - [INFO] 2024-04-25 15:43:40.349 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found com.google.code.findbugs#jsr305;3.0.0 in central
[WI-0][TI-0] - [INFO] 2024-04-25 15:43:41.352 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central
[WI-0][TI-0] - [INFO] 2024-04-25 15:43:46.764 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7867036011080332 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:43:48.357 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.commons#commons-pool2;2.11.1 in central
	downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.13/3.5.1/spark-sql-kafka-0-10_2.13-3.5.1.jar ...
[WI-0][TI-0] - [INFO] 2024-04-25 15:43:48.783 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8136986301369863 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:43:49.794 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7493403693931399 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:43:51.812 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7617728531855955 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:43:56.370 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1!spark-sql-kafka-0-10_2.13.jar (8756ms)
[WI-0][TI-0] - [INFO] 2024-04-25 15:43:57.371 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.13/3.5.1/spark-token-provider-kafka-0-10_2.13-3.5.1.jar ...
[WI-0][TI-0] - [INFO] 2024-04-25 15:43:58.373 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1!spark-token-provider-kafka-0-10_2.13.jar (1360ms)
	downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-parallel-collections_2.13/1.0.4/scala-parallel-collections_2.13-1.0.4.jar ...
[WI-0][TI-0] - [INFO] 2024-04-25 15:44:05.889 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7253731343283581 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:44:06.910 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7589645092667697 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:44:07.911 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7681412200876305 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:44:08.915 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7354651162790697 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:44:09.917 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7090395480225989 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:44:10.918 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7047353760445682 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:44:11.927 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7356948228882835 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:44:14.941 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8096514745308312 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:44:16.017 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9261213720316622 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:44:17.026 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7181571815718157 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:44:21.459 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4!scala-parallel-collections_2.13.jar (23014ms)
	downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...
[WI-0][TI-0] - [INFO] 2024-04-25 15:45:59.843 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar (98584ms)
	downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...
[WI-0][TI-0] - [INFO] 2024-04-25 15:46:00.849 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (1314ms)
	downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...
[WI-0][TI-0] - [INFO] 2024-04-25 15:46:06.862 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (5438ms)
	downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...
[WI-0][TI-0] - [INFO] 2024-04-25 15:50:46.638 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8066298342541437 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:50:47.672 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7765957446808511 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:50:48.677 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7073863636363636 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:50:49.683 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7889182058047494 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:50:50.684 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8054054054054054 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:50:51.688 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7798408488063661 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:50:55.856 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[55] - Receive TaskInstanceKillRequest: TaskInstanceKillRequest(taskInstanceId=1788)
[WI-0][TI-1788] - [ERROR] 2024-04-25 15:50:55.863 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[139] - kill task error
java.io.IOException: Cannot run program "pstree": error=2, No such file or directory
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.runCommand(AbstractShell.java:138)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.run(AbstractShell.java:118)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execute(ShellExecutor.java:125)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:103)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:86)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeShell(OSUtils.java:345)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeCmd(OSUtils.java:334)
	at org.apache.dolphinscheduler.plugin.task.api.utils.ProcessUtils.getPidsStr(ProcessUtils.java:129)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.killProcess(TaskInstanceKillOperationFunction.java:130)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.doKill(TaskInstanceKillOperationFunction.java:96)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.operate(TaskInstanceKillOperationFunction.java:69)
	at org.apache.dolphinscheduler.server.worker.rpc.TaskInstanceOperatorImpl.killTask(TaskInstanceOperatorImpl.java:49)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.dolphinscheduler.extract.base.server.ServerMethodInvokerImpl.invoke(ServerMethodInvokerImpl.java:41)
	at org.apache.dolphinscheduler.extract.base.server.JdkDynamicServerHandler.lambda$processReceived$0(JdkDynamicServerHandler.java:108)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: error=2, No such file or directory
	at java.lang.UNIXProcess.forkAndExec(Native Method)
	at java.lang.UNIXProcess.<init>(UNIXProcess.java:247)
	at java.lang.ProcessImpl.start(ProcessImpl.java:134)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	... 21 common frames omitted
[WI-0][TI-1788] - [INFO] 2024-04-25 15:50:55.864 +0800 o.a.d.p.t.a.u.ProcessUtils:[182] - Get appIds from worker 172.18.1.1:1234, taskLogPath: /opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1788.log
[WI-0][TI-1788] - [INFO] 2024-04-25 15:50:55.864 +0800 o.a.d.p.t.a.u.LogUtils:[79] - Start finding appId in /opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1788.log, fetch way: log 
[WI-0][TI-1788] - [INFO] 2024-04-25 15:50:55.866 +0800 o.a.d.p.t.a.u.ProcessUtils:[188] - The appId is empty
[WI-0][TI-1788] - [INFO] 2024-04-25 15:50:55.866 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[220] - Begin to kill process process, pid is : 4159
[WI-0][TI-0] - [INFO] 2024-04-25 15:50:56.710 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.937125748502994 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1788] - [INFO] 2024-04-25 15:51:00.876 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[225] - Success kill task: 660_1788, pid: 4159
[WI-0][TI-1788] - [INFO] 2024-04-25 15:51:00.877 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[119] - kill task by cancelApplication, taskInstanceId: 1788
[WI-0][TI-0] - [INFO] 2024-04-25 15:51:07.511 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1789, taskName=read kafka, firstSubmitTime=1714031467505, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=10, appIds=null, processInstanceId=660, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"input_topic","direct":"OUT","type":"VARCHAR","value":""},{"prop":"output_topic","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n","resourceList":[]}, environmentConfig=export JAVA_HOME=/opt/java/openjdk, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=OUT, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='read kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=OUT, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1789'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386664399040'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425155107'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='660'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"input_topic","direct":"OUT","type":"VARCHAR","value":"reddit_post_processed_topic"},{"prop":"output_topic","direct":"OUT","type":"VARCHAR","value":"reddit_post_sa_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.513 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: read kafka to wait queue success
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.513 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.515 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.516 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.516 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.516 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714031467516
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.516 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 660_1789
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.517 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1789,
  "taskName" : "read kafka",
  "firstSubmitTime" : 1714031467505,
  "startTime" : 1714031467516,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1789.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 10,
  "processInstanceId" : 660,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"input_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"},{\"prop\":\"output_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\ntopics=\\\"reddit_post_processed_topic|reddit_post_sa_topic\\\"\\n\\n# parse the topics string into input and output topics\\nIFS='|' read -r input_topic output_topic <<< \\\"$topics\\\"\\n\\n# Print the input and output topics\\necho \\\"Input Topic: $input_topic\\\"\\necho \\\"Output Topic: $output_topic\\\"\\n\\necho \\\"#{setValue(input_topic=${input_topic})}\\\"\\necho \\\"#{setValue(output_topic=${output_topic})}\\\"\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export JAVA_HOME=/opt/java/openjdk",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "OUT",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "read kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "OUT",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1789"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386664399040"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425155107"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "660"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "660_1789",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"input_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"},{\"prop\":\"output_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.517 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.517 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.517 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.523 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.524 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.524 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1789 check successfully
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.524 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.525 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.525 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.525 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.526 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "input_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  }, {
    "prop" : "output_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n",
  "resourceList" : [ ]
}
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.526 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.526 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"input_topic","direct":"OUT","type":"VARCHAR","value":"reddit_post_processed_topic"},{"prop":"output_topic","direct":"OUT","type":"VARCHAR","value":"reddit_post_sa_topic"}] successfully
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.527 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.527 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.527 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.527 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.527 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.528 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export JAVA_HOME=/opt/java/openjdk
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

topics="reddit_post_processed_topic|reddit_post_sa_topic"

# parse the topics string into input and output topics
IFS='|' read -r input_topic output_topic <<< "$topics"

# Print the input and output topics
echo "Input Topic: $input_topic"
echo "Output Topic: $output_topic"

echo "#{setValue(input_topic=reddit_post_processed_topic)}"
echo "#{setValue(output_topic=reddit_post_sa_topic)}"

[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.528 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.528 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1789/660_1789.sh
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:07.532 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 4310
[WI-0][TI-1789] - [INFO] 2024-04-25 15:51:08.353 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1789, success=true)
[WI-0][TI-1789] - [INFO] 2024-04-25 15:51:08.362 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1789)
[WI-0][TI-0] - [INFO] 2024-04-25 15:51:08.534 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Input Topic: reddit_post_processed_topic
	Output Topic: reddit_post_sa_topic
	#{setValue(input_topic=reddit_post_processed_topic)}
	#{setValue(output_topic=reddit_post_sa_topic)}
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:08.540 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1789, processId:4310 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:08.540 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:08.540 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:08.540 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:08.541 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:08.544 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:08.544 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:08.544 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1789
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:08.545 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1789
[WI-660][TI-1789] - [INFO] 2024-04-25 15:51:08.545 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1789] - [INFO] 2024-04-25 15:51:09.356 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1789, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:51:09.388 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1790, taskName=sentiment anaysis, firstSubmitTime=1714031469380, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=10, appIds=null, processInstanceId=660, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1790'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425155109'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='660'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.389 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.389 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.391 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.392 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.392 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.392 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714031469392
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.392 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 660_1790
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.392 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1790,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714031469380,
  "startTime" : 1714031469392,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1790.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 10,
  "processInstanceId" : 660,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1790"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425155109"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "660"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "660_1790",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.393 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.393 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.393 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.396 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.397 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.398 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790 check successfully
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.398 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.398 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.399 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.400 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.400 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.401 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.401 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.402 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.402 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.403 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.403 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790/py_660_1790.py
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.403 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.404 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.404 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.404 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790/py_660_1790.py
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.404 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.405 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790/660_1790.sh
[WI-660][TI-1790] - [INFO] 2024-04-25 15:51:09.409 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 4321
[WI-0][TI-1790] - [INFO] 2024-04-25 15:51:10.396 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1790, success=true)
[WI-0][TI-1790] - [INFO] 2024-04-25 15:51:10.409 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1790)
[WI-0][TI-0] - [INFO] 2024-04-25 15:51:10.410 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 15:51:10.788 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7717717717717717 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:51:12.438 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-fb6a056c-fe1d-48ea-8d03-0b3cc04c7ae2;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
[WI-0][TI-0] - [INFO] 2024-04-25 15:51:12.821 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7728613569321534 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:51:13.440 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
[WI-0][TI-0] - [INFO] 2024-04-25 15:51:14.441 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...
[WI-0][TI-0] - [INFO] 2024-04-25 15:51:36.927 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.74 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:51:40.982 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7180232558139534 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:51:47.046 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7478991596638656 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [ERROR] 2024-04-25 15:56:29.756 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[279] - Parse var pool error
java.io.IOException: Stream closed
	at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:170)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:283)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.io.BufferedReader.readLine(BufferedReader.java:324)
	at java.io.BufferedReader.readLine(BufferedReader.java:389)
	at org.apache.dolphinscheduler.plugin.task.api.AbstractCommandExecutor.lambda$parseProcessOutput$1(AbstractCommandExecutor.java:273)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[WI-660][TI-1788] - [INFO] 2024-04-25 15:56:30.460 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has killed. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1788, processId:4159 ,exitStatusCode:137 ,processWaitForStatus:true ,processExitValue:137
[WI-660][TI-1788] - [INFO] 2024-04-25 15:56:30.461 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1788] - [INFO] 2024-04-25 15:56:30.461 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-660][TI-1788] - [INFO] 2024-04-25 15:56:30.461 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1788] - [INFO] 2024-04-25 15:56:30.461 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-660][TI-1788] - [INFO] 2024-04-25 15:56:30.465 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: KILL to master : 172.18.1.1:1234
[WI-660][TI-1788] - [INFO] 2024-04-25 15:56:30.466 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-660][TI-1788] - [INFO] 2024-04-25 15:56:30.466 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1788
[WI-660][TI-1788] - [INFO] 2024-04-25 15:56:30.467 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1788
[WI-660][TI-1788] - [INFO] 2024-04-25 15:56:30.467 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1788] - [INFO] 2024-04-25 15:56:31.357 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1788, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:58:50.600 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7008547008547008 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:58:51.611 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7741935483870968 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:58:52.613 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7499999999999999 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:58:58.631 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.728813559322034 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:58:58.915 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[55] - Receive TaskInstanceKillRequest: TaskInstanceKillRequest(taskInstanceId=1790)
[WI-0][TI-1790] - [ERROR] 2024-04-25 15:58:58.919 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[139] - kill task error
java.io.IOException: Cannot run program "pstree": error=2, No such file or directory
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.runCommand(AbstractShell.java:138)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.run(AbstractShell.java:118)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execute(ShellExecutor.java:125)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:103)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:86)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeShell(OSUtils.java:345)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeCmd(OSUtils.java:334)
	at org.apache.dolphinscheduler.plugin.task.api.utils.ProcessUtils.getPidsStr(ProcessUtils.java:129)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.killProcess(TaskInstanceKillOperationFunction.java:130)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.doKill(TaskInstanceKillOperationFunction.java:96)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.operate(TaskInstanceKillOperationFunction.java:69)
	at org.apache.dolphinscheduler.server.worker.rpc.TaskInstanceOperatorImpl.killTask(TaskInstanceOperatorImpl.java:49)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.dolphinscheduler.extract.base.server.ServerMethodInvokerImpl.invoke(ServerMethodInvokerImpl.java:41)
	at org.apache.dolphinscheduler.extract.base.server.JdkDynamicServerHandler.lambda$processReceived$0(JdkDynamicServerHandler.java:108)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: error=2, No such file or directory
	at java.lang.UNIXProcess.forkAndExec(Native Method)
	at java.lang.UNIXProcess.<init>(UNIXProcess.java:247)
	at java.lang.ProcessImpl.start(ProcessImpl.java:134)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	... 21 common frames omitted
[WI-0][TI-1790] - [INFO] 2024-04-25 15:58:58.920 +0800 o.a.d.p.t.a.u.ProcessUtils:[182] - Get appIds from worker 172.18.1.1:1234, taskLogPath: /opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1790.log
[WI-0][TI-1790] - [INFO] 2024-04-25 15:58:58.920 +0800 o.a.d.p.t.a.u.LogUtils:[79] - Start finding appId in /opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1790.log, fetch way: log 
[WI-0][TI-1790] - [INFO] 2024-04-25 15:58:58.921 +0800 o.a.d.p.t.a.u.ProcessUtils:[188] - The appId is empty
[WI-0][TI-1790] - [INFO] 2024-04-25 15:58:58.921 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[220] - Begin to kill process process, pid is : 4321
[WI-0][TI-0] - [INFO] 2024-04-25 15:58:59.638 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7988505747126436 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:59:00.643 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8045977011494253 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1790] - [INFO] 2024-04-25 15:59:03.930 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[225] - Success kill task: 660_1790, pid: 4321
[WI-0][TI-1790] - [INFO] 2024-04-25 15:59:03.932 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[119] - kill task by cancelApplication, taskInstanceId: 1790
[WI-0][TI-0] - [INFO] 2024-04-25 15:59:04.668 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7914285714285715 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:59:08.513 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1791, taskName=read kafka, firstSubmitTime=1714031948477, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=10, appIds=null, processInstanceId=661, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"input_topic","direct":"OUT","type":"VARCHAR","value":""},{"prop":"output_topic","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n","resourceList":[]}, environmentConfig=export JAVA_HOME=/opt/java/openjdk, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='read kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='661'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1791'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386664399040'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425155908'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.515 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: read kafka to wait queue success
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.515 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.518 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.518 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.518 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.518 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714031948518
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.519 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 661_1791
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.519 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1791,
  "taskName" : "read kafka",
  "firstSubmitTime" : 1714031948477,
  "startTime" : 1714031948518,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/10/661/1791.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 10,
  "processInstanceId" : 661,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"input_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"},{\"prop\":\"output_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\ntopics=\\\"reddit_post_processed_topic|reddit_post_sa_topic\\\"\\n\\n# parse the topics string into input and output topics\\nIFS='|' read -r input_topic output_topic <<< \\\"$topics\\\"\\n\\n# Print the input and output topics\\necho \\\"Input Topic: $input_topic\\\"\\necho \\\"Output Topic: $output_topic\\\"\\n\\necho \\\"#{setValue(input_topic=${input_topic})}\\\"\\necho \\\"#{setValue(output_topic=${output_topic})}\\\"\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export JAVA_HOME=/opt/java/openjdk",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "read kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "661"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1791"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386664399040"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425155908"
    }
  },
  "taskAppId" : "661_1791",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.519 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.519 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.519 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.524 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.525 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.526 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1791 check successfully
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.530 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.531 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.534 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.535 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.536 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "input_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  }, {
    "prop" : "output_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n",
  "resourceList" : [ ]
}
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.536 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.536 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.537 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.537 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.537 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.537 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.538 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.538 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export JAVA_HOME=/opt/java/openjdk
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

topics="reddit_post_processed_topic|reddit_post_sa_topic"

# parse the topics string into input and output topics
IFS='|' read -r input_topic output_topic <<< "$topics"

# Print the input and output topics
echo "Input Topic: $input_topic"
echo "Output Topic: $output_topic"

echo "#{setValue(input_topic=${input_topic})}"
echo "#{setValue(output_topic=${output_topic})}"

[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.538 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.538 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1791/661_1791.sh
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:08.564 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 4468
[WI-0][TI-1791] - [INFO] 2024-04-25 15:59:08.748 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1791, success=true)
[WI-0][TI-1791] - [INFO] 2024-04-25 15:59:08.755 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1791)
[WI-0][TI-0] - [INFO] 2024-04-25 15:59:09.573 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Input Topic: reddit_post_processed_topic
	Output Topic: reddit_post_sa_topic
	#{setValue(input_topic=reddit_post_processed_topic)}
	#{setValue(output_topic=reddit_post_sa_topic)}
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:09.576 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1791, processId:4468 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:09.577 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:09.577 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:09.578 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:09.578 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:09.609 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:09.611 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:09.614 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1791
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:09.615 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1791
[WI-661][TI-1791] - [INFO] 2024-04-25 15:59:09.616 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1791] - [INFO] 2024-04-25 15:59:09.746 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1791, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 15:59:09.845 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1792, taskName=sentiment anaysis, firstSubmitTime=1714031949822, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=10, appIds=null, processInstanceId=661, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1792'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425155909'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='661'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.847 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.847 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.849 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.850 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.850 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.850 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714031949850
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.850 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 661_1792
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.851 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1792,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714031949822,
  "startTime" : 1714031949850,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/10/661/1792.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 10,
  "processInstanceId" : 661,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1792"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425155909"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "661"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "661_1792",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.851 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.851 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.851 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.864 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.865 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.868 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792 check successfully
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.870 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.870 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.870 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.870 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.871 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.871 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.871 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.871 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.871 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.871 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.873 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.873 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.878 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792/py_661_1792.py
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.880 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.885 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.887 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.887 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792/py_661_1792.py
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.887 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.889 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792/661_1792.sh
[WI-661][TI-1792] - [INFO] 2024-04-25 15:59:09.912 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 4479
[WI-0][TI-0] - [INFO] 2024-04-25 15:59:09.913 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1792] - [INFO] 2024-04-25 15:59:10.748 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1792, success=true)
[WI-0][TI-1792] - [INFO] 2024-04-25 15:59:10.764 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1792)
[WI-0][TI-0] - [INFO] 2024-04-25 15:59:11.744 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7956329226394977 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:59:12.761 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7643835616438357 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 15:59:12.942 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[WI-0][TI-0] - [INFO] 2024-04-25 15:59:13.947 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-f8a2c4d4-c0e6-41d5-890b-c627e39f9f51;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
[WI-0][TI-0] - [INFO] 2024-04-25 15:59:14.949 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...
