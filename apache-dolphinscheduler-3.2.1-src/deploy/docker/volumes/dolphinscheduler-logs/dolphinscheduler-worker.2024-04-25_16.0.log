[WI-0][TI-0] - [INFO] 2024-04-25 16:00:22.072 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7632311977715878 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:00:28.123 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7203647416413373 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:00:38.256 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7543352601156069 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:00:39.318 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7951482479784368 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:00:56.421 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7694524495677233 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:00:57.447 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8160919540229885 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:00:58.461 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8583815028901733 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:00:59.471 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9222222222222223 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:01:10.547 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7633136094674556 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:02:11.549 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[55] - Receive TaskInstanceKillRequest: TaskInstanceKillRequest(taskInstanceId=1792)
[WI-0][TI-1792] - [ERROR] 2024-04-25 16:02:11.562 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[139] - kill task error
java.io.IOException: Cannot run program "pstree": error=2, No such file or directory
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.runCommand(AbstractShell.java:138)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.run(AbstractShell.java:118)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execute(ShellExecutor.java:125)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:103)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:86)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeShell(OSUtils.java:345)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeCmd(OSUtils.java:334)
	at org.apache.dolphinscheduler.plugin.task.api.utils.ProcessUtils.getPidsStr(ProcessUtils.java:129)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.killProcess(TaskInstanceKillOperationFunction.java:130)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.doKill(TaskInstanceKillOperationFunction.java:96)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.operate(TaskInstanceKillOperationFunction.java:69)
	at org.apache.dolphinscheduler.server.worker.rpc.TaskInstanceOperatorImpl.killTask(TaskInstanceOperatorImpl.java:49)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.dolphinscheduler.extract.base.server.ServerMethodInvokerImpl.invoke(ServerMethodInvokerImpl.java:41)
	at org.apache.dolphinscheduler.extract.base.server.JdkDynamicServerHandler.lambda$processReceived$0(JdkDynamicServerHandler.java:108)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: error=2, No such file or directory
	at java.lang.UNIXProcess.forkAndExec(Native Method)
	at java.lang.UNIXProcess.<init>(UNIXProcess.java:247)
	at java.lang.ProcessImpl.start(ProcessImpl.java:134)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	... 21 common frames omitted
[WI-0][TI-1792] - [INFO] 2024-04-25 16:02:11.563 +0800 o.a.d.p.t.a.u.ProcessUtils:[182] - Get appIds from worker 172.18.1.1:1234, taskLogPath: /opt/dolphinscheduler/logs/20240425/13386218435520/10/661/1792.log
[WI-0][TI-1792] - [INFO] 2024-04-25 16:02:11.563 +0800 o.a.d.p.t.a.u.LogUtils:[79] - Start finding appId in /opt/dolphinscheduler/logs/20240425/13386218435520/10/661/1792.log, fetch way: log 
[WI-0][TI-1792] - [INFO] 2024-04-25 16:02:11.564 +0800 o.a.d.p.t.a.u.ProcessUtils:[188] - The appId is empty
[WI-0][TI-1792] - [INFO] 2024-04-25 16:02:11.564 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[220] - Begin to kill process process, pid is : 4479
[WI-0][TI-0] - [INFO] 2024-04-25 16:02:12.834 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7398843930635839 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:02:14.890 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.72 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:02:15.931 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7768817204301075 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1792] - [INFO] 2024-04-25 16:02:16.567 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[225] - Success kill task: 661_1792, pid: 4479
[WI-0][TI-1792] - [INFO] 2024-04-25 16:02:16.568 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[119] - kill task by cancelApplication, taskInstanceId: 1792
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:07.177 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7188405797101449 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:08.186 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8176638176638177 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:13.218 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7262247838616714 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:14.251 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7394957983193278 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:15.253 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8592814371257484 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:16.254 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.747945205479452 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:18.294 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7035928143712575 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:19.311 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7317073170731707 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:20.489 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1793, taskName=read kafka, firstSubmitTime=1714032200476, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=11, appIds=null, processInstanceId=662, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"input_topic","direct":"OUT","type":"VARCHAR","value":""},{"prop":"output_topic","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n","resourceList":[]}, environmentConfig=export JAVA_HOME=/opt/java/openjdk, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='read kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='662'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1793'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386664399040'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425160320'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.495 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: read kafka to wait queue success
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.496 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.497 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.497 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.497 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.498 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714032200498
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.498 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 662_1793
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.529 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1793,
  "taskName" : "read kafka",
  "firstSubmitTime" : 1714032200476,
  "startTime" : 1714032200498,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/11/662/1793.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 11,
  "processInstanceId" : 662,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"input_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"},{\"prop\":\"output_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\ntopics=\\\"reddit_post_processed_topic|reddit_post_sa_topic\\\"\\n\\n# parse the topics string into input and output topics\\nIFS='|' read -r input_topic output_topic <<< \\\"$topics\\\"\\n\\n# Print the input and output topics\\necho \\\"Input Topic: $input_topic\\\"\\necho \\\"Output Topic: $output_topic\\\"\\n\\necho \\\"#{setValue(input_topic=${input_topic})}\\\"\\necho \\\"#{setValue(output_topic=${output_topic})}\\\"\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export JAVA_HOME=/opt/java/openjdk",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "read kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "662"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1793"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386664399040"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425160320"
    }
  },
  "taskAppId" : "662_1793",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.531 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.534 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.544 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.555 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.567 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.571 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_11/662/1793 check successfully
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.572 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.572 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.573 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.573 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.573 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "input_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  }, {
    "prop" : "output_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n",
  "resourceList" : [ ]
}
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.573 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.573 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.573 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.573 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.573 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.575 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.575 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.575 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export JAVA_HOME=/opt/java/openjdk
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

topics="reddit_post_processed_topic|reddit_post_sa_topic"

# parse the topics string into input and output topics
IFS='|' read -r input_topic output_topic <<< "$topics"

# Print the input and output topics
echo "Input Topic: $input_topic"
echo "Output Topic: $output_topic"

echo "#{setValue(input_topic=${input_topic})}"
echo "#{setValue(output_topic=${output_topic})}"

[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.575 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.575 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_11/662/1793/662_1793.sh
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:20.588 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 4572
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:20.603 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1793] - [INFO] 2024-04-25 16:03:21.151 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1793, success=true)
[WI-0][TI-1793] - [INFO] 2024-04-25 16:03:21.158 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1793)
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:21.618 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Input Topic: reddit_post_processed_topic
	Output Topic: reddit_post_sa_topic
	#{setValue(input_topic=reddit_post_processed_topic)}
	#{setValue(output_topic=reddit_post_sa_topic)}
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:21.619 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_11/662/1793, processId:4572 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:21.622 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:21.622 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:21.622 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:21.622 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:21.638 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:21.638 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:21.638 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_11/662/1793
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:21.639 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_11/662/1793
[WI-662][TI-1793] - [INFO] 2024-04-25 16:03:21.639 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1793] - [INFO] 2024-04-25 16:03:22.142 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1793, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:22.217 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1794, taskName=sentiment anaysis, firstSubmitTime=1714032202210, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=11, appIds=null, processInstanceId=662, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1794'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425160322'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='662'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.219 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.219 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.220 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.220 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.220 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.220 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714032202220
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.220 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 662_1794
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.220 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1794,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714032202210,
  "startTime" : 1714032202220,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/11/662/1794.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 11,
  "processInstanceId" : 662,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1794"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425160322"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "662"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "662_1794",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.221 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.221 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.221 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.223 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.224 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.224 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_11/662/1794 check successfully
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.224 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.225 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.225 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.225 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.225 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.225 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.226 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.226 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.226 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.226 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.226 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.227 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_11/662/1794
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.227 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_11/662/1794/py_662_1794.py
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.227 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.227 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.228 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.228 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_11/662/1794/py_662_1794.py
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.228 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.228 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_11/662/1794/662_1794.sh
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:22.239 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 4583
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:22.444 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7091183177484275 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1794] - [INFO] 2024-04-25 16:03:23.145 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1794, success=true)
[WI-0][TI-1794] - [INFO] 2024-04-25 16:03:23.161 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1794)
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:23.243 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:23.506 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8275078312814161 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:24.507 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9484240687679083 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:25.509 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8328445747800587 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:26.510 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8556701030927836 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:27.267 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-278cfc51-3b93-4632-b987-a28de9e208e5;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:27.516 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9357798165137614 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:28.269 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:28.527 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9500260281103592 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:29.531 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8722044728434505 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:30.330 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:30.575 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8600000000000001 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:31.610 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8473684210526317 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:32.611 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7771739130434783 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:34.340 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (6085ms)
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:34.633 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7947976878612718 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:35.349 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...
		[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (408ms)
	:: resolution report :: resolve 1345ms :: artifacts dl 6550ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   12  |   0   |   0   |   0   ||   12  |   2   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-278cfc51-3b93-4632-b987-a28de9e208e5
		confs: [default]
		12 artifacts copied, 0 already retrieved (57876kB/515ms)
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:35.653 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8798798798798799 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:36.355 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:03:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:36.655 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9104938271604938 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:37.362 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:37.672 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9006024096385542 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:38.753 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7972972972972974 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:42.374 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:48.393 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:49.395 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:51.400 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 1:>                                                          (0 + 1) / 1]
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:52.402 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 2:>                                                          (0 + 1) / 1]
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:53.403 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	|         content|             context|           datetime|     id|score|subreddit|type|                 url|           user|negative|neutral|positive|compound|
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	|singapore is bad|21apr2024 orchard...|2024-04-21 22:10:42|1c9itat|  131|singapore|post|https://www.reddi...|premiumplatinum|   0.636|  0.364|     0.0| -0.5423|
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_11/662/1794/py_662_1794.py", line 79, in <module>
	    .save()
	     ^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py", line 1461, in save
	    self._jwrite.save()
	  File "/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
	    return f(*a, **kw)
	           ^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/py4j/protocol.py", line 326, in get_return_value
	    raise Py4JJavaError(
	py4j.protocol.Py4JJavaError: An error occurred while calling o100.save.
	: java.lang.NoClassDefFoundError: scala/$less$colon$less
		at org.apache.spark.sql.kafka010.KafkaSourceProvider.createRelation(KafkaSourceProvider.scala:180)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
		at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
		at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
		at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
		at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
		at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:282)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: java.lang.ClassNotFoundException: scala.$less$colon$less
		at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
		at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
		at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
		... 42 more
	
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:53.404 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_11/662/1794, processId:4583 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:53.407 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:53.408 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:53.408 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:53.409 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:53.411 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:53.412 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:53.413 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_11/662/1794
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:53.414 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_11/662/1794
[WI-662][TI-1794] - [INFO] 2024-04-25 16:03:53.415 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1794] - [INFO] 2024-04-25 16:03:54.288 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1794, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:54.397 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1795, taskName=sentiment anaysis, firstSubmitTime=1714032234388, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=11, appIds=null, processInstanceId=662, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1795'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425160354'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='662'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.398 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.398 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.400 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.400 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714032234401
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 662_1795
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1795,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714032234388,
  "startTime" : 1714032234401,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/11/662/1795.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 11,
  "processInstanceId" : 662,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1795"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425160354"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "662"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "662_1795",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.402 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.402 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.402 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.407 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.408 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.408 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_11/662/1795 check successfully
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.409 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.409 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.409 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.409 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.409 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.410 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.410 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.410 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.410 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.410 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.410 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.411 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_11/662/1795
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.411 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_11/662/1795/py_662_1795.py
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.411 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.412 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.412 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.412 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_11/662/1795/py_662_1795.py
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.412 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.413 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_11/662/1795/662_1795.sh
[WI-662][TI-1795] - [INFO] 2024-04-25 16:03:54.416 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 4819
[WI-0][TI-1795] - [INFO] 2024-04-25 16:03:55.288 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1795, success=true)
[WI-0][TI-1795] - [INFO] 2024-04-25 16:03:55.299 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1795)
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:55.416 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:57.419 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-f20b14d3-0517-4247-bee0-d244c02e3b73;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 428ms :: artifacts dl 11ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-f20b14d3-0517-4247-bee0-d244c02e3b73
		confs: [default]
		2 artifacts copied, 10 already retrieved (48382kB/179ms)
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:57.819 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8311688311688312 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:03:58.422 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:03:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-04-25 16:04:00.869 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8353658536585366 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:04:02.424 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-04-25 16:04:06.913 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8456375838926176 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:04:07.924 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.852112676056338 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:04:08.928 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7380191693290735 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:04:09.442 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-25 16:04:10.447 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 16:04:11.986 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7028753993610224 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:04:12.473 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 1:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-25 16:04:13.475 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 16:04:14.478 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	|         content|             context|           datetime|     id|score|subreddit|type|                 url|           user|negative|neutral|positive|compound|
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	|singapore is bad|21apr2024 orchard...|2024-04-21 22:10:42|1c9itat|  131|singapore|post|https://www.reddi...|premiumplatinum|   0.636|  0.364|     0.0| -0.5423|
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_11/662/1795/py_662_1795.py", line 79, in <module>
	    .save()
	     ^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py", line 1461, in save
	    self._jwrite.save()
	  File "/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
	    return f(*a, **kw)
	           ^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/py4j/protocol.py", line 326, in get_return_value
	    raise Py4JJavaError(
	py4j.protocol.Py4JJavaError: An error occurred while calling o100.save.
	: java.lang.NoClassDefFoundError: scala/$less$colon$less
		at org.apache.spark.sql.kafka010.KafkaSourceProvider.createRelation(KafkaSourceProvider.scala:180)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
		at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
		at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
		at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
		at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
		at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:282)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: java.lang.ClassNotFoundException: scala.$less$colon$less
		at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
		at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
		at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
		... 42 more
	
[WI-662][TI-1795] - [INFO] 2024-04-25 16:04:15.480 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_11/662/1795, processId:4819 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-662][TI-1795] - [INFO] 2024-04-25 16:04:15.491 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-662][TI-1795] - [INFO] 2024-04-25 16:04:15.491 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-662][TI-1795] - [INFO] 2024-04-25 16:04:15.491 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-662][TI-1795] - [INFO] 2024-04-25 16:04:15.492 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-662][TI-1795] - [INFO] 2024-04-25 16:04:15.494 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-662][TI-1795] - [INFO] 2024-04-25 16:04:15.495 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-662][TI-1795] - [INFO] 2024-04-25 16:04:15.495 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_11/662/1795
[WI-662][TI-1795] - [INFO] 2024-04-25 16:04:15.496 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_11/662/1795
[WI-662][TI-1795] - [INFO] 2024-04-25 16:04:15.496 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1795] - [INFO] 2024-04-25 16:04:16.336 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1795, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:04:21.016 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7603550295857988 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [ERROR] 2024-04-25 16:04:28.379 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[279] - Parse var pool error
java.io.IOException: Stream closed
	at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:170)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:283)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.io.BufferedReader.readLine(BufferedReader.java:324)
	at java.io.BufferedReader.readLine(BufferedReader.java:389)
	at org.apache.dolphinscheduler.plugin.task.api.AbstractCommandExecutor.lambda$parseProcessOutput$1(AbstractCommandExecutor.java:273)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[WI-660][TI-1790] - [INFO] 2024-04-25 16:04:29.159 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has killed. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790, processId:4321 ,exitStatusCode:137 ,processWaitForStatus:true ,processExitValue:137
[WI-660][TI-1790] - [INFO] 2024-04-25 16:04:29.168 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1790] - [INFO] 2024-04-25 16:04:29.169 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-660][TI-1790] - [INFO] 2024-04-25 16:04:29.171 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-660][TI-1790] - [INFO] 2024-04-25 16:04:29.173 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-660][TI-1790] - [INFO] 2024-04-25 16:04:29.181 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: KILL to master : 172.18.1.1:1234
[WI-660][TI-1790] - [INFO] 2024-04-25 16:04:29.181 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-660][TI-1790] - [INFO] 2024-04-25 16:04:29.181 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790
[WI-660][TI-1790] - [INFO] 2024-04-25 16:04:29.181 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790
[WI-660][TI-1790] - [INFO] 2024-04-25 16:04:29.181 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-04-25 16:04:44.139 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7041420118343196 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:04:45.151 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7079646017699115 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:21.371 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7072463768115942 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:23.393 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7661971830985915 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:27.160 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1798, taskName=read kafka, firstSubmitTime=1714032327152, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=12, appIds=null, processInstanceId=663, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"input_topic","direct":"OUT","type":"VARCHAR","value":""},{"prop":"output_topic","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n","resourceList":[]}, environmentConfig=export JAVA_HOME=/opt/java/openjdk, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='read kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='663'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1798'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386664399040'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425160527'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.162 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: read kafka to wait queue success
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.163 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.164 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.164 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.164 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.164 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714032327164
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.164 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 663_1798
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.165 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1798,
  "taskName" : "read kafka",
  "firstSubmitTime" : 1714032327152,
  "startTime" : 1714032327164,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/12/663/1798.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 12,
  "processInstanceId" : 663,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"input_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"},{\"prop\":\"output_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\ntopics=\\\"reddit_post_processed_topic|reddit_post_sa_topic\\\"\\n\\n# parse the topics string into input and output topics\\nIFS='|' read -r input_topic output_topic <<< \\\"$topics\\\"\\n\\n# Print the input and output topics\\necho \\\"Input Topic: $input_topic\\\"\\necho \\\"Output Topic: $output_topic\\\"\\n\\necho \\\"#{setValue(input_topic=${input_topic})}\\\"\\necho \\\"#{setValue(output_topic=${output_topic})}\\\"\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export JAVA_HOME=/opt/java/openjdk",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "read kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "663"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1798"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386664399040"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425160527"
    }
  },
  "taskAppId" : "663_1798",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.165 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.167 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.167 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.178 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.180 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.180 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_12/663/1798 check successfully
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.181 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.181 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.181 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.182 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.182 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "input_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  }, {
    "prop" : "output_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n",
  "resourceList" : [ ]
}
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.182 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.183 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.183 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.183 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.183 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.184 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.184 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.184 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export JAVA_HOME=/opt/java/openjdk
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

topics="reddit_post_processed_topic|reddit_post_sa_topic"

# parse the topics string into input and output topics
IFS='|' read -r input_topic output_topic <<< "$topics"

# Print the input and output topics
echo "Input Topic: $input_topic"
echo "Output Topic: $output_topic"

echo "#{setValue(input_topic=${input_topic})}"
echo "#{setValue(output_topic=${output_topic})}"

[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.184 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.184 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_12/663/1798/663_1798.sh
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:27.195 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 5055
[WI-0][TI-1798] - [INFO] 2024-04-25 16:05:27.531 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1798, success=true)
[WI-0][TI-1798] - [INFO] 2024-04-25 16:05:27.540 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1798)
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:28.196 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Input Topic: reddit_post_processed_topic
	Output Topic: reddit_post_sa_topic
	#{setValue(input_topic=reddit_post_processed_topic)}
	#{setValue(output_topic=reddit_post_sa_topic)}
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:28.206 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_12/663/1798, processId:5055 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:28.218 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:28.218 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:28.218 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:28.218 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:28.223 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:28.224 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:28.227 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_12/663/1798
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:28.234 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_12/663/1798
[WI-663][TI-1798] - [INFO] 2024-04-25 16:05:28.234 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:28.439 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7085714285714286 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1798] - [INFO] 2024-04-25 16:05:28.513 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1798, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:28.640 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1799, taskName=sentiment anaysis, firstSubmitTime=1714032328631, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=12, appIds=null, processInstanceId=663, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1799'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425160528'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='663'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.641 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.641 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.642 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.643 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.643 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.643 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714032328643
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.643 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 663_1799
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.643 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1799,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714032328631,
  "startTime" : 1714032328643,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/12/663/1799.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 12,
  "processInstanceId" : 663,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1799"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425160528"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "663"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "663_1799",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.643 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.644 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.644 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.646 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.647 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.647 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_12/663/1799 check successfully
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.647 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.647 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.647 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.648 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.648 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.648 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.648 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.648 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.648 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.648 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.648 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.649 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_12/663/1799
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.649 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_12/663/1799/py_663_1799.py
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.649 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.650 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.651 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.651 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_12/663/1799/py_663_1799.py
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.651 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.651 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_12/663/1799/663_1799.sh
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:28.664 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 5065
[WI-0][TI-1799] - [INFO] 2024-04-25 16:05:29.525 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1799, success=true)
[WI-0][TI-1799] - [INFO] 2024-04-25 16:05:29.546 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1799)
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:29.664 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:30.477 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8555555555555556 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:31.499 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8543956043956045 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:32.500 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7134986225895317 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [ERROR] 2024-04-25 16:05:32.516 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[279] - Parse var pool error
java.io.IOException: Stream closed
	at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:170)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:283)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.io.BufferedReader.readLine(BufferedReader.java:324)
	at java.io.BufferedReader.readLine(BufferedReader.java:389)
	at org.apache.dolphinscheduler.plugin.task.api.AbstractCommandExecutor.lambda$parseProcessOutput$1(AbstractCommandExecutor.java:273)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:32.669 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-f00f0da3-67a6-4be2-8abc-c2c1a375362b;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central
[WI-661][TI-1792] - [INFO] 2024-04-25 16:05:32.755 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has killed. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792, processId:4479 ,exitStatusCode:137 ,processWaitForStatus:true ,processExitValue:137
[WI-661][TI-1792] - [INFO] 2024-04-25 16:05:32.756 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-661][TI-1792] - [INFO] 2024-04-25 16:05:32.756 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-661][TI-1792] - [INFO] 2024-04-25 16:05:32.757 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-661][TI-1792] - [INFO] 2024-04-25 16:05:32.760 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-661][TI-1792] - [INFO] 2024-04-25 16:05:32.770 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: KILL to master : 172.18.1.1:1234
[WI-661][TI-1792] - [INFO] 2024-04-25 16:05:32.770 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-661][TI-1792] - [INFO] 2024-04-25 16:05:32.770 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792
[WI-661][TI-1792] - [INFO] 2024-04-25 16:05:32.770 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792
[WI-661][TI-1792] - [INFO] 2024-04-25 16:05:32.771 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:33.503 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8365384615384615 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:33.675 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 677ms :: artifacts dl 23ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-f00f0da3-67a6-4be2-8abc-c2c1a375362b
		confs: [default]
		2 artifacts copied, 10 already retrieved (48382kB/367ms)
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:34.678 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:05:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:35.525 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.764516807704895 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:36.556 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7962382445141065 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:37.558 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7520215633423181 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:39.689 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:44.711 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:>                                                          (0 + 1) / 1]
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:46.971 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 1:>                                                          (0 + 1) / 1]
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:47.989 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	|         content|             context|           datetime|     id|score|subreddit|type|                 url|           user|negative|neutral|positive|compound|
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	|singapore is bad|21apr2024 orchard...|2024-04-21 22:10:42|1c9itat|  131|singapore|post|https://www.reddi...|premiumplatinum|   0.636|  0.364|     0.0| -0.5423|
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_12/663/1799/py_663_1799.py", line 79, in <module>
	    .save()
	     ^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py", line 1461, in save
	    self._jwrite.save()
	  File "/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
	    return f(*a, **kw)
	           ^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/py4j/protocol.py", line 326, in get_return_value
	    raise Py4JJavaError(
	py4j.protocol.Py4JJavaError: An error occurred while calling o100.save.
	: java.lang.NoClassDefFoundError: scala/$less$colon$less
		at org.apache.spark.sql.kafka010.KafkaSourceProvider.createRelation(KafkaSourceProvider.scala:180)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
		at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
		at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
		at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
		at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
		at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:282)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: java.lang.ClassNotFoundException: scala.$less$colon$less
		at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
		at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
		at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
		... 42 more
	
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:48.992 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_12/663/1799, processId:5065 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:48.994 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:48.994 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:48.995 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:48.995 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:48.997 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:48.998 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:48.998 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_12/663/1799
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:48.999 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_12/663/1799
[WI-663][TI-1799] - [INFO] 2024-04-25 16:05:49.000 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1799] - [INFO] 2024-04-25 16:05:49.565 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1799, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:49.621 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1800, taskName=sentiment anaysis, firstSubmitTime=1714032349610, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=12, appIds=null, processInstanceId=663, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1800'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425160549'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='663'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.622 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.622 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.624 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.624 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.624 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.624 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714032349624
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.624 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 663_1800
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.624 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1800,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714032349610,
  "startTime" : 1714032349624,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/12/663/1800.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 12,
  "processInstanceId" : 663,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1800"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425160549"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "663"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "663_1800",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.625 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.625 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.625 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.627 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.628 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.629 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_12/663/1800 check successfully
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.629 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.629 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.630 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.630 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.631 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.631 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.631 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.632 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.632 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.632 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.632 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.633 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_12/663/1800
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.633 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_12/663/1800/py_663_1800.py
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.633 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.634 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.634 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.640 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_12/663/1800/py_663_1800.py
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.640 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.641 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_12/663/1800/663_1800.sh
[WI-663][TI-1800] - [INFO] 2024-04-25 16:05:49.643 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 5278
[WI-0][TI-1800] - [INFO] 2024-04-25 16:05:50.569 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1800, success=true)
[WI-0][TI-1800] - [INFO] 2024-04-25 16:05:50.576 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1800)
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:50.645 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:51.652 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:52.653 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-236212f1-945d-4192-b440-d31308daa439;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 564ms :: artifacts dl 9ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-236212f1-945d-4192-b440-d31308daa439
		confs: [default]
		0 artifacts copied, 12 already retrieved (0kB/17ms)
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:52.998 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8705882352941177 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:53.685 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:05:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-04-25 16:05:57.689 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-04-25 16:06:03.083 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7730061349693251 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:06:04.774 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-25 16:06:05.776 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 16:06:06.259 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8646864686468647 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:06:07.279 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8727810650887574 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:06:08.281 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8095238095238095 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:06:08.804 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 1:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-25 16:06:09.299 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8690476190476191 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:06:09.811 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 16:06:10.346 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9403409090909091 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:06:10.870 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 2:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-25 16:06:11.347 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8213058419243986 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:06:11.881 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	|         content|             context|           datetime|     id|score|subreddit|type|                 url|           user|negative|neutral|positive|compound|
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	|singapore is bad|21apr2024 orchard...|2024-04-21 22:10:42|1c9itat|  131|singapore|post|https://www.reddi...|premiumplatinum|   0.636|  0.364|     0.0| -0.5423|
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_12/663/1800/py_663_1800.py", line 79, in <module>
	    .save()
	     ^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py", line 1461, in save
	    self._jwrite.save()
	  File "/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
	    return f(*a, **kw)
	           ^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/py4j/protocol.py", line 326, in get_return_value
	    raise Py4JJavaError(
	py4j.protocol.Py4JJavaError: An error occurred while calling o100.save.
	: java.lang.NoClassDefFoundError: scala/$less$colon$less
		at org.apache.spark.sql.kafka010.KafkaSourceProvider.createRelation(KafkaSourceProvider.scala:180)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
		at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
		at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
		at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
		at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
		at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:282)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: java.lang.ClassNotFoundException: scala.$less$colon$less
		at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
		at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
		at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
		... 42 more
	
[WI-663][TI-1800] - [INFO] 2024-04-25 16:06:12.901 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_12/663/1800, processId:5278 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-663][TI-1800] - [INFO] 2024-04-25 16:06:12.902 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-663][TI-1800] - [INFO] 2024-04-25 16:06:12.902 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-663][TI-1800] - [INFO] 2024-04-25 16:06:12.902 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-663][TI-1800] - [INFO] 2024-04-25 16:06:12.902 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-663][TI-1800] - [INFO] 2024-04-25 16:06:12.904 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-663][TI-1800] - [INFO] 2024-04-25 16:06:12.905 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-663][TI-1800] - [INFO] 2024-04-25 16:06:12.905 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_12/663/1800
[WI-663][TI-1800] - [INFO] 2024-04-25 16:06:12.905 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_12/663/1800
[WI-663][TI-1800] - [INFO] 2024-04-25 16:06:12.905 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1800] - [INFO] 2024-04-25 16:06:13.693 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1800, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:06:19.372 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7146892655367232 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:06:23.440 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8222222222222222 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:06:24.452 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7983425414364641 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:06:25.453 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7966573816155988 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:06:28.468 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7698863636363635 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:04.729 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7267605633802816 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:29.809 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7734806629834254 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:34.836 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8160919540229885 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:39.928 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1803, taskName=read kafka, firstSubmitTime=1714032459918, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=13, appIds=null, processInstanceId=664, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"input_topic","direct":"OUT","type":"VARCHAR","value":""},{"prop":"output_topic","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n","resourceList":[]}, environmentConfig=export JAVA_HOME=/opt/java/openjdk, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='read kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='664'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1803'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386664399040'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425160739'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.929 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: read kafka to wait queue success
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.929 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.948 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.948 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.949 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.949 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714032459949
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.949 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 664_1803
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.949 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1803,
  "taskName" : "read kafka",
  "firstSubmitTime" : 1714032459918,
  "startTime" : 1714032459949,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/13/664/1803.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 13,
  "processInstanceId" : 664,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"input_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"},{\"prop\":\"output_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\ntopics=\\\"reddit_post_processed_topic|reddit_post_sa_topic\\\"\\n\\n# parse the topics string into input and output topics\\nIFS='|' read -r input_topic output_topic <<< \\\"$topics\\\"\\n\\n# Print the input and output topics\\necho \\\"Input Topic: $input_topic\\\"\\necho \\\"Output Topic: $output_topic\\\"\\n\\necho \\\"#{setValue(input_topic=${input_topic})}\\\"\\necho \\\"#{setValue(output_topic=${output_topic})}\\\"\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export JAVA_HOME=/opt/java/openjdk",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "read kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "664"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1803"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386664399040"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425160739"
    }
  },
  "taskAppId" : "664_1803",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.949 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.950 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.950 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.960 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.960 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.965 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1803 check successfully
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.965 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.965 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.966 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.966 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.967 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "input_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  }, {
    "prop" : "output_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n",
  "resourceList" : [ ]
}
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.967 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.967 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.967 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.967 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.968 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.968 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.973 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.973 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export JAVA_HOME=/opt/java/openjdk
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

topics="reddit_post_processed_topic|reddit_post_sa_topic"

# parse the topics string into input and output topics
IFS='|' read -r input_topic output_topic <<< "$topics"

# Print the input and output topics
echo "Input Topic: $input_topic"
echo "Output Topic: $output_topic"

echo "#{setValue(input_topic=${input_topic})}"
echo "#{setValue(output_topic=${output_topic})}"

[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.974 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.974 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1803/664_1803.sh
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:39.988 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 5517
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:39.993 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1803] - [INFO] 2024-04-25 16:07:40.787 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1803, success=true)
[WI-0][TI-1803] - [INFO] 2024-04-25 16:07:40.795 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1803)
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:40.856 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7092651757188497 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:41.009 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Input Topic: reddit_post_processed_topic
	Output Topic: reddit_post_sa_topic
	#{setValue(input_topic=reddit_post_processed_topic)}
	#{setValue(output_topic=reddit_post_sa_topic)}
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:41.010 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1803, processId:5517 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:41.011 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:41.012 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:41.014 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:41.015 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:41.047 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:41.051 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:41.053 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1803
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:41.054 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1803
[WI-664][TI-1803] - [INFO] 2024-04-25 16:07:41.055 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1803] - [INFO] 2024-04-25 16:07:41.787 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1803, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:41.843 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1804, taskName=sentiment anaysis, firstSubmitTime=1714032461835, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=13, appIds=null, processInstanceId=664, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1804'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425160741'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='664'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.844 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.845 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.848 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.848 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.849 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.849 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714032461849
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.850 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 664_1804
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.850 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1804,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714032461835,
  "startTime" : 1714032461849,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/13/664/1804.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 13,
  "processInstanceId" : 664,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1804"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425160741"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "664"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "664_1804",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.852 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.852 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.853 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.858 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.859 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.860 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1804 check successfully
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.861 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.862 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.863 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.863 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.864 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.866 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.869 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.870 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.870 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.870 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.871 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.873 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1804
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.874 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1804/py_664_1804.py
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.875 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.878 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.879 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.879 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1804/py_664_1804.py
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.881 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.881 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1804/664_1804.sh
[WI-664][TI-1804] - [INFO] 2024-04-25 16:07:41.885 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 5528
[WI-0][TI-1804] - [INFO] 2024-04-25 16:07:42.839 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1804, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:42.890 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1804] - [INFO] 2024-04-25 16:07:42.894 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1804)
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:43.886 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8596491228070176 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:45.913 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-64993bd6-830a-483b-b32a-2d18a7574143;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:45.947 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8179190751445087 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:46.915 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 1049ms :: artifacts dl 80ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-64993bd6-830a-483b-b32a-2d18a7574143
		confs: [default]
		0 artifacts copied, 12 already retrieved (0kB/21ms)
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:46.970 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8782608695652173 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:47.916 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:07:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:47.972 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.896551724137931 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:48.938 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:48.976 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9125364431486881 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:49.979 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8668831168831168 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:50.991 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9541284403669725 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:52.004 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7035928143712575 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:53.007 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7232142857142857 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:55.030 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8736462093862816 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:56.035 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:56.064 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9253731343283581 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:57.075 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9259259259259259 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:07:59.092 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8197674418604651 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:04.081 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:05.084 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:06.087 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 1:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:07.090 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:07.160 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7155963302752293 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:08.092 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	|         content|             context|           datetime|     id|score|subreddit|type|                 url|           user|negative|neutral|positive|compound|
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	|singapore is bad|21apr2024 orchard...|2024-04-21 22:10:42|1c9itat|  131|singapore|post|https://www.reddi...|premiumplatinum|   0.636|  0.364|     0.0| -0.5423|
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1804/py_664_1804.py", line 79, in <module>
	    .save()
	     ^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py", line 1461, in save
	    self._jwrite.save()
	  File "/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
	    return f(*a, **kw)
	           ^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/py4j/protocol.py", line 326, in get_return_value
	    raise Py4JJavaError(
	py4j.protocol.Py4JJavaError: An error occurred while calling o100.save.
	: java.lang.NoClassDefFoundError: scala/$less$colon$less
		at org.apache.spark.sql.kafka010.KafkaSourceProvider.createRelation(KafkaSourceProvider.scala:180)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
		at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
		at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
		at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
		at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
		at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:282)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: java.lang.ClassNotFoundException: scala.$less$colon$less
		at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
		at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
		at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
		... 42 more
	
[WI-0][TI-1792] - [INFO] 2024-04-25 16:08:08.854 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1792, processInstanceId=661, status=9, startTime=1714031949850, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/661/1792.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792, endTime=1714032332757, processId=4479, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1792] - [INFO] 2024-04-25 16:08:08.857 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1792, processInstanceId=661, status=9, startTime=1714031949850, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/661/1792.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792, endTime=1714032332757, processId=4479, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714032488854)
[WI-0][TI-1790] - [INFO] 2024-04-25 16:08:08.857 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1790, processInstanceId=660, status=9, startTime=1714031469392, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1790.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790, endTime=1714032269172, processId=4321, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1790] - [INFO] 2024-04-25 16:08:08.860 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1790, processInstanceId=660, status=9, startTime=1714031469392, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1790.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790, endTime=1714032269172, processId=4321, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714032488854)
[WI-664][TI-1804] - [INFO] 2024-04-25 16:08:09.608 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1804, processId:5528 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-664][TI-1804] - [INFO] 2024-04-25 16:08:09.610 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1804] - [INFO] 2024-04-25 16:08:09.611 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-664][TI-1804] - [INFO] 2024-04-25 16:08:09.612 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1804] - [INFO] 2024-04-25 16:08:09.614 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-664][TI-1804] - [INFO] 2024-04-25 16:08:09.630 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-664][TI-1804] - [INFO] 2024-04-25 16:08:09.630 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-664][TI-1804] - [INFO] 2024-04-25 16:08:09.631 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1804
[WI-664][TI-1804] - [INFO] 2024-04-25 16:08:09.631 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1804
[WI-664][TI-1804] - [INFO] 2024-04-25 16:08:09.632 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1804] - [INFO] 2024-04-25 16:08:09.865 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1804, processInstanceId=664, status=6, startTime=1714032461849, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/13/664/1804.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1804, endTime=1714032489613, processId=5528, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1804] - [INFO] 2024-04-25 16:08:09.871 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1804, processInstanceId=664, status=6, startTime=1714032461849, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/13/664/1804.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1804, endTime=1714032489613, processId=5528, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714032489863)
[WI-0][TI-1804] - [INFO] 2024-04-25 16:08:10.603 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1804, success=true)
[WI-0][TI-1804] - [INFO] 2024-04-25 16:08:10.606 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1804, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:10.640 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1805, taskName=sentiment anaysis, firstSubmitTime=1714032490631, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=13, appIds=null, processInstanceId=664, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1805'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425160810'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='664'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.642 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.647 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.650 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.650 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.650 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.650 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714032490650
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.650 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 664_1805
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.650 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1805,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714032490631,
  "startTime" : 1714032490650,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/13/664/1805.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 13,
  "processInstanceId" : 664,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1805"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425160810"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "664"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "664_1805",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.651 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.651 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.651 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.654 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.654 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.655 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1805 check successfully
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.656 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.656 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.656 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.657 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.657 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.658 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.658 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.658 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.659 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.659 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.659 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.661 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1805
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.661 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1805/py_664_1805.py
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.661 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.662 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.662 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.662 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1805/py_664_1805.py
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.662 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.662 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1805/664_1805.sh
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:10.665 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 5746
[WI-0][TI-1805] - [INFO] 2024-04-25 16:08:10.873 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1805, processInstanceId=664, startTime=1714032490650, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/13/664/1805.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1805] - [INFO] 2024-04-25 16:08:10.876 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1805, processInstanceId=664, startTime=1714032490650, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/13/664/1805.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714032490872)
[WI-0][TI-1805] - [INFO] 2024-04-25 16:08:10.876 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1805, processInstanceId=664, startTime=1714032490650, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1805] - [INFO] 2024-04-25 16:08:10.879 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1805, processInstanceId=664, startTime=1714032490650, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714032490872)
[WI-0][TI-1805] - [INFO] 2024-04-25 16:08:11.607 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1805, success=true)
[WI-0][TI-1805] - [INFO] 2024-04-25 16:08:11.616 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1805)
[WI-0][TI-1805] - [INFO] 2024-04-25 16:08:11.634 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1805, success=true)
[WI-0][TI-1805] - [INFO] 2024-04-25 16:08:11.644 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1805)
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:11.667 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:12.678 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-7c070306-ae02-4e5a-98fb-aa9a2d624672;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:13.711 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 421ms :: artifacts dl 14ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-7c070306-ae02-4e5a-98fb-aa9a2d624672
		confs: [default]
		0 artifacts copied, 12 already retrieved (0kB/8ms)
	24/04/25 16:08:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:16.713 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:23.727 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:>                                                          (0 + 1) / 1]
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:24.734 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7230320699708455 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:25.745 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 1:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:26.751 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:26.764 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8404907975460123 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:27.755 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	|         content|             context|           datetime|     id|score|subreddit|type|                 url|           user|negative|neutral|positive|compound|
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	|singapore is bad|21apr2024 orchard...|2024-04-21 22:10:42|1c9itat|  131|singapore|post|https://www.reddi...|premiumplatinum|   0.636|  0.364|     0.0| -0.5423|
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1805/py_664_1805.py", line 79, in <module>
	    .save()
	     ^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py", line 1461, in save
	    self._jwrite.save()
	  File "/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
	    return f(*a, **kw)
	           ^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/py4j/protocol.py", line 326, in get_return_value
	    raise Py4JJavaError(
	py4j.protocol.Py4JJavaError: An error occurred while calling o100.save.
	: java.lang.NoClassDefFoundError: scala/$less$colon$less
		at org.apache.spark.sql.kafka010.KafkaSourceProvider.createRelation(KafkaSourceProvider.scala:180)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
		at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
		at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
		at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
		at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
		at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:282)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: java.lang.ClassNotFoundException: scala.$less$colon$less
		at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
		at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
		at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
		... 42 more
	
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:28.764 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1805, processId:5746 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:28.764 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:28.764 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:28.764 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:28.764 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:28.778 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:28.778 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:28.779 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1805
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:28.780 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1805
[WI-664][TI-1805] - [INFO] 2024-04-25 16:08:28.780 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1805] - [INFO] 2024-04-25 16:08:29.360 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1805, processInstanceId=664, status=6, startTime=1714032490650, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/13/664/1805.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1805, endTime=1714032508764, processId=5746, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1805] - [INFO] 2024-04-25 16:08:29.368 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1805, processInstanceId=664, status=6, startTime=1714032490650, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/13/664/1805.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1805, endTime=1714032508764, processId=5746, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714032509359)
[WI-0][TI-1805] - [INFO] 2024-04-25 16:08:29.653 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1805, success=true)
[WI-0][TI-1805] - [INFO] 2024-04-25 16:08:29.657 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1805, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:29.766 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1806, taskName=sentiment anaysis, firstSubmitTime=1714032509750, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=13, appIds=null, processInstanceId=664, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1806'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425160829'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='664'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.768 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.768 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.770 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.770 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.770 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.770 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714032509770
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.770 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 664_1806
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.771 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1806,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714032509750,
  "startTime" : 1714032509770,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/13/664/1806.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 13,
  "processInstanceId" : 664,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1806"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425160829"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "664"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "664_1806",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.772 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.772 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.772 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.777 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.778 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.779 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1806 check successfully
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.779 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.779 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.779 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.780 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.780 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.780 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.781 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.781 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.781 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.782 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.782 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.783 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1806
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.791 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1806/py_664_1806.py
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.792 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.792 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.793 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.793 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1806/py_664_1806.py
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.793 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.794 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1806/664_1806.sh
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:29.797 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 5974
[WI-0][TI-1806] - [INFO] 2024-04-25 16:08:30.380 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1806, processInstanceId=664, startTime=1714032509770, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/13/664/1806.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1806] - [INFO] 2024-04-25 16:08:30.388 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1806, processInstanceId=664, startTime=1714032509770, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/13/664/1806.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714032510379)
[WI-0][TI-1806] - [INFO] 2024-04-25 16:08:30.388 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1806, processInstanceId=664, startTime=1714032509770, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1806] - [INFO] 2024-04-25 16:08:30.390 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1806, processInstanceId=664, startTime=1714032509770, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714032510379)
[WI-0][TI-1806] - [INFO] 2024-04-25 16:08:30.664 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1806, success=true)
[WI-0][TI-1806] - [INFO] 2024-04-25 16:08:30.671 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1806)
[WI-0][TI-1806] - [INFO] 2024-04-25 16:08:30.678 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1806, success=true)
[WI-0][TI-1806] - [INFO] 2024-04-25 16:08:30.690 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1806)
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:30.797 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:32.801 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-b4324077-100b-46dc-b03c-b0ab0a458d04;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 460ms :: artifacts dl 41ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-b4324077-100b-46dc-b03c-b0ab0a458d04
		confs: [default]
		0 artifacts copied, 12 already retrieved (0kB/25ms)
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:33.819 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:08:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:33.842 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7507788161993769 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:34.881 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7884615384615384 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:37.865 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:45.997 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:47.999 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:49.000 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 1:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:51.322 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:52.325 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 2:>                                                          (0 + 1) / 1]
	
	                                                                                
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	|         content|             context|           datetime|     id|score|subreddit|type|                 url|           user|negative|neutral|positive|compound|
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	|singapore is bad|21apr2024 orchard...|2024-04-21 22:10:42|1c9itat|  131|singapore|post|https://www.reddi...|premiumplatinum|   0.636|  0.364|     0.0| -0.5423|
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1806/py_664_1806.py", line 79, in <module>
	    .save()
	     ^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py", line 1461, in save
	    self._jwrite.save()
	  File "/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
	    return f(*a, **kw)
	           ^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/py4j/protocol.py", line 326, in get_return_value
	    raise Py4JJavaError(
	py4j.protocol.Py4JJavaError: An error occurred while calling o100.save.
	: java.lang.NoClassDefFoundError: scala/$less$colon$less
		at org.apache.spark.sql.kafka010.KafkaSourceProvider.createRelation(KafkaSourceProvider.scala:180)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
		at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
		at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
		at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
		at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
		at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:282)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: java.lang.ClassNotFoundException: scala.$less$colon$less
		at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
		at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
		at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
		... 42 more
	
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:53.343 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1806, processId:5974 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:53.344 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:53.344 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:53.345 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:53.345 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:53.347 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:53.348 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:53.348 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1806
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:53.348 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1806
[WI-664][TI-1806] - [INFO] 2024-04-25 16:08:53.348 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1806] - [INFO] 2024-04-25 16:08:53.611 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1806, processInstanceId=664, status=6, startTime=1714032509770, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/13/664/1806.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1806, endTime=1714032533345, processId=5974, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1806] - [INFO] 2024-04-25 16:08:53.615 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1806, processInstanceId=664, status=6, startTime=1714032509770, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/13/664/1806.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1806, endTime=1714032533345, processId=5974, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714032533609)
[WI-0][TI-1806] - [INFO] 2024-04-25 16:08:53.758 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1806, success=true)
[WI-0][TI-1806] - [INFO] 2024-04-25 16:08:53.763 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1806, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:53.875 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1807, taskName=sentiment anaysis, firstSubmitTime=1714032533862, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=13, appIds=null, processInstanceId=664, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1807'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425160853'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='664'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.875 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.876 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.877 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.878 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.879 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.879 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714032533879
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.879 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 664_1807
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.879 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1807,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714032533862,
  "startTime" : 1714032533879,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/13/664/1807.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 13,
  "processInstanceId" : 664,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is bad\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# create a new column to temporarily insert the sentiment scores\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1807"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425160853"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "664"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "664_1807",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.881 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.881 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.882 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.884 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.885 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.886 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1807 check successfully
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.886 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.887 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.887 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.888 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.888 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is bad\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# create a new column to temporarily insert the sentiment scores\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.889 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.889 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.889 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.889 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.889 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.889 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.890 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1807
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.891 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1807/py_664_1807.py
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.893 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is bad",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# create a new column to temporarily insert the sentiment scores
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.894 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.894 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.895 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1807/py_664_1807.py
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.895 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.895 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1807/664_1807.sh
[WI-664][TI-1807] - [INFO] 2024-04-25 16:08:53.899 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6194
[WI-0][TI-1807] - [INFO] 2024-04-25 16:08:54.616 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1807, processInstanceId=664, startTime=1714032533879, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/13/664/1807.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1807] - [INFO] 2024-04-25 16:08:54.619 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1807, processInstanceId=664, startTime=1714032533879, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/13/664/1807.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714032534616)
[WI-0][TI-1807] - [INFO] 2024-04-25 16:08:54.620 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1807, processInstanceId=664, startTime=1714032533879, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1807] - [INFO] 2024-04-25 16:08:54.622 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1807, processInstanceId=664, startTime=1714032533879, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714032534616)
[WI-0][TI-1807] - [INFO] 2024-04-25 16:08:54.763 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1807, success=true)
[WI-0][TI-1807] - [INFO] 2024-04-25 16:08:54.780 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1807)
[WI-0][TI-1807] - [INFO] 2024-04-25 16:08:54.789 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1807, success=true)
[WI-0][TI-1807] - [INFO] 2024-04-25 16:08:54.797 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1807)
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:54.900 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:56.367 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8512261191901911 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:57.439 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8591954022988506 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:57.912 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:58.444 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8725212464589235 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:58.918 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-64ba415d-acb7-4360-bf9a-6a8b0beb9bed;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:59.479 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8810289389067524 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:08:59.947 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 1451ms :: artifacts dl 13ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-64ba415d-acb7-4360-bf9a-6a8b0beb9bed
		confs: [default]
		0 artifacts copied, 12 already retrieved (0kB/23ms)
[WI-0][TI-0] - [INFO] 2024-04-25 16:09:00.481 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7455089820359282 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:09:00.952 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:09:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-04-25 16:09:05.971 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-04-25 16:09:06.583 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8605341246290801 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:09:07.700 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.726384364820847 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:09:08.701 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8484848484848484 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:09:09.704 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8055555555555556 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:09:11.724 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7142857142857142 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:09:13.770 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8378378378378378 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:09:15.880 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8409893992932862 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:09:16.032 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-25 16:09:16.916 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8119658119658119 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:09:17.041 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 16:09:17.918 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8253968253968254 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:09:18.920 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7450331125827815 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:09:20.220 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 1:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-25 16:09:21.228 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-25 16:09:22.088 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.771875 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:09:22.233 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 2:>                                                          (0 + 1) / 1]
	
	                                                                                
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	|         content|             context|           datetime|     id|score|subreddit|type|                 url|           user|negative|neutral|positive|compound|
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	|singapore is bad|21apr2024 orchard...|2024-04-21 22:10:42|1c9itat|  131|singapore|post|https://www.reddi...|premiumplatinum|   0.636|  0.364|     0.0| -0.5423|
	+----------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+--------+-------+--------+--------+
	
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1807/py_664_1807.py", line 79, in <module>
	    .save()
	     ^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py", line 1461, in save
	    self._jwrite.save()
	  File "/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
	    return f(*a, **kw)
	           ^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/py4j/protocol.py", line 326, in get_return_value
	    raise Py4JJavaError(
	py4j.protocol.Py4JJavaError: An error occurred while calling o100.save.
	: java.lang.NoClassDefFoundError: scala/$less$colon$less
		at org.apache.spark.sql.kafka010.KafkaSourceProvider.createRelation(KafkaSourceProvider.scala:180)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
		at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
		at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
		at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
		at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
		at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:282)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: java.lang.ClassNotFoundException: scala.$less$colon$less
		at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
		at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
		at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
		... 42 more
	
[WI-0][TI-0] - [INFO] 2024-04-25 16:09:23.131 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8699505738460249 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-664][TI-1807] - [INFO] 2024-04-25 16:09:24.245 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1807, processId:6194 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-664][TI-1807] - [INFO] 2024-04-25 16:09:24.248 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1807] - [INFO] 2024-04-25 16:09:24.249 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-664][TI-1807] - [INFO] 2024-04-25 16:09:24.249 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-664][TI-1807] - [INFO] 2024-04-25 16:09:24.252 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-664][TI-1807] - [INFO] 2024-04-25 16:09:24.256 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-664][TI-1807] - [INFO] 2024-04-25 16:09:24.257 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-664][TI-1807] - [INFO] 2024-04-25 16:09:24.257 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1807
[WI-664][TI-1807] - [INFO] 2024-04-25 16:09:24.259 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1807
[WI-664][TI-1807] - [INFO] 2024-04-25 16:09:24.260 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1807] - [INFO] 2024-04-25 16:09:24.945 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1807, processInstanceId=664, status=6, startTime=1714032533879, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/13/664/1807.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1807, endTime=1714032564249, processId=6194, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1807] - [INFO] 2024-04-25 16:09:24.960 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1807, success=true)
[WI-0][TI-1807] - [INFO] 2024-04-25 16:09:24.967 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1807, success=true)
[WI-0][TI-1807] - [INFO] 2024-04-25 16:09:24.967 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1807, processInstanceId=664, status=6, startTime=1714032533879, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/13/664/1807.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_13/664/1807, endTime=1714032564249, processId=6194, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714032564932)
[WI-0][TI-1807] - [WARN] 2024-04-25 16:09:24.967 +0800 o.a.d.s.w.m.MessageRetryRunner:[139] - Retry send message to master error
java.util.ConcurrentModificationException: null
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:911)
	at java.util.ArrayList$Itr.next(ArrayList.java:861)
	at org.apache.dolphinscheduler.server.worker.message.MessageRetryRunner.run(MessageRetryRunner.java:127)
[WI-0][TI-0] - [INFO] 2024-04-25 16:12:22.429 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1808, taskName=Extract Reddit Data, firstSubmitTime=1714032742409, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13045665829504, processDefineVersion=15, appIds=null, processInstanceId=665, scheduleTime=0, globalParams=[{"prop":"subreddit","direct":"IN","type":"VARCHAR","value":"singapore"},{"prop":"limit","direct":"IN","type":"VARCHAR","value":"3"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"data","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"import praw\nimport json\n\nclass RedditPostDataExtractor:\n    def __init__(self):\n        self.data = []\n\n    def extract_data(self, hot_posts):\n        if hot_posts:\n            for post in hot_posts:\n                post.comments.replace_more(limit=None)\n                # initialise a dictionary to contain the data of the post\n                post_data = {}\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the poster's name\n                if post.author is not None:\n                    post_data['author'] = post.author.name\n                else:\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the post was created\n                post_data['unix_time'] = post.created_utc\n                post_data['permalink'] = post.permalink\n                post_data['score'] = post.score\n                post_data['subreddit'] = post.subreddit.display_name\n                post_data['post_title'] = post.title\n                post_data['body'] = post.selftext\n\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\n                # t3 represents that it is a post\n                post_data['name'] = post.name\n\n                # store the posts data in the dictionary where the name is used as the key\n                self.data.append(post_data)\n\n                # extract the comments from the post\n                comments = post.comments.list()\n\n                # extract the data in the comments and store them inside the dictionary\n                self.extract_comments_data(comments)\n        \n        return self.data\n\n    def extract_comments_data(self, comments):\n        if comments:\n            for comment in comments:\n                # initialise a dictionary to contain the data of the comment\n                comment_data = {}\n\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the commenter name\n                if comment.author is not None:\n                    comment_data['author'] = comment.author.name\n                else:\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the comment was created\n                comment_data['unix_time'] = comment.created_utc\n                comment_data['permalink'] = comment.permalink\n                comment_data['score'] = comment.score\n                comment_data['subreddit'] = comment.subreddit.display_name\n                comment_data['post_title'] = comment.submission.title\n                comment_data['body'] = comment.body\n\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\n                # t1 represents that it is a post\n                comment_data['name'] = comment.name\n\n                # store the comment's data in the dictionary where the name is used as the key\n                self.data.append(comment_data)\n\n                # extract the comment's replies from the post\n                # replies = comment.replies\n\n                # extract the data in the comments and store them inside the dictionary\n                # self.extract_comments_data(replies)\n        \n        return\n    \ndef get_hot_posts(reddit_instance, subreddit_name, limit):\n    # Define the subreddit\n    subreddit = reddit_instance.subreddit(subreddit_name)\n\n    # Get hot posts from the subreddit\n    hot_posts = subreddit.hot(limit=limit)\n\n    # Filter out posts that are not pinned\n    filtered_posts = [post for post in hot_posts if not post.stickied]\n    return filtered_posts","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='Extract Reddit Data'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1808'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13045662268160'}, subreddit=Property{prop='subreddit', direct=IN, type=VARCHAR, value='singapore'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425161222'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='665'}, limit=Property{prop='limit', direct=IN, type=VARCHAR, value='3'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13045665829504'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.467 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: Extract Reddit Data to wait queue success
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.468 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.482 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.483 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.483 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.483 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714032742483
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.483 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 665_1808
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.485 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1808,
  "taskName" : "Extract Reddit Data",
  "firstSubmitTime" : 1714032742409,
  "startTime" : 1714032742483,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13045665829504/15/665/1808.log",
  "processId" : 0,
  "processDefineCode" : 13045665829504,
  "processDefineVersion" : 15,
  "processInstanceId" : 665,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"subreddit\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"singapore\"},{\"prop\":\"limit\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"3\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"data\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"import praw\\nimport json\\n\\nclass RedditPostDataExtractor:\\n    def __init__(self):\\n        self.data = []\\n\\n    def extract_data(self, hot_posts):\\n        if hot_posts:\\n            for post in hot_posts:\\n                post.comments.replace_more(limit=None)\\n                # initialise a dictionary to contain the data of the post\\n                post_data = {}\\n\\n                # get the relevant post's data and store it inside post_data\\n\\n                # get the poster's name\\n                if post.author is not None:\\n                    post_data['author'] = post.author.name\\n                else:\\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\\n\\n                # get the unix time in which the post was created\\n                post_data['unix_time'] = post.created_utc\\n                post_data['permalink'] = post.permalink\\n                post_data['score'] = post.score\\n                post_data['subreddit'] = post.subreddit.display_name\\n                post_data['post_title'] = post.title\\n                post_data['body'] = post.selftext\\n\\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\\n                # t3 represents that it is a post\\n                post_data['name'] = post.name\\n\\n                # store the posts data in the dictionary where the name is used as the key\\n                self.data.append(post_data)\\n\\n                # extract the comments from the post\\n                comments = post.comments.list()\\n\\n                # extract the data in the comments and store them inside the dictionary\\n                self.extract_comments_data(comments)\\n        \\n        return self.data\\n\\n    def extract_comments_data(self, comments):\\n        if comments:\\n            for comment in comments:\\n                # initialise a dictionary to contain the data of the comment\\n                comment_data = {}\\n\\n\\n                # get the relevant post's data and store it inside post_data\\n\\n                # get the commenter name\\n                if comment.author is not None:\\n                    comment_data['author'] = comment.author.name\\n                else:\\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\\n\\n                # get the unix time in which the comment was created\\n                comment_data['unix_time'] = comment.created_utc\\n                comment_data['permalink'] = comment.permalink\\n                comment_data['score'] = comment.score\\n                comment_data['subreddit'] = comment.subreddit.display_name\\n                comment_data['post_title'] = comment.submission.title\\n                comment_data['body'] = comment.body\\n\\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\\n                # t1 represents that it is a post\\n                comment_data['name'] = comment.name\\n\\n                # store the comment's data in the dictionary where the name is used as the key\\n                self.data.append(comment_data)\\n\\n                # extract the comment's replies from the post\\n                # replies = comment.replies\\n\\n                # extract the data in the comments and store them inside the dictionary\\n                # self.extract_comments_data(replies)\\n        \\n        return\\n    \\ndef get_hot_posts(reddit_instance, subreddit_name, limit):\\n    # Define the subreddit\\n    subreddit = reddit_instance.subreddit(subreddit_name)\\n\\n    # Get hot posts from the subreddit\\n    hot_posts = subreddit.hot(limit=limit)\\n\\n    # Filter out posts that are not pinned\\n    filtered_posts = [post for post in hot_posts if not post.stickied]\\n    return filtered_posts\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "Extract Reddit Data"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1808"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13045662268160"
    },
    "subreddit" : {
      "prop" : "subreddit",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "singapore"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425161222"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "665"
    },
    "limit" : {
      "prop" : "limit",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13045665829504"
    }
  },
  "taskAppId" : "665_1808",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.487 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.487 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.487 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.499 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.499 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.502 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_15/665/1808 check successfully
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.505 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.506 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.507 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.507 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.508 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "data",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "import praw\nimport json\n\nclass RedditPostDataExtractor:\n    def __init__(self):\n        self.data = []\n\n    def extract_data(self, hot_posts):\n        if hot_posts:\n            for post in hot_posts:\n                post.comments.replace_more(limit=None)\n                # initialise a dictionary to contain the data of the post\n                post_data = {}\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the poster's name\n                if post.author is not None:\n                    post_data['author'] = post.author.name\n                else:\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the post was created\n                post_data['unix_time'] = post.created_utc\n                post_data['permalink'] = post.permalink\n                post_data['score'] = post.score\n                post_data['subreddit'] = post.subreddit.display_name\n                post_data['post_title'] = post.title\n                post_data['body'] = post.selftext\n\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\n                # t3 represents that it is a post\n                post_data['name'] = post.name\n\n                # store the posts data in the dictionary where the name is used as the key\n                self.data.append(post_data)\n\n                # extract the comments from the post\n                comments = post.comments.list()\n\n                # extract the data in the comments and store them inside the dictionary\n                self.extract_comments_data(comments)\n        \n        return self.data\n\n    def extract_comments_data(self, comments):\n        if comments:\n            for comment in comments:\n                # initialise a dictionary to contain the data of the comment\n                comment_data = {}\n\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the commenter name\n                if comment.author is not None:\n                    comment_data['author'] = comment.author.name\n                else:\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the comment was created\n                comment_data['unix_time'] = comment.created_utc\n                comment_data['permalink'] = comment.permalink\n                comment_data['score'] = comment.score\n                comment_data['subreddit'] = comment.subreddit.display_name\n                comment_data['post_title'] = comment.submission.title\n                comment_data['body'] = comment.body\n\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\n                # t1 represents that it is a post\n                comment_data['name'] = comment.name\n\n                # store the comment's data in the dictionary where the name is used as the key\n                self.data.append(comment_data)\n\n                # extract the comment's replies from the post\n                # replies = comment.replies\n\n                # extract the data in the comments and store them inside the dictionary\n                # self.extract_comments_data(replies)\n        \n        return\n    \ndef get_hot_posts(reddit_instance, subreddit_name, limit):\n    # Define the subreddit\n    subreddit = reddit_instance.subreddit(subreddit_name)\n\n    # Get hot posts from the subreddit\n    hot_posts = subreddit.hot(limit=limit)\n\n    # Filter out posts that are not pinned\n    filtered_posts = [post for post in hot_posts if not post.stickied]\n    return filtered_posts",
  "resourceList" : [ ]
}
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.508 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.509 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.509 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.509 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.510 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.510 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import praw
import json

class RedditPostDataExtractor:
    def __init__(self):
        self.data = []

    def extract_data(self, hot_posts):
        if hot_posts:
            for post in hot_posts:
                post.comments.replace_more(limit=None)
                # initialise a dictionary to contain the data of the post
                post_data = {}

                # get the relevant post's data and store it inside post_data

                # get the poster's name
                if post.author is not None:
                    post_data['author'] = post.author.name
                else:
                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the post was created
                post_data['unix_time'] = post.created_utc
                post_data['permalink'] = post.permalink
                post_data['score'] = post.score
                post_data['subreddit'] = post.subreddit.display_name
                post_data['post_title'] = post.title
                post_data['body'] = post.selftext

                # get the unique name of the post (name refers to the id of the post prefixed with t3_)
                # t3 represents that it is a post
                post_data['name'] = post.name

                # store the posts data in the dictionary where the name is used as the key
                self.data.append(post_data)

                # extract the comments from the post
                comments = post.comments.list()

                # extract the data in the comments and store them inside the dictionary
                self.extract_comments_data(comments)
        
        return self.data

    def extract_comments_data(self, comments):
        if comments:
            for comment in comments:
                # initialise a dictionary to contain the data of the comment
                comment_data = {}


                # get the relevant post's data and store it inside post_data

                # get the commenter name
                if comment.author is not None:
                    comment_data['author'] = comment.author.name
                else:
                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the comment was created
                comment_data['unix_time'] = comment.created_utc
                comment_data['permalink'] = comment.permalink
                comment_data['score'] = comment.score
                comment_data['subreddit'] = comment.subreddit.display_name
                comment_data['post_title'] = comment.submission.title
                comment_data['body'] = comment.body

                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)
                # t1 represents that it is a post
                comment_data['name'] = comment.name

                # store the comment's data in the dictionary where the name is used as the key
                self.data.append(comment_data)

                # extract the comment's replies from the post
                # replies = comment.replies

                # extract the data in the comments and store them inside the dictionary
                # self.extract_comments_data(replies)
        
        return
    
def get_hot_posts(reddit_instance, subreddit_name, limit):
    # Define the subreddit
    subreddit = reddit_instance.subreddit(subreddit_name)

    # Get hot posts from the subreddit
    hot_posts = subreddit.hot(limit=limit)

    # Filter out posts that are not pinned
    filtered_posts = [post for post in hot_posts if not post.stickied]
    return filtered_posts
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.514 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_15/665/1808
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.515 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_15/665/1808/py_665_1808.py
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.515 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import praw
import json

class RedditPostDataExtractor:
    def __init__(self):
        self.data = []

    def extract_data(self, hot_posts):
        if hot_posts:
            for post in hot_posts:
                post.comments.replace_more(limit=None)
                # initialise a dictionary to contain the data of the post
                post_data = {}

                # get the relevant post's data and store it inside post_data

                # get the poster's name
                if post.author is not None:
                    post_data['author'] = post.author.name
                else:
                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the post was created
                post_data['unix_time'] = post.created_utc
                post_data['permalink'] = post.permalink
                post_data['score'] = post.score
                post_data['subreddit'] = post.subreddit.display_name
                post_data['post_title'] = post.title
                post_data['body'] = post.selftext

                # get the unique name of the post (name refers to the id of the post prefixed with t3_)
                # t3 represents that it is a post
                post_data['name'] = post.name

                # store the posts data in the dictionary where the name is used as the key
                self.data.append(post_data)

                # extract the comments from the post
                comments = post.comments.list()

                # extract the data in the comments and store them inside the dictionary
                self.extract_comments_data(comments)
        
        return self.data

    def extract_comments_data(self, comments):
        if comments:
            for comment in comments:
                # initialise a dictionary to contain the data of the comment
                comment_data = {}


                # get the relevant post's data and store it inside post_data

                # get the commenter name
                if comment.author is not None:
                    comment_data['author'] = comment.author.name
                else:
                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the comment was created
                comment_data['unix_time'] = comment.created_utc
                comment_data['permalink'] = comment.permalink
                comment_data['score'] = comment.score
                comment_data['subreddit'] = comment.subreddit.display_name
                comment_data['post_title'] = comment.submission.title
                comment_data['body'] = comment.body

                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)
                # t1 represents that it is a post
                comment_data['name'] = comment.name

                # store the comment's data in the dictionary where the name is used as the key
                self.data.append(comment_data)

                # extract the comment's replies from the post
                # replies = comment.replies

                # extract the data in the comments and store them inside the dictionary
                # self.extract_comments_data(replies)
        
        return
    
def get_hot_posts(reddit_instance, subreddit_name, limit):
    # Define the subreddit
    subreddit = reddit_instance.subreddit(subreddit_name)

    # Get hot posts from the subreddit
    hot_posts = subreddit.hot(limit=limit)

    # Filter out posts that are not pinned
    filtered_posts = [post for post in hot_posts if not post.stickied]
    return filtered_posts
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.517 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.517 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.517 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_15/665/1808/py_665_1808.py
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.517 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.517 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_15/665/1808/665_1808.sh
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:22.533 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6448
[WI-0][TI-0] - [INFO] 2024-04-25 16:12:22.534 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1808] - [INFO] 2024-04-25 16:12:22.993 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1808, success=true)
[WI-0][TI-1808] - [INFO] 2024-04-25 16:12:23.002 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1808)
[WI-0][TI-0] - [INFO] 2024-04-25 16:12:23.378 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7701863354037267 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:23.558 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_15/665/1808, processId:6448 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:23.561 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:23.561 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:23.561 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:23.562 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:23.568 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:23.568 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:23.569 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_15/665/1808
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:23.570 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_15/665/1808
[WI-665][TI-1808] - [INFO] 2024-04-25 16:12:23.570 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1808] - [INFO] 2024-04-25 16:12:23.979 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1808, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:12:31.541 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1809, taskName=search for intake JSON files, firstSubmitTime=1714032751514, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=113, appIds=null, processInstanceId=666, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1809'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340113777664'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425161231'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='666'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.542 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake JSON files to wait queue success
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.543 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.545 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.548 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.549 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.552 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714032751552
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.552 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 666_1809
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.553 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1809,
  "taskName" : "search for intake JSON files",
  "firstSubmitTime" : 1714032751514,
  "startTime" : 1714032751552,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13201021801792/113/666/1809.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 113,
  "processInstanceId" : 666,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set raw_file_dir to null\\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1809"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340113777664"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425161231"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "666"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "666_1809",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.554 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.554 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.554 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.567 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.567 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.568 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/666/1809 check successfully
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.569 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.569 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.569 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.569 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.569 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"",
  "resourceList" : [ ]
}
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.569 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.569 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.569 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.570 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.570 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.570 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.570 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.570 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/reddit/in -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set raw_file_dir to null
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
fi

echo "#{setValue(raw_file_dir=${raw_file_dir})}"
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.570 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.570 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/666/1809/666_1809.sh
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:31.592 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6467
[WI-0][TI-0] - [INFO] 2024-04-25 16:12:31.607 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1809] - [INFO] 2024-04-25 16:12:32.019 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1809, success=true)
[WI-0][TI-1809] - [INFO] 2024-04-25 16:12:32.026 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1809)
[WI-0][TI-0] - [INFO] 2024-04-25 16:12:32.617 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	#{setValue(raw_file_dir=null)}
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:32.619 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/666/1809, processId:6467 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:32.619 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:32.619 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:32.620 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:32.620 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:32.624 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:32.624 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:32.624 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/666/1809
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:32.625 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/666/1809
[WI-666][TI-1809] - [INFO] 2024-04-25 16:12:32.625 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1809] - [INFO] 2024-04-25 16:12:32.993 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1809, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:12:34.455 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7557471264367817 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:12:35.104 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1811, taskName=end workflow, firstSubmitTime=1714032755092, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=113, appIds=null, processInstanceId=666, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"echo No Files Detected - End Workflow","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='end workflow'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1811'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13348985859488'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425161235'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='666'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='null'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"null"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.105 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: end workflow to wait queue success
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.106 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.109 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.109 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.109 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.109 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714032755109
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.109 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 666_1811
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.109 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1811,
  "taskName" : "end workflow",
  "firstSubmitTime" : 1714032755092,
  "startTime" : 1714032755109,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13201021801792/113/666/1811.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 113,
  "processInstanceId" : 666,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"echo No Files Detected - End Workflow\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "end workflow"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1811"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13348985859488"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425161235"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "666"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "null"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "666_1811",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"null\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.109 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.109 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.109 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.116 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.116 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.117 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/666/1811 check successfully
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.117 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.117 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.117 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.117 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.117 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "echo No Files Detected - End Workflow",
  "resourceList" : [ ]
}
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.117 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.117 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"null"}] successfully
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.117 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.117 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.117 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.118 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.118 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.118 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
echo No Files Detected - End Workflow
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.118 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.118 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/666/1811/666_1811.sh
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:35.127 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6480
[WI-0][TI-0] - [INFO] 2024-04-25 16:12:35.165 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1811] - [INFO] 2024-04-25 16:12:35.393 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1811, processInstanceId=666, startTime=1714032755109, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/113/666/1811.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1811] - [INFO] 2024-04-25 16:12:35.417 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1811, processInstanceId=666, startTime=1714032755109, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/113/666/1811.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714032755388)
[WI-0][TI-1811] - [INFO] 2024-04-25 16:12:35.418 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1811, processInstanceId=666, startTime=1714032755109, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1811] - [INFO] 2024-04-25 16:12:35.420 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1811, processInstanceId=666, startTime=1714032755109, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714032755388)
[WI-0][TI-1811] - [INFO] 2024-04-25 16:12:35.997 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1811, success=true)
[WI-0][TI-1811] - [INFO] 2024-04-25 16:12:36.017 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1811)
[WI-0][TI-1811] - [INFO] 2024-04-25 16:12:36.022 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1811, success=true)
[WI-0][TI-1811] - [INFO] 2024-04-25 16:12:36.031 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1811)
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:36.198 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/666/1811, processId:6480 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:36.205 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:36.217 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:36.218 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:36.219 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:36.228 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:36.229 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:36.230 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/666/1811
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:36.230 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/666/1811
[WI-666][TI-1811] - [INFO] 2024-04-25 16:12:36.231 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1811] - [INFO] 2024-04-25 16:12:36.422 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1811, processInstanceId=666, status=7, startTime=1714032755109, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/113/666/1811.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/666/1811, endTime=1714032756218, processId=6480, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"null"}], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1811] - [INFO] 2024-04-25 16:12:36.427 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1811, processInstanceId=666, status=7, startTime=1714032755109, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/113/666/1811.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/666/1811, endTime=1714032756218, processId=6480, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"null"}], eventCreateTime=0, eventSendTime=1714032756422)
[WI-0][TI-1811] - [INFO] 2024-04-25 16:12:36.999 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1811, success=true)
[WI-0][TI-1811] - [INFO] 2024-04-25 16:12:37.003 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1811, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:12:39.560 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1812, taskName=extract from preprocessed queue, firstSubmitTime=1714032759550, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=12, appIds=null, processInstanceId=667, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"message","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n    --bootstrap-server kafka:9092 \\\n    --topic reddit_preprocessed \\\n    --group preprocessed_consumers \\\n    --max-messages 1 \\\n    --timeout-ms 10000)\n\necho \"#{setValue(message=${message})}\"","resourceList":[]}, environmentConfig=export JAVA_HOME=/opt/java/openjdk, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract from preprocessed queue'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1812'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13375898458848'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425161239'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='667'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_data'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.561 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract from preprocessed queue to wait queue success
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.561 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.563 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.563 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.563 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.563 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714032759563
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.563 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 667_1812
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.563 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1812,
  "taskName" : "extract from preprocessed queue",
  "firstSubmitTime" : 1714032759550,
  "startTime" : 1714032759563,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13377949373536/12/667/1812.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 12,
  "processInstanceId" : 667,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"message\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\\\\n    --bootstrap-server kafka:9092 \\\\\\n    --topic reddit_preprocessed \\\\\\n    --group preprocessed_consumers \\\\\\n    --max-messages 1 \\\\\\n    --timeout-ms 10000)\\n\\necho \\\"#{setValue(message=${message})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export JAVA_HOME=/opt/java/openjdk",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract from preprocessed queue"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1812"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13375898458848"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425161239"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "667"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_data"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "667_1812",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.564 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.564 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.564 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.566 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.567 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.567 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1812 check successfully
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.567 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.568 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.568 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.568 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.568 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "message",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n    --bootstrap-server kafka:9092 \\\n    --topic reddit_preprocessed \\\n    --group preprocessed_consumers \\\n    --max-messages 1 \\\n    --timeout-ms 10000)\n\necho \"#{setValue(message=${message})}\"",
  "resourceList" : [ ]
}
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.568 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.568 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.569 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.569 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.569 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.569 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.570 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.570 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export JAVA_HOME=/opt/java/openjdk
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

message=$(${KAFKA_DIR}/kafka-console-consumer.sh \
    --bootstrap-server kafka:9092 \
    --topic reddit_preprocessed \
    --group preprocessed_consumers \
    --max-messages 1 \
    --timeout-ms 10000)

echo "#{setValue(message=${message})}"
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.570 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.570 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1812/667_1812.sh
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:39.574 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6490
[WI-0][TI-1812] - [INFO] 2024-04-25 16:12:40.003 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1812, success=true)
[WI-0][TI-1812] - [INFO] 2024-04-25 16:12:40.009 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1812)
[WI-0][TI-0] - [INFO] 2024-04-25 16:12:40.574 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1812/667_1812.sh: line 12: /opt/kafka_2.13-3.7.0/bin/kafka-console-consumer.sh: No such file or directory
	#{setValue(message=)}
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:40.577 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1812, processId:6490 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-667][TI-1812] - [WARN] 2024-04-25 16:12:40.577 +0800 o.a.d.p.t.a.p.AbstractParameters:[152] - Cannot find the output parameter message in the task output parameters
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:40.577 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:40.577 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:40.577 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:40.578 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:40.580 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:40.581 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:40.581 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1812
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:40.581 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1812
[WI-667][TI-1812] - [INFO] 2024-04-25 16:12:40.581 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1812] - [INFO] 2024-04-25 16:12:40.998 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1812, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:12:41.044 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1813, taskName=keyword filtering, firstSubmitTime=1714032761039, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=12, appIds=null, processInstanceId=667, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1813'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425161241'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='667'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_data'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.046 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.047 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.047 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.047 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.047 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.047 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714032761047
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.049 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 667_1813
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.049 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1813,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1714032761039,
  "startTime" : 1714032761047,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13377949373536/12/667/1813.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 12,
  "processInstanceId" : 667,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1813"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425161241"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "667"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_data"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "667_1813",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.050 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.051 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.051 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.053 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.053 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.054 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1813 check successfully
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.054 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.054 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.054 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.054 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.054 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.054 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.054 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.054 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.054 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.054 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.054 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.055 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1813
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.055 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1813/py_667_1813.py
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.055 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.056 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.056 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.056 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1813/py_667_1813.py
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.056 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.056 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1813/667_1813.sh
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:41.059 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6501
[WI-0][TI-1813] - [INFO] 2024-04-25 16:12:41.435 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1813, processInstanceId=667, startTime=1714032761047, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/12/667/1813.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1813] - [INFO] 2024-04-25 16:12:41.439 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1813, processInstanceId=667, startTime=1714032761047, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/12/667/1813.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714032761434)
[WI-0][TI-1813] - [INFO] 2024-04-25 16:12:41.439 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1813, processInstanceId=667, startTime=1714032761047, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1813] - [INFO] 2024-04-25 16:12:41.441 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1813, processInstanceId=667, startTime=1714032761047, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714032761434)
[WI-0][TI-1813] - [INFO] 2024-04-25 16:12:42.011 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1813, success=true)
[WI-0][TI-1813] - [INFO] 2024-04-25 16:12:42.023 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1813)
[WI-0][TI-1813] - [INFO] 2024-04-25 16:12:42.049 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1813, success=true)
[WI-0][TI-1813] - [INFO] 2024-04-25 16:12:42.055 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1813)
[WI-0][TI-0] - [INFO] 2024-04-25 16:12:42.059 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1813/667_1813.sh: line 7: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1813/py_667_1813.py: Permission denied
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:42.061 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1813, processId:6501 ,exitStatusCode:126 ,processWaitForStatus:true ,processExitValue:126
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:42.062 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:42.063 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:42.063 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:42.063 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:42.065 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:42.065 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:42.065 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1813
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:42.065 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1813
[WI-667][TI-1813] - [INFO] 2024-04-25 16:12:42.066 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1813] - [INFO] 2024-04-25 16:12:42.442 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1813, processInstanceId=667, status=6, startTime=1714032761047, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/12/667/1813.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1813, endTime=1714032762063, processId=6501, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1813] - [INFO] 2024-04-25 16:12:42.462 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1813, processInstanceId=667, status=6, startTime=1714032761047, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/12/667/1813.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1813, endTime=1714032762063, processId=6501, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714032762442)
[WI-0][TI-1813] - [INFO] 2024-04-25 16:12:43.013 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1813, success=true)
[WI-0][TI-1813] - [INFO] 2024-04-25 16:12:43.016 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1813, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:12:43.087 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1814, taskName=keyword filtering, firstSubmitTime=1714032763079, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=12, appIds=null, processInstanceId=667, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1814'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425161243'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='667'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_data'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.088 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.098 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.099 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.100 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.100 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.100 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714032763100
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.100 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 667_1814
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.100 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1814,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1714032763079,
  "startTime" : 1714032763100,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13377949373536/12/667/1814.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 12,
  "processInstanceId" : 667,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1814"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425161243"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "667"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_data"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "667_1814",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.101 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.101 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.101 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.107 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.108 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.111 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1814 check successfully
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.111 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.111 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.111 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.112 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.112 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.113 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.113 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.113 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.113 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.113 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.113 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.113 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1814
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.113 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1814/py_667_1814.py
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.113 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.114 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.114 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.114 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1814/py_667_1814.py
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.114 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.114 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1814/667_1814.sh
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:43.127 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6512
[WI-0][TI-1814] - [INFO] 2024-04-25 16:12:43.465 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1814, processInstanceId=667, startTime=1714032763100, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/12/667/1814.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1814] - [INFO] 2024-04-25 16:12:43.469 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1814, processInstanceId=667, startTime=1714032763100, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/12/667/1814.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714032763465)
[WI-0][TI-1814] - [INFO] 2024-04-25 16:12:43.469 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1814, processInstanceId=667, startTime=1714032763100, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1814] - [INFO] 2024-04-25 16:12:43.475 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1814, processInstanceId=667, startTime=1714032763100, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714032763465)
[WI-0][TI-1814] - [INFO] 2024-04-25 16:12:44.014 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1814, success=true)
[WI-0][TI-1814] - [INFO] 2024-04-25 16:12:44.020 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1814)
[WI-0][TI-1814] - [INFO] 2024-04-25 16:12:44.026 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1814, success=true)
[WI-0][TI-1814] - [INFO] 2024-04-25 16:12:44.036 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1814)
[WI-0][TI-0] - [INFO] 2024-04-25 16:12:44.132 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1814/667_1814.sh: line 7: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1814/py_667_1814.py: Permission denied
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:44.134 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1814, processId:6512 ,exitStatusCode:126 ,processWaitForStatus:true ,processExitValue:126
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:44.134 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:44.134 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:44.135 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:44.135 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:44.137 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:44.137 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:44.137 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1814
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:44.138 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1814
[WI-667][TI-1814] - [INFO] 2024-04-25 16:12:44.138 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1814] - [INFO] 2024-04-25 16:12:44.477 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1814, processInstanceId=667, status=6, startTime=1714032763100, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/12/667/1814.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1814, endTime=1714032764135, processId=6512, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1814] - [INFO] 2024-04-25 16:12:44.480 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1814, processInstanceId=667, status=6, startTime=1714032763100, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/12/667/1814.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1814, endTime=1714032764135, processId=6512, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714032764477)
[WI-0][TI-1814] - [INFO] 2024-04-25 16:12:45.007 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1814, success=true)
[WI-0][TI-1814] - [INFO] 2024-04-25 16:12:45.013 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1814, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:12:45.132 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1815, taskName=keyword filtering, firstSubmitTime=1714032765119, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=12, appIds=null, processInstanceId=667, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1815'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425161245'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='667'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_data'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.133 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.134 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.135 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.135 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.135 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.135 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714032765135
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.135 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 667_1815
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.136 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1815,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1714032765119,
  "startTime" : 1714032765135,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13377949373536/12/667/1815.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 12,
  "processInstanceId" : 667,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1815"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425161245"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "667"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_data"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "667_1815",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.136 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.136 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.136 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.141 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.142 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.142 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1815 check successfully
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.142 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.143 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.143 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.143 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.143 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.143 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.143 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.143 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.143 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.143 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.143 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.144 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1815
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.144 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1815/py_667_1815.py
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.144 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.144 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.144 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.144 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1815/py_667_1815.py
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.145 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.145 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1815/667_1815.sh
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:45.153 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6523
[WI-0][TI-1815] - [INFO] 2024-04-25 16:12:45.481 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1815, processInstanceId=667, startTime=1714032765135, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/12/667/1815.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1815] - [INFO] 2024-04-25 16:12:45.485 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1815, processInstanceId=667, startTime=1714032765135, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/12/667/1815.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714032765480)
[WI-0][TI-1815] - [INFO] 2024-04-25 16:12:45.486 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1815, processInstanceId=667, startTime=1714032765135, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1815] - [INFO] 2024-04-25 16:12:45.489 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1815, processInstanceId=667, startTime=1714032765135, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714032765480)
[WI-0][TI-0] - [INFO] 2024-04-25 16:12:45.490 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8033707865168539 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1815] - [INFO] 2024-04-25 16:12:46.026 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1815, success=true)
[WI-0][TI-1815] - [INFO] 2024-04-25 16:12:46.051 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1815)
[WI-0][TI-1815] - [INFO] 2024-04-25 16:12:46.071 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1815, success=true)
[WI-0][TI-1815] - [INFO] 2024-04-25 16:12:46.080 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1815)
[WI-0][TI-0] - [INFO] 2024-04-25 16:12:46.155 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1815/667_1815.sh: line 7: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1815/py_667_1815.py: Permission denied
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:46.158 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1815, processId:6523 ,exitStatusCode:126 ,processWaitForStatus:true ,processExitValue:126
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:46.158 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:46.159 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:46.159 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:46.159 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:46.161 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:46.161 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:46.162 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1815
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:46.162 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1815
[WI-667][TI-1815] - [INFO] 2024-04-25 16:12:46.163 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1815] - [INFO] 2024-04-25 16:12:46.490 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1815, processInstanceId=667, status=6, startTime=1714032765135, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/12/667/1815.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1815, endTime=1714032766159, processId=6523, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1815] - [INFO] 2024-04-25 16:12:46.492 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1815, processInstanceId=667, status=6, startTime=1714032765135, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/12/667/1815.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1815, endTime=1714032766159, processId=6523, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714032766490)
[WI-0][TI-1815] - [INFO] 2024-04-25 16:12:47.009 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1815, success=true)
[WI-0][TI-1815] - [INFO] 2024-04-25 16:12:47.013 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1815, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:12:47.042 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1816, taskName=keyword filtering, firstSubmitTime=1714032767033, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=12, appIds=null, processInstanceId=667, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1816'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425161247'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='667'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_data'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.048 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.048 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.049 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.049 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.049 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.049 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714032767049
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.049 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 667_1816
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.050 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1816,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1714032767033,
  "startTime" : 1714032767049,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13377949373536/12/667/1816.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 12,
  "processInstanceId" : 667,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1816"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425161247"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "667"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_data"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "667_1816",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.050 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.050 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.050 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.053 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.054 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.055 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1816 check successfully
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.055 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.055 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.055 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.055 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.055 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.055 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.056 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.056 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.056 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.056 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.056 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.056 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1816
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.056 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1816/py_667_1816.py
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.057 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.057 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.057 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.057 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1816/py_667_1816.py
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.057 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.058 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1816/667_1816.sh
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:47.061 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6534
[WI-0][TI-1816] - [INFO] 2024-04-25 16:12:47.501 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1816, processInstanceId=667, startTime=1714032767049, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/12/667/1816.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1816] - [INFO] 2024-04-25 16:12:47.508 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1816, processInstanceId=667, startTime=1714032767049, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/12/667/1816.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714032767493)
[WI-0][TI-1816] - [INFO] 2024-04-25 16:12:47.508 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1816, processInstanceId=667, startTime=1714032767049, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1816] - [INFO] 2024-04-25 16:12:47.510 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1816, processInstanceId=667, startTime=1714032767049, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714032767493)
[WI-0][TI-1816] - [INFO] 2024-04-25 16:12:48.012 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1816, success=true)
[WI-0][TI-1816] - [INFO] 2024-04-25 16:12:48.020 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1816)
[WI-0][TI-1816] - [INFO] 2024-04-25 16:12:48.034 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1816, success=true)
[WI-0][TI-1816] - [INFO] 2024-04-25 16:12:48.042 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1816)
[WI-0][TI-0] - [INFO] 2024-04-25 16:12:48.062 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1816/667_1816.sh: line 7: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1816/py_667_1816.py: Permission denied
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:48.065 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1816, processId:6534 ,exitStatusCode:126 ,processWaitForStatus:true ,processExitValue:126
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:48.065 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:48.066 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:48.066 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:48.066 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:48.072 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:48.072 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:48.073 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1816
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:48.074 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1816
[WI-667][TI-1816] - [INFO] 2024-04-25 16:12:48.074 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1816] - [INFO] 2024-04-25 16:12:48.511 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1816, processInstanceId=667, status=6, startTime=1714032767049, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/12/667/1816.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1816, endTime=1714032768066, processId=6534, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1816] - [INFO] 2024-04-25 16:12:48.523 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1816, processInstanceId=667, status=6, startTime=1714032767049, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/12/667/1816.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_12/667/1816, endTime=1714032768066, processId=6534, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714032768511)
[WI-0][TI-1816] - [INFO] 2024-04-25 16:12:49.008 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1816, success=true)
[WI-0][TI-1816] - [INFO] 2024-04-25 16:12:49.012 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1816, success=true)
[WI-0][TI-1792] - [INFO] 2024-04-25 16:13:09.563 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1792, processInstanceId=661, status=9, startTime=1714031949850, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/661/1792.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792, endTime=1714032332757, processId=4479, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714032488854)
[WI-0][TI-1792] - [INFO] 2024-04-25 16:13:09.567 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1792, processInstanceId=661, status=9, startTime=1714031949850, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/661/1792.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792, endTime=1714032332757, processId=4479, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714032789563)
[WI-0][TI-1790] - [INFO] 2024-04-25 16:13:09.568 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1790, processInstanceId=660, status=9, startTime=1714031469392, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1790.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790, endTime=1714032269172, processId=4321, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714032488854)
[WI-0][TI-1790] - [INFO] 2024-04-25 16:13:09.571 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1790, processInstanceId=660, status=9, startTime=1714031469392, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1790.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790, endTime=1714032269172, processId=4321, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714032789563)
[WI-0][TI-0] - [INFO] 2024-04-25 16:13:24.647 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8028985507246377 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:14:56.042 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7098591549295774 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:15:18.148 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1817, taskName=extract from preprocessed queue, firstSubmitTime=1714032918142, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=13, appIds=null, processInstanceId=668, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"message","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n    --bootstrap-server kafka:9092 \\\n    --topic reddit_preprocessed \\\n    --group preprocessed_consumers \\\n    --max-messages 1 \\\n    --timeout-ms 10000)\n\necho \"#{setValue(message=${message})}\"","resourceList":[]}, environmentConfig=export JAVA_HOME=/opt/java/openjdk, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract from preprocessed queue'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1817'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13375898458848'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425161518'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='668'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_data'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.149 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract from preprocessed queue to wait queue success
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.149 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.153 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.153 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.153 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.153 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714032918153
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.153 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 668_1817
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.154 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1817,
  "taskName" : "extract from preprocessed queue",
  "firstSubmitTime" : 1714032918142,
  "startTime" : 1714032918153,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13377949373536/13/668/1817.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 13,
  "processInstanceId" : 668,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"message\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\\\\n    --bootstrap-server kafka:9092 \\\\\\n    --topic reddit_preprocessed \\\\\\n    --group preprocessed_consumers \\\\\\n    --max-messages 1 \\\\\\n    --timeout-ms 10000)\\n\\necho \\\"#{setValue(message=${message})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export JAVA_HOME=/opt/java/openjdk",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract from preprocessed queue"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1817"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13375898458848"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425161518"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "668"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_data"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "668_1817",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.154 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.154 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.155 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.157 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.158 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.158 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1817 check successfully
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.159 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.159 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.159 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.159 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.160 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "message",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n    --bootstrap-server kafka:9092 \\\n    --topic reddit_preprocessed \\\n    --group preprocessed_consumers \\\n    --max-messages 1 \\\n    --timeout-ms 10000)\n\necho \"#{setValue(message=${message})}\"",
  "resourceList" : [ ]
}
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.160 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.160 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.160 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.161 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.171 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.171 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.171 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.171 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export JAVA_HOME=/opt/java/openjdk
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

message=$(${KAFKA_DIR}/kafka-console-consumer.sh \
    --bootstrap-server kafka:9092 \
    --topic reddit_preprocessed \
    --group preprocessed_consumers \
    --max-messages 1 \
    --timeout-ms 10000)

echo "#{setValue(message=${message})}"
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.172 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.172 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1817/668_1817.sh
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:18.175 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6573
[WI-0][TI-1817] - [INFO] 2024-04-25 16:15:18.639 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1817, success=true)
[WI-0][TI-1817] - [INFO] 2024-04-25 16:15:18.645 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1817)
[WI-0][TI-0] - [INFO] 2024-04-25 16:15:19.176 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1817/668_1817.sh: line 12: /opt/kafka_2.13-3.7.0/bin/kafka-console-consumer.sh: No such file or directory
	#{setValue(message=)}
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:19.179 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1817, processId:6573 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-668][TI-1817] - [WARN] 2024-04-25 16:15:19.180 +0800 o.a.d.p.t.a.p.AbstractParameters:[152] - Cannot find the output parameter message in the task output parameters
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:19.180 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:19.180 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:19.180 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:19.181 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:19.183 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:19.183 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:19.183 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1817
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:19.184 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1817
[WI-668][TI-1817] - [INFO] 2024-04-25 16:15:19.184 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1817] - [INFO] 2024-04-25 16:15:19.641 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1817, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:15:19.777 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1818, taskName=keyword filtering, firstSubmitTime=1714032919756, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=13, appIds=null, processInstanceId=668, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1818'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425161519'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='668'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_data'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.778 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.779 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.780 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.780 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.780 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.780 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714032919780
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.780 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 668_1818
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.780 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1818,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1714032919756,
  "startTime" : 1714032919780,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13377949373536/13/668/1818.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 13,
  "processInstanceId" : 668,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1818"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425161519"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "668"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_data"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "668_1818",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.781 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.781 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.781 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.786 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.787 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.787 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1818 check successfully
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.787 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.788 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.788 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.788 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.788 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.789 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.789 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.789 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.789 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.789 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.789 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.789 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1818
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.790 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1818/py_668_1818.py
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.791 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.792 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.792 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.792 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1818/py_668_1818.py
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.792 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.792 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1818/668_1818.sh
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:19.807 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6585
[WI-0][TI-1818] - [INFO] 2024-04-25 16:15:20.075 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1818, processInstanceId=668, startTime=1714032919780, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/668/1818.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1818] - [INFO] 2024-04-25 16:15:20.086 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1818, processInstanceId=668, startTime=1714032919780, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/668/1818.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714032920075)
[WI-0][TI-1818] - [INFO] 2024-04-25 16:15:20.086 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1818, processInstanceId=668, startTime=1714032919780, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1818] - [INFO] 2024-04-25 16:15:20.087 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1818, processInstanceId=668, startTime=1714032919780, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714032920075)
[WI-0][TI-1818] - [INFO] 2024-04-25 16:15:20.649 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1818, success=true)
[WI-0][TI-1818] - [INFO] 2024-04-25 16:15:20.684 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1818)
[WI-0][TI-1818] - [INFO] 2024-04-25 16:15:20.706 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1818, success=true)
[WI-0][TI-1818] - [INFO] 2024-04-25 16:15:20.714 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1818)
[WI-0][TI-0] - [INFO] 2024-04-25 16:15:20.808 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1818/py_668_1818.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:20.812 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1818, processId:6585 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:20.815 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:20.815 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:20.816 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:20.816 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:20.819 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:20.819 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:20.819 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1818
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:20.820 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1818
[WI-668][TI-1818] - [INFO] 2024-04-25 16:15:20.820 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1818] - [INFO] 2024-04-25 16:15:21.093 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1818, processInstanceId=668, status=6, startTime=1714032919780, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/668/1818.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1818, endTime=1714032920816, processId=6585, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1818] - [INFO] 2024-04-25 16:15:21.096 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1818, processInstanceId=668, status=6, startTime=1714032919780, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/668/1818.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1818, endTime=1714032920816, processId=6585, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714032921093)
[WI-0][TI-1818] - [INFO] 2024-04-25 16:15:21.645 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1818, success=true)
[WI-0][TI-1818] - [INFO] 2024-04-25 16:15:21.648 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1818, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:15:21.701 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1819, taskName=keyword filtering, firstSubmitTime=1714032921691, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=13, appIds=null, processInstanceId=668, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1819'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425161521'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='668'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_data'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.702 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.702 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.704 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.704 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.704 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.704 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714032921704
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.704 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 668_1819
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.704 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1819,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1714032921691,
  "startTime" : 1714032921704,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13377949373536/13/668/1819.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 13,
  "processInstanceId" : 668,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1819"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425161521"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "668"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_data"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "668_1819",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.705 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.705 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.705 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.708 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.709 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.710 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1819 check successfully
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.710 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.710 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.710 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.711 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.711 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.711 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.712 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.712 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.712 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.712 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.712 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.713 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1819
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.714 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1819/py_668_1819.py
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.714 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.715 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.715 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.715 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1819/py_668_1819.py
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.715 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.716 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1819/668_1819.sh
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:21.724 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6596
[WI-0][TI-1819] - [INFO] 2024-04-25 16:15:22.104 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1819, processInstanceId=668, startTime=1714032921704, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/668/1819.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1819] - [INFO] 2024-04-25 16:15:22.118 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1819, processInstanceId=668, startTime=1714032921704, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/668/1819.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714032922097)
[WI-0][TI-1819] - [INFO] 2024-04-25 16:15:22.118 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1819, processInstanceId=668, startTime=1714032921704, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1819] - [INFO] 2024-04-25 16:15:22.121 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1819, processInstanceId=668, startTime=1714032921704, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714032922097)
[WI-0][TI-1819] - [INFO] 2024-04-25 16:15:22.650 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1819, success=true)
[WI-0][TI-1819] - [INFO] 2024-04-25 16:15:22.657 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1819)
[WI-0][TI-1819] - [INFO] 2024-04-25 16:15:22.663 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1819, success=true)
[WI-0][TI-1819] - [INFO] 2024-04-25 16:15:22.671 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1819)
[WI-0][TI-0] - [INFO] 2024-04-25 16:15:22.729 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1819/py_668_1819.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:22.748 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1819, processId:6596 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:22.749 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:22.749 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:22.749 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:22.749 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:22.752 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:22.752 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:22.753 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1819
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:22.753 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1819
[WI-668][TI-1819] - [INFO] 2024-04-25 16:15:22.754 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1819] - [INFO] 2024-04-25 16:15:23.122 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1819, processInstanceId=668, status=6, startTime=1714032921704, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/668/1819.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1819, endTime=1714032922749, processId=6596, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1819] - [INFO] 2024-04-25 16:15:23.125 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1819, processInstanceId=668, status=6, startTime=1714032921704, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/668/1819.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1819, endTime=1714032922749, processId=6596, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714032923122)
[WI-0][TI-1819] - [INFO] 2024-04-25 16:15:23.656 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1819, success=true)
[WI-0][TI-1819] - [INFO] 2024-04-25 16:15:23.661 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1819, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:15:23.749 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1820, taskName=keyword filtering, firstSubmitTime=1714032923739, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=13, appIds=null, processInstanceId=668, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1820'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425161523'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='668'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_data'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.764 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.765 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.766 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.767 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.767 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.767 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714032923767
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.767 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 668_1820
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.767 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1820,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1714032923739,
  "startTime" : 1714032923767,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13377949373536/13/668/1820.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 13,
  "processInstanceId" : 668,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1820"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425161523"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "668"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_data"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "668_1820",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.768 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.768 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.768 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.771 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.771 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.772 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1820 check successfully
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.772 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.773 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.773 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.773 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.773 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.774 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.774 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.774 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.774 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.774 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.775 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.776 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1820
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.776 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1820/py_668_1820.py
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.776 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.777 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.777 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.777 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1820/py_668_1820.py
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.777 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.777 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1820/668_1820.sh
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:23.781 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6607
[WI-0][TI-1820] - [INFO] 2024-04-25 16:15:24.126 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1820, processInstanceId=668, startTime=1714032923767, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/668/1820.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1820] - [INFO] 2024-04-25 16:15:24.130 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1820, processInstanceId=668, startTime=1714032923767, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/668/1820.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714032924125)
[WI-0][TI-1820] - [INFO] 2024-04-25 16:15:24.130 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1820, processInstanceId=668, startTime=1714032923767, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1820] - [INFO] 2024-04-25 16:15:24.132 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1820, processInstanceId=668, startTime=1714032923767, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714032924125)
[WI-0][TI-1820] - [INFO] 2024-04-25 16:15:24.658 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1820, success=true)
[WI-0][TI-1820] - [INFO] 2024-04-25 16:15:24.663 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1820)
[WI-0][TI-1820] - [INFO] 2024-04-25 16:15:24.670 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1820, success=true)
[WI-0][TI-1820] - [INFO] 2024-04-25 16:15:24.703 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1820)
[WI-0][TI-0] - [INFO] 2024-04-25 16:15:24.784 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1820/py_668_1820.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:24.794 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1820, processId:6607 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:24.798 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:24.799 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:24.799 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:24.799 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:24.802 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:24.802 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:24.803 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1820
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:24.803 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1820
[WI-668][TI-1820] - [INFO] 2024-04-25 16:15:24.804 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1820] - [INFO] 2024-04-25 16:15:25.136 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1820, processInstanceId=668, status=6, startTime=1714032923767, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/668/1820.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1820, endTime=1714032924799, processId=6607, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1820] - [INFO] 2024-04-25 16:15:25.141 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1820, processInstanceId=668, status=6, startTime=1714032923767, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/668/1820.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1820, endTime=1714032924799, processId=6607, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714032925136)
[WI-0][TI-0] - [INFO] 2024-04-25 16:15:25.144 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8746518105849582 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1820] - [INFO] 2024-04-25 16:15:25.680 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1820, success=true)
[WI-0][TI-1820] - [INFO] 2024-04-25 16:15:25.683 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1820, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:15:25.797 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1821, taskName=keyword filtering, firstSubmitTime=1714032925787, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=13, appIds=null, processInstanceId=668, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1821'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425161525'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='668'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_data'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.798 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.799 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.801 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.801 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.801 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.801 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714032925801
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.801 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 668_1821
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.801 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1821,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1714032925787,
  "startTime" : 1714032925801,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13377949373536/13/668/1821.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 13,
  "processInstanceId" : 668,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1821"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425161525"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "668"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_data"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "668_1821",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.802 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.802 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.802 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.819 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.820 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.820 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1821 check successfully
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.820 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.820 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.820 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.821 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.821 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.821 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.821 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.821 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.821 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.821 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.821 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.822 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1821
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.822 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1821/py_668_1821.py
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.822 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.823 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.823 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.823 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1821/py_668_1821.py
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.823 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.824 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1821/668_1821.sh
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:25.828 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6618
[WI-0][TI-1821] - [INFO] 2024-04-25 16:15:26.142 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1821, processInstanceId=668, startTime=1714032925801, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/668/1821.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1821] - [INFO] 2024-04-25 16:15:26.145 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1821, processInstanceId=668, startTime=1714032925801, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/668/1821.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714032926142)
[WI-0][TI-1821] - [INFO] 2024-04-25 16:15:26.146 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1821, processInstanceId=668, startTime=1714032925801, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1821] - [INFO] 2024-04-25 16:15:26.149 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1821, processInstanceId=668, startTime=1714032925801, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714032926142)
[WI-0][TI-1821] - [INFO] 2024-04-25 16:15:26.660 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1821, success=true)
[WI-0][TI-1821] - [INFO] 2024-04-25 16:15:26.674 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1821)
[WI-0][TI-1821] - [INFO] 2024-04-25 16:15:26.682 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1821, success=true)
[WI-0][TI-1821] - [INFO] 2024-04-25 16:15:26.690 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1821)
[WI-0][TI-0] - [INFO] 2024-04-25 16:15:26.829 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1821/py_668_1821.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:26.845 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1821, processId:6618 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:26.846 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:26.847 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:26.847 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:26.847 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:26.851 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:26.852 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:26.852 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1821
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:26.853 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1821
[WI-668][TI-1821] - [INFO] 2024-04-25 16:15:26.853 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1821] - [INFO] 2024-04-25 16:15:27.149 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1821, processInstanceId=668, status=6, startTime=1714032925801, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/668/1821.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1821, endTime=1714032926847, processId=6618, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1821] - [INFO] 2024-04-25 16:15:27.153 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1821, processInstanceId=668, status=6, startTime=1714032925801, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/668/1821.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/668/1821, endTime=1714032926847, processId=6618, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714032927149)
[WI-0][TI-1821] - [INFO] 2024-04-25 16:15:27.659 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1821, success=true)
[WI-0][TI-1821] - [INFO] 2024-04-25 16:15:27.662 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1821, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:15:28.191 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7036088027085257 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:15:29.202 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8132183908045978 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:15:30.205 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7692307692307693 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:16:02.348 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8355795148247979 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:16:09.445 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7620396600566571 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:16:12.508 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7885714285714285 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:17:09.826 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1822, taskName=Extract Reddit Data, firstSubmitTime=1714033029808, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13045665829504, processDefineVersion=15, appIds=null, processInstanceId=669, scheduleTime=0, globalParams=[{"prop":"subreddit","direct":"IN","type":"VARCHAR","value":"singapore"},{"prop":"limit","direct":"IN","type":"VARCHAR","value":"3"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"data","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"import praw\nimport json\n\nclass RedditPostDataExtractor:\n    def __init__(self):\n        self.data = []\n\n    def extract_data(self, hot_posts):\n        if hot_posts:\n            for post in hot_posts:\n                post.comments.replace_more(limit=None)\n                # initialise a dictionary to contain the data of the post\n                post_data = {}\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the poster's name\n                if post.author is not None:\n                    post_data['author'] = post.author.name\n                else:\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the post was created\n                post_data['unix_time'] = post.created_utc\n                post_data['permalink'] = post.permalink\n                post_data['score'] = post.score\n                post_data['subreddit'] = post.subreddit.display_name\n                post_data['post_title'] = post.title\n                post_data['body'] = post.selftext\n\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\n                # t3 represents that it is a post\n                post_data['name'] = post.name\n\n                # store the posts data in the dictionary where the name is used as the key\n                self.data.append(post_data)\n\n                # extract the comments from the post\n                comments = post.comments.list()\n\n                # extract the data in the comments and store them inside the dictionary\n                self.extract_comments_data(comments)\n        \n        return self.data\n\n    def extract_comments_data(self, comments):\n        if comments:\n            for comment in comments:\n                # initialise a dictionary to contain the data of the comment\n                comment_data = {}\n\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the commenter name\n                if comment.author is not None:\n                    comment_data['author'] = comment.author.name\n                else:\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the comment was created\n                comment_data['unix_time'] = comment.created_utc\n                comment_data['permalink'] = comment.permalink\n                comment_data['score'] = comment.score\n                comment_data['subreddit'] = comment.subreddit.display_name\n                comment_data['post_title'] = comment.submission.title\n                comment_data['body'] = comment.body\n\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\n                # t1 represents that it is a post\n                comment_data['name'] = comment.name\n\n                # store the comment's data in the dictionary where the name is used as the key\n                self.data.append(comment_data)\n\n                # extract the comment's replies from the post\n                # replies = comment.replies\n\n                # extract the data in the comments and store them inside the dictionary\n                # self.extract_comments_data(replies)\n        \n        return\n    \ndef get_hot_posts(reddit_instance, subreddit_name, limit):\n    # Define the subreddit\n    subreddit = reddit_instance.subreddit(subreddit_name)\n\n    # Get hot posts from the subreddit\n    hot_posts = subreddit.hot(limit=limit)\n\n    # Filter out posts that are not pinned\n    filtered_posts = [post for post in hot_posts if not post.stickied]\n    return filtered_posts","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='Extract Reddit Data'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1822'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13045662268160'}, subreddit=Property{prop='subreddit', direct=IN, type=VARCHAR, value='singapore'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425161709'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='669'}, limit=Property{prop='limit', direct=IN, type=VARCHAR, value='3'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13045665829504'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.837 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: Extract Reddit Data to wait queue success
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.840 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.851 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.851 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.851 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.851 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714033029851
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.851 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 669_1822
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.851 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1822,
  "taskName" : "Extract Reddit Data",
  "firstSubmitTime" : 1714033029808,
  "startTime" : 1714033029851,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13045665829504/15/669/1822.log",
  "processId" : 0,
  "processDefineCode" : 13045665829504,
  "processDefineVersion" : 15,
  "processInstanceId" : 669,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"subreddit\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"singapore\"},{\"prop\":\"limit\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"3\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"data\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"import praw\\nimport json\\n\\nclass RedditPostDataExtractor:\\n    def __init__(self):\\n        self.data = []\\n\\n    def extract_data(self, hot_posts):\\n        if hot_posts:\\n            for post in hot_posts:\\n                post.comments.replace_more(limit=None)\\n                # initialise a dictionary to contain the data of the post\\n                post_data = {}\\n\\n                # get the relevant post's data and store it inside post_data\\n\\n                # get the poster's name\\n                if post.author is not None:\\n                    post_data['author'] = post.author.name\\n                else:\\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\\n\\n                # get the unix time in which the post was created\\n                post_data['unix_time'] = post.created_utc\\n                post_data['permalink'] = post.permalink\\n                post_data['score'] = post.score\\n                post_data['subreddit'] = post.subreddit.display_name\\n                post_data['post_title'] = post.title\\n                post_data['body'] = post.selftext\\n\\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\\n                # t3 represents that it is a post\\n                post_data['name'] = post.name\\n\\n                # store the posts data in the dictionary where the name is used as the key\\n                self.data.append(post_data)\\n\\n                # extract the comments from the post\\n                comments = post.comments.list()\\n\\n                # extract the data in the comments and store them inside the dictionary\\n                self.extract_comments_data(comments)\\n        \\n        return self.data\\n\\n    def extract_comments_data(self, comments):\\n        if comments:\\n            for comment in comments:\\n                # initialise a dictionary to contain the data of the comment\\n                comment_data = {}\\n\\n\\n                # get the relevant post's data and store it inside post_data\\n\\n                # get the commenter name\\n                if comment.author is not None:\\n                    comment_data['author'] = comment.author.name\\n                else:\\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\\n\\n                # get the unix time in which the comment was created\\n                comment_data['unix_time'] = comment.created_utc\\n                comment_data['permalink'] = comment.permalink\\n                comment_data['score'] = comment.score\\n                comment_data['subreddit'] = comment.subreddit.display_name\\n                comment_data['post_title'] = comment.submission.title\\n                comment_data['body'] = comment.body\\n\\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\\n                # t1 represents that it is a post\\n                comment_data['name'] = comment.name\\n\\n                # store the comment's data in the dictionary where the name is used as the key\\n                self.data.append(comment_data)\\n\\n                # extract the comment's replies from the post\\n                # replies = comment.replies\\n\\n                # extract the data in the comments and store them inside the dictionary\\n                # self.extract_comments_data(replies)\\n        \\n        return\\n    \\ndef get_hot_posts(reddit_instance, subreddit_name, limit):\\n    # Define the subreddit\\n    subreddit = reddit_instance.subreddit(subreddit_name)\\n\\n    # Get hot posts from the subreddit\\n    hot_posts = subreddit.hot(limit=limit)\\n\\n    # Filter out posts that are not pinned\\n    filtered_posts = [post for post in hot_posts if not post.stickied]\\n    return filtered_posts\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "Extract Reddit Data"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1822"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13045662268160"
    },
    "subreddit" : {
      "prop" : "subreddit",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "singapore"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425161709"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "669"
    },
    "limit" : {
      "prop" : "limit",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13045665829504"
    }
  },
  "taskAppId" : "669_1822",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.852 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.852 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.852 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.878 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.879 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.879 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_15/669/1822 check successfully
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.881 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.881 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.881 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.881 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.882 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "data",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "import praw\nimport json\n\nclass RedditPostDataExtractor:\n    def __init__(self):\n        self.data = []\n\n    def extract_data(self, hot_posts):\n        if hot_posts:\n            for post in hot_posts:\n                post.comments.replace_more(limit=None)\n                # initialise a dictionary to contain the data of the post\n                post_data = {}\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the poster's name\n                if post.author is not None:\n                    post_data['author'] = post.author.name\n                else:\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the post was created\n                post_data['unix_time'] = post.created_utc\n                post_data['permalink'] = post.permalink\n                post_data['score'] = post.score\n                post_data['subreddit'] = post.subreddit.display_name\n                post_data['post_title'] = post.title\n                post_data['body'] = post.selftext\n\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\n                # t3 represents that it is a post\n                post_data['name'] = post.name\n\n                # store the posts data in the dictionary where the name is used as the key\n                self.data.append(post_data)\n\n                # extract the comments from the post\n                comments = post.comments.list()\n\n                # extract the data in the comments and store them inside the dictionary\n                self.extract_comments_data(comments)\n        \n        return self.data\n\n    def extract_comments_data(self, comments):\n        if comments:\n            for comment in comments:\n                # initialise a dictionary to contain the data of the comment\n                comment_data = {}\n\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the commenter name\n                if comment.author is not None:\n                    comment_data['author'] = comment.author.name\n                else:\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the comment was created\n                comment_data['unix_time'] = comment.created_utc\n                comment_data['permalink'] = comment.permalink\n                comment_data['score'] = comment.score\n                comment_data['subreddit'] = comment.subreddit.display_name\n                comment_data['post_title'] = comment.submission.title\n                comment_data['body'] = comment.body\n\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\n                # t1 represents that it is a post\n                comment_data['name'] = comment.name\n\n                # store the comment's data in the dictionary where the name is used as the key\n                self.data.append(comment_data)\n\n                # extract the comment's replies from the post\n                # replies = comment.replies\n\n                # extract the data in the comments and store them inside the dictionary\n                # self.extract_comments_data(replies)\n        \n        return\n    \ndef get_hot_posts(reddit_instance, subreddit_name, limit):\n    # Define the subreddit\n    subreddit = reddit_instance.subreddit(subreddit_name)\n\n    # Get hot posts from the subreddit\n    hot_posts = subreddit.hot(limit=limit)\n\n    # Filter out posts that are not pinned\n    filtered_posts = [post for post in hot_posts if not post.stickied]\n    return filtered_posts",
  "resourceList" : [ ]
}
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.882 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.882 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.882 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.883 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.883 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.883 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import praw
import json

class RedditPostDataExtractor:
    def __init__(self):
        self.data = []

    def extract_data(self, hot_posts):
        if hot_posts:
            for post in hot_posts:
                post.comments.replace_more(limit=None)
                # initialise a dictionary to contain the data of the post
                post_data = {}

                # get the relevant post's data and store it inside post_data

                # get the poster's name
                if post.author is not None:
                    post_data['author'] = post.author.name
                else:
                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the post was created
                post_data['unix_time'] = post.created_utc
                post_data['permalink'] = post.permalink
                post_data['score'] = post.score
                post_data['subreddit'] = post.subreddit.display_name
                post_data['post_title'] = post.title
                post_data['body'] = post.selftext

                # get the unique name of the post (name refers to the id of the post prefixed with t3_)
                # t3 represents that it is a post
                post_data['name'] = post.name

                # store the posts data in the dictionary where the name is used as the key
                self.data.append(post_data)

                # extract the comments from the post
                comments = post.comments.list()

                # extract the data in the comments and store them inside the dictionary
                self.extract_comments_data(comments)
        
        return self.data

    def extract_comments_data(self, comments):
        if comments:
            for comment in comments:
                # initialise a dictionary to contain the data of the comment
                comment_data = {}


                # get the relevant post's data and store it inside post_data

                # get the commenter name
                if comment.author is not None:
                    comment_data['author'] = comment.author.name
                else:
                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the comment was created
                comment_data['unix_time'] = comment.created_utc
                comment_data['permalink'] = comment.permalink
                comment_data['score'] = comment.score
                comment_data['subreddit'] = comment.subreddit.display_name
                comment_data['post_title'] = comment.submission.title
                comment_data['body'] = comment.body

                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)
                # t1 represents that it is a post
                comment_data['name'] = comment.name

                # store the comment's data in the dictionary where the name is used as the key
                self.data.append(comment_data)

                # extract the comment's replies from the post
                # replies = comment.replies

                # extract the data in the comments and store them inside the dictionary
                # self.extract_comments_data(replies)
        
        return
    
def get_hot_posts(reddit_instance, subreddit_name, limit):
    # Define the subreddit
    subreddit = reddit_instance.subreddit(subreddit_name)

    # Get hot posts from the subreddit
    hot_posts = subreddit.hot(limit=limit)

    # Filter out posts that are not pinned
    filtered_posts = [post for post in hot_posts if not post.stickied]
    return filtered_posts
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.884 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_15/669/1822
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.884 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_15/669/1822/py_669_1822.py
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.884 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import praw
import json

class RedditPostDataExtractor:
    def __init__(self):
        self.data = []

    def extract_data(self, hot_posts):
        if hot_posts:
            for post in hot_posts:
                post.comments.replace_more(limit=None)
                # initialise a dictionary to contain the data of the post
                post_data = {}

                # get the relevant post's data and store it inside post_data

                # get the poster's name
                if post.author is not None:
                    post_data['author'] = post.author.name
                else:
                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the post was created
                post_data['unix_time'] = post.created_utc
                post_data['permalink'] = post.permalink
                post_data['score'] = post.score
                post_data['subreddit'] = post.subreddit.display_name
                post_data['post_title'] = post.title
                post_data['body'] = post.selftext

                # get the unique name of the post (name refers to the id of the post prefixed with t3_)
                # t3 represents that it is a post
                post_data['name'] = post.name

                # store the posts data in the dictionary where the name is used as the key
                self.data.append(post_data)

                # extract the comments from the post
                comments = post.comments.list()

                # extract the data in the comments and store them inside the dictionary
                self.extract_comments_data(comments)
        
        return self.data

    def extract_comments_data(self, comments):
        if comments:
            for comment in comments:
                # initialise a dictionary to contain the data of the comment
                comment_data = {}


                # get the relevant post's data and store it inside post_data

                # get the commenter name
                if comment.author is not None:
                    comment_data['author'] = comment.author.name
                else:
                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the comment was created
                comment_data['unix_time'] = comment.created_utc
                comment_data['permalink'] = comment.permalink
                comment_data['score'] = comment.score
                comment_data['subreddit'] = comment.subreddit.display_name
                comment_data['post_title'] = comment.submission.title
                comment_data['body'] = comment.body

                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)
                # t1 represents that it is a post
                comment_data['name'] = comment.name

                # store the comment's data in the dictionary where the name is used as the key
                self.data.append(comment_data)

                # extract the comment's replies from the post
                # replies = comment.replies

                # extract the data in the comments and store them inside the dictionary
                # self.extract_comments_data(replies)
        
        return
    
def get_hot_posts(reddit_instance, subreddit_name, limit):
    # Define the subreddit
    subreddit = reddit_instance.subreddit(subreddit_name)

    # Get hot posts from the subreddit
    hot_posts = subreddit.hot(limit=limit)

    # Filter out posts that are not pinned
    filtered_posts = [post for post in hot_posts if not post.stickied]
    return filtered_posts
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.885 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.885 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.885 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_15/669/1822/py_669_1822.py
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.885 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.886 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_15/669/1822/669_1822.sh
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:09.915 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6653
[WI-0][TI-0] - [INFO] 2024-04-25 16:17:09.915 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1822] - [INFO] 2024-04-25 16:17:10.663 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1822, processInstanceId=669, startTime=1714033029851, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13045665829504/15/669/1822.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1822] - [INFO] 2024-04-25 16:17:10.667 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1822, processInstanceId=669, startTime=1714033029851, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13045665829504/15/669/1822.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714033030663)
[WI-0][TI-1822] - [INFO] 2024-04-25 16:17:10.668 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1822, processInstanceId=669, startTime=1714033029851, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1822] - [INFO] 2024-04-25 16:17:10.669 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1822, processInstanceId=669, startTime=1714033029851, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714033030663)
[WI-0][TI-1822] - [INFO] 2024-04-25 16:17:10.693 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1822, success=true)
[WI-0][TI-1822] - [INFO] 2024-04-25 16:17:10.699 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1822)
[WI-0][TI-1822] - [INFO] 2024-04-25 16:17:10.704 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1822, success=true)
[WI-0][TI-1822] - [INFO] 2024-04-25 16:17:10.708 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1822)
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:10.933 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_15/669/1822, processId:6653 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:10.935 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:10.935 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:10.935 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:10.935 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:10.944 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:10.945 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:10.945 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_15/669/1822
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:10.946 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_15/669/1822
[WI-669][TI-1822] - [INFO] 2024-04-25 16:17:10.947 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1822] - [INFO] 2024-04-25 16:17:11.670 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1822, processInstanceId=669, status=7, startTime=1714033029851, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13045665829504/15/669/1822.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_15/669/1822, endTime=1714033030935, processId=6653, appIds=null, varPool=[{"prop":"data","direct":"OUT","type":"VARCHAR","value":""}], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1822] - [INFO] 2024-04-25 16:17:11.673 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1822, processInstanceId=669, status=7, startTime=1714033029851, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13045665829504/15/669/1822.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_15/669/1822, endTime=1714033030935, processId=6653, appIds=null, varPool=[{"prop":"data","direct":"OUT","type":"VARCHAR","value":""}], eventCreateTime=0, eventSendTime=1714033031670)
[WI-0][TI-1822] - [INFO] 2024-04-25 16:17:11.688 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1822, success=true)
[WI-0][TI-1822] - [INFO] 2024-04-25 16:17:11.691 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1822, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:17:17.783 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7298050139275766 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:17:35.865 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7771260997067448 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1792] - [INFO] 2024-04-25 16:18:09.796 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1792, processInstanceId=661, status=9, startTime=1714031949850, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/661/1792.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792, endTime=1714032332757, processId=4479, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714032789563)
[WI-0][TI-1792] - [INFO] 2024-04-25 16:18:09.800 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1792, processInstanceId=661, status=9, startTime=1714031949850, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/661/1792.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792, endTime=1714032332757, processId=4479, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714033089796)
[WI-0][TI-1790] - [INFO] 2024-04-25 16:18:09.800 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1790, processInstanceId=660, status=9, startTime=1714031469392, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1790.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790, endTime=1714032269172, processId=4321, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714032789563)
[WI-0][TI-1790] - [INFO] 2024-04-25 16:18:09.813 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1790, processInstanceId=660, status=9, startTime=1714031469392, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1790.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790, endTime=1714032269172, processId=4321, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714033089796)
[WI-0][TI-0] - [INFO] 2024-04-25 16:18:13.026 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7304347826086957 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:19:27.338 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7046783625730993 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:19:30.368 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7256637168141593 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:19:47.264 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1823, taskName=Extract Reddit Data, firstSubmitTime=1714033187252, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13045665829504, processDefineVersion=16, appIds=null, processInstanceId=670, scheduleTime=0, globalParams=[{"prop":"subreddit","direct":"IN","type":"VARCHAR","value":"singapore"},{"prop":"limit","direct":"IN","type":"VARCHAR","value":"3"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"data","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"import praw\nimport json\n\nclass RedditPostDataExtractor:\n    def __init__(self):\n        self.data = []\n\n    def extract_data(self, hot_posts):\n        if hot_posts:\n            for post in hot_posts:\n                post.comments.replace_more(limit=None)\n                # initialise a dictionary to contain the data of the post\n                post_data = {}\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the poster's name\n                if post.author is not None:\n                    post_data['author'] = post.author.name\n                else:\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the post was created\n                post_data['unix_time'] = post.created_utc\n                post_data['permalink'] = post.permalink\n                post_data['score'] = post.score\n                post_data['subreddit'] = post.subreddit.display_name\n                post_data['post_title'] = post.title\n                post_data['body'] = post.selftext\n\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\n                # t3 represents that it is a post\n                post_data['name'] = post.name\n\n                # store the posts data in the dictionary where the name is used as the key\n                self.data.append(post_data)\n\n                # extract the comments from the post\n                comments = post.comments.list()\n\n                # extract the data in the comments and store them inside the dictionary\n                self.extract_comments_data(comments)\n        \n        return self.data\n\n    def extract_comments_data(self, comments):\n        if comments:\n            for comment in comments:\n                # initialise a dictionary to contain the data of the comment\n                comment_data = {}\n\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the commenter name\n                if comment.author is not None:\n                    comment_data['author'] = comment.author.name\n                else:\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the comment was created\n                comment_data['unix_time'] = comment.created_utc\n                comment_data['permalink'] = comment.permalink\n                comment_data['score'] = comment.score\n                comment_data['subreddit'] = comment.subreddit.display_name\n                comment_data['post_title'] = comment.submission.title\n                comment_data['body'] = comment.body\n\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\n                # t1 represents that it is a post\n                comment_data['name'] = comment.name\n\n                # store the comment's data in the dictionary where the name is used as the key\n                self.data.append(comment_data)\n\n                # extract the comment's replies from the post\n                # replies = comment.replies\n\n                # extract the data in the comments and store them inside the dictionary\n                # self.extract_comments_data(replies)\n        \n        return\n    \ndef get_hot_posts(reddit_instance, subreddit_name, limit):\n    # Define the subreddit\n    subreddit = reddit_instance.subreddit(subreddit_name)\n\n    # Get hot posts from the subreddit\n    hot_posts = subreddit.hot(limit=limit)\n\n    # Filter out posts that are not pinned\n    filtered_posts = [post for post in hot_posts if not post.stickied]\n    return filtered_posts\n\n\n# initialize Reddit instance\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\n                     user_agent='Prawlingv1')\n\n\n# choose the subreddit to extract data from\nsubreddit = ${subreddit}\nlimit = ${limit}\n\n# get hot posts from the subreddit\nhot_posts = get_hot_posts(reddit, subreddit, limit)\n\n# initialise a reddit data extractor object\ndata_extractor = RedditPostDataExtractor()\n\n# extract data from the hot posts from r/singapore\ndata = data_extractor.extract_data(hot_posts)\ndata","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='Extract Reddit Data'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1823'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13045662268160'}, subreddit=Property{prop='subreddit', direct=IN, type=VARCHAR, value='singapore'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425161947'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='670'}, limit=Property{prop='limit', direct=IN, type=VARCHAR, value='3'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13045665829504'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.266 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: Extract Reddit Data to wait queue success
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.266 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.268 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.268 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.268 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.269 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714033187269
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.269 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 670_1823
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.269 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1823,
  "taskName" : "Extract Reddit Data",
  "firstSubmitTime" : 1714033187252,
  "startTime" : 1714033187269,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13045665829504/16/670/1823.log",
  "processId" : 0,
  "processDefineCode" : 13045665829504,
  "processDefineVersion" : 16,
  "processInstanceId" : 670,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"subreddit\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"singapore\"},{\"prop\":\"limit\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"3\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"data\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"import praw\\nimport json\\n\\nclass RedditPostDataExtractor:\\n    def __init__(self):\\n        self.data = []\\n\\n    def extract_data(self, hot_posts):\\n        if hot_posts:\\n            for post in hot_posts:\\n                post.comments.replace_more(limit=None)\\n                # initialise a dictionary to contain the data of the post\\n                post_data = {}\\n\\n                # get the relevant post's data and store it inside post_data\\n\\n                # get the poster's name\\n                if post.author is not None:\\n                    post_data['author'] = post.author.name\\n                else:\\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\\n\\n                # get the unix time in which the post was created\\n                post_data['unix_time'] = post.created_utc\\n                post_data['permalink'] = post.permalink\\n                post_data['score'] = post.score\\n                post_data['subreddit'] = post.subreddit.display_name\\n                post_data['post_title'] = post.title\\n                post_data['body'] = post.selftext\\n\\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\\n                # t3 represents that it is a post\\n                post_data['name'] = post.name\\n\\n                # store the posts data in the dictionary where the name is used as the key\\n                self.data.append(post_data)\\n\\n                # extract the comments from the post\\n                comments = post.comments.list()\\n\\n                # extract the data in the comments and store them inside the dictionary\\n                self.extract_comments_data(comments)\\n        \\n        return self.data\\n\\n    def extract_comments_data(self, comments):\\n        if comments:\\n            for comment in comments:\\n                # initialise a dictionary to contain the data of the comment\\n                comment_data = {}\\n\\n\\n                # get the relevant post's data and store it inside post_data\\n\\n                # get the commenter name\\n                if comment.author is not None:\\n                    comment_data['author'] = comment.author.name\\n                else:\\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\\n\\n                # get the unix time in which the comment was created\\n                comment_data['unix_time'] = comment.created_utc\\n                comment_data['permalink'] = comment.permalink\\n                comment_data['score'] = comment.score\\n                comment_data['subreddit'] = comment.subreddit.display_name\\n                comment_data['post_title'] = comment.submission.title\\n                comment_data['body'] = comment.body\\n\\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\\n                # t1 represents that it is a post\\n                comment_data['name'] = comment.name\\n\\n                # store the comment's data in the dictionary where the name is used as the key\\n                self.data.append(comment_data)\\n\\n                # extract the comment's replies from the post\\n                # replies = comment.replies\\n\\n                # extract the data in the comments and store them inside the dictionary\\n                # self.extract_comments_data(replies)\\n        \\n        return\\n    \\ndef get_hot_posts(reddit_instance, subreddit_name, limit):\\n    # Define the subreddit\\n    subreddit = reddit_instance.subreddit(subreddit_name)\\n\\n    # Get hot posts from the subreddit\\n    hot_posts = subreddit.hot(limit=limit)\\n\\n    # Filter out posts that are not pinned\\n    filtered_posts = [post for post in hot_posts if not post.stickied]\\n    return filtered_posts\\n\\n\\n# initialize Reddit instance\\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\\n                     user_agent='Prawlingv1')\\n\\n\\n# choose the subreddit to extract data from\\nsubreddit = ${subreddit}\\nlimit = ${limit}\\n\\n# get hot posts from the subreddit\\nhot_posts = get_hot_posts(reddit, subreddit, limit)\\n\\n# initialise a reddit data extractor object\\ndata_extractor = RedditPostDataExtractor()\\n\\n# extract data from the hot posts from r/singapore\\ndata = data_extractor.extract_data(hot_posts)\\ndata\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "Extract Reddit Data"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1823"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13045662268160"
    },
    "subreddit" : {
      "prop" : "subreddit",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "singapore"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425161947"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "670"
    },
    "limit" : {
      "prop" : "limit",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13045665829504"
    }
  },
  "taskAppId" : "670_1823",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.270 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.270 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.270 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.273 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.273 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.274 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_16/670/1823 check successfully
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.274 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.275 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.275 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.275 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.275 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "data",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "import praw\nimport json\n\nclass RedditPostDataExtractor:\n    def __init__(self):\n        self.data = []\n\n    def extract_data(self, hot_posts):\n        if hot_posts:\n            for post in hot_posts:\n                post.comments.replace_more(limit=None)\n                # initialise a dictionary to contain the data of the post\n                post_data = {}\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the poster's name\n                if post.author is not None:\n                    post_data['author'] = post.author.name\n                else:\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the post was created\n                post_data['unix_time'] = post.created_utc\n                post_data['permalink'] = post.permalink\n                post_data['score'] = post.score\n                post_data['subreddit'] = post.subreddit.display_name\n                post_data['post_title'] = post.title\n                post_data['body'] = post.selftext\n\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\n                # t3 represents that it is a post\n                post_data['name'] = post.name\n\n                # store the posts data in the dictionary where the name is used as the key\n                self.data.append(post_data)\n\n                # extract the comments from the post\n                comments = post.comments.list()\n\n                # extract the data in the comments and store them inside the dictionary\n                self.extract_comments_data(comments)\n        \n        return self.data\n\n    def extract_comments_data(self, comments):\n        if comments:\n            for comment in comments:\n                # initialise a dictionary to contain the data of the comment\n                comment_data = {}\n\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the commenter name\n                if comment.author is not None:\n                    comment_data['author'] = comment.author.name\n                else:\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the comment was created\n                comment_data['unix_time'] = comment.created_utc\n                comment_data['permalink'] = comment.permalink\n                comment_data['score'] = comment.score\n                comment_data['subreddit'] = comment.subreddit.display_name\n                comment_data['post_title'] = comment.submission.title\n                comment_data['body'] = comment.body\n\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\n                # t1 represents that it is a post\n                comment_data['name'] = comment.name\n\n                # store the comment's data in the dictionary where the name is used as the key\n                self.data.append(comment_data)\n\n                # extract the comment's replies from the post\n                # replies = comment.replies\n\n                # extract the data in the comments and store them inside the dictionary\n                # self.extract_comments_data(replies)\n        \n        return\n    \ndef get_hot_posts(reddit_instance, subreddit_name, limit):\n    # Define the subreddit\n    subreddit = reddit_instance.subreddit(subreddit_name)\n\n    # Get hot posts from the subreddit\n    hot_posts = subreddit.hot(limit=limit)\n\n    # Filter out posts that are not pinned\n    filtered_posts = [post for post in hot_posts if not post.stickied]\n    return filtered_posts\n\n\n# initialize Reddit instance\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\n                     user_agent='Prawlingv1')\n\n\n# choose the subreddit to extract data from\nsubreddit = ${subreddit}\nlimit = ${limit}\n\n# get hot posts from the subreddit\nhot_posts = get_hot_posts(reddit, subreddit, limit)\n\n# initialise a reddit data extractor object\ndata_extractor = RedditPostDataExtractor()\n\n# extract data from the hot posts from r/singapore\ndata = data_extractor.extract_data(hot_posts)\ndata",
  "resourceList" : [ ]
}
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.276 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.276 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.276 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.276 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.276 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.276 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import praw
import json

class RedditPostDataExtractor:
    def __init__(self):
        self.data = []

    def extract_data(self, hot_posts):
        if hot_posts:
            for post in hot_posts:
                post.comments.replace_more(limit=None)
                # initialise a dictionary to contain the data of the post
                post_data = {}

                # get the relevant post's data and store it inside post_data

                # get the poster's name
                if post.author is not None:
                    post_data['author'] = post.author.name
                else:
                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the post was created
                post_data['unix_time'] = post.created_utc
                post_data['permalink'] = post.permalink
                post_data['score'] = post.score
                post_data['subreddit'] = post.subreddit.display_name
                post_data['post_title'] = post.title
                post_data['body'] = post.selftext

                # get the unique name of the post (name refers to the id of the post prefixed with t3_)
                # t3 represents that it is a post
                post_data['name'] = post.name

                # store the posts data in the dictionary where the name is used as the key
                self.data.append(post_data)

                # extract the comments from the post
                comments = post.comments.list()

                # extract the data in the comments and store them inside the dictionary
                self.extract_comments_data(comments)
        
        return self.data

    def extract_comments_data(self, comments):
        if comments:
            for comment in comments:
                # initialise a dictionary to contain the data of the comment
                comment_data = {}


                # get the relevant post's data and store it inside post_data

                # get the commenter name
                if comment.author is not None:
                    comment_data['author'] = comment.author.name
                else:
                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the comment was created
                comment_data['unix_time'] = comment.created_utc
                comment_data['permalink'] = comment.permalink
                comment_data['score'] = comment.score
                comment_data['subreddit'] = comment.subreddit.display_name
                comment_data['post_title'] = comment.submission.title
                comment_data['body'] = comment.body

                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)
                # t1 represents that it is a post
                comment_data['name'] = comment.name

                # store the comment's data in the dictionary where the name is used as the key
                self.data.append(comment_data)

                # extract the comment's replies from the post
                # replies = comment.replies

                # extract the data in the comments and store them inside the dictionary
                # self.extract_comments_data(replies)
        
        return
    
def get_hot_posts(reddit_instance, subreddit_name, limit):
    # Define the subreddit
    subreddit = reddit_instance.subreddit(subreddit_name)

    # Get hot posts from the subreddit
    hot_posts = subreddit.hot(limit=limit)

    # Filter out posts that are not pinned
    filtered_posts = [post for post in hot_posts if not post.stickied]
    return filtered_posts


# initialize Reddit instance
reddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',
                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',
                     user_agent='Prawlingv1')


# choose the subreddit to extract data from
subreddit = ${subreddit}
limit = ${limit}

# get hot posts from the subreddit
hot_posts = get_hot_posts(reddit, subreddit, limit)

# initialise a reddit data extractor object
data_extractor = RedditPostDataExtractor()

# extract data from the hot posts from r/singapore
data = data_extractor.extract_data(hot_posts)
data
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.278 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_16/670/1823
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.278 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_16/670/1823/py_670_1823.py
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.278 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import praw
import json

class RedditPostDataExtractor:
    def __init__(self):
        self.data = []

    def extract_data(self, hot_posts):
        if hot_posts:
            for post in hot_posts:
                post.comments.replace_more(limit=None)
                # initialise a dictionary to contain the data of the post
                post_data = {}

                # get the relevant post's data and store it inside post_data

                # get the poster's name
                if post.author is not None:
                    post_data['author'] = post.author.name
                else:
                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the post was created
                post_data['unix_time'] = post.created_utc
                post_data['permalink'] = post.permalink
                post_data['score'] = post.score
                post_data['subreddit'] = post.subreddit.display_name
                post_data['post_title'] = post.title
                post_data['body'] = post.selftext

                # get the unique name of the post (name refers to the id of the post prefixed with t3_)
                # t3 represents that it is a post
                post_data['name'] = post.name

                # store the posts data in the dictionary where the name is used as the key
                self.data.append(post_data)

                # extract the comments from the post
                comments = post.comments.list()

                # extract the data in the comments and store them inside the dictionary
                self.extract_comments_data(comments)
        
        return self.data

    def extract_comments_data(self, comments):
        if comments:
            for comment in comments:
                # initialise a dictionary to contain the data of the comment
                comment_data = {}


                # get the relevant post's data and store it inside post_data

                # get the commenter name
                if comment.author is not None:
                    comment_data['author'] = comment.author.name
                else:
                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the comment was created
                comment_data['unix_time'] = comment.created_utc
                comment_data['permalink'] = comment.permalink
                comment_data['score'] = comment.score
                comment_data['subreddit'] = comment.subreddit.display_name
                comment_data['post_title'] = comment.submission.title
                comment_data['body'] = comment.body

                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)
                # t1 represents that it is a post
                comment_data['name'] = comment.name

                # store the comment's data in the dictionary where the name is used as the key
                self.data.append(comment_data)

                # extract the comment's replies from the post
                # replies = comment.replies

                # extract the data in the comments and store them inside the dictionary
                # self.extract_comments_data(replies)
        
        return
    
def get_hot_posts(reddit_instance, subreddit_name, limit):
    # Define the subreddit
    subreddit = reddit_instance.subreddit(subreddit_name)

    # Get hot posts from the subreddit
    hot_posts = subreddit.hot(limit=limit)

    # Filter out posts that are not pinned
    filtered_posts = [post for post in hot_posts if not post.stickied]
    return filtered_posts


# initialize Reddit instance
reddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',
                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',
                     user_agent='Prawlingv1')


# choose the subreddit to extract data from
subreddit = singapore
limit = 3

# get hot posts from the subreddit
hot_posts = get_hot_posts(reddit, subreddit, limit)

# initialise a reddit data extractor object
data_extractor = RedditPostDataExtractor()

# extract data from the hot posts from r/singapore
data = data_extractor.extract_data(hot_posts)
data
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.279 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.279 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.280 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_16/670/1823/py_670_1823.py
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.280 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.280 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_16/670/1823/670_1823.sh
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:47.294 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6695
[WI-0][TI-0] - [INFO] 2024-04-25 16:19:47.324 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1823] - [INFO] 2024-04-25 16:19:48.031 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1823, success=true)
[WI-0][TI-1823] - [INFO] 2024-04-25 16:19:48.037 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1823)
[WI-0][TI-0] - [INFO] 2024-04-25 16:19:48.331 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_16/670/1823/py_670_1823.py", line 105, in <module>
	    subreddit = singapore
	                ^^^^^^^^^
	NameError: name 'singapore' is not defined
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:48.333 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_16/670/1823, processId:6695 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:48.335 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:48.335 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:48.335 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:48.336 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:48.338 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:48.338 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:48.338 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_16/670/1823
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:48.339 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_16/670/1823
[WI-670][TI-1823] - [INFO] 2024-04-25 16:19:48.339 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1823] - [INFO] 2024-04-25 16:19:49.044 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1823, processInstanceId=670, status=6, startTime=1714033187269, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13045665829504/16/670/1823.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_16/670/1823, endTime=1714033188336, processId=6695, appIds=null, varPool=[{"prop":"data","direct":"OUT","type":"VARCHAR","value":""}], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1823] - [INFO] 2024-04-25 16:19:49.050 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1823, success=true)
[WI-0][TI-1823] - [INFO] 2024-04-25 16:19:49.052 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1823, processInstanceId=670, status=6, startTime=1714033187269, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13045665829504/16/670/1823.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_16/670/1823, endTime=1714033188336, processId=6695, appIds=null, varPool=[{"prop":"data","direct":"OUT","type":"VARCHAR","value":""}], eventCreateTime=0, eventSendTime=1714033189044)
[WI-0][TI-1823] - [WARN] 2024-04-25 16:19:49.052 +0800 o.a.d.s.w.m.MessageRetryRunner:[139] - Retry send message to master error
java.util.ConcurrentModificationException: null
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:911)
	at java.util.ArrayList$Itr.next(ArrayList.java:861)
	at org.apache.dolphinscheduler.server.worker.message.MessageRetryRunner.run(MessageRetryRunner.java:127)
[WI-0][TI-1823] - [INFO] 2024-04-25 16:19:49.062 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1823, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:20:01.479 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7613636363636364 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:20:13.379 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1824, taskName=search for intake JSON files, firstSubmitTime=1714033213358, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=114, appIds=null, processInstanceId=671, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1824'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340113777664'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425162013'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='671'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.380 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake JSON files to wait queue success
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.380 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.384 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.384 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.384 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.384 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714033213384
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.384 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 671_1824
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.385 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1824,
  "taskName" : "search for intake JSON files",
  "firstSubmitTime" : 1714033213358,
  "startTime" : 1714033213384,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13201021801792/114/671/1824.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 114,
  "processInstanceId" : 671,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set raw_file_dir to null\\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1824"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340113777664"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425162013"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "671"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "671_1824",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.385 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.385 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.385 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.394 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.395 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.396 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/671/1824 check successfully
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.397 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.397 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.397 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.397 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.397 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"",
  "resourceList" : [ ]
}
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.397 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.397 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.397 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.398 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.398 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.398 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.398 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.398 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/reddit/in -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set raw_file_dir to null
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
fi

echo "#{setValue(raw_file_dir=${raw_file_dir})}"
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.398 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.398 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/671/1824/671_1824.sh
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:13.422 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6712
[WI-0][TI-1824] - [INFO] 2024-04-25 16:20:14.072 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1824, success=true)
[WI-0][TI-1824] - [INFO] 2024-04-25 16:20:14.078 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1824)
[WI-0][TI-0] - [INFO] 2024-04-25 16:20:14.425 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	#{setValue(raw_file_dir=null)}
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:14.428 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/671/1824, processId:6712 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:14.429 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:14.430 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:14.430 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:14.432 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:14.434 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:14.435 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:14.435 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/671/1824
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:14.436 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/671/1824
[WI-671][TI-1824] - [INFO] 2024-04-25 16:20:14.436 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1824] - [INFO] 2024-04-25 16:20:15.067 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1824, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:20:16.240 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1826, taskName=end workflow, firstSubmitTime=1714033216224, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=114, appIds=null, processInstanceId=671, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"echo No Files Detected - End Workflow","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='end workflow'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1826'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13348985859488'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425162016'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='671'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='null'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"null"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.241 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: end workflow to wait queue success
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.241 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.243 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.243 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.243 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.243 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714033216243
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.243 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 671_1826
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.243 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1826,
  "taskName" : "end workflow",
  "firstSubmitTime" : 1714033216224,
  "startTime" : 1714033216243,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13201021801792/114/671/1826.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 114,
  "processInstanceId" : 671,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"echo No Files Detected - End Workflow\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "end workflow"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1826"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13348985859488"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425162016"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "671"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "null"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "671_1826",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"null\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.244 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.244 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.244 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.253 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.253 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.254 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/671/1826 check successfully
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.254 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.255 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.255 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.255 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.255 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "echo No Files Detected - End Workflow",
  "resourceList" : [ ]
}
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.255 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.255 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"null"}] successfully
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.256 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.256 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.256 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.257 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.257 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.257 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
echo No Files Detected - End Workflow
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.257 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.257 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/671/1826/671_1826.sh
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:16.268 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6725
[WI-0][TI-0] - [INFO] 2024-04-25 16:20:16.586 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7712609970674487 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1826] - [INFO] 2024-04-25 16:20:17.081 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1826, success=true)
[WI-0][TI-1826] - [INFO] 2024-04-25 16:20:17.090 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1826)
[WI-0][TI-0] - [INFO] 2024-04-25 16:20:17.283 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	No Files Detected - End Workflow
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:17.289 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/671/1826, processId:6725 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:17.290 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:17.290 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:17.291 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:17.293 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:17.295 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:17.295 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:17.296 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/671/1826
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:17.296 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/671/1826
[WI-671][TI-1826] - [INFO] 2024-04-25 16:20:17.296 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1826] - [INFO] 2024-04-25 16:20:18.081 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1826, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:20:30.686 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8155619596541787 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:20:43.839 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7458563535911602 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:20:44.847 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8801089918256132 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:20:47.561 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1827, taskName=Extract Reddit Data, firstSubmitTime=1714033247550, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13045665829504, processDefineVersion=16, appIds=null, processInstanceId=672, scheduleTime=0, globalParams=[{"prop":"subreddit","direct":"IN","type":"VARCHAR","value":"singapore"},{"prop":"limit","direct":"IN","type":"VARCHAR","value":"3"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"data","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"import praw\nimport json\n\nclass RedditPostDataExtractor:\n    def __init__(self):\n        self.data = []\n\n    def extract_data(self, hot_posts):\n        if hot_posts:\n            for post in hot_posts:\n                post.comments.replace_more(limit=None)\n                # initialise a dictionary to contain the data of the post\n                post_data = {}\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the poster's name\n                if post.author is not None:\n                    post_data['author'] = post.author.name\n                else:\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the post was created\n                post_data['unix_time'] = post.created_utc\n                post_data['permalink'] = post.permalink\n                post_data['score'] = post.score\n                post_data['subreddit'] = post.subreddit.display_name\n                post_data['post_title'] = post.title\n                post_data['body'] = post.selftext\n\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\n                # t3 represents that it is a post\n                post_data['name'] = post.name\n\n                # store the posts data in the dictionary where the name is used as the key\n                self.data.append(post_data)\n\n                # extract the comments from the post\n                comments = post.comments.list()\n\n                # extract the data in the comments and store them inside the dictionary\n                self.extract_comments_data(comments)\n        \n        return self.data\n\n    def extract_comments_data(self, comments):\n        if comments:\n            for comment in comments:\n                # initialise a dictionary to contain the data of the comment\n                comment_data = {}\n\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the commenter name\n                if comment.author is not None:\n                    comment_data['author'] = comment.author.name\n                else:\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the comment was created\n                comment_data['unix_time'] = comment.created_utc\n                comment_data['permalink'] = comment.permalink\n                comment_data['score'] = comment.score\n                comment_data['subreddit'] = comment.subreddit.display_name\n                comment_data['post_title'] = comment.submission.title\n                comment_data['body'] = comment.body\n\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\n                # t1 represents that it is a post\n                comment_data['name'] = comment.name\n\n                # store the comment's data in the dictionary where the name is used as the key\n                self.data.append(comment_data)\n\n                # extract the comment's replies from the post\n                # replies = comment.replies\n\n                # extract the data in the comments and store them inside the dictionary\n                # self.extract_comments_data(replies)\n        \n        return\n    \ndef get_hot_posts(reddit_instance, subreddit_name, limit):\n    # Define the subreddit\n    subreddit = reddit_instance.subreddit(subreddit_name)\n\n    # Get hot posts from the subreddit\n    hot_posts = subreddit.hot(limit=limit)\n\n    # Filter out posts that are not pinned\n    filtered_posts = [post for post in hot_posts if not post.stickied]\n    return filtered_posts\n\n\n# initialize Reddit instance\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\n                     user_agent='Prawlingv1')\n\n\n# choose the subreddit to extract data from\nsubreddit = ${subreddit}\nlimit = ${limit}\n\n# get hot posts from the subreddit\nhot_posts = get_hot_posts(reddit, subreddit, limit)\n\n# initialise a reddit data extractor object\ndata_extractor = RedditPostDataExtractor()\n\n# extract data from the hot posts from r/singapore\ndata = data_extractor.extract_data(hot_posts)\ndata","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='Extract Reddit Data'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1827'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13045662268160'}, subreddit=Property{prop='subreddit', direct=IN, type=VARCHAR, value='singapore'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425162047'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='672'}, limit=Property{prop='limit', direct=IN, type=VARCHAR, value='3'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13045665829504'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.565 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: Extract Reddit Data to wait queue success
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.565 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.571 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.571 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.572 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.572 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714033247572
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.572 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 672_1827
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.572 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1827,
  "taskName" : "Extract Reddit Data",
  "firstSubmitTime" : 1714033247550,
  "startTime" : 1714033247572,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13045665829504/16/672/1827.log",
  "processId" : 0,
  "processDefineCode" : 13045665829504,
  "processDefineVersion" : 16,
  "processInstanceId" : 672,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"subreddit\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"singapore\"},{\"prop\":\"limit\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"3\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"data\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"import praw\\nimport json\\n\\nclass RedditPostDataExtractor:\\n    def __init__(self):\\n        self.data = []\\n\\n    def extract_data(self, hot_posts):\\n        if hot_posts:\\n            for post in hot_posts:\\n                post.comments.replace_more(limit=None)\\n                # initialise a dictionary to contain the data of the post\\n                post_data = {}\\n\\n                # get the relevant post's data and store it inside post_data\\n\\n                # get the poster's name\\n                if post.author is not None:\\n                    post_data['author'] = post.author.name\\n                else:\\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\\n\\n                # get the unix time in which the post was created\\n                post_data['unix_time'] = post.created_utc\\n                post_data['permalink'] = post.permalink\\n                post_data['score'] = post.score\\n                post_data['subreddit'] = post.subreddit.display_name\\n                post_data['post_title'] = post.title\\n                post_data['body'] = post.selftext\\n\\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\\n                # t3 represents that it is a post\\n                post_data['name'] = post.name\\n\\n                # store the posts data in the dictionary where the name is used as the key\\n                self.data.append(post_data)\\n\\n                # extract the comments from the post\\n                comments = post.comments.list()\\n\\n                # extract the data in the comments and store them inside the dictionary\\n                self.extract_comments_data(comments)\\n        \\n        return self.data\\n\\n    def extract_comments_data(self, comments):\\n        if comments:\\n            for comment in comments:\\n                # initialise a dictionary to contain the data of the comment\\n                comment_data = {}\\n\\n\\n                # get the relevant post's data and store it inside post_data\\n\\n                # get the commenter name\\n                if comment.author is not None:\\n                    comment_data['author'] = comment.author.name\\n                else:\\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\\n\\n                # get the unix time in which the comment was created\\n                comment_data['unix_time'] = comment.created_utc\\n                comment_data['permalink'] = comment.permalink\\n                comment_data['score'] = comment.score\\n                comment_data['subreddit'] = comment.subreddit.display_name\\n                comment_data['post_title'] = comment.submission.title\\n                comment_data['body'] = comment.body\\n\\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\\n                # t1 represents that it is a post\\n                comment_data['name'] = comment.name\\n\\n                # store the comment's data in the dictionary where the name is used as the key\\n                self.data.append(comment_data)\\n\\n                # extract the comment's replies from the post\\n                # replies = comment.replies\\n\\n                # extract the data in the comments and store them inside the dictionary\\n                # self.extract_comments_data(replies)\\n        \\n        return\\n    \\ndef get_hot_posts(reddit_instance, subreddit_name, limit):\\n    # Define the subreddit\\n    subreddit = reddit_instance.subreddit(subreddit_name)\\n\\n    # Get hot posts from the subreddit\\n    hot_posts = subreddit.hot(limit=limit)\\n\\n    # Filter out posts that are not pinned\\n    filtered_posts = [post for post in hot_posts if not post.stickied]\\n    return filtered_posts\\n\\n\\n# initialize Reddit instance\\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\\n                     user_agent='Prawlingv1')\\n\\n\\n# choose the subreddit to extract data from\\nsubreddit = ${subreddit}\\nlimit = ${limit}\\n\\n# get hot posts from the subreddit\\nhot_posts = get_hot_posts(reddit, subreddit, limit)\\n\\n# initialise a reddit data extractor object\\ndata_extractor = RedditPostDataExtractor()\\n\\n# extract data from the hot posts from r/singapore\\ndata = data_extractor.extract_data(hot_posts)\\ndata\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "Extract Reddit Data"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1827"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13045662268160"
    },
    "subreddit" : {
      "prop" : "subreddit",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "singapore"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425162047"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "672"
    },
    "limit" : {
      "prop" : "limit",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13045665829504"
    }
  },
  "taskAppId" : "672_1827",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.573 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.573 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.573 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.580 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.583 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.583 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_16/672/1827 check successfully
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.584 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.584 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.584 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.584 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.584 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "data",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "import praw\nimport json\n\nclass RedditPostDataExtractor:\n    def __init__(self):\n        self.data = []\n\n    def extract_data(self, hot_posts):\n        if hot_posts:\n            for post in hot_posts:\n                post.comments.replace_more(limit=None)\n                # initialise a dictionary to contain the data of the post\n                post_data = {}\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the poster's name\n                if post.author is not None:\n                    post_data['author'] = post.author.name\n                else:\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the post was created\n                post_data['unix_time'] = post.created_utc\n                post_data['permalink'] = post.permalink\n                post_data['score'] = post.score\n                post_data['subreddit'] = post.subreddit.display_name\n                post_data['post_title'] = post.title\n                post_data['body'] = post.selftext\n\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\n                # t3 represents that it is a post\n                post_data['name'] = post.name\n\n                # store the posts data in the dictionary where the name is used as the key\n                self.data.append(post_data)\n\n                # extract the comments from the post\n                comments = post.comments.list()\n\n                # extract the data in the comments and store them inside the dictionary\n                self.extract_comments_data(comments)\n        \n        return self.data\n\n    def extract_comments_data(self, comments):\n        if comments:\n            for comment in comments:\n                # initialise a dictionary to contain the data of the comment\n                comment_data = {}\n\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the commenter name\n                if comment.author is not None:\n                    comment_data['author'] = comment.author.name\n                else:\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the comment was created\n                comment_data['unix_time'] = comment.created_utc\n                comment_data['permalink'] = comment.permalink\n                comment_data['score'] = comment.score\n                comment_data['subreddit'] = comment.subreddit.display_name\n                comment_data['post_title'] = comment.submission.title\n                comment_data['body'] = comment.body\n\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\n                # t1 represents that it is a post\n                comment_data['name'] = comment.name\n\n                # store the comment's data in the dictionary where the name is used as the key\n                self.data.append(comment_data)\n\n                # extract the comment's replies from the post\n                # replies = comment.replies\n\n                # extract the data in the comments and store them inside the dictionary\n                # self.extract_comments_data(replies)\n        \n        return\n    \ndef get_hot_posts(reddit_instance, subreddit_name, limit):\n    # Define the subreddit\n    subreddit = reddit_instance.subreddit(subreddit_name)\n\n    # Get hot posts from the subreddit\n    hot_posts = subreddit.hot(limit=limit)\n\n    # Filter out posts that are not pinned\n    filtered_posts = [post for post in hot_posts if not post.stickied]\n    return filtered_posts\n\n\n# initialize Reddit instance\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\n                     user_agent='Prawlingv1')\n\n\n# choose the subreddit to extract data from\nsubreddit = ${subreddit}\nlimit = ${limit}\n\n# get hot posts from the subreddit\nhot_posts = get_hot_posts(reddit, subreddit, limit)\n\n# initialise a reddit data extractor object\ndata_extractor = RedditPostDataExtractor()\n\n# extract data from the hot posts from r/singapore\ndata = data_extractor.extract_data(hot_posts)\ndata",
  "resourceList" : [ ]
}
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.585 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.585 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.585 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.585 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.585 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.585 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import praw
import json

class RedditPostDataExtractor:
    def __init__(self):
        self.data = []

    def extract_data(self, hot_posts):
        if hot_posts:
            for post in hot_posts:
                post.comments.replace_more(limit=None)
                # initialise a dictionary to contain the data of the post
                post_data = {}

                # get the relevant post's data and store it inside post_data

                # get the poster's name
                if post.author is not None:
                    post_data['author'] = post.author.name
                else:
                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the post was created
                post_data['unix_time'] = post.created_utc
                post_data['permalink'] = post.permalink
                post_data['score'] = post.score
                post_data['subreddit'] = post.subreddit.display_name
                post_data['post_title'] = post.title
                post_data['body'] = post.selftext

                # get the unique name of the post (name refers to the id of the post prefixed with t3_)
                # t3 represents that it is a post
                post_data['name'] = post.name

                # store the posts data in the dictionary where the name is used as the key
                self.data.append(post_data)

                # extract the comments from the post
                comments = post.comments.list()

                # extract the data in the comments and store them inside the dictionary
                self.extract_comments_data(comments)
        
        return self.data

    def extract_comments_data(self, comments):
        if comments:
            for comment in comments:
                # initialise a dictionary to contain the data of the comment
                comment_data = {}


                # get the relevant post's data and store it inside post_data

                # get the commenter name
                if comment.author is not None:
                    comment_data['author'] = comment.author.name
                else:
                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the comment was created
                comment_data['unix_time'] = comment.created_utc
                comment_data['permalink'] = comment.permalink
                comment_data['score'] = comment.score
                comment_data['subreddit'] = comment.subreddit.display_name
                comment_data['post_title'] = comment.submission.title
                comment_data['body'] = comment.body

                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)
                # t1 represents that it is a post
                comment_data['name'] = comment.name

                # store the comment's data in the dictionary where the name is used as the key
                self.data.append(comment_data)

                # extract the comment's replies from the post
                # replies = comment.replies

                # extract the data in the comments and store them inside the dictionary
                # self.extract_comments_data(replies)
        
        return
    
def get_hot_posts(reddit_instance, subreddit_name, limit):
    # Define the subreddit
    subreddit = reddit_instance.subreddit(subreddit_name)

    # Get hot posts from the subreddit
    hot_posts = subreddit.hot(limit=limit)

    # Filter out posts that are not pinned
    filtered_posts = [post for post in hot_posts if not post.stickied]
    return filtered_posts


# initialize Reddit instance
reddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',
                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',
                     user_agent='Prawlingv1')


# choose the subreddit to extract data from
subreddit = ${subreddit}
limit = ${limit}

# get hot posts from the subreddit
hot_posts = get_hot_posts(reddit, subreddit, limit)

# initialise a reddit data extractor object
data_extractor = RedditPostDataExtractor()

# extract data from the hot posts from r/singapore
data = data_extractor.extract_data(hot_posts)
data
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.594 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_16/672/1827
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.594 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_16/672/1827/py_672_1827.py
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.596 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import praw
import json

class RedditPostDataExtractor:
    def __init__(self):
        self.data = []

    def extract_data(self, hot_posts):
        if hot_posts:
            for post in hot_posts:
                post.comments.replace_more(limit=None)
                # initialise a dictionary to contain the data of the post
                post_data = {}

                # get the relevant post's data and store it inside post_data

                # get the poster's name
                if post.author is not None:
                    post_data['author'] = post.author.name
                else:
                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the post was created
                post_data['unix_time'] = post.created_utc
                post_data['permalink'] = post.permalink
                post_data['score'] = post.score
                post_data['subreddit'] = post.subreddit.display_name
                post_data['post_title'] = post.title
                post_data['body'] = post.selftext

                # get the unique name of the post (name refers to the id of the post prefixed with t3_)
                # t3 represents that it is a post
                post_data['name'] = post.name

                # store the posts data in the dictionary where the name is used as the key
                self.data.append(post_data)

                # extract the comments from the post
                comments = post.comments.list()

                # extract the data in the comments and store them inside the dictionary
                self.extract_comments_data(comments)
        
        return self.data

    def extract_comments_data(self, comments):
        if comments:
            for comment in comments:
                # initialise a dictionary to contain the data of the comment
                comment_data = {}


                # get the relevant post's data and store it inside post_data

                # get the commenter name
                if comment.author is not None:
                    comment_data['author'] = comment.author.name
                else:
                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the comment was created
                comment_data['unix_time'] = comment.created_utc
                comment_data['permalink'] = comment.permalink
                comment_data['score'] = comment.score
                comment_data['subreddit'] = comment.subreddit.display_name
                comment_data['post_title'] = comment.submission.title
                comment_data['body'] = comment.body

                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)
                # t1 represents that it is a post
                comment_data['name'] = comment.name

                # store the comment's data in the dictionary where the name is used as the key
                self.data.append(comment_data)

                # extract the comment's replies from the post
                # replies = comment.replies

                # extract the data in the comments and store them inside the dictionary
                # self.extract_comments_data(replies)
        
        return
    
def get_hot_posts(reddit_instance, subreddit_name, limit):
    # Define the subreddit
    subreddit = reddit_instance.subreddit(subreddit_name)

    # Get hot posts from the subreddit
    hot_posts = subreddit.hot(limit=limit)

    # Filter out posts that are not pinned
    filtered_posts = [post for post in hot_posts if not post.stickied]
    return filtered_posts


# initialize Reddit instance
reddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',
                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',
                     user_agent='Prawlingv1')


# choose the subreddit to extract data from
subreddit = singapore
limit = 3

# get hot posts from the subreddit
hot_posts = get_hot_posts(reddit, subreddit, limit)

# initialise a reddit data extractor object
data_extractor = RedditPostDataExtractor()

# extract data from the hot posts from r/singapore
data = data_extractor.extract_data(hot_posts)
data
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.598 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.598 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.599 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_16/672/1827/py_672_1827.py
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.599 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.599 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_16/672/1827/672_1827.sh
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:47.630 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6741
[WI-0][TI-1827] - [INFO] 2024-04-25 16:20:48.160 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1827, processInstanceId=672, startTime=1714033247572, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13045665829504/16/672/1827.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1827] - [INFO] 2024-04-25 16:20:48.160 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1827, success=true)
[WI-0][TI-1827] - [INFO] 2024-04-25 16:20:48.165 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1827, processInstanceId=672, startTime=1714033247572, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13045665829504/16/672/1827.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714033248150)
[WI-0][TI-1827] - [INFO] 2024-04-25 16:20:48.168 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1827)
[WI-0][TI-1827] - [INFO] 2024-04-25 16:20:48.175 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1827, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:20:48.632 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_16/672/1827/py_672_1827.py", line 105, in <module>
	    subreddit = singapore
	                ^^^^^^^^^
	NameError: name 'singapore' is not defined
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:48.637 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_16/672/1827, processId:6741 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:48.640 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:48.641 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:48.641 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:48.641 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:48.644 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:48.645 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:48.645 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_16/672/1827
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:48.645 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_16/672/1827
[WI-672][TI-1827] - [INFO] 2024-04-25 16:20:48.646 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1827] - [INFO] 2024-04-25 16:20:49.141 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1827, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:20:51.866 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7941176470588235 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:20:53.904 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7066666666666667 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:20:54.913 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7206703910614525 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:21:06.969 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7621776504297995 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:21:11.015 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7134328358208956 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:21:23.088 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7287671232876712 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:21:24.146 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.773841961852861 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:21:28.172 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7023809523809523 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:21:29.739 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1828, taskName=Extract Reddit Data, firstSubmitTime=1714033289723, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13045665829504, processDefineVersion=17, appIds=null, processInstanceId=673, scheduleTime=0, globalParams=[{"prop":"subreddit","direct":"IN","type":"VARCHAR","value":"singapore"},{"prop":"limit","direct":"IN","type":"VARCHAR","value":"3"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"data","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"import praw\nimport json\n\nclass RedditPostDataExtractor:\n    def __init__(self):\n        self.data = []\n\n    def extract_data(self, hot_posts):\n        if hot_posts:\n            for post in hot_posts:\n                post.comments.replace_more(limit=None)\n                # initialise a dictionary to contain the data of the post\n                post_data = {}\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the poster's name\n                if post.author is not None:\n                    post_data['author'] = post.author.name\n                else:\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the post was created\n                post_data['unix_time'] = post.created_utc\n                post_data['permalink'] = post.permalink\n                post_data['score'] = post.score\n                post_data['subreddit'] = post.subreddit.display_name\n                post_data['post_title'] = post.title\n                post_data['body'] = post.selftext\n\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\n                # t3 represents that it is a post\n                post_data['name'] = post.name\n\n                # store the posts data in the dictionary where the name is used as the key\n                self.data.append(post_data)\n\n                # extract the comments from the post\n                comments = post.comments.list()\n\n                # extract the data in the comments and store them inside the dictionary\n                self.extract_comments_data(comments)\n        \n        return self.data\n\n    def extract_comments_data(self, comments):\n        if comments:\n            for comment in comments:\n                # initialise a dictionary to contain the data of the comment\n                comment_data = {}\n\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the commenter name\n                if comment.author is not None:\n                    comment_data['author'] = comment.author.name\n                else:\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the comment was created\n                comment_data['unix_time'] = comment.created_utc\n                comment_data['permalink'] = comment.permalink\n                comment_data['score'] = comment.score\n                comment_data['subreddit'] = comment.subreddit.display_name\n                comment_data['post_title'] = comment.submission.title\n                comment_data['body'] = comment.body\n\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\n                # t1 represents that it is a post\n                comment_data['name'] = comment.name\n\n                # store the comment's data in the dictionary where the name is used as the key\n                self.data.append(comment_data)\n\n                # extract the comment's replies from the post\n                # replies = comment.replies\n\n                # extract the data in the comments and store them inside the dictionary\n                # self.extract_comments_data(replies)\n        \n        return\n    \ndef get_hot_posts(reddit_instance, subreddit_name, limit):\n    # Define the subreddit\n    subreddit = reddit_instance.subreddit(subreddit_name)\n\n    # Get hot posts from the subreddit\n    hot_posts = subreddit.hot(limit=limit)\n\n    # Filter out posts that are not pinned\n    filtered_posts = [post for post in hot_posts if not post.stickied]\n    return filtered_posts\n\n\n# initialize Reddit instance\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\n                     user_agent='Prawlingv1')\n\n\n# choose the subreddit to extract data from\nsubreddit = \"${subreddit}\"\nlimit = ${limit}\n\n# get hot posts from the subreddit\nhot_posts = get_hot_posts(reddit, subreddit, limit)\n\n# initialise a reddit data extractor object\ndata_extractor = RedditPostDataExtractor()\n\n# extract data from the hot posts from r/singapore\ndata = data_extractor.extract_data(hot_posts)\ndata","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='Extract Reddit Data'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1828'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13045662268160'}, subreddit=Property{prop='subreddit', direct=IN, type=VARCHAR, value='singapore'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425162129'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='673'}, limit=Property{prop='limit', direct=IN, type=VARCHAR, value='3'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13045665829504'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.740 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: Extract Reddit Data to wait queue success
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.740 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.745 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.745 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.745 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.745 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714033289745
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.746 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 673_1828
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.746 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1828,
  "taskName" : "Extract Reddit Data",
  "firstSubmitTime" : 1714033289723,
  "startTime" : 1714033289745,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13045665829504/17/673/1828.log",
  "processId" : 0,
  "processDefineCode" : 13045665829504,
  "processDefineVersion" : 17,
  "processInstanceId" : 673,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"subreddit\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"singapore\"},{\"prop\":\"limit\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"3\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"data\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"import praw\\nimport json\\n\\nclass RedditPostDataExtractor:\\n    def __init__(self):\\n        self.data = []\\n\\n    def extract_data(self, hot_posts):\\n        if hot_posts:\\n            for post in hot_posts:\\n                post.comments.replace_more(limit=None)\\n                # initialise a dictionary to contain the data of the post\\n                post_data = {}\\n\\n                # get the relevant post's data and store it inside post_data\\n\\n                # get the poster's name\\n                if post.author is not None:\\n                    post_data['author'] = post.author.name\\n                else:\\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\\n\\n                # get the unix time in which the post was created\\n                post_data['unix_time'] = post.created_utc\\n                post_data['permalink'] = post.permalink\\n                post_data['score'] = post.score\\n                post_data['subreddit'] = post.subreddit.display_name\\n                post_data['post_title'] = post.title\\n                post_data['body'] = post.selftext\\n\\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\\n                # t3 represents that it is a post\\n                post_data['name'] = post.name\\n\\n                # store the posts data in the dictionary where the name is used as the key\\n                self.data.append(post_data)\\n\\n                # extract the comments from the post\\n                comments = post.comments.list()\\n\\n                # extract the data in the comments and store them inside the dictionary\\n                self.extract_comments_data(comments)\\n        \\n        return self.data\\n\\n    def extract_comments_data(self, comments):\\n        if comments:\\n            for comment in comments:\\n                # initialise a dictionary to contain the data of the comment\\n                comment_data = {}\\n\\n\\n                # get the relevant post's data and store it inside post_data\\n\\n                # get the commenter name\\n                if comment.author is not None:\\n                    comment_data['author'] = comment.author.name\\n                else:\\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\\n\\n                # get the unix time in which the comment was created\\n                comment_data['unix_time'] = comment.created_utc\\n                comment_data['permalink'] = comment.permalink\\n                comment_data['score'] = comment.score\\n                comment_data['subreddit'] = comment.subreddit.display_name\\n                comment_data['post_title'] = comment.submission.title\\n                comment_data['body'] = comment.body\\n\\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\\n                # t1 represents that it is a post\\n                comment_data['name'] = comment.name\\n\\n                # store the comment's data in the dictionary where the name is used as the key\\n                self.data.append(comment_data)\\n\\n                # extract the comment's replies from the post\\n                # replies = comment.replies\\n\\n                # extract the data in the comments and store them inside the dictionary\\n                # self.extract_comments_data(replies)\\n        \\n        return\\n    \\ndef get_hot_posts(reddit_instance, subreddit_name, limit):\\n    # Define the subreddit\\n    subreddit = reddit_instance.subreddit(subreddit_name)\\n\\n    # Get hot posts from the subreddit\\n    hot_posts = subreddit.hot(limit=limit)\\n\\n    # Filter out posts that are not pinned\\n    filtered_posts = [post for post in hot_posts if not post.stickied]\\n    return filtered_posts\\n\\n\\n# initialize Reddit instance\\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\\n                     user_agent='Prawlingv1')\\n\\n\\n# choose the subreddit to extract data from\\nsubreddit = \\\"${subreddit}\\\"\\nlimit = ${limit}\\n\\n# get hot posts from the subreddit\\nhot_posts = get_hot_posts(reddit, subreddit, limit)\\n\\n# initialise a reddit data extractor object\\ndata_extractor = RedditPostDataExtractor()\\n\\n# extract data from the hot posts from r/singapore\\ndata = data_extractor.extract_data(hot_posts)\\ndata\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "Extract Reddit Data"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1828"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13045662268160"
    },
    "subreddit" : {
      "prop" : "subreddit",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "singapore"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425162129"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "673"
    },
    "limit" : {
      "prop" : "limit",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13045665829504"
    }
  },
  "taskAppId" : "673_1828",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.749 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.750 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.750 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.753 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.753 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.755 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_17/673/1828 check successfully
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.756 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.756 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.756 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.756 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.757 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "data",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "import praw\nimport json\n\nclass RedditPostDataExtractor:\n    def __init__(self):\n        self.data = []\n\n    def extract_data(self, hot_posts):\n        if hot_posts:\n            for post in hot_posts:\n                post.comments.replace_more(limit=None)\n                # initialise a dictionary to contain the data of the post\n                post_data = {}\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the poster's name\n                if post.author is not None:\n                    post_data['author'] = post.author.name\n                else:\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the post was created\n                post_data['unix_time'] = post.created_utc\n                post_data['permalink'] = post.permalink\n                post_data['score'] = post.score\n                post_data['subreddit'] = post.subreddit.display_name\n                post_data['post_title'] = post.title\n                post_data['body'] = post.selftext\n\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\n                # t3 represents that it is a post\n                post_data['name'] = post.name\n\n                # store the posts data in the dictionary where the name is used as the key\n                self.data.append(post_data)\n\n                # extract the comments from the post\n                comments = post.comments.list()\n\n                # extract the data in the comments and store them inside the dictionary\n                self.extract_comments_data(comments)\n        \n        return self.data\n\n    def extract_comments_data(self, comments):\n        if comments:\n            for comment in comments:\n                # initialise a dictionary to contain the data of the comment\n                comment_data = {}\n\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the commenter name\n                if comment.author is not None:\n                    comment_data['author'] = comment.author.name\n                else:\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the comment was created\n                comment_data['unix_time'] = comment.created_utc\n                comment_data['permalink'] = comment.permalink\n                comment_data['score'] = comment.score\n                comment_data['subreddit'] = comment.subreddit.display_name\n                comment_data['post_title'] = comment.submission.title\n                comment_data['body'] = comment.body\n\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\n                # t1 represents that it is a post\n                comment_data['name'] = comment.name\n\n                # store the comment's data in the dictionary where the name is used as the key\n                self.data.append(comment_data)\n\n                # extract the comment's replies from the post\n                # replies = comment.replies\n\n                # extract the data in the comments and store them inside the dictionary\n                # self.extract_comments_data(replies)\n        \n        return\n    \ndef get_hot_posts(reddit_instance, subreddit_name, limit):\n    # Define the subreddit\n    subreddit = reddit_instance.subreddit(subreddit_name)\n\n    # Get hot posts from the subreddit\n    hot_posts = subreddit.hot(limit=limit)\n\n    # Filter out posts that are not pinned\n    filtered_posts = [post for post in hot_posts if not post.stickied]\n    return filtered_posts\n\n\n# initialize Reddit instance\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\n                     user_agent='Prawlingv1')\n\n\n# choose the subreddit to extract data from\nsubreddit = \"${subreddit}\"\nlimit = ${limit}\n\n# get hot posts from the subreddit\nhot_posts = get_hot_posts(reddit, subreddit, limit)\n\n# initialise a reddit data extractor object\ndata_extractor = RedditPostDataExtractor()\n\n# extract data from the hot posts from r/singapore\ndata = data_extractor.extract_data(hot_posts)\ndata",
  "resourceList" : [ ]
}
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.758 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.758 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.759 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.759 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.759 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.759 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import praw
import json

class RedditPostDataExtractor:
    def __init__(self):
        self.data = []

    def extract_data(self, hot_posts):
        if hot_posts:
            for post in hot_posts:
                post.comments.replace_more(limit=None)
                # initialise a dictionary to contain the data of the post
                post_data = {}

                # get the relevant post's data and store it inside post_data

                # get the poster's name
                if post.author is not None:
                    post_data['author'] = post.author.name
                else:
                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the post was created
                post_data['unix_time'] = post.created_utc
                post_data['permalink'] = post.permalink
                post_data['score'] = post.score
                post_data['subreddit'] = post.subreddit.display_name
                post_data['post_title'] = post.title
                post_data['body'] = post.selftext

                # get the unique name of the post (name refers to the id of the post prefixed with t3_)
                # t3 represents that it is a post
                post_data['name'] = post.name

                # store the posts data in the dictionary where the name is used as the key
                self.data.append(post_data)

                # extract the comments from the post
                comments = post.comments.list()

                # extract the data in the comments and store them inside the dictionary
                self.extract_comments_data(comments)
        
        return self.data

    def extract_comments_data(self, comments):
        if comments:
            for comment in comments:
                # initialise a dictionary to contain the data of the comment
                comment_data = {}


                # get the relevant post's data and store it inside post_data

                # get the commenter name
                if comment.author is not None:
                    comment_data['author'] = comment.author.name
                else:
                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the comment was created
                comment_data['unix_time'] = comment.created_utc
                comment_data['permalink'] = comment.permalink
                comment_data['score'] = comment.score
                comment_data['subreddit'] = comment.subreddit.display_name
                comment_data['post_title'] = comment.submission.title
                comment_data['body'] = comment.body

                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)
                # t1 represents that it is a post
                comment_data['name'] = comment.name

                # store the comment's data in the dictionary where the name is used as the key
                self.data.append(comment_data)

                # extract the comment's replies from the post
                # replies = comment.replies

                # extract the data in the comments and store them inside the dictionary
                # self.extract_comments_data(replies)
        
        return
    
def get_hot_posts(reddit_instance, subreddit_name, limit):
    # Define the subreddit
    subreddit = reddit_instance.subreddit(subreddit_name)

    # Get hot posts from the subreddit
    hot_posts = subreddit.hot(limit=limit)

    # Filter out posts that are not pinned
    filtered_posts = [post for post in hot_posts if not post.stickied]
    return filtered_posts


# initialize Reddit instance
reddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',
                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',
                     user_agent='Prawlingv1')


# choose the subreddit to extract data from
subreddit = "${subreddit}"
limit = ${limit}

# get hot posts from the subreddit
hot_posts = get_hot_posts(reddit, subreddit, limit)

# initialise a reddit data extractor object
data_extractor = RedditPostDataExtractor()

# extract data from the hot posts from r/singapore
data = data_extractor.extract_data(hot_posts)
data
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.760 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_17/673/1828
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.761 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_17/673/1828/py_673_1828.py
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.761 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import praw
import json

class RedditPostDataExtractor:
    def __init__(self):
        self.data = []

    def extract_data(self, hot_posts):
        if hot_posts:
            for post in hot_posts:
                post.comments.replace_more(limit=None)
                # initialise a dictionary to contain the data of the post
                post_data = {}

                # get the relevant post's data and store it inside post_data

                # get the poster's name
                if post.author is not None:
                    post_data['author'] = post.author.name
                else:
                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the post was created
                post_data['unix_time'] = post.created_utc
                post_data['permalink'] = post.permalink
                post_data['score'] = post.score
                post_data['subreddit'] = post.subreddit.display_name
                post_data['post_title'] = post.title
                post_data['body'] = post.selftext

                # get the unique name of the post (name refers to the id of the post prefixed with t3_)
                # t3 represents that it is a post
                post_data['name'] = post.name

                # store the posts data in the dictionary where the name is used as the key
                self.data.append(post_data)

                # extract the comments from the post
                comments = post.comments.list()

                # extract the data in the comments and store them inside the dictionary
                self.extract_comments_data(comments)
        
        return self.data

    def extract_comments_data(self, comments):
        if comments:
            for comment in comments:
                # initialise a dictionary to contain the data of the comment
                comment_data = {}


                # get the relevant post's data and store it inside post_data

                # get the commenter name
                if comment.author is not None:
                    comment_data['author'] = comment.author.name
                else:
                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the comment was created
                comment_data['unix_time'] = comment.created_utc
                comment_data['permalink'] = comment.permalink
                comment_data['score'] = comment.score
                comment_data['subreddit'] = comment.subreddit.display_name
                comment_data['post_title'] = comment.submission.title
                comment_data['body'] = comment.body

                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)
                # t1 represents that it is a post
                comment_data['name'] = comment.name

                # store the comment's data in the dictionary where the name is used as the key
                self.data.append(comment_data)

                # extract the comment's replies from the post
                # replies = comment.replies

                # extract the data in the comments and store them inside the dictionary
                # self.extract_comments_data(replies)
        
        return
    
def get_hot_posts(reddit_instance, subreddit_name, limit):
    # Define the subreddit
    subreddit = reddit_instance.subreddit(subreddit_name)

    # Get hot posts from the subreddit
    hot_posts = subreddit.hot(limit=limit)

    # Filter out posts that are not pinned
    filtered_posts = [post for post in hot_posts if not post.stickied]
    return filtered_posts


# initialize Reddit instance
reddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',
                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',
                     user_agent='Prawlingv1')


# choose the subreddit to extract data from
subreddit = "singapore"
limit = 3

# get hot posts from the subreddit
hot_posts = get_hot_posts(reddit, subreddit, limit)

# initialise a reddit data extractor object
data_extractor = RedditPostDataExtractor()

# extract data from the hot posts from r/singapore
data = data_extractor.extract_data(hot_posts)
data
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.762 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.762 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.763 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_17/673/1828/py_673_1828.py
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.763 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.763 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_17/673/1828/673_1828.sh
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:29.830 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6758
[WI-0][TI-0] - [INFO] 2024-04-25 16:21:29.833 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 16:21:30.230 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7442528735632185 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1828] - [INFO] 2024-04-25 16:21:30.252 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1828, processInstanceId=673, startTime=1714033289745, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13045665829504/17/673/1828.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1828] - [INFO] 2024-04-25 16:21:30.255 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1828, success=true)
[WI-0][TI-1828] - [INFO] 2024-04-25 16:21:30.257 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1828, processInstanceId=673, startTime=1714033289745, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13045665829504/17/673/1828.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714033290244)
[WI-0][TI-1828] - [INFO] 2024-04-25 16:21:30.314 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1828)
[WI-0][TI-1828] - [INFO] 2024-04-25 16:21:30.326 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1828, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:21:32.910 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1829, taskName=search for intake JSON files, firstSubmitTime=1714033292864, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=114, appIds=null, processInstanceId=674, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1829'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340113777664'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425162132'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='674'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.916 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake JSON files to wait queue success
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.918 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.919 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.920 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.920 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.922 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714033292922
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.925 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 674_1829
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.926 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1829,
  "taskName" : "search for intake JSON files",
  "firstSubmitTime" : 1714033292864,
  "startTime" : 1714033292922,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13201021801792/114/674/1829.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 114,
  "processInstanceId" : 674,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set raw_file_dir to null\\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1829"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340113777664"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425162132"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "674"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "674_1829",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.926 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.941 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.941 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.946 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.948 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.953 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/674/1829 check successfully
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.953 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.954 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.954 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.954 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.955 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"",
  "resourceList" : [ ]
}
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.955 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.955 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.955 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.955 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.955 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.956 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.957 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.957 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/reddit/in -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set raw_file_dir to null
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
fi

echo "#{setValue(raw_file_dir=${raw_file_dir})}"
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.957 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.957 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/674/1829/674_1829.sh
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:32.969 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6775
[WI-0][TI-1829] - [INFO] 2024-04-25 16:21:33.229 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1829, success=true)
[WI-0][TI-1829] - [INFO] 2024-04-25 16:21:33.243 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1829)
[WI-0][TI-0] - [INFO] 2024-04-25 16:21:33.254 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8470948012232415 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:33.876 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_17/673/1828, processId:6758 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:33.880 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:33.881 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:33.881 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:33.881 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:33.886 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:33.886 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:33.886 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_17/673/1828
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:33.887 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_17/673/1828
[WI-673][TI-1828] - [INFO] 2024-04-25 16:21:33.887 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-04-25 16:21:33.971 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	#{setValue(raw_file_dir=null)}
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:33.974 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/674/1829, processId:6775 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:33.975 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:33.975 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:33.975 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:33.975 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:33.983 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:33.983 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:33.984 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/674/1829
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:33.985 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/674/1829
[WI-674][TI-1829] - [INFO] 2024-04-25 16:21:33.985 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1828] - [INFO] 2024-04-25 16:21:34.229 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1828, success=true)
[WI-0][TI-1829] - [INFO] 2024-04-25 16:21:34.235 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1829, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:21:35.325 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1831, taskName=end workflow, firstSubmitTime=1714033295313, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=114, appIds=null, processInstanceId=674, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"echo No Files Detected - End Workflow","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='end workflow'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1831'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13348985859488'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425162135'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='674'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='null'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"null"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.331 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: end workflow to wait queue success
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.331 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.332 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.332 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.332 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.332 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714033295332
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.332 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 674_1831
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.332 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1831,
  "taskName" : "end workflow",
  "firstSubmitTime" : 1714033295313,
  "startTime" : 1714033295332,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13201021801792/114/674/1831.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 114,
  "processInstanceId" : 674,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"echo No Files Detected - End Workflow\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "end workflow"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1831"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13348985859488"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425162135"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "674"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "null"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "674_1831",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"null\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.336 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.336 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.336 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.339 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.340 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.340 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/674/1831 check successfully
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.340 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.340 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.340 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.341 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.341 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "echo No Files Detected - End Workflow",
  "resourceList" : [ ]
}
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.341 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.341 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"null"}] successfully
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.341 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.341 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.341 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.341 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.341 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.342 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
echo No Files Detected - End Workflow
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.342 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.342 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/674/1831/674_1831.sh
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:35.347 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6789
[WI-0][TI-1831] - [INFO] 2024-04-25 16:21:36.237 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1831, success=true)
[WI-0][TI-1831] - [INFO] 2024-04-25 16:21:36.241 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1831)
[WI-0][TI-0] - [INFO] 2024-04-25 16:21:36.347 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	No Files Detected - End Workflow
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:36.355 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/674/1831, processId:6789 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:36.356 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:36.356 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:36.356 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:36.357 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:36.358 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:36.359 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:36.359 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/674/1831
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:36.359 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/674/1831
[WI-674][TI-1831] - [INFO] 2024-04-25 16:21:36.359 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1831] - [INFO] 2024-04-25 16:21:37.236 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1831, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:21:45.322 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7146974063400576 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:21:47.363 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8176795580110497 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:22:03.476 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.718213058419244 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:23:07.829 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7138728323699421 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1792] - [INFO] 2024-04-25 16:23:10.484 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1792, processInstanceId=661, status=9, startTime=1714031949850, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/661/1792.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792, endTime=1714032332757, processId=4479, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714033089796)
[WI-0][TI-1792] - [INFO] 2024-04-25 16:23:10.495 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1792, processInstanceId=661, status=9, startTime=1714031949850, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/661/1792.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792, endTime=1714032332757, processId=4479, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714033390484)
[WI-0][TI-1790] - [INFO] 2024-04-25 16:23:10.498 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1790, processInstanceId=660, status=9, startTime=1714031469392, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1790.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790, endTime=1714032269172, processId=4321, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714033089796)
[WI-0][TI-1790] - [INFO] 2024-04-25 16:23:10.516 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1790, processInstanceId=660, status=9, startTime=1714031469392, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1790.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790, endTime=1714032269172, processId=4321, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714033390484)
[WI-0][TI-0] - [INFO] 2024-04-25 16:23:11.392 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1832, taskName=Extract Reddit Data, firstSubmitTime=1714033391358, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13045665829504, processDefineVersion=18, appIds=null, processInstanceId=675, scheduleTime=0, globalParams=[{"prop":"subreddit","direct":"IN","type":"VARCHAR","value":"singapore"},{"prop":"limit","direct":"IN","type":"VARCHAR","value":"3"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"data","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"import praw\nimport json\n\nclass RedditPostDataExtractor:\n    def __init__(self):\n        self.data = []\n\n    def extract_data(self, hot_posts):\n        if hot_posts:\n            for post in hot_posts:\n                post.comments.replace_more(limit=None)\n                # initialise a dictionary to contain the data of the post\n                post_data = {}\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the poster's name\n                if post.author is not None:\n                    post_data['author'] = post.author.name\n                else:\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the post was created\n                post_data['unix_time'] = post.created_utc\n                post_data['permalink'] = post.permalink\n                post_data['score'] = post.score\n                post_data['subreddit'] = post.subreddit.display_name\n                post_data['post_title'] = post.title\n                post_data['body'] = post.selftext\n\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\n                # t3 represents that it is a post\n                post_data['name'] = post.name\n\n                # store the posts data in the dictionary where the name is used as the key\n                self.data.append(post_data)\n\n                # extract the comments from the post\n                comments = post.comments.list()\n\n                # extract the data in the comments and store them inside the dictionary\n                self.extract_comments_data(comments)\n        \n        return self.data\n\n    def extract_comments_data(self, comments):\n        if comments:\n            for comment in comments:\n                # initialise a dictionary to contain the data of the comment\n                comment_data = {}\n\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the commenter name\n                if comment.author is not None:\n                    comment_data['author'] = comment.author.name\n                else:\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the comment was created\n                comment_data['unix_time'] = comment.created_utc\n                comment_data['permalink'] = comment.permalink\n                comment_data['score'] = comment.score\n                comment_data['subreddit'] = comment.subreddit.display_name\n                comment_data['post_title'] = comment.submission.title\n                comment_data['body'] = comment.body\n\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\n                # t1 represents that it is a post\n                comment_data['name'] = comment.name\n\n                # store the comment's data in the dictionary where the name is used as the key\n                self.data.append(comment_data)\n\n                # extract the comment's replies from the post\n                # replies = comment.replies\n\n                # extract the data in the comments and store them inside the dictionary\n                # self.extract_comments_data(replies)\n        \n        return\n    \ndef get_hot_posts(reddit_instance, subreddit_name, limit):\n    # Define the subreddit\n    subreddit = reddit_instance.subreddit(subreddit_name)\n\n    # Get hot posts from the subreddit\n    hot_posts = subreddit.hot(limit=limit)\n\n    # Filter out posts that are not pinned\n    filtered_posts = [post for post in hot_posts if not post.stickied]\n    return filtered_posts\n\n\n# initialize Reddit instance\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\n                     user_agent='Prawlingv1')\n\n\n# choose the subreddit to extract data from\nsubreddit = \"${subreddit}\"\nlimit = ${limit}\n\n# get hot posts from the subreddit\nhot_posts = get_hot_posts(reddit, subreddit, limit)\n\n# initialise a reddit data extractor object\ndata_extractor = RedditPostDataExtractor()\n\n# extract data from the hot posts from r/singapore\ndata = data_extractor.extract_data(hot_posts)\n\nprint(\"#setValue(data=%s)\" % data)","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='Extract Reddit Data'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1832'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13045662268160'}, subreddit=Property{prop='subreddit', direct=IN, type=VARCHAR, value='singapore'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425162311'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='675'}, limit=Property{prop='limit', direct=IN, type=VARCHAR, value='3'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13045665829504'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.394 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: Extract Reddit Data to wait queue success
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.394 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.397 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.398 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.398 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.398 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714033391398
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.398 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 675_1832
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.398 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1832,
  "taskName" : "Extract Reddit Data",
  "firstSubmitTime" : 1714033391358,
  "startTime" : 1714033391398,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13045665829504/18/675/1832.log",
  "processId" : 0,
  "processDefineCode" : 13045665829504,
  "processDefineVersion" : 18,
  "processInstanceId" : 675,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"subreddit\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"singapore\"},{\"prop\":\"limit\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"3\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"data\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"import praw\\nimport json\\n\\nclass RedditPostDataExtractor:\\n    def __init__(self):\\n        self.data = []\\n\\n    def extract_data(self, hot_posts):\\n        if hot_posts:\\n            for post in hot_posts:\\n                post.comments.replace_more(limit=None)\\n                # initialise a dictionary to contain the data of the post\\n                post_data = {}\\n\\n                # get the relevant post's data and store it inside post_data\\n\\n                # get the poster's name\\n                if post.author is not None:\\n                    post_data['author'] = post.author.name\\n                else:\\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\\n\\n                # get the unix time in which the post was created\\n                post_data['unix_time'] = post.created_utc\\n                post_data['permalink'] = post.permalink\\n                post_data['score'] = post.score\\n                post_data['subreddit'] = post.subreddit.display_name\\n                post_data['post_title'] = post.title\\n                post_data['body'] = post.selftext\\n\\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\\n                # t3 represents that it is a post\\n                post_data['name'] = post.name\\n\\n                # store the posts data in the dictionary where the name is used as the key\\n                self.data.append(post_data)\\n\\n                # extract the comments from the post\\n                comments = post.comments.list()\\n\\n                # extract the data in the comments and store them inside the dictionary\\n                self.extract_comments_data(comments)\\n        \\n        return self.data\\n\\n    def extract_comments_data(self, comments):\\n        if comments:\\n            for comment in comments:\\n                # initialise a dictionary to contain the data of the comment\\n                comment_data = {}\\n\\n\\n                # get the relevant post's data and store it inside post_data\\n\\n                # get the commenter name\\n                if comment.author is not None:\\n                    comment_data['author'] = comment.author.name\\n                else:\\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\\n\\n                # get the unix time in which the comment was created\\n                comment_data['unix_time'] = comment.created_utc\\n                comment_data['permalink'] = comment.permalink\\n                comment_data['score'] = comment.score\\n                comment_data['subreddit'] = comment.subreddit.display_name\\n                comment_data['post_title'] = comment.submission.title\\n                comment_data['body'] = comment.body\\n\\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\\n                # t1 represents that it is a post\\n                comment_data['name'] = comment.name\\n\\n                # store the comment's data in the dictionary where the name is used as the key\\n                self.data.append(comment_data)\\n\\n                # extract the comment's replies from the post\\n                # replies = comment.replies\\n\\n                # extract the data in the comments and store them inside the dictionary\\n                # self.extract_comments_data(replies)\\n        \\n        return\\n    \\ndef get_hot_posts(reddit_instance, subreddit_name, limit):\\n    # Define the subreddit\\n    subreddit = reddit_instance.subreddit(subreddit_name)\\n\\n    # Get hot posts from the subreddit\\n    hot_posts = subreddit.hot(limit=limit)\\n\\n    # Filter out posts that are not pinned\\n    filtered_posts = [post for post in hot_posts if not post.stickied]\\n    return filtered_posts\\n\\n\\n# initialize Reddit instance\\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\\n                     user_agent='Prawlingv1')\\n\\n\\n# choose the subreddit to extract data from\\nsubreddit = \\\"${subreddit}\\\"\\nlimit = ${limit}\\n\\n# get hot posts from the subreddit\\nhot_posts = get_hot_posts(reddit, subreddit, limit)\\n\\n# initialise a reddit data extractor object\\ndata_extractor = RedditPostDataExtractor()\\n\\n# extract data from the hot posts from r/singapore\\ndata = data_extractor.extract_data(hot_posts)\\n\\nprint(\\\"#setValue(data=%s)\\\" % data)\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "Extract Reddit Data"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1832"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13045662268160"
    },
    "subreddit" : {
      "prop" : "subreddit",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "singapore"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425162311"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "675"
    },
    "limit" : {
      "prop" : "limit",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13045665829504"
    }
  },
  "taskAppId" : "675_1832",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.399 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.399 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.399 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.409 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.410 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.415 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_18/675/1832 check successfully
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.415 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.418 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.418 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.419 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.419 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "data",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "import praw\nimport json\n\nclass RedditPostDataExtractor:\n    def __init__(self):\n        self.data = []\n\n    def extract_data(self, hot_posts):\n        if hot_posts:\n            for post in hot_posts:\n                post.comments.replace_more(limit=None)\n                # initialise a dictionary to contain the data of the post\n                post_data = {}\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the poster's name\n                if post.author is not None:\n                    post_data['author'] = post.author.name\n                else:\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the post was created\n                post_data['unix_time'] = post.created_utc\n                post_data['permalink'] = post.permalink\n                post_data['score'] = post.score\n                post_data['subreddit'] = post.subreddit.display_name\n                post_data['post_title'] = post.title\n                post_data['body'] = post.selftext\n\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\n                # t3 represents that it is a post\n                post_data['name'] = post.name\n\n                # store the posts data in the dictionary where the name is used as the key\n                self.data.append(post_data)\n\n                # extract the comments from the post\n                comments = post.comments.list()\n\n                # extract the data in the comments and store them inside the dictionary\n                self.extract_comments_data(comments)\n        \n        return self.data\n\n    def extract_comments_data(self, comments):\n        if comments:\n            for comment in comments:\n                # initialise a dictionary to contain the data of the comment\n                comment_data = {}\n\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the commenter name\n                if comment.author is not None:\n                    comment_data['author'] = comment.author.name\n                else:\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the comment was created\n                comment_data['unix_time'] = comment.created_utc\n                comment_data['permalink'] = comment.permalink\n                comment_data['score'] = comment.score\n                comment_data['subreddit'] = comment.subreddit.display_name\n                comment_data['post_title'] = comment.submission.title\n                comment_data['body'] = comment.body\n\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\n                # t1 represents that it is a post\n                comment_data['name'] = comment.name\n\n                # store the comment's data in the dictionary where the name is used as the key\n                self.data.append(comment_data)\n\n                # extract the comment's replies from the post\n                # replies = comment.replies\n\n                # extract the data in the comments and store them inside the dictionary\n                # self.extract_comments_data(replies)\n        \n        return\n    \ndef get_hot_posts(reddit_instance, subreddit_name, limit):\n    # Define the subreddit\n    subreddit = reddit_instance.subreddit(subreddit_name)\n\n    # Get hot posts from the subreddit\n    hot_posts = subreddit.hot(limit=limit)\n\n    # Filter out posts that are not pinned\n    filtered_posts = [post for post in hot_posts if not post.stickied]\n    return filtered_posts\n\n\n# initialize Reddit instance\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\n                     user_agent='Prawlingv1')\n\n\n# choose the subreddit to extract data from\nsubreddit = \"${subreddit}\"\nlimit = ${limit}\n\n# get hot posts from the subreddit\nhot_posts = get_hot_posts(reddit, subreddit, limit)\n\n# initialise a reddit data extractor object\ndata_extractor = RedditPostDataExtractor()\n\n# extract data from the hot posts from r/singapore\ndata = data_extractor.extract_data(hot_posts)\n\nprint(\"#setValue(data=%s)\" % data)",
  "resourceList" : [ ]
}
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.420 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.421 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.421 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.421 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.423 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.424 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import praw
import json

class RedditPostDataExtractor:
    def __init__(self):
        self.data = []

    def extract_data(self, hot_posts):
        if hot_posts:
            for post in hot_posts:
                post.comments.replace_more(limit=None)
                # initialise a dictionary to contain the data of the post
                post_data = {}

                # get the relevant post's data and store it inside post_data

                # get the poster's name
                if post.author is not None:
                    post_data['author'] = post.author.name
                else:
                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the post was created
                post_data['unix_time'] = post.created_utc
                post_data['permalink'] = post.permalink
                post_data['score'] = post.score
                post_data['subreddit'] = post.subreddit.display_name
                post_data['post_title'] = post.title
                post_data['body'] = post.selftext

                # get the unique name of the post (name refers to the id of the post prefixed with t3_)
                # t3 represents that it is a post
                post_data['name'] = post.name

                # store the posts data in the dictionary where the name is used as the key
                self.data.append(post_data)

                # extract the comments from the post
                comments = post.comments.list()

                # extract the data in the comments and store them inside the dictionary
                self.extract_comments_data(comments)
        
        return self.data

    def extract_comments_data(self, comments):
        if comments:
            for comment in comments:
                # initialise a dictionary to contain the data of the comment
                comment_data = {}


                # get the relevant post's data and store it inside post_data

                # get the commenter name
                if comment.author is not None:
                    comment_data['author'] = comment.author.name
                else:
                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the comment was created
                comment_data['unix_time'] = comment.created_utc
                comment_data['permalink'] = comment.permalink
                comment_data['score'] = comment.score
                comment_data['subreddit'] = comment.subreddit.display_name
                comment_data['post_title'] = comment.submission.title
                comment_data['body'] = comment.body

                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)
                # t1 represents that it is a post
                comment_data['name'] = comment.name

                # store the comment's data in the dictionary where the name is used as the key
                self.data.append(comment_data)

                # extract the comment's replies from the post
                # replies = comment.replies

                # extract the data in the comments and store them inside the dictionary
                # self.extract_comments_data(replies)
        
        return
    
def get_hot_posts(reddit_instance, subreddit_name, limit):
    # Define the subreddit
    subreddit = reddit_instance.subreddit(subreddit_name)

    # Get hot posts from the subreddit
    hot_posts = subreddit.hot(limit=limit)

    # Filter out posts that are not pinned
    filtered_posts = [post for post in hot_posts if not post.stickied]
    return filtered_posts


# initialize Reddit instance
reddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',
                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',
                     user_agent='Prawlingv1')


# choose the subreddit to extract data from
subreddit = "${subreddit}"
limit = ${limit}

# get hot posts from the subreddit
hot_posts = get_hot_posts(reddit, subreddit, limit)

# initialise a reddit data extractor object
data_extractor = RedditPostDataExtractor()

# extract data from the hot posts from r/singapore
data = data_extractor.extract_data(hot_posts)

print("#setValue(data=%s)" % data)
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.427 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_18/675/1832
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.428 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_18/675/1832/py_675_1832.py
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.428 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import praw
import json

class RedditPostDataExtractor:
    def __init__(self):
        self.data = []

    def extract_data(self, hot_posts):
        if hot_posts:
            for post in hot_posts:
                post.comments.replace_more(limit=None)
                # initialise a dictionary to contain the data of the post
                post_data = {}

                # get the relevant post's data and store it inside post_data

                # get the poster's name
                if post.author is not None:
                    post_data['author'] = post.author.name
                else:
                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the post was created
                post_data['unix_time'] = post.created_utc
                post_data['permalink'] = post.permalink
                post_data['score'] = post.score
                post_data['subreddit'] = post.subreddit.display_name
                post_data['post_title'] = post.title
                post_data['body'] = post.selftext

                # get the unique name of the post (name refers to the id of the post prefixed with t3_)
                # t3 represents that it is a post
                post_data['name'] = post.name

                # store the posts data in the dictionary where the name is used as the key
                self.data.append(post_data)

                # extract the comments from the post
                comments = post.comments.list()

                # extract the data in the comments and store them inside the dictionary
                self.extract_comments_data(comments)
        
        return self.data

    def extract_comments_data(self, comments):
        if comments:
            for comment in comments:
                # initialise a dictionary to contain the data of the comment
                comment_data = {}


                # get the relevant post's data and store it inside post_data

                # get the commenter name
                if comment.author is not None:
                    comment_data['author'] = comment.author.name
                else:
                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the comment was created
                comment_data['unix_time'] = comment.created_utc
                comment_data['permalink'] = comment.permalink
                comment_data['score'] = comment.score
                comment_data['subreddit'] = comment.subreddit.display_name
                comment_data['post_title'] = comment.submission.title
                comment_data['body'] = comment.body

                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)
                # t1 represents that it is a post
                comment_data['name'] = comment.name

                # store the comment's data in the dictionary where the name is used as the key
                self.data.append(comment_data)

                # extract the comment's replies from the post
                # replies = comment.replies

                # extract the data in the comments and store them inside the dictionary
                # self.extract_comments_data(replies)
        
        return
    
def get_hot_posts(reddit_instance, subreddit_name, limit):
    # Define the subreddit
    subreddit = reddit_instance.subreddit(subreddit_name)

    # Get hot posts from the subreddit
    hot_posts = subreddit.hot(limit=limit)

    # Filter out posts that are not pinned
    filtered_posts = [post for post in hot_posts if not post.stickied]
    return filtered_posts


# initialize Reddit instance
reddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',
                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',
                     user_agent='Prawlingv1')


# choose the subreddit to extract data from
subreddit = "singapore"
limit = 3

# get hot posts from the subreddit
hot_posts = get_hot_posts(reddit, subreddit, limit)

# initialise a reddit data extractor object
data_extractor = RedditPostDataExtractor()

# extract data from the hot posts from r/singapore
data = data_extractor.extract_data(hot_posts)

print("#setValue(data=%s)" % data)
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.431 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.431 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.431 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_18/675/1832/py_675_1832.py
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.432 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.432 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_18/675/1832/675_1832.sh
[WI-0][TI-1832] - [INFO] 2024-04-25 16:23:11.500 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1832, success=true)
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:11.547 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6817
[WI-0][TI-0] - [INFO] 2024-04-25 16:23:11.549 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 16:23:11.885 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8885793871866295 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1832] - [INFO] 2024-04-25 16:23:12.485 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1832)
[WI-0][TI-0] - [INFO] 2024-04-25 16:23:12.953 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7452574525745258 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:23:16.576 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	#setValue(data=[{'author': 'Contriod76', 'unix_time': 1714011699.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/', 'score': 411, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': " https://www.channelnewsasia.com/singapore/tampines-accident-man-charged-dangerous-driving-causing-death-black-saab-4290401?cid=internal_sharetool_androidphone_25042024_cna\n\nSINGAPORE: A driver involved in a multi-vehicle accident in Tampines that killed two people was charged in court on Thursday (Apr 25).\n\nMuhammad Syafie Ismail, 42, was charged with four offences related to Monday's incident. ", 'name': 't3_1ccgms0'}, {'author': 'etulf', 'unix_time': 1714014245.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15bp2u/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Well, we were waiting for this when the news was shared that the driver would be charged today. With his name in public domain, we would like to remind users that it is still against Reddit\'s [policies](https://support.reddithelp.com/hc/en-us/articles/205926439-Reddiquette) to "Post someone\'s personal information,\xa0or post links to personal information".\n\nWe understand that emotions may still be running high but please exercise care in what you comment and post.\n\nI\'ve gone ahead to remove another post on the same topic; we\'ll try to channel all discussions here so that other meaningful conversations can still take place on our subs.\n\nThanks all, and stay safe.', 'name': 't1_l15bp2u'}, {'author': 'hamiwin', 'unix_time': 1714013239.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l159emd/', 'score': 400, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'The maximum punishment of 8 years of jail still sounds light for 2 lives killed, imho.', 'name': 't1_l159emd'}, {'author': 'avocadoooforlife', 'unix_time': 1714013057.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l158zfj/', 'score': 288, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Driver seems to be so arrogant and no remorse.... The way he replied to the judge that he thinks it is not necessary to put an e-tag', 'name': 't1_l158zfj'}, {'author': 'AbrocomaIcy1947', 'unix_time': 1714013165.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l1598j1/', 'score': 176, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'his lack of remorse enrages me', 'name': 't1_l1598j1'}, {'author': 'Living-Can-2160', 'unix_time': 1714013723.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15aizm/', 'score': 145, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'This guy psychopath ah ? Why he talk like no remorse', 'name': 't1_l15aizm'}, {'author': 'fateoftheg0dz', 'unix_time': 1714013660.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15adr0/', 'score': 148, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'How did this fucker only get away with a broken arm from the accident???', 'name': 't1_l15adr0'}, {'author': 'illEagle96', 'unix_time': 1714013860.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15au4n/', 'score': 175, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Buto this guy, tahu sedap main racing. Kill people already want to chol wear shades, cap and mask.\n\n\nI hope people inside sebat you sedap2 at Changi kimak', 'name': 't1_l15au4n'}, {'author': 'dxflr', 'unix_time': 1714012653.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15812n/', 'score': 97, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Scumbag and coward wearing the mask', 'name': 't1_l15812n'}, {'author': 'erisestarrs', 'unix_time': 1714014535.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15cbux/', 'score': 59, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Dude needed to be escorted because he's public enemy no 1 in Singapore rn. I hope he gets the maximum sentences for all his charges but tbh even that won't be enough for the tragic loss of lives.", 'name': 't1_l15cbux'}, {'author': 'tiredsingaporean5274', 'unix_time': 1714013488.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l159zhg/', 'score': 87, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Putting it out here big big:\nSCREW YOU MUHAMMAD SYAFIE ISMAIL\xa0', 'name': 't1_l159zhg'}, {'author': 'SG_wormsblink', 'unix_time': 1714012757.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l1589ye/', 'score': 39, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Question, is it one count of dangerous driving causing death or two counts, since there are 2 victims.\n\nie is it possible that Muhammad Syafie Ismail will be jailed for up to 8 years or up to 16 years.', 'name': 't1_l1589ye'}, {'author': 'im_a_good_goat', 'unix_time': 1714015986.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15ffxi/', 'score': 38, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'This POS gets to live to 40+ years while the JC girl didn’t have a chance to even learn driving. Life on earth short-lived.', 'name': 't1_l15ffxi'}, {'author': 'KopiSiewSiewDai', 'unix_time': 1714013110.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l1593xb/', 'score': 65, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Internet CSI brigade: Chiong ahhhhh dig dig dig!!!!', 'name': 't1_l1593xb'}, {'author': 'Bright-Head-7777', 'unix_time': 1714012845.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l158hg4/', 'score': 32, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Your stupid actions has caused irrevocable consequences. You turn lives topsy-turvy in an instance for the families of the deceased and the injured!!!', 'name': 't1_l158hg4'}, {'author': 'Radiant_Guess7277', 'unix_time': 1714013482.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l159yze/', 'score': 29, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Because of one man's stupidity, so many others and their families suffer. And the max sentence is not even enough to deter other rash drivers from behaving like idiots too 😮\u200d💨", 'name': 't1_l159yze'}, {'author': 'xHarleyy', 'unix_time': 1714015041.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15dfpc/', 'score': 28, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Not hard to be smug about it when you know you are not getting death no matter what. \n\nEven murderers show more remorse than him.', 'name': 't1_l15dfpc'}, {'author': 'rynthms', 'unix_time': 1714024669.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15v7v5/', 'score': 8, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Watch this Syafie fucker get 2 years only and strut out and arrogantly claim he didn’t do anything wrong.', 'name': 't1_l15v7v5'}, {'author': 'dreamer_eater', 'unix_time': 1714013674.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15aewl/', 'score': 24, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'This is way too lenient honestly. Hope our laws can be revised....', 'name': 't1_l15aewl'}, {'author': 'MolassesBulky', 'unix_time': 1714016531.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15glah/', 'score': 31, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': '**Not the usual case of beating the red light**\n\nThe video at the junction clearly shows that a white small lorry was already stationary for a while waiting for the lights to turn green. So it has been red for a long while. Yet he went straight thru.\n\nIn my view, dangerous driving causing death would not be appropriate. He should face a charge of manslaughter. AG has the right to upgrade the charge. I think it is unprecedented but this would be more appropriate charge.\n\n**Lack of remorse**\n\nThe way he spoke to the judge on e-tag on his leg while on bail suggest he does not realise or chose to ignore the level of harm he had done by his actions. Looks like he still wants to look good in public without the e-tag.', 'name': 't1_l15glah'}, {'author': 'avendyr', 'unix_time': 1714014723.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15cqov/', 'score': 21, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': '"Syafie arrived at court escorted by three men with lanyards – two in uniform and one in an AETOS polo T-shirt. "\n\nThe scumbag needed 3 AETOS to escort him.', 'name': 't1_l15cqov'}, {'author': 'Only-Tangerine-9851', 'unix_time': 1714022631.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15rxfs/', 'score': 11, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Why in SG cunts like these always seems to prosper, own nice cars, businesses have pretty wife or many girls whereas people who follow the rules and path will become ignored and nobodies unless you're already born with generational wealth, why our society so special lol. \n\nAlso another question this cunt is already 42 given his farking behavior how the fuck he been hiding in the shadows all these times? Or he just collected his car 1 day ago...?", 'name': 't1_l15rxfs'}, {'author': 'HANAEMILK', 'unix_time': 1714013158.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l1597zc/', 'score': 20, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Max sentence pls at least', 'name': 't1_l1597zc'}, {'author': 'Hydrohomie1337', 'unix_time': 1714014599.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15cgva/', 'score': 14, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Major FUCK this guy', 'name': 't1_l15cgva'}, {'author': 'anangrypudge', 'unix_time': 1714017251.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15i1nt/', 'score': 15, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Looking remarkably uninjured and unbothered...', 'name': 't1_l15i1nt'}, {'author': 'Konstanx', 'unix_time': 1714013791.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15aohj/', 'score': 13, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Fuck this dude', 'name': 't1_l15aohj'}, {'author': 'polarbare91', 'unix_time': 1714014081.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15bc0o/', 'score': 29, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'By the time he gets out of prison, most will forget. His belligerence and lack of remorse tells me he knows this all too well. This is where the justice system has failed us. If the punishment was far harsher (i.e. death penalty), he wouldn’t be behaving this way.', 'name': 't1_l15bc0o'}, {'author': 'operaduck289', 'unix_time': 1714018454.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15kery/', 'score': 29, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'The judge had asked him if he has anything to say. If he had been sincerely remorseful, he could have used this opportunity to apologise to the families of those he has killed/injured. Instead, he said he’ll be engaging lawyer. No apologies. \n\nPerhaps he does not wish to implicate himself further with any apology (admission of guilt). But with all the video evidence, there is no way to escape culpability. I’m no lawyer, but I think if he had taken the opportunity to apologise in court today, it is a small point in his favour as that at least shows he is human, that he is capable of remorse. \n\nNow i’m wondering which brave lawyer will choose to take on this case.\n\nEdit:  given this high profile, some media hungry criminal lawyers will prob be knocking on scumbag’s door.', 'name': 't1_l15kery'}, {'author': 'Jammy_buttons2', 'unix_time': 1714012698.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l1584z4/', 'score': 24, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'I am surprised no drug or drunk driving charges :o\n\nEdit: Sorry saw the line where police are still investigating other possible offenses.', 'name': 't1_l1584z4'}, {'author': 'sonertimotei', 'unix_time': 1714015213.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15dt3w/', 'score': 16, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Ppl are still laying hospital and 2 died because of him, but this guy is walking to court. F', 'name': 't1_l15dt3w'}, {'author': 'TWENTYFOUR2', 'unix_time': 1714026731.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15yen0/', 'score': 2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'MUHAMMAD SYAFIE ISMAIL', 'name': 't1_l15yen0'}, {'author': 'eliseusmoo', 'unix_time': 1714028735.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l161cpn/', 'score': 4, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Hope drugs over 1kg will suddenly be “found” in his trunk 🙏', 'name': 't1_l161cpn'}, {'author': 'throwaway209152', 'unix_time': 1714014706.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15cpb2/', 'score': 21, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'I saw so much speculation and almost racist comments on the supposed race of the driver. Interesting development.', 'name': 't1_l15cpb2'}, {'author': '_box_box', 'unix_time': 1714014016.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15b6sm/', 'score': 15, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'penalty for dangerous driving causing death: 2 - 8 years\n\n\njust 8 years?? for ending two lives and causing immeasurable pain and trauma to multiple families?! \n\n8 years is a joke', 'name': 't1_l15b6sm'}, {'author': 'ailes_d', 'unix_time': 1714014058.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15ba7f/', 'score': 12, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Dumb fuck, go break another arm', 'name': 't1_l15ba7f'}, {'author': 'ilovesupermartsg', 'unix_time': 1714014513.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15ca4w/', 'score': 16, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Should bring him in cuffs to the graves of the 2 innocent parties. Make him kneel down, look at the tombstone and seek forgiveness from his God.', 'name': 't1_l15ca4w'}, {'author': 'wank_for_peace', 'unix_time': 1714013342.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l159ncy/', 'score': 10, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "2 to 8 years for dangerous driving causing death. Out sooner if good behavior. We'll fucking done.", 'name': 't1_l159ncy'}, {'author': 'NIDORAX', 'unix_time': 1714017033.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15hlwp/', 'score': 8, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Just lock that guy up and permaban him from driving even a PMD.', 'name': 't1_l15hlwp'}, {'author': 'cantoilmate', 'unix_time': 1714017506.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15ik1q/', 'score': 3, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Fuck you, Muhammad Syafie Ismail. May you suffer for the rest of your life for this.', 'name': 't1_l15ik1q'}, {'author': 'No-Composer-3152', 'unix_time': 1714030139.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l163caj/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Appearing in court, Syafie stated that he was planning to engage legal counsel, but had not decided whether he would apply for a lawyer from the Public Defender's Office or if he would engage a private lawyer.\n\nThe Public Defender's Office provides criminal defence aid to people facing non-capital charges who cannot afford\xa0a\xa0lawyer.\n\nfrom Today's article [https://www.todayonline.com/singapore/driver-fatal-tampines-accident-charged-4-offences-including-dangerous-driving-causing-death-2410556](https://www.todayonline.com/singapore/driver-fatal-tampines-accident-charged-4-offences-including-dangerous-driving-causing-death-2410556)\n\n  \nStrange how he could afford a Saab, or was it driven without permission? Hence he was in a hurry to get away fast after the first accident.", 'name': 't1_l163caj'}, {'author': 'Starzap', 'unix_time': 1714014370.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15byxh/', 'score': 7, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'This trash will be out a few years ready to kill again! /s', 'name': 't1_l15byxh'}, {'author': 'Material_Foot_9733', 'unix_time': 1714014710.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15cpkk/', 'score': 6, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'This guy is a dead man walking', 'name': 't1_l15cpkk'}, {'author': '4queuetoo', 'unix_time': 1714013188.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l159aec/', 'score': 8, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'hope he gets to see the families of the lives he destroyed to realise he fucked up big time.', 'name': 't1_l159aec'}, {'author': 'Altruistic-Coyote425', 'unix_time': 1714015336.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15e2ly/', 'score': 8, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Whatever punishment he gets, is not enough to bring back the two lives he has taken from this fatal accident he had caused. That being said, our traffic crime laws are way too lenient. \n\nI hope this guy is gonna get what he deserves real soon, if the law doesnt punish him.. the public cfm will take action.', 'name': 't1_l15e2ly'}, {'author': 'LaustinSpayce', 'unix_time': 1714012781.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l158bzz/', 'score': 9, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Throw the book at him!\n\nAnd we need to be insistent, *and keep insisting*, to our officials that our roads in Singapore be safer; and to commit to a **Vision Zero** policy where we aim to have *zero* deaths or serious injuries on our roads.', 'name': 't1_l158bzz'}, {'author': 'majingou', 'unix_time': 1714022653.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15rypj/', 'score': 4, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': '42-year-old killer, you mean.', 'name': 't1_l15rypj'}, {'author': 'brokolili', 'unix_time': 1714019503.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15mdnv/', 'score': 6, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'memalukan gila pe nama "Muhammad" + "Syafie"', 'name': 't1_l15mdnv'}, {'author': 'Noobcakes19', 'unix_time': 1714015527.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15eh9l/', 'score': 5, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'ahh the they finally reveal the identity of this fucker.', 'name': 't1_l15eh9l'}, {'author': 'Park-Super', 'unix_time': 1714018423.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15kclf/', 'score': 2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'fucking cunt', 'name': 't1_l15kclf'}, {'author': 'werkbij', 'unix_time': 1714024930.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15vmko/', 'score': 2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Cunt. I hope he gets his ass roundly kicked multiple times. May he never have a good night's sleep, and I hope his world crumbles down all around him with zero chance of recovery.", 'name': 't1_l15vmko'}, {'author': 'Pilotboi', 'unix_time': 1714031658.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l165emd/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'My blood boiling the way this knn ccb answer in court and even with all the mask and sunglasses worn by this joker, it still so visible that he got the stuck up attitude.\n\nI still want to say more, but I confirm kena banned and my comment deleted :D', 'name': 't1_l165emd'}, {'author': 'Super-Pension2542', 'unix_time': 1714032377.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l166dox/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Is this an isolated incident or a part of a larger issue we are facing? Nowadays, I feel increasingly unsafe driving on roads filled with zigzagging and speeding cars, vans, trucks, and PMDs. I miss those days when police cars patrolled regularly and campaigns promoted safe driving. If the authorities' approach remains solely evidence-based, we risk being perpetually reactive, with lives lost before action is taken. It's a deep-seated issue among some Singaporean drivers that demands prompt rectification.", 'name': 't1_l166dox'}, {'author': 'New_Pack_4351', 'unix_time': 1714016631.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15gsl6/', 'score': 3, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "The moral of the story, don't let your ego take over you. this is the outcome and look at all the comments about fking him up and down, CSI him, and bringing all his history into the WWW to shame him. when I look at this troll. life is so fragile", 'name': 't1_l15gsl6'}, {'author': 'lila_fauns', 'unix_time': 1714018012.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15jju8/', 'score': 3, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'the indifference in his tone is astonishing. it’s borderline sociopathic. i hope a good night’s sleep evades him for the rest of his pathetic existence.', 'name': 't1_l15jju8'}, {'author': '_Juicyy', 'unix_time': 1714017615.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15irtx/', 'score': 3, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': '2 to 8 years? Why are we so nice to this remorseless little fucker', 'name': 't1_l15irtx'}, {'author': 'Big-Tea-9729', 'unix_time': 1714015717.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15evqs/', 'score': 3, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Hope u bring your guiltiness in prison. Remember you killed 2 innocent people. It's gonna haunt you for the rest of your life bro.", 'name': 't1_l15evqs'}, {'author': 'wolf-bot', 'unix_time': 1714016512.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15gjuw/', 'score': 2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'He shouldn’t be the one that’s still walking around.', 'name': 't1_l15gjuw'}, {'author': 'virtual_nxm', 'unix_time': 1714017894.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15jbof/', 'score': 3, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Sentences for dangerous/drunk driving that causes death should be compensated with either life imprisonment or death sentence. Sentence is too light for these jerks to take it seriously. Feel so heartbroken for the family and friends of those who passed on from such accidents.', 'name': 't1_l15jbof'}, {'author': 'Raitoumightou', 'unix_time': 1714014917.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15d5zv/', 'score': 4, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'At this rate, the only way to guarantee a death sentence for reckless driving is if the car was found with drugs.\n\nThe sentence is beyond retarded, people died, this is straight up murder. If someone runs through a public crowd with a sword swinging wildly and people died, you mean to tell me he will just get 8 years jail maximum?', 'name': 't1_l15d5zv'}, {'author': 'Agreeable-Panda-8049', 'unix_time': 1714016508.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15gjkm/', 'score': 4, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'How is this not attempted murder given that he beat the red light on purpose?? If our traffic rules are so relax everyone could become a murderer tomorrow since the sentence will be so light! Two innocent lives were taken away just like that because of someone’s ego.', 'name': 't1_l15gjkm'}, {'author': 'Vegetable-Act-1158', 'unix_time': 1714016527.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15gkz9/', 'score': 5, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'He doesn’t look smart enough to pass even N levels but took the life of a young girl from a good JC. This really breaks my heart.', 'name': 't1_l15gkz9'}, {'author': 'Original-Bug-7056', 'unix_time': 1714015351.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15e3sk/', 'score': 4, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Is the bail in sg always so low. He could afford a car, so he 30k seems low or?', 'name': 't1_l15e3sk'}, {'author': 'No-Preparation2277', 'unix_time': 1714022304.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15rdhp/', 'score': 2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Something is off.\n\nHe's either too smug or he is giving the court a blank cheque. It seems that he doesn't care what will happen to himself.\n\nWas he suicidal or of unsound mind when he ploughed through the traffic?\n\nThis is senseless. I feel for the families (although I can't imagine what they are still going through) of the victims. I pray they find closure and strength.", 'name': 't1_l15rdhp'}, {'author': 'Valuable-Path9747', 'unix_time': 1714022782.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15s6d6/', 'score': 2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'This guy need to be given the max… no remorseful shown at all! 😡', 'name': 't1_l15s6d6'}, {'author': 'stickytofw', 'unix_time': 1714023113.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15sq2k/', 'score': 2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Even if his life could be traded for the victims’, that’s still not enough', 'name': 't1_l15sq2k'}, {'author': 'dreamer_eater', 'unix_time': 1714013829.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15armd/', 'score': 2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Genuine question, how is he so intact physically when others sustained worse injuries? At high speed does ramming into someone cause less injuries to the driver or is he just lucky? I thought he'd have more injuries than this....", 'name': 't1_l15armd'}, {'author': 'Original_Chemist_635', 'unix_time': 1714020566.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15obfu/', 'score': 2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'So this guy wasn’t even driving under influence of any substance? Just purely psychopathic??', 'name': 't1_l15obfu'}, {'author': 'Starwind13', 'unix_time': 1714022140.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15r39l/', 'score': 2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "If he's jailed, please put him together with the lifers so that they can give him a good welcome.", 'name': 't1_l15r39l'}, {'author': 'Harmoniinus', 'unix_time': 1714023683.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15tn64/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "I hope the Saab driver's family don't defend him, he should face the consequences as he is in the wrong and many lives were affected. But at the same time, I hope people don't stoop so low ah like doxxing his family/close ones' phone numbers, home address and harrassing them. Reacting on behalf of the victims' families by harrassing the offender(s) family (just for the offender to feel what it's like for his family to get hurt) is so wrong and stirs fear\n\nE.g: The person who uploaded the Merc POV dashcam on his FB turns out to be a car workshop person who forgot to put a disclaimer that he wasn't the Merc driver and uploading on the driver's behalf. Ended up people leaked his home address, phone number, called his children names and harrassed him via calls.", 'name': 't1_l15tn64'}, {'author': 'Suspicious_Clock_577', 'unix_time': 1714016427.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15gdd1/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Well well well', 'name': 't1_l15gdd1'}, {'author': 'ghostcryp', 'unix_time': 1714017531.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15iltu/', 'score': 2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Burn in hell. This guy deserves zero liberty', 'name': 't1_l15iltu'}, {'author': 'deweye', 'unix_time': 1714015517.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15egib/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Scum bag. Put him in jail and throw away the key.', 'name': 't1_l15egib'}, {'author': 'ieatritalinforbrkfst', 'unix_time': 1714017585.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15ipmr/', 'score': 2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'We should petition for the death penalty for this dude, really deserves it.', 'name': 't1_l15ipmr'}, {'author': 'WebApprehensive4944', 'unix_time': 1714012943.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l158prq/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Fucking worthless scumbag, rot in prison for eternity. Anything less than a death penalty is unacceptable.', 'name': 't1_l158prq'}, {'author': 'MandyLei', 'unix_time': 1714032101.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l1660lm/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "car technologies have evolved. cars are getting faster, and people owning car in sg are getting richer.. the penalties need to keep up. but again.. i don't think this is an accident. it's deliberate and blatant red light beating in a moving junction...his actions with the machine is lethal!", 'name': 't1_l1660lm'}, {'author': 'Earlgreymilkteh', 'unix_time': 1714032242.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l1667b8/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Why are our laws so lax when it comes to cars.', 'name': 't1_l1667b8'}, {'author': 'Pilotboi', 'unix_time': 1714032385.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l166e2g/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': '>He is out on bail of S$30,000 and will return to court for a pre-trial conference on June 7\n\nThis babi fucker got people to bail him out also? confimr got money to engage lawyer and go out scott free', 'name': 't1_l166e2g'}, {'author': 'khitho1', 'unix_time': 1714032966.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l1675g6/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Should have bash him unconscious, tie him up and chain him to his car.\n\nWhen he wakes up, the only thing he hear is "I wanna to play a game!!! with the SAW theme song blasting"\n\nThe only thing he see is a Leopard 2 tank charging towards him, crushing him to rojak\n\nThis would have be real justice!\n\n(instead of our fucked up laws which is a joke really)', 'name': 't1_l1675g6'}, {'author': 'Ok-Carpet-3520', 'unix_time': 1714033137.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l167dkn/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'The urge to slap his face continuously for 10 mins is strong here.. no remorse butoh this one. Plus, who the hell wakes up one day and decided to buy that ugly Saab car?? Wth? No taste', 'name': 't1_l167dkn'}, {'author': 'Henjbh', 'unix_time': 1714024276.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15ule6/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Singapore need stronger laws, 2 years is bullshit, specially with 2 ppl involved', 'name': 't1_l15ule6'}, {'author': 'ntq1507', 'unix_time': 1714013054.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l158z5h/', 'score': 0, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Hope he repents and see the harm he’s caused.', 'name': 't1_l158z5h'}, {'author': 'Vegetable-Fly-7402', 'unix_time': 1714016511.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15gjrt/', 'score': -2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'whoever posted bail on his behalf is complicit and condoning his action', 'name': 't1_l15gjrt'}, {'author': 'AlphaOmega1337', 'unix_time': 1714017252.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15i1rq/', 'score': 0, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'punishment should be strapped to a car and get T-boned honestly', 'name': 't1_l15i1rq'}, {'author': 'Beginning-Travel838', 'unix_time': 1714019388.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15m5yz/', 'score': 0, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Huat ah!!  Crazy Murderers should come to SG. Only 8 years max for the killer of 2 at tampines.. Siao liao!!', 'name': 't1_l15m5yz'}, {'author': 'Alucardeus', 'unix_time': 1714021549.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15q2by/', 'score': -11, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'So many hate comments here!! What he did is wrong but wishing for his death is also wrong.', 'name': 't1_l15q2by'}, {'author': '[deleted]', 'unix_time': 1714012754.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l1589oa/', 'score': -5, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': '[removed]', 'name': 't1_l1589oa'}, {'author': '3ply', 'unix_time': 1714016177.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15fucn/', 'score': -6, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'I wonder why the prosecution thinks he could be a flight risk. Is he not local?', 'name': 't1_l15fucn'}, {'author': 'Realistic-Nail6835', 'unix_time': 1714015965.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15fe8x/', 'score': -15, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'I think the maximum of 8 years will be sufficient.', 'name': 't1_l15fe8x'}, {'author': 'hychael2020', 'unix_time': 1714023721.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15tpe4/', 'score': 27, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "I'm imploring everyone angry to press your MPs in the future and make this a talking point during the elections this year. Please don't waste your time and effort on doxxing, please instead utilise our democracy to make changes to sentencing if you guys think it's light.\n\nWe can definitely make changes if we try and work together. However doxxing isn't the way", 'name': 't1_l15tpe4'}, {'author': 'Popofish', 'unix_time': 1714017367.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15i9y3/', 'score': -41, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Kena censored on mainstream media not enough, Reddit also want to censor.', 'name': 't1_l15i9y3'}, {'author': 'Electrical-Usual5147', 'unix_time': 1714016528.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15gl14/', 'score': -76, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'emotion may still be running high? 2 lifes were killed for nothing! dont sit on high horses and made such comment just because you are a moderator!', 'name': 't1_l15gl14'}, {'author': 'bluecheeseplate', 'unix_time': 1714014202.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15blqq/', 'score': 178, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'I know there is a legal and moral difference between murder and manslaughter but... this guy and his attitude is really blurring the lines.', 'name': 't1_l15blqq'}, {'author': 'feyeraband', 'unix_time': 1714017799.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15j4v2/', 'score': 35, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'I believe that’s after they increased it. Just 10 years ago, a guy killed a whole South Korean family that was visiting. 4 people. He got 5 years because that was the maximum then. I believe they might have raised it because of that incident.\n\nhttps://www.straitstimes.com/singapore/courts-crime/man-gets-5-years-and-20-years-ban-for-cte-fatal-accident-that-killed-korean', 'name': 't1_l15j4v2'}, {'author': 'FdPros', 'unix_time': 1714015139.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15dncv/', 'score': 16, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'nothing new, our laws for injuring/killing whilst driving is jokingly light', 'name': 't1_l15dncv'}, {'author': '_truth_hurts', 'unix_time': 1714021017.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15p4j9/', 'score': 4, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'And it’s seldom that the maximum sentence is meted out.', 'name': 't1_l15p4j9'}, {'author': 'Orenisshii', 'unix_time': 1714017051.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15hn5e/', 'score': -2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Any chance to increase the jail term for this mfer? This should read as an intent to kill', 'name': 't1_l15hn5e'}, {'author': 'Hetares', 'unix_time': 1714022662.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15rz88/', 'score': 0, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "That's the MAXIMUM!? For fuck's sake...", 'name': 't1_l15rz88'}, {'author': 'Shoki81', 'unix_time': 1714015711.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15ev91/', 'score': 86, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Knn the way he replied damn guai lan...', 'name': 't1_l15ev91'}, {'author': 'KoishiChan92', 'unix_time': 1714022800.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15s7gz/', 'score': 22, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Right? This motherfucker seriously, wearing an e tag is literally nothing compared to the families who are grieving their lost family members and worrying about their hospitalised family members.', 'name': 't1_l15s7gz'}, {'author': 'Jonathan-Ang', 'unix_time': 1714013344.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l159nhr/', 'score': 22, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'where are you getting this info?', 'name': 't1_l159nhr'}, {'author': 'Koei7', 'unix_time': 1714017030.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15hloj/', 'score': -4, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'I am not defending what he said, but just want to point out he isn’t entirely wrong. He has all his travel documents taken, no way he could leave the country legally & is it worth it for him to be on the run for life (meaning he finds a way to leave illegally) for max 8 years jail & probably 6 years for good conduct? So then is the electronic tag really necessary bcos he might be a flight risk?\n\nIf I were him, I would not leave esp if my family & kids are here. I would be out before I am 50. Just stating my personal opinion.', 'name': 't1_l15hloj'}, {'author': 'Hivacal', 'unix_time': 1714013769.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15ampi/', 'score': -2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Did he reply to the judge? I didn't see it in the article.", 'name': 't1_l15ampi'}, {'author': 'xHarleyy', 'unix_time': 1714025828.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15x160/', 'score': 10, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Why will he feel remorse when he know he is not getting death no matter what.', 'name': 't1_l15x160'}, {'author': 'DarkCartier43', 'unix_time': 1714018260.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15k184/', 'score': 3, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "maybe he's feeling hopeless? or just don't care about anything anymore.", 'name': 't1_l15k184'}, {'author': 'seazboy', 'unix_time': 1714019462.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15maz6/', 'score': 74, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Cause he doesn't have any. You would think that he would be guilty for causing a huge accident but no. He was the first to get out of the car amongst all the victims and went to look for his belongings without even helping the victims", 'name': 't1_l15maz6'}, {'author': 'Azurebold', 'unix_time': 1714025839.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15x1td/', 'score': 10, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Cause he doesn’t really have any. He knows he isn’t going to get a lot of time even though there’s blood on his hands. Driving laws in Singapore are a joke at best, and even the max sentence is laughable. \n\nIf they do decide to up his sentence, I’d love to see the priceless look on his face though.', 'name': 't1_l15x1td'}, {'author': 'Living-Can-2160', 'unix_time': 1714020310.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15nuuv/', 'score': 0, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Damn man', 'name': 't1_l15nuuv'}, {'author': 'sct_trooper', 'unix_time': 1714013980.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15b3wb/', 'score': 131, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'someone on edmw commented because he was driving a saab. continental car + collision head first meant that ironically, he had the most protection/buffer compared to those he rammed through.', 'name': 't1_l15b3wb'}, {'author': 'precipiceblades', 'unix_time': 1714014702.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15coz2/', 'score': 27, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Arm not even in a cast, most likely not even broken :c', 'name': 't1_l15coz2'}, {'author': 'Noobcakes19', 'unix_time': 1714015612.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15enq2/', 'score': 20, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "drivers will usually survive a head-on collision against something. Unless it's a head-on collision with an on-coming vehicle.   \n  \nThe ones getting T-boned usually GG.", 'name': 't1_l15enq2'}, {'author': 'Vegetable-Fly-7402', 'unix_time': 1714016628.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15gsda/', 'score': 10, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'he was using his mobile device on his injured arm in the video', 'name': 't1_l15gsda'}, {'author': 'Original-Bug-7056', 'unix_time': 1714015241.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15dvak/', 'score': 6, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'No broken. He can use his phone and has no cast…', 'name': 't1_l15dvak'}, {'author': 'spdknght', 'unix_time': 1714023895.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15tzh4/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "1)Speed of which he crash into other\n2)Angle of impact he crash head on, into the sides of the cars.\n3)Curb Weight if you Google Saab 93 aero is ~1.5 tons(1,490 KG) \n4)It's a continetial as many have said.", 'name': 't1_l15tzh4'}, {'author': 'Prov0st', 'unix_time': 1714015751.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15eybp/', 'score': 98, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'As a fellow Melayu, this piece of shit deserve to die.', 'name': 't1_l15eybp'}, {'author': 'neokai', 'unix_time': 1714014726.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15cqvj/', 'score': 41, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': ">wear shades, cap and mask.\n\nThis is anti-press measures, don't want clear photo in newspapers.", 'name': 't1_l15cqvj'}, {'author': 'dxflr', 'unix_time': 1714014558.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15cdpv/', 'score': 75, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': '[Zb link redacted. See sub comments for context why]\xa0\n\n\nFor those interested, zaobao has a photo of the alleged driver of the Saab on the day of the accident. This is probably Mr. Syafie, no snapback cap, mask and sunglass can hide.', 'name': 't1_l15cdpv'}, {'author': 'Traxgen', 'unix_time': 1714012893.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l158ljk/', 'score': 17, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Wear shades and cap to obscure some more.', 'name': 't1_l158ljk'}, {'author': 'Interesting_Ad2986', 'unix_time': 1714015337.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15e2o5/', 'score': 6, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'His face make me puke', 'name': 't1_l15e2o5'}, {'author': 'Purpledragon84', 'unix_time': 1714027013.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15ytmn/', 'score': 8, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Agreed, man really is public enemy no.1 right now. Regardless of race language or religion all fking hate this guy.', 'name': 't1_l15ytmn'}, {'author': 'No-Establishment-885', 'unix_time': 1714030510.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l163uiz/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Max sentence of 8 years felt too light', 'name': 't1_l163uiz'}, {'author': 'yujuismypuppy', 'unix_time': 1714019504.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15mdqp/', 'score': 27, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'You mean the black Saab driver who caused the deaths of two innocent citizens, Muhammad Syafie Ismail?', 'name': 't1_l15mdqp'}, {'author': 'Pilotboi', 'unix_time': 1714032849.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l16700u/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'you scolding too soft for a that babi', 'name': 't1_l16700u'}, {'author': 'dukeshytalker', 'unix_time': 1714013187.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l159abc/', 'score': 41, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'its one incident with two victims, dont think they can make it consecutive even if two charges. realistically speaking though so what if he can get 16 years. w good behaviour mebbe 10+ years he is out. 2 lives taken entrie communities affected several with life changing injuries .... quite a bummer that SG allowed this shitty state of affairs for so long. The other Tampines case the feller literally got 7 years without a permanent ban. Plus so many reckless driving cases in the past year. I blame the authorities.', 'name': 't1_l159abc'}, {'author': 'Scorchster1138', 'unix_time': 1714013757.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15alql/', 'score': 62, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Apparently the EDMW CSI experts already dug out his name a few days ago', 'name': 't1_l15alql'}, {'author': 'dxflr', 'unix_time': 1714013892.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15aws4/', 'score': 26, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "No point for this fella. As mentioned in another comment, about his reply to being on etag, clearly a sociopath/psychopath. He can't empathize, and only cares about himself. Showing others in pain will have little effect. This one is best to just bring direct suffering on him and his ego.\xa0", 'name': 't1_l15aws4'}, {'author': 'HalcyoNighT', 'unix_time': 1714030223.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l163gi1/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Out in one week for good behavior', 'name': 't1_l163gi1'}, {'author': 'robozom', 'unix_time': 1714018646.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15krsr/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "We need a new law for this kind of asshole - not to deal with ordinary recklessness but reckless without regard for other road user's safety.", 'name': 't1_l15krsr'}, {'author': 'ironicfall', 'unix_time': 1714022421.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15rkl8/', 'score': 3, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'imagine being that lorry driver during that incident. you’re completely stopped and waiting for the traffic and you see a car freaking zooming past you into people in front. his dash cam video probably the most disturbing', 'name': 't1_l15rkl8'}, {'author': 'Jammy_buttons2', 'unix_time': 1714021817.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15qj5a/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'We do not have manslaughter in our penal code, only culpable homicide not amounting to murder', 'name': 't1_l15qj5a'}, {'author': 'denasher', 'unix_time': 1714014877.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15d2tu/', 'score': 31, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'I mean he’s currently persona non grata with the entire country, I’m not sure if 2 are even enough if people decided to take matters into their own hands', 'name': 't1_l15d2tu'}, {'author': 'thethinkingbrain', 'unix_time': 1714025279.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15w6h3/', 'score': 6, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'With that logic, anyone who kills another in a traffic accident should be hanged regardless. We are going to send more people in the [gallows](https://www.straitstimes.com/singapore/fatal-road-accidents-up-in-2023-many-motorists-have-irresponsible-driving-behaviours-traffic-police) every year then, given that [less than 20 people are hanged annually for murder or drug-related offenses](https://www.statista.com/statistics/961037/number-of-capital-executions-by-crime-committee-singapore/).\n\nIf there was a deliberate motive for murder and it was pre-meditated, then yes, the death penalty is warranted. Do you see any forms of pre-meditation or malicious intent here? If anything, this case is instigated by provocation, at best culpable homicide not amounting to murder.\n\nI am not trying to justify this vile man’s actions here, he’s in the wrong for taking two innocent lives away, but can you listen to yourself first? There is definitely work to be done in our justice system, but suggesting something so preposterous like “death penalty to all traffic murderers” is just going to sweep off any mitigating factors within every legal case. You are letting your emotion run high instead of enacting justice for all parties involved.', 'name': 't1_l15w6h3'}, {'author': 'Sea_Consequence_6506', 'unix_time': 1714019368.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15m4og/', 'score': 28, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "If criminal lawyers only take on cases that they can win or only choose to represent 'palatable' criminals, then they can all close shop and choose another line of work already", 'name': 't1_l15m4og'}, {'author': 'Isares', 'unix_time': 1714016795.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15h4km/', 'score': 16, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Unfortunately driving while stupid doesn't count as DUI.", 'name': 't1_l15h4km'}, {'author': 'Ensis_Aurora', 'unix_time': 1714020182.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15nmjy/', 'score': 4, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Cause he drives a SAAB, that thing is literally indestructible. Ppl say Volvo is safest, only cause SAAB went bankrupt....\n\nEdit: Like I also remember got 1 incident towards Tuas checkpoint on AYE, some merc drove full speed towards oncoming and hit a Japanese car. Japanese car driver died on the spot, merc driver only light injuries.... Fck up.\n\nhttps://youtu.be/uHHV3x79w3Q', 'name': 't1_l15nmjy'}, {'author': 'BubbleTea199', 'unix_time': 1714024579.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15v2up/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'When emotions are running high, race often brought in…', 'name': 't1_l15v2up'}, {'author': 'Federal_Run3818', 'unix_time': 1714014477.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15c7al/', 'score': 9, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Better yet, hope he breaks his neck, but not enough to kill him, just turn him into a living vegetable. It'll be an upgrade for him anyway.", 'name': 't1_l15c7al'}, {'author': 'Purpledragon84', 'unix_time': 1714027403.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15zec4/', 'score': 2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Thats not a bad idea. But this fker also not remorseful.', 'name': 't1_l15zec4'}, {'author': 'Sir-Spork', 'unix_time': 1714019444.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15m9sn/', 'score': -3, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'For just one of the charges, he has 4', 'name': 't1_l15m9sn'}, {'author': 'machopsychologist', 'unix_time': 1714017806.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15j5db/', 'score': 5, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'I have trouble believing that one would willfully risk their own lives by running directly into traffic unless he was suicidal… it was not even trying to beat a red; there was clearly active traffic in the intersection, and there was no sign of attempting to avoid collision at all. \n\nEither under the influence, or was so high on adrenaline from the previous hit that he was so focus  on whether the merc was chasing him\n\nNot to discount his actions but the law takes everything into account.', 'name': 't1_l15j5db'}, {'author': 'komplete10', 'unix_time': 1714017533.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15ilz4/', 'score': 3, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'I guess the question is, did he break the red light because he was trying to kill someone? Like you say, that would make everyone who goes over on red an attempted murderer.', 'name': 't1_l15ilz4'}, {'author': 'toggafsyk', 'unix_time': 1714014092.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15bcxa/', 'score': 21, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "front of the car has a crumple zone + airbags \n\nmeanwhile you're fked if you get t boned", 'name': 't1_l15bcxa'}, {'author': '_Synchronicity-', 'unix_time': 1714014294.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15bsvf/', 'score': 17, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Firstly, because it's a saab known for their sturdy cars and design.\n\nSecondly, the brunt of the force is absorbed by the front of the chassis while the girl who got hit faced the full impact because she was in the front passenger seat where the saab hit directly. -| < something like that where the dash is the car and | is the victim's car\n\n That's also partly why the girl's father survived as the girl took most of the impact.\n\n3rdly, the saab's airbags was doing it's job.", 'name': 't1_l15bsvf'}, {'author': 'osshhh', 'unix_time': 1714014002.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15b5od/', 'score': 8, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'cause saab cars legit fucking tanky one', 'name': 't1_l15b5od'}, {'author': 'Haunting_Reality_158', 'unix_time': 1714014125.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15bfl1/', 'score': 4, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'also collision head first, got your engine infront as crumble zone. most severe ones are those getting T-boned or even flipped', 'name': 't1_l15bfl1'}, {'author': 'Radiant_Guess7277', 'unix_time': 1714014026.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15b7kf/', 'score': 2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Could be that because the other vehicles absorbed the impact and because he was saved by the car's airbags. Never have I ever hated upon the invention of airbags but how I wish his didn't deploy", 'name': 't1_l15b7kf'}, {'author': 'la_gusa', 'unix_time': 1714014162.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15bihb/', 'score': 2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'It is because he is ramming. The front of the cars have the engine and tons of space where structures deform to absorb the impact. He rammed other vehicles from the side, where less protection and impact absorbing structures are present. This is not including that all cars provide front airbags, but only some provide side courtains (and defenetely a minibus will not)', 'name': 't1_l15bihb'}, {'author': 'yaykaboom', 'unix_time': 1714017603.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15iqyl/', 'score': 0, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'A simple analogy would be, someone wearing a helmet headbutting you.\n\nThe helmet guy might feel it a bit but you not wearing a helmet is in a world of pain.', 'name': 't1_l15iqyl'}, {'author': 'blahhh87', 'unix_time': 1714021268.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15pkgh/', 'score': -1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Cars are designed to take impact from the front (plus it being a Saab). Secondly, the victims were t-honed by him. Had he drove a Japanese car and the victims in Saab or other conti cars, they still would likely died because there is little to no protection from the sides.', 'name': 't1_l15pkgh'}, {'author': 'gaoxingdcf', 'unix_time': 1714025694.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15wtpd/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Cannot lah.\n\nWait taxpayer money need to spend to redo the lock.\n\n Maybe keep it so secure like in our submarines.', 'name': 't1_l15wtpd'}, {'author': 'Radiant_Guess7277', 'unix_time': 1714015834.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15f4ju/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Sadly that's not even in the max sentence for all the charges that he is charged with.", 'name': 't1_l15f4ju'}, {'author': 'gaoxingdcf', 'unix_time': 1714025786.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15wysb/', 'score': 2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'If no one posted bail, one will stay in prison?', 'name': 't1_l15wysb'}, {'author': 'Azurebold', 'unix_time': 1714026847.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15ykt9/', 'score': 5, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Yes, because two people very literally dying by your own selfish needs and irresponsibility is on-par with people condemning you with hypotheticals. \n\nI don’t think there’s an underlying obligation to be kumbaya. Not everyone needs kindness all the time, especially when they do this kinda thing.', 'name': 't1_l15ykt9'}, {'author': 'SG_wormsblink', 'unix_time': 1714012945.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l158pz7/', 'score': 15, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'His race shouldn’t matter in the judgement, all are equal before the law.', 'name': 't1_l158pz7'}, {'author': 'Pyhet', 'unix_time': 1714012953.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l158qmi/', 'score': 9, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'what are you spouting man', 'name': 't1_l158qmi'}, {'author': 'queeenvee', 'unix_time': 1714016674.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15gvqq/', 'score': 15, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': '“In court, the prosecuting officer requested for Syafie to wear an electronic tag. This will allow the authorities to monitor him.\n\nThis is because he failed to stop after hitting another car, which showed poor conduct and an element of flight risk, the prosecuting officer said.” - Mothership', 'name': 't1_l15gvqq'}, {'author': 'Fensirulfr', 'unix_time': 1714021417.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15ptw4/', 'score': 5, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Why would being local mean that he is not a flight risk?', 'name': 't1_l15ptw4'}, {'author': 'SuzeeWu', 'unix_time': 1714027466.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15zhmm/', 'score': 6, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Yes, one MP (Bishan Toa Payoh) has filed questions for Parliament. Let's hope for stronger/deterrent sentences!!!", 'name': 't1_l15zhmm'}, {'author': '-avenged-', 'unix_time': 1714024692.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15v962/', 'score': 14, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "How is this censorship? You know his name, face, and crime charged already.\n\nOr do you also want to know where he stays, which school he went to, who he works for, his family members' names and faces?", 'name': 't1_l15v962'}, {'author': 'WangmasterX', 'unix_time': 1714019960.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15n82s/', 'score': 45, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'You are exactly the problem mods are trying to prevent. Please get yourself a mirror.', 'name': 't1_l15n82s'}, {'author': 'Radaxen', 'unix_time': 1714019689.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15mpz5/', 'score': 38, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "You're proving their point.", 'name': 't1_l15mpz5'}, {'author': 'trashtrottingtrout', 'unix_time': 1714021004.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15p3o0/', 'score': 26, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Then what? You bitching on the internet can bring them back to life?', 'name': 't1_l15p3o0'}, {'author': 'raymmm', 'unix_time': 1714015436.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15eaba/', 'score': 69, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "To be honest, if you drive at reckless speed or drive under the influence of alcohol then you consciously decided to put someone else's life in danger. Not sure about legal definition, but morally it's premeditation to me.", 'name': 't1_l15eaba'}, {'author': 'that_one_guy_2123', 'unix_time': 1714014585.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15cfsn/', 'score': 70, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'If you drive at that speed and on a busy intersection too. How is that manslaughter. Seems like he has intentions for someone to die.', 'name': 't1_l15cfsn'}, {'author': 'KeenStudent', 'unix_time': 1714025156.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15vz7h/', 'score': 5, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Ya section 64 amended in 2019 but unlikely it was due to that incident', 'name': 't1_l15vz7h'}, {'author': 'Fensirulfr', 'unix_time': 1714020766.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15oo94/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'No. The maximun penalty for each type of offence is stated in the law, and any change in law cannot be retroactively applied. \n\nAlso, intent to kill is harder to prove than you think.', 'name': 't1_l15oo94'}, {'author': 'TimTamDrake', 'unix_time': 1714013564.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15a5r2/', 'score': 260, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "In another article here: [Driver involved in Tampines fatal crash identified; handed four charges | The Straits Times](https://www.straitstimes.com/singapore/courts-crime/driver-involved-in-tampines-fatal-crash-identified-handed-four-charges)\n\n>In response, Muhammad said: “I don’t think it’s necessary to put an e-tag on me as all my documents, my passport, have been seized by the police. I’m on bail and am already present today and will be present for all the court matters.\n\n>“But it is up to the court to make the decision.”\n\nSidenote; Fucking piece of shit, fuck you Muhammad Syafie Ismail if you're reading this.", 'name': 't1_l15a5r2'}, {'author': 'twoeasy3', 'unix_time': 1714013524.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15a2g0/', 'score': 37, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': '>In response, Muhammad said: “I don’t think it’s necessary to put an e-tag on me as all my documents, my passport, have been seized by the police. I’m on bail and am already present today and will be present for all the court matters.\n\n[Straits Times.](https://www.straitstimes.com/singapore/courts-crime/driver-involved-in-tampines-fatal-crash-identified-handed-four-charges)', 'name': 't1_l15a2g0'}, {'author': 'avocadoooforlife', 'unix_time': 1714013481.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l159yxi/', 'score': 5, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Straits Times', 'name': 't1_l159yxi'}, {'author': 'vancomyxin', 'unix_time': 1714013511.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15a1bp/', 'score': 4, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': '[Mothership](https://mothership.sg/2024/04/driver-tampines-accident-charged/?fbclid=IwZXh0bgNhZW0BMQABHfrp_KKZ3j-itVsD2rpgL6mUCNYOhvQBa9Bo6Z5dxSumIQ4zOWPnkAksKA_aem_AZOi5F1E3J-QQk0_71MztoiJevrYJyABpy5UjCypKfC10wjPU_AhV7hB0ICFpmlzbDI) say one', 'name': 't1_l15a1bp'}, {'author': 'Musical_Walrus', 'unix_time': 1714018218.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15jy83/', 'score': 64, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': '“If you were him” you are making the classic mistake of assuming everyone has the same morals as you. This guy clearly doesn’t.', 'name': 't1_l15jy83'}, {'author': 'Fat-Solid591', 'unix_time': 1714022692.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15s0yd/', 'score': 4, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'The fact that he replied the judge, show his arrogance. Being at the centre of this court case, he should just shut up & follow all the process.', 'name': 't1_l15s0yd'}, {'author': 'PreviousFly920', 'unix_time': 1714015035.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15df8z/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Go see ST post on IG. He replied to the judge.', 'name': 't1_l15df8z'}, {'author': 'yujuismypuppy', 'unix_time': 1714019439.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15m9h3/', 'score': 34, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'hopeless/dont feel anything should be looking down at the ground in shame, trying to hide his face at all costs, not step use phone check SMS while on the way in. who the fuck still wanna message this guy anyway?', 'name': 't1_l15m9h3'}, {'author': 'seazboy', 'unix_time': 1714020723.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15olgi/', 'score': 20, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Hopeless? I think he is just entitled. Ram into others causing harm and even death. But he is the first one to step out. Didn't even go and help others but went to look for his belongings. Asking for help from scdf peeps to look for his belongings while they busy trying to help others.", 'name': 't1_l15olgi'}, {'author': 'Depressed-Gonk', 'unix_time': 1714026107.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15xgis/', 'score': 7, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Wtf .. Someone saw that happening?', 'name': 't1_l15xgis'}, {'author': 'onlyreverie', 'unix_time': 1714021298.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15pmbd/', 'score': 19, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Fuck, for real??? I really hope karma gets him.', 'name': 't1_l15pmbd'}, {'author': 'Pilotboi', 'unix_time': 1714032007.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l165vvz/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'wtf? are you for real?\n\nthis fucker came out unscratched?', 'name': 't1_l165vvz'}, {'author': 'Pilotboi', 'unix_time': 1714032062.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l165yo1/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'I really hope they do and a new law magically gets created just nice for this babi to rot in jail', 'name': 't1_l165yo1'}, {'author': 'CommieBird', 'unix_time': 1714014233.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15bo50/', 'score': 58, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'If you look at the accident photos the car did its job of protecting the driver (can’t make more future sales if your customer is dead). Ironically the other vehicles involved were in far worse shape than the SAAB.', 'name': 't1_l15bo50'}, {'author': 'go_zarian', 'unix_time': 1714015114.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15dldp/', 'score': 25, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Yep, continental cars are built to withstand punishment.\n\nSaab, Volvo, you name it. They can take a terrific beating.\n\nFrom what I understand, though, it comes at the cost of fuel efficiency. Feel free to correct me if I'm wrong.", 'name': 't1_l15dldp'}, {'author': 'davechua', 'unix_time': 1714017279.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15i3pw/', 'score': 10, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Just pretending to get sympathy? Wouldn’t be surprised.', 'name': 't1_l15i3pw'}, {'author': 'carrotwise', 'unix_time': 1714021834.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15qk7o/', 'score': 3, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'i broke my arm really bad and had to have a surgery to put a metal in and i didnt have a cast…not saying he broke it but idt a cast is a sign', 'name': 't1_l15qk7o'}, {'author': 'yagrain', 'unix_time': 1714021298.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15pmc7/', 'score': 6, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Omg chao geng! Can't even pretend to be injured or remorseful. The palace in his heart must be huge", 'name': 't1_l15pmc7'}, {'author': 'MilkTeaRamen', 'unix_time': 1714015662.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15erhl/', 'score': 9, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'I think just wrist pain from the impact, and the hospital just put it on as a precautionary measure.\n\nMaybe he wants some sympathy points and never took it down. \n\nLike you said, no cast… and the way he could still use his phone hmm', 'name': 't1_l15erhl'}, {'author': 'Party-Ring445', 'unix_time': 1714018747.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15kype/', 'score': 32, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Same.. sampah masyarakat', 'name': 't1_l15kype'}, {'author': 'illEagle96', 'unix_time': 1714016347.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15g7d6/', 'score': 9, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Happy cake day bro 😔', 'name': 't1_l15g7d6'}, {'author': 'Pilotboi', 'unix_time': 1714032497.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l166jcd/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'scared all will recognize his babi face in public and whack him', 'name': 't1_l166jcd'}, {'author': 'etulf', 'unix_time': 1714015281.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15dyf4/', 'score': 87, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'say what you want about journalism standards in the chinese papers, they are damn good at digging up stuff sometimes.', 'name': 't1_l15dyf4'}, {'author': 'yellow_quarantine', 'unix_time': 1714017897.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15jbvo/', 'score': -18, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Keyword: ‘Probably’. How do you know you’re not doxing someone. Seriously, why are y’all so hungry to see his face?', 'name': 't1_l15jbvo'}, {'author': 'tiredsingaporean5274', 'unix_time': 1714026270.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15xpm3/', 'score': 14, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Yes I mean Muhammad Syafie Ismail, the driver who killed an innocent JC student going to a school event as well as a lady just on her way to work. His actions has negatively impacted the lives of two families, as well as several friends who will now have a hole in their lives, no thanks to one guy who just had to speed.', 'name': 't1_l15xpm3'}, {'author': 'sie-waitforit-ghart', 'unix_time': 1714014640.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15ck56/', 'score': 35, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Cause if we jail him long enough, he will come out when he is so old that integrating back to society is near impossible and actually a mental torture. \n\nImagine after 15 or 20 years, you are so used to jail life. Having food and your needs taken care of. \n\nThen when you are 60 years old, you are released back to sg society. You might not be able to keep up with all the changes. Just to list a few things that change in our daily lives in a matter of 10 years:\n1. We tap our phones instead of wallet/card for public transport\n2. We can order food or hail a cab through our phone\n3. We need to keep our trays at hawkers\n4. More mrt lines\n\nThese people will have a mental breakdown after a while, live their rest of their lives in poverty (cause family are long gone, forgotten or abandon him) or just finding other ways to go back to jail. \n\nI do not wish death on these kind of people, instead a long and hard life ahead.', 'name': 't1_l15ck56'}, {'author': 'Hydrohomie1337', 'unix_time': 1714014466.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15c6g3/', 'score': 74, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Ya man, damn zhun somemore, apparently his own colleagues ratted him out', 'name': 't1_l15c6g3'}, {'author': 'KopiSiewSiewDai', 'unix_time': 1714014156.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15bi10/', 'score': 21, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Wow. How sia. God tier CSI as usual', 'name': 't1_l15bi10'}, {'author': 'gaoxingdcf', 'unix_time': 1714025542.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15wl8y/', 'score': 5, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Must be damn rabak that his colleagues ratted him out. \n\nAgain impressive on how he walked away with just a broken arm. Continental cars are indeed a safety device.', 'name': 't1_l15wl8y'}, {'author': 'MolassesBulky', 'unix_time': 1714022947.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15sg7m/', 'score': -1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Colloquially called mansalughter. Google “manslaughter Singapore”', 'name': 't1_l15sg7m'}, {'author': 'Additional-Ad-1644', 'unix_time': 1714027095.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15yy33/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Ya just hang.\n\nBeat a red light? HANG\n\nChew gum in Singapore? Believe it or not, HANG', 'name': 't1_l15yy33'}, {'author': 'x1243', 'unix_time': 1714024906.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15vl6g/', 'score': 3, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "really? I know the og Volvo which was built like a tank was near indestructible.. my aunt's car got hit by a pickup truck.. the Volvo only had slight scratches.. the front of the pickup was smashed", 'name': 't1_l15vl6g'}, {'author': 'denasher', 'unix_time': 1714014958.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15d98d/', 'score': 4, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Vegetable means he won’t be aware of anything, what you want is probably paralyze from neck down and needing people to manage his daily needs', 'name': 't1_l15d98d'}, {'author': 'LuminousSnow', 'unix_time': 1714020276.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15nskw/', 'score': 0, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'not enough. and not likely for max sentencing for all 4 charges also', 'name': 't1_l15nskw'}, {'author': 'dreamer_eater', 'unix_time': 1714015826.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15f3yv/', 'score': 2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Ahhh that makes sense, well that's a shame in this case :( feels unfair", 'name': 't1_l15f3yv'}, {'author': 'DarkCartier43', 'unix_time': 1714018936.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15lbil/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'somehow, reading your comment reminds me of a friend who died when his uber got T-Boned. on April 10 2018. he was so young.', 'name': 't1_l15lbil'}, {'author': 'dreamer_eater', 'unix_time': 1714015872.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15f7eg/', 'score': 0, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Thanks for the explanation, I guess it makes full sense but tragic in this scenario :/', 'name': 't1_l15f7eg'}, {'author': 'MyNameIsOnce', 'unix_time': 1714014365.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15bykg/', 'score': 6, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Saab and Volvo, the two Swedish 'tanks'.\n\nAlso, front impact is always 'safer' compared to getting hit from the side, especially in an older Japanese car", 'name': 't1_l15bykg'}, {'author': 'dreamer_eater', 'unix_time': 1714015975.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15ff31/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Makes sense but sucks that the sides of vehicles are so poorly protected as compared to the front of the vehicle', 'name': 't1_l15ff31'}, {'author': 'HANAEMILK', 'unix_time': 1714013246.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l159f6t/', 'score': 4, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Luckily SG judicial system is not like Los Angeles...', 'name': 't1_l159f6t'}, {'author': 'Sea_Consequence_6506', 'unix_time': 1714023385.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15t5yi/', 'score': 5, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "The fewer links you have to a place, the more likely it is that you may jump bail. If your roots, family and assets, and whatever other means you need to survive, are pretty much all based in Singapore, you are a lower flight risk. \n\nThere's no single factor though. it is a multifaceted enquiry.", 'name': 't1_l15t5yi'}, {'author': 'thethinkingbrain', 'unix_time': 1714029989.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l1634vp/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'I have faith in our institutions. If we can enact change and encourage debate from matters such as SimplyGo and Udemy, we can do it again.\n\nThere’s a reason why that man is an MP and why you aren’t.', 'name': 't1_l1634vp'}, {'author': 'hychael2020', 'unix_time': 1714023238.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15sxcu/', 'score': -7, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "No. However, we can channel this energy into something good instead of complaining. For instance, advocate for more strict road laws and stricter punishments for reckless driving. Educate everyone on dos and don'ts on the road", 'name': 't1_l15sxcu'}, {'author': 'Bcpjw', 'unix_time': 1714021248.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15pj33/', 'score': 10, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Yea, like carrying a weapon, even if you don’t plan to hurt anyone, just one slip and someone is going to get hurt.', 'name': 't1_l15pj33'}, {'author': 'Dawnana', 'unix_time': 1714032526.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l166kqd/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Yeah, a car is as much a weapon as a tool. Its like i bring a knife out and wave it around a crowded place and “accidentally” kill someone, can i say i didnt intend to harm anyone', 'name': 't1_l166kqd'}, {'author': 'MrFickless', 'unix_time': 1714016102.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15fom0/', 'score': 41, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Unfortunately, that’s not how the law interprets the situation. \n\nHad he known his act of speeding could cause an accident? Most likely yes.\n\nWas it to cause as much carnage as he did? Probably not.', 'name': 't1_l15fom0'}, {'author': 'Zestyclose-Swing-345', 'unix_time': 1714014877.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15d2tw/', 'score': 43, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': '**intention**: desiring a result to occur or knowing that the result is a virtual certainty of your actions\n\n**recklessness**: being aware of a risk, and then goes on unreasonably to take it\n\nhitting someone at a busy intersection isn’t a virtual certainty, and i think you surely can’t be saying he desired to kill someone, so its recklessness and therefore manslaughter', 'name': 't1_l15d2tw'}, {'author': 'Yeah_Right_Mister', 'unix_time': 1714018386.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15ka0o/', 'score': -11, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Agree. If I put on a blindfold and walk out onto a busy street with a rifle then start spinning and shooting, killing 20+ people in the process, how culpable am I? \n\nAnd unlike this car accident, the people seeing me appear with a gun could've fled. If you think 8 years is a sufficient sentence for this driver, then surely 8 years is also a sufficient sentence for a blindfolded mass shooter.\n\n(yeah, it's hard to get a gun in SG, but it's just an analogy since I can't think of many ways to mass-kill people in SG besides a car, a gun, or an explosive)", 'name': 't1_l15ka0o'}, {'author': 'sian_half', 'unix_time': 1714023133.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15sr9n/', 'score': 0, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'I believe until the traffic police releases their assessment of what the speed was, the speed will have to be left out of the picture. When the speed has been determined, then more charges will be served', 'name': 't1_l15sr9n'}, {'author': 'Bcpjw', 'unix_time': 1714015586.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15elq5/', 'score': 54, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Definitely sounds like a little shit and odds are he will start saying it’s the car’s faulty brakes, steering issues or even slippery road.\n\n![gif](giphy|Qumf2QovTD4QxHPjy5)', 'name': 't1_l15elq5'}, {'author': 'nohken8', 'unix_time': 1714016219.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15fxm5/', 'score': 37, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Still wants to continue to live his life as if he didn't end 2 and completely change several others", 'name': 't1_l15fxm5'}, {'author': 'Jonathan-Ang', 'unix_time': 1714013626.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15aayj/', 'score': 15, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Thank you.', 'name': 't1_l15aayj'}, {'author': 'Imperiax731st', 'unix_time': 1714016640.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15gt9y/', 'score': 16, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'He should have just perish in the accident and saved us all the anguish.', 'name': 't1_l15gt9y'}, {'author': 'Aphelion', 'unix_time': 1714025905.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15x5hk/', 'score': 4, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Still dare to be passive-aggressive. I think he still hasn't registered that he took 2 innocent lives.", 'name': 't1_l15x5hk'}, {'author': 'Dawnana', 'unix_time': 1714032589.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l166nqr/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'We all need to collectively curse this guy, place a hex on him or whatever so he suffers for life', 'name': 't1_l166nqr'}, {'author': '[deleted]', 'unix_time': 1714015706.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15euv4/', 'score': 4, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': '[removed]', 'name': 't1_l15euv4'}, {'author': 'la_gusa', 'unix_time': 1714014005.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15b5x0/', 'score': 34, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Yeah, court trying to prevent another case of "I will find some boat to run to MY or ID"', 'name': 't1_l15b5x0'}, {'author': 'Koei7', 'unix_time': 1714018824.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15l3yo/', 'score': -1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'It is not just about morals but also the reality of having to be on the run for life vs serving 6-8 years in jail (before good conduct). And the fact he was offered bail probably also showed that the judge thinks he is unlikely to flee the country. This is just my take on what he might be thinking when he said what he said.', 'name': 't1_l15l3yo'}, {'author': 'Koei7', 'unix_time': 1714024691.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15v943/', 'score': 0, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'My reply was not about his attitude but on whether does it make sense for him to flee & hence if the electronic tag is really necessary. I am not defending him as a person but just simply about him fleeing & the tag.', 'name': 't1_l15v943'}, {'author': 'CervezaPorFavor', 'unix_time': 1714017519.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15ikzg/', 'score': 0, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'No. He replied to the prosecution, not the judge.', 'name': 't1_l15ikzg'}, {'author': 'Normal_Coat_6325', 'unix_time': 1714020004.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15nb10/', 'score': 2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'i love the way u say "step use phone check" haha...but yah ini mat, memang sampah masyarakat.', 'name': 't1_l15nb10'}, {'author': 'Sinkingdumplings', 'unix_time': 1714019899.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15n3xz/', 'score': 0, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'LOL', 'name': 't1_l15n3xz'}, {'author': 'Dawnana', 'unix_time': 1714032728.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l166ucx/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Wow, the more i hear about this fooker the more it boils my blood. Someone should have just taken one for the team and rammed into him', 'name': 't1_l166ucx'}, {'author': 'KoishiChan92', 'unix_time': 1714022735.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15s3lc/', 'score': 2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Where did you get this detail from', 'name': 't1_l15s3lc'}, {'author': '[deleted]', 'unix_time': 1714023589.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15thoi/', 'score': -1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': '[deleted]', 'name': 't1_l15thoi'}, {'author': 'seazboy', 'unix_time': 1714032547.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l166lqw/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'He broke an arm thats all', 'name': 't1_l166lqw'}, {'author': 'Arcturion', 'unix_time': 1714015979.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15ffdy/', 'score': 68, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Reminds me of Saab\'s slogan in the 90s, *"A Saab will surrender its own life to save yours"*. Lowkey impressed they delivered.\n\nhttps://en.wikipedia.org/wiki/Saab_Automobile', 'name': 't1_l15ffdy'}, {'author': 'neokai', 'unix_time': 1714014793.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15cw7y/', 'score': 40, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "It's harder to protect the sides imo, smaller available volume for crumple zone.", 'name': 't1_l15cw7y'}, {'author': 'dodgethis_sg', 'unix_time': 1714015062.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15dhf1/', 'score': 6, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Saab is already dead lol', 'name': 't1_l15dhf1'}, {'author': 'KanseiDorifto', 'unix_time': 1714018056.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15jmwu/', 'score': 15, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Swedish cars are thought of to be the safest among all. Volvo invented the safety belt as well, IIRC.', 'name': 't1_l15jmwu'}, {'author': 'AnAnnoyedSpectator', 'unix_time': 1714026432.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15xyi7/', 'score': 4, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "My childhood friend was driving a Volvo and chased by some idiots (who thought she was someone else) into a telephone poll as a teen - the car was completely destroyed and first responders were surprised a live person was pulled from the wreck - but it was a Volvo and she mostly survived (Wasn't the same person unfortunately, but it's still a miracle she is still with us).", 'name': 't1_l15xyi7'}, {'author': 'sarhan182', 'unix_time': 1714024247.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15ujrx/', 'score': 3, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Young2 dowant die, old old kill people', 'name': 't1_l15ujrx'}, {'author': 'x1243', 'unix_time': 1714025575.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15wn2b/', 'score': 5, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "because the English papers don't have journalists already.. they have ppl who just swallow everything and regurgitate.. see the poly lecturer who faked his qualifications or the poly student who pretended to be some super successful business man..", 'name': 't1_l15wn2b'}, {'author': 'dxflr', 'unix_time': 1714026708.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15yddz/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "You have a point on the possible doxxing. There's a non zero chance ZB's photograph is wrong and that's not Mr. Syafie, and just some innocent random dude.\n\n\nWill redact link. We'll just wait for the official mugshot.", 'name': 't1_l15yddz'}, {'author': 'WebApprehensive4944', 'unix_time': 1714016165.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15ftgt/', 'score': 11, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'I hope this happens and he is tortured mentally and physically', 'name': 't1_l15ftgt'}, {'author': 'blahhh87', 'unix_time': 1714021588.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15q4ok/', 'score': 4, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "I agree. What angers me about Ma Chi (Ferrari driver in the bugis accident, years back) is that he died instantly. He didn't lived to suffer the consequences of his actions. This guy survives and I hope it haunts him for the rest of his life.", 'name': 't1_l15q4ok'}, {'author': 'sie-waitforit-ghart', 'unix_time': 1714016292.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15g37r/', 'score': 9, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Hi hi, add on as I see the upvotes. \n\nDespite wishing the worst for such perpetrators as a very specific standalone case, it also show case the challenges of reintegration of ex-convicts back into society. I hope us as individuals be more welcoming and give them the 2nd chance each person deserves. Support the yellow ribbon project and help them break the cycle.', 'name': 't1_l15g37r'}, {'author': 'etulf', 'unix_time': 1714015316.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15e13p/', 'score': 34, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'makes you wonder about his interpersonal relationships, huh.', 'name': 't1_l15e13p'}, {'author': 'DarkCartier43', 'unix_time': 1714018410.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15kbqb/', 'score': 11, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'his colleagues? where and what?', 'name': 't1_l15kbqb'}, {'author': 'Scorchster1138', 'unix_time': 1714015480.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15edqc/', 'score': 6, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Haha. I don’t know. I’m not well versed in the ways of the EDMW', 'name': 't1_l15edqc'}, {'author': 'Purpledragon84', 'unix_time': 1714027363.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15zca0/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Apparently another guy posted say volvo after bought by geely then standard drop.', 'name': 't1_l15zca0'}, {'author': 'ailes_d', 'unix_time': 1714015890.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15f8nm/', 'score': -1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Then ruin lives of more people again…', 'name': 't1_l15f8nm'}, {'author': 'la_gusa', 'unix_time': 1714017870.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15j9yd/', 'score': 0, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Well, nothing you can do unless you want to make the vehicle like much wider and dors much heavier', 'name': 't1_l15j9yd'}, {'author': 'SuzeeWu', 'unix_time': 1714032923.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l1673dz/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Hello, I'm not sure if I understand you at all.", 'name': 't1_l1673dz'}, {'author': 'trashtrottingtrout', 'unix_time': 1714023482.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15tbj3/', 'score': 10, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "And that can be done without doxxing, which somehow, the people replying to the mod don't seem to comprehend.", 'name': 't1_l15tbj3'}, {'author': 'Sweaty-Run-2881', 'unix_time': 1714032828.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l166z1x/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Agreed. Imagine a guy waving a knife around at everyone  and walking through in a public venue, and saying that he meant no one harm. Does it now become the fault of the innocent who fail to avoid his swinging knife and getting hurt? It should not be such. \n\nSimilarly, this maniac is driving above the speed limits and side swiping other vehicles while fast approaching a traffic light. Singapore is no autobahn so he should be fully responsible for the deaths and injuries he caused since it is done in a fully conscious state of mind.', 'name': 't1_l166z1x'}, {'author': 'Purpledragon84', 'unix_time': 1714018736.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15kxwo/', 'score': -10, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'My question is. How would u know? Maybe he got scammed 10million and wanted to kill someone by driving like that. \n\nBut to ur point it is ultimately what can be proven in court. The only logical reason imo that he is charged with manslaughter is because thats only what we can prove in court.', 'name': 't1_l15kxwo'}, {'author': 'livinglifeingrieve', 'unix_time': 1714016528.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15gl1g/', 'score': -28, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Fk your technicality', 'name': 't1_l15gl1g'}, {'author': 'AlexHollows', 'unix_time': 1714018764.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15kztc/', 'score': 11, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'That’s not a good analogy. You taking a rifle out and firing it in a busy street already implies that you had knowledge of and intent to cause bodily harm. Why else would you take a rifle out and fire it in a busy street?\n\nI think a better analogy would be if say you are hired to shoot and kill crows at a neighbourhood.\n\nYou then decide that it would be more effective for you to randomly spam an entire magazine at a tree. So happens that in doing so you accidentally shot and killed 2 random onlookers at the block behind the tree.\n\nWhat would you then say is a valid sentence. You didn’t have an intention to kill, but you did take the actions that lead to the elevated risk and resulted in the deaths.', 'name': 't1_l15kztc'}, {'author': 'Zestyclose-Swing-345', 'unix_time': 1714019994.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15naah/', 'score': 6, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'other comments have covered good points too but a serious logical flaw in your analogy is that the preparation to commit the crime (getting a rifle, putting on a blindfold — which means you considered this law) already prove you intend to commit it. read my definition of intention above, but in short it’s logical that it’s a virtual certainty you’ll kill someone on a busy street with a rifle compared to driving your car in a busy intersection.', 'name': 't1_l15naah'}, {'author': 'TapComprehensive6735', 'unix_time': 1714020681.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15oit4/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Extremely flawed statement. You take rifle knowing well it\'s loaded with bullet and you blindfold yourself and shoot anyhow. OBVIOUSLY your intention is to kill someone.\n\nThe driver speed with the mercs, was he driving recklessly with intention? Yes without a doubt, but did he tell himself "okay i today drive and i hopefully can hit someone and they must die" ? No obviously. \n\n  \nBefore emerging with statements like this just because your angry, look at how you present it LOL', 'name': 't1_l15oit4'}, {'author': 'Fensirulfr', 'unix_time': 1714019590.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15mjky/', 'score': -1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Very poor analogy. The Arms Offences Act already covers the scenario you described, and any attempt to unlawfully use firearms is already punishable by death.You do not need to have actually fired a shot to be charged.', 'name': 't1_l15mjky'}, {'author': 'Lukaku1sttouch', 'unix_time': 1714022462.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15rn4j/', 'score': 0, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'I’m all for analogies cause it brings a different perspective.\n\nBut sorry, it’s a fucking stupid analogy here.', 'name': 't1_l15rn4j'}, {'author': 'davechua', 'unix_time': 1714017333.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15i7m4/', 'score': 11, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Or he dropped his phone on the accelerator or some other lame excuse.', 'name': 't1_l15i7m4'}, {'author': 'Bcpjw', 'unix_time': 1714021784.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15qh1s/', 'score': 12, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Scary thought is if he didn’t crashed his car, would he have stopped or continue speeding creating havoc?\n\nIf only he could grasp the fact that he was wrong and the punishments will never be enough in 2 lifetimes. \n\nJust bite the bullet and cooperate, then maybe there’s a glimpse of hope.', 'name': 't1_l15qh1s'}, {'author': 'Pilotboi', 'unix_time': 1714031941.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l165smj/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'yes, pity the girl who have had all the life dreams and ambitions all gone in a second and the lady who just happened to have celebrated her first and last hari raya in her new home.\n\n  \nThis dirty motherfucker still have the attitude like no his fault and they deserve it.\n\nMuhammad Syafie Ismail, you are a babi fucker in the society', 'name': 't1_l165smj'}, {'author': 'Descartes350', 'unix_time': 1714022393.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15rivp/', 'score': 62, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Mob justice is bad. When the dashcam footage from the other car was posted, people thought the poster was the driver, and refused to believe otherwise. \n\nAfter proper investigation, it was found that the poster was the mechanic.\n\nBy then, the mechanic already received death threats and had his personal information leaked.\n\nSimilarly, people of a certain community believed that the Saab driver’s identity was withheld because he’s a Chinese who killed two Malays. As we now know, the driver is a Malay.\n\nMob justice is bad because there isn’t a proper process for fact finding. They jump to conclusions and take action without verifying information.', 'name': 't1_l15rivp'}, {'author': '[deleted]', 'unix_time': 1714021683.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15qand/', 'score': 0, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': '[removed]', 'name': 't1_l15qand'}, {'author': 'AnAnnoyedSpectator', 'unix_time': 1714026283.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15xqd9/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Yah, that all makes sense. But people are going to be mad at you for not blindly hating everything coming out of his mouth like they are at this point.', 'name': 't1_l15xqd9'}, {'author': 'seazboy', 'unix_time': 1714022849.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15sae1/', 'score': 19, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "A friend who was onsite in the one of the emergency units . According to other comments too, which mentioned he only had a broken arm. There's an article on zaobao showing him with close to no injuries seen in the picture looking into his own car", 'name': 't1_l15sae1'}, {'author': 'seazboy', 'unix_time': 1714024538.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15v0l5/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': '[https://www.reddit.com/r/singapore/comments/1ccgms0/comment/l15sae1/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/singapore/comments/1ccgms0/comment/l15sae1/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)', 'name': 't1_l15v0l5'}, {'author': 't_25_t', 'unix_time': 1714024404.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15usug/', 'score': 13, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': '> Reminds me of Saab\'s slogan in the 90s, "A Saab will surrender its own life to save yours". Lowkey impressed they delivered.\n\nThe Scandanavians are obsessed with safety. Look at their competitor Volvo. Not as much these days after Geely took over.', 'name': 't1_l15usug'}, {'author': 'KoishiChan92', 'unix_time': 1714022973.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15shq2/', 'score': 5, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Oh damn the company went bankrupt. This incident seems to be a testament to it's build safety.", 'name': 't1_l15shq2'}, {'author': 't3rmina1', 'unix_time': 1714016505.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15gjc2/', 'score': 6, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "A Saab will surrender your victims' lives to save yours", 'name': 't1_l15gjc2'}, {'author': 't_25_t', 'unix_time': 1714024526.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15uzvy/', 'score': 4, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': '> Volvo invented the safety belt as well, IIRC.\n\nNot just the safety belt.\n\n\nRearward facing child set, side impact protection, anti whiplash, curtain air bags, abs and probably some others I forgot.\n\n\nTheir car bodies were also consistantly fared better in an accident.', 'name': 't1_l15uzvy'}, {'author': 'dukeshytalker', 'unix_time': 1714019828.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15mzbh/', 'score': 12, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Nice to support repentance and to give second chances. \n\nBut whos gonna give all the affected people second chances? People died, families shattered overnight. 20 years later people would still be living with the brokenness.\n\nI feell that there is a certain class of crimes that people should simply never attempt. I think that its a stretch right now before he has demonstrated any remorse or any attempts to make amends to simply say, "yellow ribbon, welcome back".\n\nSorry, a bit too woke for me. \n\nPales in comparison to the real yellow ribbon repentees who have demonstrated remorse and taken active steps to get their act together.', 'name': 't1_l15mzbh'}, {'author': 'x1243', 'unix_time': 1714028659.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l1618or/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'sad.. loved the old Volvo.. ugly but built like a tank..', 'name': 't1_l1618or'}, {'author': 'hychael2020', 'unix_time': 1714023627.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15tjto/', 'score': -1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Yeah true. I just hope that this will start to make others press for changes in road laws and the sentencing issue in general. Instead of doxxing(what can it do already? The guy will be at Changi soon enough), I'm imploring everyone angry to press your MPs in the future and make this a talking point during the elections this year", 'name': 't1_l15tjto'}, {'author': 'Zestyclose-Swing-345', 'unix_time': 1714019454.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15mafn/', 'score': 10, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'for your hypo specifically, prosecution would usually gather evidence relating to D’s state of mind at that time, which would definitely include bank statements and stuff. if 10million suddenly disappears of course you can establish intention.\n\nbut you’re right — law is based on the facts at hand. and also recall the threshold of proof. it is easy to prove beyond reasonable doubt that D was racing with the white merc, and that is a reckless act in a busy intersection — hence the charges imposed upon him. but it’s impossible to say that it was a virtual certainty that he’ll kill someone at an intersection, and its highly unlikely that he desired to kill someone here. issues of this desire becomes even more complicated when we consider that there isn’t a specific someone he set out to kill. \n\nso best is to get him on what can easily be proven. in this case, quite a hefty fine and moderately long imprisonment sentence for his 4 charges along with a side of license suspension.', 'name': 't1_l15mafn'}, {'author': 'Fensirulfr', 'unix_time': 1714019113.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15lndl/', 'score': 2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'He is not charged with manslaughter, nor "causing death due to negligience act", which are defined in the Penal Code. Instead, as this is a traffic incident, the charges are under the Road Traffic Act instead', 'name': 't1_l15lndl'}, {'author': 'TapComprehensive6735', 'unix_time': 1714020522.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15o8m8/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "And my question to you is who are you to judge what are his intention? He was recklessly driving period however you can't prove he has any direct intention to hurt anyone.", 'name': 't1_l15o8m8'}, {'author': 'Zestyclose-Swing-345', 'unix_time': 1714016873.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15haba/', 'score': 19, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'just saying the law as it is buddy. also i don’t think you know what the word technicality means.', 'name': 't1_l15haba'}, {'author': 'idiotnoobx', 'unix_time': 1714018389.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15ka8v/', 'score': 2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Not his, it’s the law', 'name': 't1_l15ka8v'}, {'author': 'Yeah_Right_Mister', 'unix_time': 1714019729.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15msnh/', 'score': 4, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "yeah except he drove at high speed through a red light at a busy intersection, not in some ulu area where a kid ran across the road. you'd think the red light is warning enough, let alone the cars driving past perpendicular to him.", 'name': 't1_l15msnh'}, {'author': 'Yeah_Right_Mister', 'unix_time': 1714019879.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15n2of/', 'score': 0, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "that's my entire point - that the penalties for dangerous driving is disproportionately low", 'name': 't1_l15n2of'}, {'author': '-wmloo-', 'unix_time': 1714023999.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15u5gz/', 'score': 8, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Had to agree with what you shared. Hope cooler heads will prevail & of course due punishments to be meted out.', 'name': 't1_l15u5gz'}, {'author': 'Noobcakes19', 'unix_time': 1714026510.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15y2o8/', 'score': 6, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "i agree as well. pardon my keyboard anger and can't do much.\n\nwhoever thinks it's a race matter is someone who's racist. \n\nthe driver's identity was withheld due to the doxxing shit law.", 'name': 't1_l15y2o8'}, {'author': 'meekiatahaihiam', 'unix_time': 1714026083.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15xf7w/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'This.', 'name': 't1_l15xf7w'}, {'author': 'Noobcakes19', 'unix_time': 1714022745.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15s45o/', 'score': 0, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Correction : post fact-finding / investigation mob justice.', 'name': 't1_l15s45o'}, {'author': 'Arcturion', 'unix_time': 1714017238.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15i0sb/', 'score': 55, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "I get your anger, but it's the user that's the asshole. The car is just a tool, and apparently a good one at that.", 'name': 't1_l15i0sb'}, {'author': 'thestudiomaster', 'unix_time': 1714031646.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l165e2j/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Saab was also renowned for pedestrian protection in an accident.', 'name': 't1_l165e2j'}, {'author': 'ShortKingsOnly69', 'unix_time': 1714022072.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15qz3o/', 'score': 0, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Good point. We should let cars decide whether to save its driver, or other road users.\xa0', 'name': 't1_l15qz3o'}, {'author': 'sie-waitforit-ghart', 'unix_time': 1714021249.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15pj5f/', 'score': 3, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Don't be sorry. It is not woke, it is just us as human being humans. I fully agree with what you said. \n\nIt really is an issue that both society and as individuals we need to take steps to tackley.", 'name': 't1_l15pj5f'}, {'author': 'Purpledragon84', 'unix_time': 1714021752.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15qezx/', 'score': 5, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "Agreed. Thats why courts in US sometimes change the chargesheet from 1st degree murder to 2nd degree because it's easier to nail the guy down based on the evidence that was available and basically how fast they wanna conclude it, despite how obvious it was to the layman that it was pre-meditated. Because if they fail on the 1st degree charge the guy gets scotfree altogether.\n\nWasnt disagreeing with u at the start, just putting out all frames/pov, in case some other folks say i was.", 'name': 't1_l15qezx'}, {'author': 'CSlv', 'unix_time': 1714019630.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15mm76/', 'score': 6, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'This is a serious loophole. Just because a vehicle is used, the Penal Code is suddenly not applicable.', 'name': 't1_l15mm76'}, {'author': 'Zestyclose-Swing-345', 'unix_time': 1714019661.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15mo90/', 'score': 4, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'but the charges include elements of recklessness, which means discussions of manslaughter are still kinda relevant.', 'name': 't1_l15mo90'}, {'author': 'Purpledragon84', 'unix_time': 1714021502.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15pzb6/', 'score': 2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Im not judging, im questioning. As the other redditor replied, nobody knows. Hence the court is there.', 'name': 't1_l15pzb6'}, {'author': 'AlexHollows', 'unix_time': 1714020973.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15p1nr/', 'score': -1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'The question then becomes “who is anyone then to prove his intention?” \n\nWhich becomes an unsolvable problem. Thats why we have the courts and the legal system to decipher it.', 'name': 't1_l15p1nr'}, {'author': 'komplete10', 'unix_time': 1714017200.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15hy1v/', 'score': 15, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Agree, this is the law. If prosecution tried to get him on murder, the defence has it easy and he walks home a free man.', 'name': 't1_l15hy1v'}, {'author': 'AlexHollows', 'unix_time': 1714020825.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15os5d/', 'score': 2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'The facts are true, indisputable. But the reality of the matter is what was the intent? \n\nWas he in the car speeding through the intersection thinking “yes I want to kill some people”? That’s the difference.', 'name': 't1_l15os5d'}, {'author': 'lurkinglurkerwholurk', 'unix_time': 1714020415.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15o1nx/', 'score': 2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Indeed you’re correct. Both simile situations should be considered the same: any person who gets into a car is automatically assumed he must be ready to intentionally ram someone later. \n\nHe must later be charged under that assumption.', 'name': 't1_l15o1nx'}, {'author': 'Descartes350', 'unix_time': 1714024451.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15uvjf/', 'score': 9, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Yeah. That being said, I think the punishments for driving-related offenses are too lenient in SG.\n\nFor instance, this guy gets a maximum of 8 years in jail for taking 2 lives and changing the lives of their families forever? That’s a gross injustice imo.\n\nSo while I advocate to follow the proper process, I think parts of the process (i.e. the penalties) suck and should be reviewed.', 'name': 't1_l15uvjf'}, {'author': 'x1243', 'unix_time': 1714025106.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15vweu/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'that sounds like a Stephen king novel', 'name': 't1_l15vweu'}, {'author': 'YtoZ', 'unix_time': 1714033301.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l167ld5/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'It’s definitely something we have to think about before self-driving cars and busses are implemented but that’s for another thread', 'name': 't1_l167ld5'}, {'author': 'dukeshytalker', 'unix_time': 1714021499.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15pz5b/', 'score': 2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'Cool - I feel you re a cool person, seldom on reddit people can disagree peacfully hahah', 'name': 't1_l15pz5b'}, {'author': 'neverspeakofme', 'unix_time': 1714025983.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15x9r2/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'No need to look to the US, very common in Singapore as well.', 'name': 't1_l15x9r2'}, {'author': 'Fensirulfr', 'unix_time': 1714022299.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15rd8m/', 'score': 2, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'No. As mentioned before, the Road Traffic Act already covers the aspect of recklessness. There is a specific section for "reckless or dangerous driving". Using the Penal Code instead would mean setting a new precedent and starting a legal debate.', 'name': 't1_l15rd8m'}, {'author': 'Zestyclose-Swing-345', 'unix_time': 1714018622.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15kq71/', 'score': 11, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'exactly. prosecution usually asks for max sentence for all charges too, in this case his 4 charges adds up to about 11.5 years according to the article. any decent defence fight a lil bit will get it down to 9-10. he’ll be 52 by the time he comes out with 0 job prospects and no life. imo that’s worse than being kept alive/healthy in prison and (in the case of people calling for the death penalty) not letting him get off “easy” at all. \n\nfrankly quite fed up with reddit legal and moral experts calling for his head when what he’ll face is what he truly deserves and what is fair and just.', 'name': 't1_l15kq71'}, {'author': '-wmloo-', 'unix_time': 1714024939.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15vn25/', 'score': 6, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'I agree that it may be too lenient for such cases, we should review to see if it’s adequate. Just glad that there’s proper process here to look at each cases objectively.', 'name': 't1_l15vn25'}, {'author': 'Visible-Beginning733', 'unix_time': 1714032657.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l166qxi/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': "I think you're spot on with both mob justice and the penalties. \nSingapore is pretty lenient on driving offences in general and in a case like this, an 8 year max  term doesn't seem at all fair or appropriate.\nI also believe that the law is way too soft on  people who leave the scene of an accident. \nPlus, not enough of the police cameras are actually recording. Many are just for show.\nI know two people who were knocked off their bikes in Singapore just last year and on both occasions the drivers didn't even stop.", 'name': 't1_l166qxi'}, {'author': 'Zestyclose-Swing-345', 'unix_time': 1714022824.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15s8xy/', 'score': 3, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'im aware. i literally said the charges include elements of recklessness. which means discussions of why its manslaughter and not murder is relevant. once we established its recklessness and not intent, then we move on to the correct way to prosecute, which is under the RTA. that’s what i mean by discussions of manslaughter. i never said we should charge him under manslaughter.', 'name': 't1_l15s8xy'}, {'author': 'neverspeakofme', 'unix_time': 1714026044.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l15xd4l/', 'score': 1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'You missed the point. The comment is talking about what the law SHOULD be, not what it is.', 'name': 't1_l15xd4l'}, {'author': 'Fensirulfr', 'unix_time': 1714028014.0, 'permalink': '/r/singapore/comments/1ccgms0/42yearold_man_handed_four_charges_over_tampines/l160aj0/', 'score': -1, 'subreddit': 'singapore', 'post_title': '42-year-old man handed four charges over Tampines accident, including dangerous driving causing death', 'body': 'I disagree.The above comment did not say how should the laws be amended, nor does it say why in principle why the Penal Code should take precedence when the Road Traffic Act is more specific.\n\nThe law and its previous application are pretty clear. The law for specifically reckless and dangerous driving exist. As for offences causes in the Penal Code, there are the categories, 1. culpable homicide amounting to murder, 2. culpable homicide not amounting to murder, and 3. causing death by\xa0rash\xa0or negligent\xa0act. Between 2 and 3, can it be shown that the accused lost the power of self-control by grave and sudden provocation?\n\nDo note that the penalties for causing death by\xa0rash\xa0or negligent\xa0act under the Penal Code is less than those for causing death by reckless driving under the Road Traffic Act.', 'name': 't1_l160aj0'}])
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:16.620 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_18/675/1832, processId:6817 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:16.622 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:16.622 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:16.623 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:16.623 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:16.626 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:16.627 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:16.627 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_18/675/1832
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:16.628 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_18/675/1832
[WI-675][TI-1832] - [INFO] 2024-04-25 16:23:16.629 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1832] - [INFO] 2024-04-25 16:23:17.498 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1832, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:23:19.986 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7865168539325842 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:23:25.097 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7681159420289856 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:23:26.165 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8031914893617021 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:23:27.167 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9515151515151514 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:23:31.251 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7485207100591716 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:23:34.284 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7450424929178471 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:23:35.295 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7360703812316716 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:23:36.309 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8607242339832869 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:23:37.311 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7304347826086957 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:23:38.314 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.788135593220339 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:23:39.317 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7002967359050445 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:23:52.607 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7791044776119403 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:24:33.773 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.835195530726257 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:24:34.785 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7865168539325842 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:24:35.791 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7988826815642458 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [WARN] 2024-04-25 16:25:28.773 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[147] - [id: 0x0557cfab, L:/172.18.1.1:1234 - R:/172.18.0.11:53034] is not writable, over high water level : 65536
[WI-0][TI-0] - [WARN] 2024-04-25 16:25:28.787 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[154] - [id: 0x0557cfab, L:/172.18.1.1:1234 - R:/172.18.0.11:53034] is writable, to low water : 32768
[WI-0][TI-0] - [INFO] 2024-04-25 16:27:43.801 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7368421052631579 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:27:48.863 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1833, taskName=Extract Reddit Data, firstSubmitTime=1714033668829, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13045665829504, processDefineVersion=19, appIds=null, processInstanceId=676, scheduleTime=0, globalParams=[{"prop":"subreddit","direct":"IN","type":"VARCHAR","value":"singapore"},{"prop":"limit","direct":"IN","type":"VARCHAR","value":"3"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import praw\nimport json\n\nclass RedditPostDataExtractor:\n    def __init__(self):\n        self.data = []\n\n    def extract_data(self, hot_posts):\n        if hot_posts:\n            for post in hot_posts:\n                post.comments.replace_more(limit=None)\n                # initialise a dictionary to contain the data of the post\n                post_data = {}\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the poster's name\n                if post.author is not None:\n                    post_data['author'] = post.author.name\n                else:\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the post was created\n                post_data['unix_time'] = post.created_utc\n                post_data['permalink'] = post.permalink\n                post_data['score'] = post.score\n                post_data['subreddit'] = post.subreddit.display_name\n                post_data['post_title'] = post.title\n                post_data['body'] = post.selftext\n\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\n                # t3 represents that it is a post\n                post_data['name'] = post.name\n\n                # store the posts data in the dictionary where the name is used as the key\n                self.data.append(post_data)\n\n                # extract the comments from the post\n                comments = post.comments.list()\n\n                # extract the data in the comments and store them inside the dictionary\n                self.extract_comments_data(comments)\n        \n        return self.data\n\n    def extract_comments_data(self, comments):\n        if comments:\n            for comment in comments:\n                # initialise a dictionary to contain the data of the comment\n                comment_data = {}\n\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the commenter name\n                if comment.author is not None:\n                    comment_data['author'] = comment.author.name\n                else:\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the comment was created\n                comment_data['unix_time'] = comment.created_utc\n                comment_data['permalink'] = comment.permalink\n                comment_data['score'] = comment.score\n                comment_data['subreddit'] = comment.subreddit.display_name\n                comment_data['post_title'] = comment.submission.title\n                comment_data['body'] = comment.body\n\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\n                # t1 represents that it is a post\n                comment_data['name'] = comment.name\n\n                # store the comment's data in the dictionary where the name is used as the key\n                self.data.append(comment_data)\n\n                # extract the comment's replies from the post\n                # replies = comment.replies\n\n                # extract the data in the comments and store them inside the dictionary\n                # self.extract_comments_data(replies)\n        \n        return\n    \ndef get_hot_posts(reddit_instance, subreddit_name, limit):\n    # Define the subreddit\n    subreddit = reddit_instance.subreddit(subreddit_name)\n\n    # Get hot posts from the subreddit\n    hot_posts = subreddit.hot(limit=limit)\n\n    # Filter out posts that are not pinned\n    filtered_posts = [post for post in hot_posts if not post.stickied]\n    return filtered_posts\n\n\n# initialize Reddit instance\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\n                     user_agent='Prawlingv1')\n\n\n# choose the subreddit to extract data from\nsubreddit = \"${subreddit}\"\nlimit = ${limit}\n\n# get hot posts from the subreddit\nhot_posts = get_hot_posts(reddit, subreddit, limit)\n\n# initialise a reddit data extractor object\ndata_extractor = RedditPostDataExtractor()\n\n# extract data from the hot posts from r/singapore\ndata = data_extractor.extract_data(hot_posts)\n\nwith open(\"/local_storage/reddit/in/${system.workflow.instance.id}-${system.datetime}.json\", \"w\") as fp:\n    json.dump(data, fp, indent=4)","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='Extract Reddit Data'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1833'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13045662268160'}, subreddit=Property{prop='subreddit', direct=IN, type=VARCHAR, value='singapore'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425162748'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='676'}, limit=Property{prop='limit', direct=IN, type=VARCHAR, value='3'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13045665829504'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.866 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: Extract Reddit Data to wait queue success
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.866 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.870 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.870 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.870 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.871 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714033668871
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.871 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 676_1833
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.871 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1833,
  "taskName" : "Extract Reddit Data",
  "firstSubmitTime" : 1714033668829,
  "startTime" : 1714033668871,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13045665829504/19/676/1833.log",
  "processId" : 0,
  "processDefineCode" : 13045665829504,
  "processDefineVersion" : 19,
  "processInstanceId" : 676,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"subreddit\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"singapore\"},{\"prop\":\"limit\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"3\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import praw\\nimport json\\n\\nclass RedditPostDataExtractor:\\n    def __init__(self):\\n        self.data = []\\n\\n    def extract_data(self, hot_posts):\\n        if hot_posts:\\n            for post in hot_posts:\\n                post.comments.replace_more(limit=None)\\n                # initialise a dictionary to contain the data of the post\\n                post_data = {}\\n\\n                # get the relevant post's data and store it inside post_data\\n\\n                # get the poster's name\\n                if post.author is not None:\\n                    post_data['author'] = post.author.name\\n                else:\\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\\n\\n                # get the unix time in which the post was created\\n                post_data['unix_time'] = post.created_utc\\n                post_data['permalink'] = post.permalink\\n                post_data['score'] = post.score\\n                post_data['subreddit'] = post.subreddit.display_name\\n                post_data['post_title'] = post.title\\n                post_data['body'] = post.selftext\\n\\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\\n                # t3 represents that it is a post\\n                post_data['name'] = post.name\\n\\n                # store the posts data in the dictionary where the name is used as the key\\n                self.data.append(post_data)\\n\\n                # extract the comments from the post\\n                comments = post.comments.list()\\n\\n                # extract the data in the comments and store them inside the dictionary\\n                self.extract_comments_data(comments)\\n        \\n        return self.data\\n\\n    def extract_comments_data(self, comments):\\n        if comments:\\n            for comment in comments:\\n                # initialise a dictionary to contain the data of the comment\\n                comment_data = {}\\n\\n\\n                # get the relevant post's data and store it inside post_data\\n\\n                # get the commenter name\\n                if comment.author is not None:\\n                    comment_data['author'] = comment.author.name\\n                else:\\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\\n\\n                # get the unix time in which the comment was created\\n                comment_data['unix_time'] = comment.created_utc\\n                comment_data['permalink'] = comment.permalink\\n                comment_data['score'] = comment.score\\n                comment_data['subreddit'] = comment.subreddit.display_name\\n                comment_data['post_title'] = comment.submission.title\\n                comment_data['body'] = comment.body\\n\\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\\n                # t1 represents that it is a post\\n                comment_data['name'] = comment.name\\n\\n                # store the comment's data in the dictionary where the name is used as the key\\n                self.data.append(comment_data)\\n\\n                # extract the comment's replies from the post\\n                # replies = comment.replies\\n\\n                # extract the data in the comments and store them inside the dictionary\\n                # self.extract_comments_data(replies)\\n        \\n        return\\n    \\ndef get_hot_posts(reddit_instance, subreddit_name, limit):\\n    # Define the subreddit\\n    subreddit = reddit_instance.subreddit(subreddit_name)\\n\\n    # Get hot posts from the subreddit\\n    hot_posts = subreddit.hot(limit=limit)\\n\\n    # Filter out posts that are not pinned\\n    filtered_posts = [post for post in hot_posts if not post.stickied]\\n    return filtered_posts\\n\\n\\n# initialize Reddit instance\\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\\n                     user_agent='Prawlingv1')\\n\\n\\n# choose the subreddit to extract data from\\nsubreddit = \\\"${subreddit}\\\"\\nlimit = ${limit}\\n\\n# get hot posts from the subreddit\\nhot_posts = get_hot_posts(reddit, subreddit, limit)\\n\\n# initialise a reddit data extractor object\\ndata_extractor = RedditPostDataExtractor()\\n\\n# extract data from the hot posts from r/singapore\\ndata = data_extractor.extract_data(hot_posts)\\n\\nwith open(\\\"/local_storage/reddit/in/${system.workflow.instance.id}-${system.datetime}.json\\\", \\\"w\\\") as fp:\\n    json.dump(data, fp, indent=4)\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "Extract Reddit Data"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1833"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13045662268160"
    },
    "subreddit" : {
      "prop" : "subreddit",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "singapore"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425162748"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "676"
    },
    "limit" : {
      "prop" : "limit",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13045665829504"
    }
  },
  "taskAppId" : "676_1833",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.872 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.872 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.872 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.877 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.877 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.877 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_19/676/1833 check successfully
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.878 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.878 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.878 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.878 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.880 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import praw\nimport json\n\nclass RedditPostDataExtractor:\n    def __init__(self):\n        self.data = []\n\n    def extract_data(self, hot_posts):\n        if hot_posts:\n            for post in hot_posts:\n                post.comments.replace_more(limit=None)\n                # initialise a dictionary to contain the data of the post\n                post_data = {}\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the poster's name\n                if post.author is not None:\n                    post_data['author'] = post.author.name\n                else:\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the post was created\n                post_data['unix_time'] = post.created_utc\n                post_data['permalink'] = post.permalink\n                post_data['score'] = post.score\n                post_data['subreddit'] = post.subreddit.display_name\n                post_data['post_title'] = post.title\n                post_data['body'] = post.selftext\n\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\n                # t3 represents that it is a post\n                post_data['name'] = post.name\n\n                # store the posts data in the dictionary where the name is used as the key\n                self.data.append(post_data)\n\n                # extract the comments from the post\n                comments = post.comments.list()\n\n                # extract the data in the comments and store them inside the dictionary\n                self.extract_comments_data(comments)\n        \n        return self.data\n\n    def extract_comments_data(self, comments):\n        if comments:\n            for comment in comments:\n                # initialise a dictionary to contain the data of the comment\n                comment_data = {}\n\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the commenter name\n                if comment.author is not None:\n                    comment_data['author'] = comment.author.name\n                else:\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the comment was created\n                comment_data['unix_time'] = comment.created_utc\n                comment_data['permalink'] = comment.permalink\n                comment_data['score'] = comment.score\n                comment_data['subreddit'] = comment.subreddit.display_name\n                comment_data['post_title'] = comment.submission.title\n                comment_data['body'] = comment.body\n\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\n                # t1 represents that it is a post\n                comment_data['name'] = comment.name\n\n                # store the comment's data in the dictionary where the name is used as the key\n                self.data.append(comment_data)\n\n                # extract the comment's replies from the post\n                # replies = comment.replies\n\n                # extract the data in the comments and store them inside the dictionary\n                # self.extract_comments_data(replies)\n        \n        return\n    \ndef get_hot_posts(reddit_instance, subreddit_name, limit):\n    # Define the subreddit\n    subreddit = reddit_instance.subreddit(subreddit_name)\n\n    # Get hot posts from the subreddit\n    hot_posts = subreddit.hot(limit=limit)\n\n    # Filter out posts that are not pinned\n    filtered_posts = [post for post in hot_posts if not post.stickied]\n    return filtered_posts\n\n\n# initialize Reddit instance\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\n                     user_agent='Prawlingv1')\n\n\n# choose the subreddit to extract data from\nsubreddit = \"${subreddit}\"\nlimit = ${limit}\n\n# get hot posts from the subreddit\nhot_posts = get_hot_posts(reddit, subreddit, limit)\n\n# initialise a reddit data extractor object\ndata_extractor = RedditPostDataExtractor()\n\n# extract data from the hot posts from r/singapore\ndata = data_extractor.extract_data(hot_posts)\n\nwith open(\"/local_storage/reddit/in/${system.workflow.instance.id}-${system.datetime}.json\", \"w\") as fp:\n    json.dump(data, fp, indent=4)",
  "resourceList" : [ ]
}
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.880 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.880 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.880 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.880 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.880 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.880 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import praw
import json

class RedditPostDataExtractor:
    def __init__(self):
        self.data = []

    def extract_data(self, hot_posts):
        if hot_posts:
            for post in hot_posts:
                post.comments.replace_more(limit=None)
                # initialise a dictionary to contain the data of the post
                post_data = {}

                # get the relevant post's data and store it inside post_data

                # get the poster's name
                if post.author is not None:
                    post_data['author'] = post.author.name
                else:
                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the post was created
                post_data['unix_time'] = post.created_utc
                post_data['permalink'] = post.permalink
                post_data['score'] = post.score
                post_data['subreddit'] = post.subreddit.display_name
                post_data['post_title'] = post.title
                post_data['body'] = post.selftext

                # get the unique name of the post (name refers to the id of the post prefixed with t3_)
                # t3 represents that it is a post
                post_data['name'] = post.name

                # store the posts data in the dictionary where the name is used as the key
                self.data.append(post_data)

                # extract the comments from the post
                comments = post.comments.list()

                # extract the data in the comments and store them inside the dictionary
                self.extract_comments_data(comments)
        
        return self.data

    def extract_comments_data(self, comments):
        if comments:
            for comment in comments:
                # initialise a dictionary to contain the data of the comment
                comment_data = {}


                # get the relevant post's data and store it inside post_data

                # get the commenter name
                if comment.author is not None:
                    comment_data['author'] = comment.author.name
                else:
                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the comment was created
                comment_data['unix_time'] = comment.created_utc
                comment_data['permalink'] = comment.permalink
                comment_data['score'] = comment.score
                comment_data['subreddit'] = comment.subreddit.display_name
                comment_data['post_title'] = comment.submission.title
                comment_data['body'] = comment.body

                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)
                # t1 represents that it is a post
                comment_data['name'] = comment.name

                # store the comment's data in the dictionary where the name is used as the key
                self.data.append(comment_data)

                # extract the comment's replies from the post
                # replies = comment.replies

                # extract the data in the comments and store them inside the dictionary
                # self.extract_comments_data(replies)
        
        return
    
def get_hot_posts(reddit_instance, subreddit_name, limit):
    # Define the subreddit
    subreddit = reddit_instance.subreddit(subreddit_name)

    # Get hot posts from the subreddit
    hot_posts = subreddit.hot(limit=limit)

    # Filter out posts that are not pinned
    filtered_posts = [post for post in hot_posts if not post.stickied]
    return filtered_posts


# initialize Reddit instance
reddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',
                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',
                     user_agent='Prawlingv1')


# choose the subreddit to extract data from
subreddit = "${subreddit}"
limit = ${limit}

# get hot posts from the subreddit
hot_posts = get_hot_posts(reddit, subreddit, limit)

# initialise a reddit data extractor object
data_extractor = RedditPostDataExtractor()

# extract data from the hot posts from r/singapore
data = data_extractor.extract_data(hot_posts)

with open("/local_storage/reddit/in/${system.workflow.instance.id}-${system.datetime}.json", "w") as fp:
    json.dump(data, fp, indent=4)
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.881 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_19/676/1833
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.881 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_19/676/1833/py_676_1833.py
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.882 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import praw
import json

class RedditPostDataExtractor:
    def __init__(self):
        self.data = []

    def extract_data(self, hot_posts):
        if hot_posts:
            for post in hot_posts:
                post.comments.replace_more(limit=None)
                # initialise a dictionary to contain the data of the post
                post_data = {}

                # get the relevant post's data and store it inside post_data

                # get the poster's name
                if post.author is not None:
                    post_data['author'] = post.author.name
                else:
                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the post was created
                post_data['unix_time'] = post.created_utc
                post_data['permalink'] = post.permalink
                post_data['score'] = post.score
                post_data['subreddit'] = post.subreddit.display_name
                post_data['post_title'] = post.title
                post_data['body'] = post.selftext

                # get the unique name of the post (name refers to the id of the post prefixed with t3_)
                # t3 represents that it is a post
                post_data['name'] = post.name

                # store the posts data in the dictionary where the name is used as the key
                self.data.append(post_data)

                # extract the comments from the post
                comments = post.comments.list()

                # extract the data in the comments and store them inside the dictionary
                self.extract_comments_data(comments)
        
        return self.data

    def extract_comments_data(self, comments):
        if comments:
            for comment in comments:
                # initialise a dictionary to contain the data of the comment
                comment_data = {}


                # get the relevant post's data and store it inside post_data

                # get the commenter name
                if comment.author is not None:
                    comment_data['author'] = comment.author.name
                else:
                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the comment was created
                comment_data['unix_time'] = comment.created_utc
                comment_data['permalink'] = comment.permalink
                comment_data['score'] = comment.score
                comment_data['subreddit'] = comment.subreddit.display_name
                comment_data['post_title'] = comment.submission.title
                comment_data['body'] = comment.body

                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)
                # t1 represents that it is a post
                comment_data['name'] = comment.name

                # store the comment's data in the dictionary where the name is used as the key
                self.data.append(comment_data)

                # extract the comment's replies from the post
                # replies = comment.replies

                # extract the data in the comments and store them inside the dictionary
                # self.extract_comments_data(replies)
        
        return
    
def get_hot_posts(reddit_instance, subreddit_name, limit):
    # Define the subreddit
    subreddit = reddit_instance.subreddit(subreddit_name)

    # Get hot posts from the subreddit
    hot_posts = subreddit.hot(limit=limit)

    # Filter out posts that are not pinned
    filtered_posts = [post for post in hot_posts if not post.stickied]
    return filtered_posts


# initialize Reddit instance
reddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',
                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',
                     user_agent='Prawlingv1')


# choose the subreddit to extract data from
subreddit = "singapore"
limit = 3

# get hot posts from the subreddit
hot_posts = get_hot_posts(reddit, subreddit, limit)

# initialise a reddit data extractor object
data_extractor = RedditPostDataExtractor()

# extract data from the hot posts from r/singapore
data = data_extractor.extract_data(hot_posts)

with open("/local_storage/reddit/in/676-20240425162748.json", "w") as fp:
    json.dump(data, fp, indent=4)
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.882 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.882 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.882 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_19/676/1833/py_676_1833.py
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.882 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.883 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_19/676/1833/676_1833.sh
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:48.916 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6881
[WI-0][TI-0] - [INFO] 2024-04-25 16:27:48.917 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1833] - [INFO] 2024-04-25 16:27:49.704 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1833, processInstanceId=676, startTime=1714033668871, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13045665829504/19/676/1833.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1833] - [INFO] 2024-04-25 16:27:49.708 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1833, processInstanceId=676, startTime=1714033668871, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13045665829504/19/676/1833.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714033669703)
[WI-0][TI-1833] - [INFO] 2024-04-25 16:27:49.708 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1833, processInstanceId=676, startTime=1714033668871, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1833] - [INFO] 2024-04-25 16:27:49.711 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1833, processInstanceId=676, startTime=1714033668871, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714033669703)
[WI-0][TI-1833] - [INFO] 2024-04-25 16:27:49.714 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1833, success=true)
[WI-0][TI-1833] - [INFO] 2024-04-25 16:27:49.738 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1833)
[WI-0][TI-1833] - [INFO] 2024-04-25 16:27:49.767 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1833, success=true)
[WI-0][TI-1833] - [INFO] 2024-04-25 16:27:49.782 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1833)
[WI-0][TI-0] - [INFO] 2024-04-25 16:27:52.865 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7591115097724482 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:52.965 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_19/676/1833, processId:6881 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:52.966 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:52.967 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:52.968 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:52.968 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:52.972 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:52.972 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:52.972 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_19/676/1833
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:52.974 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_19/676/1833
[WI-676][TI-1833] - [INFO] 2024-04-25 16:27:52.975 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1833] - [INFO] 2024-04-25 16:27:53.719 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1833, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:03.928 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1834, taskName=search for intake JSON files, firstSubmitTime=1714033683906, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=114, appIds=null, processInstanceId=677, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1834'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340113777664'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425162803'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='677'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.942 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake JSON files to wait queue success
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.944 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.974 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.975 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.975 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.975 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714033683975
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.975 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 677_1834
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.975 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1834,
  "taskName" : "search for intake JSON files",
  "firstSubmitTime" : 1714033683906,
  "startTime" : 1714033683975,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13201021801792/114/677/1834.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 114,
  "processInstanceId" : 677,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set raw_file_dir to null\\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1834"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340113777664"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425162803"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "677"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "677_1834",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.977 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.978 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.978 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.987 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.988 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.990 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/677/1834 check successfully
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.990 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.991 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.991 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.992 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.992 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"",
  "resourceList" : [ ]
}
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.992 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.992 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.993 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.993 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.993 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.994 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.994 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.994 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/reddit/in -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set raw_file_dir to null
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
fi

echo "#{setValue(raw_file_dir=${raw_file_dir})}"
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.994 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:03.995 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/677/1834/677_1834.sh
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:04.008 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6899
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:04.009 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1834] - [INFO] 2024-04-25 16:28:04.727 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1834, success=true)
[WI-0][TI-1834] - [INFO] 2024-04-25 16:28:04.754 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1834)
[WI-0][TI-1834] - [INFO] 2024-04-25 16:28:04.755 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1834, processInstanceId=677, startTime=1714033683975, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1834] - [INFO] 2024-04-25 16:28:04.763 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1834, processInstanceId=677, startTime=1714033683975, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714033684755)
[WI-0][TI-1834] - [WARN] 2024-04-25 16:28:04.763 +0800 o.a.d.s.w.m.MessageRetryRunner:[139] - Retry send message to master error
java.util.ConcurrentModificationException: null
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:911)
	at java.util.ArrayList$Itr.next(ArrayList.java:861)
	at org.apache.dolphinscheduler.server.worker.message.MessageRetryRunner.run(MessageRetryRunner.java:127)
[WI-0][TI-1834] - [INFO] 2024-04-25 16:28:04.772 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1834)
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:05.019 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	#{setValue(raw_file_dir=/local_storage/reddit/in/676-20240425162748.json)}
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:05.020 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/677/1834, processId:6899 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:05.021 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:05.021 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:05.021 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:05.022 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:05.025 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:05.026 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:05.026 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/677/1834
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:05.027 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/677/1834
[WI-677][TI-1834] - [INFO] 2024-04-25 16:28:05.028 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1834] - [INFO] 2024-04-25 16:28:05.720 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1834, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:06.781 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1836, taskName=move to processing, firstSubmitTime=1714033686775, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=114, appIds=null, processInstanceId=677, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to processing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1836'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340390094208'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425162806'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='677'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/in/676-20240425162748.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/in/676-20240425162748.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.783 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to processing to wait queue success
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.783 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.785 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.785 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.785 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.785 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714033686785
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.785 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 677_1836
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.786 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1836,
  "taskName" : "move to processing",
  "firstSubmitTime" : 1714033686775,
  "startTime" : 1714033686785,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13201021801792/114/677/1836.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 114,
  "processInstanceId" : 677,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the reddit processing folder\\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\\nfilename=$(basename \\\"${raw_file_dir}\\\")\\nif ! grep -q \\\"${REDDIT_HOME}/processing\\\" <<< \\\"${raw_file_dir}\\\"; then\\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\\nelse\\n    echo JSON is already in processing\\nfi\\n\\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\\necho \\\"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to processing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1836"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340390094208"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425162806"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "677"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/in/676-20240425162748.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "677_1836",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/in/676-20240425162748.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.787 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.787 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.787 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.789 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.789 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.790 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/677/1836 check successfully
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.790 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.790 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.790 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.796 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.796 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"",
  "resourceList" : [ ]
}
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.796 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.797 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/in/676-20240425162748.json"}] successfully
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.797 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.798 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.798 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.799 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.799 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.799 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the reddit processing folder
# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error
filename=$(basename "/local_storage/reddit/in/676-20240425162748.json")
if ! grep -q "/local_storage/reddit/processing" <<< "/local_storage/reddit/in/676-20240425162748.json"; then
    mv /local_storage/reddit/in/676-20240425162748.json /local_storage/reddit/processing
else
    echo JSON is already in processing
fi

# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing
echo "#{setValue(raw_file_dir=/local_storage/reddit/processing/${filename})}"
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.799 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.799 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/677/1836/677_1836.sh
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:06.802 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6912
[WI-0][TI-1836] - [INFO] 2024-04-25 16:28:07.729 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1836, success=true)
[WI-0][TI-1836] - [INFO] 2024-04-25 16:28:07.734 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1836)
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:07.804 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	#{setValue(raw_file_dir=/local_storage/reddit/processing/676-20240425162748.json)}
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:07.807 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/677/1836, processId:6912 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:07.808 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:07.808 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:07.808 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:07.808 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:07.811 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:07.811 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:07.811 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/677/1836
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:07.811 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/677/1836
[WI-677][TI-1836] - [INFO] 2024-04-25 16:28:07.811 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1836] - [INFO] 2024-04-25 16:28:08.723 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1836, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:08.814 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1837, taskName=preprocessing and kafka, firstSubmitTime=1714033688807, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=114, appIds=null, processInstanceId=677, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_reddit_preprocessing.py\n\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='preprocessing and kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1837'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13210058002752'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425162808'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='677'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/676-20240425162748.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.815 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: preprocessing and kafka to wait queue success
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.815 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.817 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.817 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.817 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.817 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714033688817
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.817 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 677_1837
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.818 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1837,
  "taskName" : "preprocessing and kafka",
  "firstSubmitTime" : 1714033688807,
  "startTime" : 1714033688817,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13201021801792/114/677/1837.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 114,
  "processInstanceId" : 677,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name reddit_preprocessing \\\\\\n    --files ${raw_file_dir} \\\\\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\\\\n    --conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n    spark_reddit_preprocessing.py\\n\\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\\\\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocessing and kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1837"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13210058002752"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425162808"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "677"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/676-20240425162748.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "677_1837",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/676-20240425162748.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.818 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.818 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.818 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.823 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.824 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.826 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/677/1837 check successfully
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.827 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.884 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py, resourceRelativePath=spark_reddit_preprocessing.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/677/1837/spark_reddit_preprocessing.py)})
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.886 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.886 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.890 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_reddit_preprocessing.py\n\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py",
    "res" : null
  } ]
}
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.891 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.891 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}] successfully
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.892 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.892 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.892 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.895 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.896 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.896 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master local \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name reddit_preprocessing \
    --files /local_storage/reddit/processing/676-20240425162748.json \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \
    --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
    spark_reddit_preprocessing.py

#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.896 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.896 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/677/1837/677_1837.sh
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:08.918 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6925
[WI-0][TI-1837] - [INFO] 2024-04-25 16:28:09.737 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1837, success=true)
[WI-0][TI-1837] - [INFO] 2024-04-25 16:28:09.745 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1837)
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:09.937 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:09.963 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8333333333333334 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1792] - [INFO] 2024-04-25 16:28:10.771 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1792, processInstanceId=661, status=9, startTime=1714031949850, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/661/1792.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792, endTime=1714032332757, processId=4479, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714033390484)
[WI-0][TI-1792] - [INFO] 2024-04-25 16:28:10.776 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1792, processInstanceId=661, status=9, startTime=1714031949850, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/661/1792.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792, endTime=1714032332757, processId=4479, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714033690771)
[WI-0][TI-1790] - [INFO] 2024-04-25 16:28:10.777 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1790, processInstanceId=660, status=9, startTime=1714031469392, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1790.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790, endTime=1714032269172, processId=4321, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714033390484)
[WI-0][TI-1790] - [INFO] 2024-04-25 16:28:10.786 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1790, processInstanceId=660, status=9, startTime=1714031469392, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1790.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790, endTime=1714032269172, processId=4321, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714033690771)
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:11.075 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8571428571428571 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:11.970 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-f300938d-2750-49b0-b681-6c3ea6405e59;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:12.079 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8136836997499161 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:12.971 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 696ms :: artifacts dl 38ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-f300938d-2750-49b0-b681-6c3ea6405e59
		confs: [default]
		0 artifacts copied, 12 already retrieved (0kB/25ms)
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:13.081 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8925373134328358 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:14.012 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:28:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:14.086 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7777777777777778 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:15.013 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:28:14 INFO SparkContext: Running Spark version 3.5.1
	24/04/25 16:28:14 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/04/25 16:28:14 INFO SparkContext: Java version 1.8.0_402
	24/04/25 16:28:14 INFO ResourceUtils: ==============================================================
	24/04/25 16:28:14 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/04/25 16:28:14 INFO ResourceUtils: ==============================================================
	24/04/25 16:28:14 INFO SparkContext: Submitted application: reddit_preprocessing
	24/04/25 16:28:14 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/04/25 16:28:14 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
	24/04/25 16:28:14 INFO ResourceProfileManager: Added ResourceProfile id: 0
	24/04/25 16:28:14 INFO SecurityManager: Changing view acls to: default
	24/04/25 16:28:14 INFO SecurityManager: Changing modify acls to: default
	24/04/25 16:28:14 INFO SecurityManager: Changing view acls groups to: 
	24/04/25 16:28:14 INFO SecurityManager: Changing modify acls groups to: 
	24/04/25 16:28:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
	24/04/25 16:28:14 INFO Utils: Successfully started service 'sparkDriver' on port 36319.
	24/04/25 16:28:14 INFO SparkEnv: Registering MapOutputTracker
	24/04/25 16:28:14 INFO SparkEnv: Registering BlockManagerMaster
	24/04/25 16:28:14 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/04/25 16:28:14 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/04/25 16:28:14 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/04/25 16:28:14 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-787132e4-56ea-4266-bf62-10a9ab290c3c
	24/04/25 16:28:14 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
	24/04/25 16:28:14 INFO SparkEnv: Registering OutputCommitCoordinator
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:16.014 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:28:15 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/04/25 16:28:15 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/04/25 16:28:15 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar at spark://15dedf891bed:36319/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1714033694181
	24/04/25 16:28:15 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar at spark://15dedf891bed:36319/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1714033694181
	24/04/25 16:28:15 INFO SparkContext: Added JAR file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar at spark://15dedf891bed:36319/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1714033694181
	24/04/25 16:28:15 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://15dedf891bed:36319/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714033694181
	24/04/25 16:28:15 INFO SparkContext: Added JAR file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://15dedf891bed:36319/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714033694181
	24/04/25 16:28:15 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://15dedf891bed:36319/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714033694181
	24/04/25 16:28:15 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://15dedf891bed:36319/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714033694181
	24/04/25 16:28:15 INFO SparkContext: Added JAR file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at spark://15dedf891bed:36319/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714033694181
	24/04/25 16:28:15 INFO SparkContext: Added JAR file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://15dedf891bed:36319/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714033694181
	24/04/25 16:28:15 INFO SparkContext: Added JAR file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://15dedf891bed:36319/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714033694181
	24/04/25 16:28:15 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://15dedf891bed:36319/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714033694181
	24/04/25 16:28:15 INFO SparkContext: Added JAR file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at spark://15dedf891bed:36319/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714033694181
	24/04/25 16:28:15 INFO SparkContext: Added file file:///local_storage/reddit/processing/676-20240425162748.json at file:///local_storage/reddit/processing/676-20240425162748.json with timestamp 1714033694181
	24/04/25 16:28:15 INFO Utils: Copying /local_storage/reddit/processing/676-20240425162748.json to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/676-20240425162748.json
	24/04/25 16:28:15 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1714033694181
	24/04/25 16:28:15 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar
	24/04/25 16:28:15 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1714033694181
	24/04/25 16:28:15 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar
	24/04/25 16:28:15 INFO SparkContext: Added file file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar at file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1714033694181
	24/04/25 16:28:15 INFO Utils: Copying /tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar
	24/04/25 16:28:15 INFO SparkContext: Added file file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714033694181
	24/04/25 16:28:15 INFO Utils: Copying /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/25 16:28:15 INFO SparkContext: Added file file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714033694181
	24/04/25 16:28:15 INFO Utils: Copying /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/25 16:28:15 INFO SparkContext: Added file file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714033694181
	24/04/25 16:28:15 INFO Utils: Copying /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/25 16:28:15 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714033694181
	24/04/25 16:28:15 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/25 16:28:15 INFO SparkContext: Added file file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714033694181
	24/04/25 16:28:15 INFO Utils: Copying /tmp/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.lz4_lz4-java-1.8.0.jar
	24/04/25 16:28:15 INFO SparkContext: Added file file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714033694181
	24/04/25 16:28:15 INFO Utils: Copying /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/25 16:28:15 INFO SparkContext: Added file file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714033694181
	24/04/25 16:28:15 INFO Utils: Copying /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.slf4j_slf4j-api-2.0.7.jar
	24/04/25 16:28:15 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714033694181
	24/04/25 16:28:15 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/25 16:28:15 INFO SparkContext: Added file file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714033694181
	24/04/25 16:28:15 INFO Utils: Copying /tmp/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/commons-logging_commons-logging-1.1.3.jar
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:16.113 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8542857142857143 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:17.018 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:28:16 INFO Executor: Starting executor ID driver on host 15dedf891bed
	24/04/25 16:28:16 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/04/25 16:28:16 INFO Executor: Java version 1.8.0_402
	24/04/25 16:28:16 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/04/25 16:28:16 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@5a142a20 for default.
	24/04/25 16:28:16 INFO Executor: Fetching file:///local_storage/reddit/processing/676-20240425162748.json with timestamp 1714033694181
	24/04/25 16:28:16 INFO Utils: /local_storage/reddit/processing/676-20240425162748.json has been previously copied to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/676-20240425162748.json
	24/04/25 16:28:16 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1714033694181
	24/04/25 16:28:16 INFO Utils: /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar has been previously copied to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar
	24/04/25 16:28:16 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714033694181
	24/04/25 16:28:16 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/25 16:28:16 INFO Executor: Fetching file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714033694181
	24/04/25 16:28:16 INFO Utils: /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/25 16:28:16 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1714033694181
	24/04/25 16:28:16 INFO Utils: /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar has been previously copied to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar
	24/04/25 16:28:16 INFO Executor: Fetching file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714033694181
	24/04/25 16:28:16 INFO Utils: /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/25 16:28:16 INFO Executor: Fetching file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714033694181
	24/04/25 16:28:16 INFO Utils: /tmp/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.lz4_lz4-java-1.8.0.jar
	24/04/25 16:28:16 INFO Executor: Fetching file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714033694181
	24/04/25 16:28:16 INFO Utils: /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/25 16:28:16 INFO Executor: Fetching file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714033694181
	24/04/25 16:28:16 INFO Utils: /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.slf4j_slf4j-api-2.0.7.jar
	24/04/25 16:28:16 INFO Executor: Fetching file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714033694181
	24/04/25 16:28:16 INFO Utils: /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/25 16:28:16 INFO Executor: Fetching file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714033694181
	24/04/25 16:28:16 INFO Utils: /tmp/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/commons-logging_commons-logging-1.1.3.jar
	24/04/25 16:28:16 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714033694181
	24/04/25 16:28:16 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/25 16:28:16 INFO Executor: Fetching file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1714033694181
	24/04/25 16:28:16 INFO Utils: /tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar has been previously copied to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar
	24/04/25 16:28:16 INFO Executor: Fetching spark://15dedf891bed:36319/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714033694181
	24/04/25 16:28:16 INFO TransportClientFactory: Successfully created connection to 15dedf891bed/172.18.1.1:36319 after 51 ms (0 ms spent in bootstraps)
	24/04/25 16:28:16 INFO Utils: Fetching spark://15dedf891bed:36319/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/fetchFileTemp3224159743529252733.tmp
	24/04/25 16:28:16 INFO Utils: /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/fetchFileTemp3224159743529252733.tmp has been previously copied to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/25 16:28:16 INFO Executor: Adding file:/tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
	24/04/25 16:28:16 INFO Executor: Fetching spark://15dedf891bed:36319/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714033694181
	24/04/25 16:28:16 INFO Utils: Fetching spark://15dedf891bed:36319/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/fetchFileTemp3870549914194222666.tmp
	24/04/25 16:28:16 INFO Utils: /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/fetchFileTemp3870549914194222666.tmp has been previously copied to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/25 16:28:16 INFO Executor: Adding file:/tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
	24/04/25 16:28:16 INFO Executor: Fetching spark://15dedf891bed:36319/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714033694181
	24/04/25 16:28:16 INFO Utils: Fetching spark://15dedf891bed:36319/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/fetchFileTemp429499119233277940.tmp
	24/04/25 16:28:16 INFO Utils: /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/fetchFileTemp429499119233277940.tmp has been previously copied to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/25 16:28:16 INFO Executor: Adding file:/tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
	24/04/25 16:28:16 INFO Executor: Fetching spark://15dedf891bed:36319/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714033694181
	24/04/25 16:28:16 INFO Utils: Fetching spark://15dedf891bed:36319/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/fetchFileTemp5351020522701281641.tmp
	24/04/25 16:28:16 INFO Utils: /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/fetchFileTemp5351020522701281641.tmp has been previously copied to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/25 16:28:16 INFO Executor: Adding file:/tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
	24/04/25 16:28:16 INFO Executor: Fetching spark://15dedf891bed:36319/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1714033694181
	24/04/25 16:28:16 INFO Utils: Fetching spark://15dedf891bed:36319/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/fetchFileTemp5920796217116667923.tmp
	24/04/25 16:28:16 INFO Utils: /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/fetchFileTemp5920796217116667923.tmp has been previously copied to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar
	24/04/25 16:28:16 INFO Executor: Adding file:/tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar to class loader default
	24/04/25 16:28:16 INFO Executor: Fetching spark://15dedf891bed:36319/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714033694181
	24/04/25 16:28:16 INFO Utils: Fetching spark://15dedf891bed:36319/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/fetchFileTemp4143248722473052797.tmp
	24/04/25 16:28:16 INFO Utils: /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/fetchFileTemp4143248722473052797.tmp has been previously copied to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.lz4_lz4-java-1.8.0.jar
	24/04/25 16:28:16 INFO Executor: Adding file:/tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.lz4_lz4-java-1.8.0.jar to class loader default
	24/04/25 16:28:16 INFO Executor: Fetching spark://15dedf891bed:36319/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714033694181
	24/04/25 16:28:16 INFO Utils: Fetching spark://15dedf891bed:36319/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/fetchFileTemp2479903447014787108.tmp
	24/04/25 16:28:16 INFO Utils: /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/fetchFileTemp2479903447014787108.tmp has been previously copied to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/commons-logging_commons-logging-1.1.3.jar
	24/04/25 16:28:16 INFO Executor: Adding file:/tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/commons-logging_commons-logging-1.1.3.jar to class loader default
	24/04/25 16:28:16 INFO Executor: Fetching spark://15dedf891bed:36319/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1714033694181
	24/04/25 16:28:16 INFO Utils: Fetching spark://15dedf891bed:36319/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/fetchFileTemp5597305974012949226.tmp
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:17.139 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7098591549295774 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:18.023 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:28:17 INFO Utils: /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/fetchFileTemp5597305974012949226.tmp has been previously copied to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar
	24/04/25 16:28:17 INFO Executor: Adding file:/tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar to class loader default
	24/04/25 16:28:17 INFO Executor: Fetching spark://15dedf891bed:36319/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714033694181
	24/04/25 16:28:17 INFO Utils: Fetching spark://15dedf891bed:36319/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/fetchFileTemp5874974531177613609.tmp
	24/04/25 16:28:17 INFO Utils: /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/fetchFileTemp5874974531177613609.tmp has been previously copied to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.slf4j_slf4j-api-2.0.7.jar
	24/04/25 16:28:17 INFO Executor: Adding file:/tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.slf4j_slf4j-api-2.0.7.jar to class loader default
	24/04/25 16:28:17 INFO Executor: Fetching spark://15dedf891bed:36319/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714033694181
	24/04/25 16:28:17 INFO Utils: Fetching spark://15dedf891bed:36319/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/fetchFileTemp170145558701005603.tmp
	24/04/25 16:28:17 INFO Utils: /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/fetchFileTemp170145558701005603.tmp has been previously copied to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/25 16:28:17 INFO Executor: Adding file:/tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
	24/04/25 16:28:17 INFO Executor: Fetching spark://15dedf891bed:36319/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714033694181
	24/04/25 16:28:17 INFO Utils: Fetching spark://15dedf891bed:36319/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/fetchFileTemp8256387140655673992.tmp
	24/04/25 16:28:17 INFO Utils: /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/fetchFileTemp8256387140655673992.tmp has been previously copied to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/25 16:28:17 INFO Executor: Adding file:/tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
	24/04/25 16:28:17 INFO Executor: Fetching spark://15dedf891bed:36319/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1714033694181
	24/04/25 16:28:17 INFO Utils: Fetching spark://15dedf891bed:36319/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/fetchFileTemp7965669196869015784.tmp
	24/04/25 16:28:17 INFO Utils: /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/fetchFileTemp7965669196869015784.tmp has been previously copied to /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar
	24/04/25 16:28:17 INFO Executor: Adding file:/tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar to class loader default
	24/04/25 16:28:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39583.
	24/04/25 16:28:17 INFO NettyBlockTransferService: Server created on 15dedf891bed:39583
	24/04/25 16:28:17 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/04/25 16:28:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 15dedf891bed, 39583, None)
	24/04/25 16:28:17 INFO BlockManagerMasterEndpoint: Registering block manager 15dedf891bed:39583 with 366.3 MiB RAM, BlockManagerId(driver, 15dedf891bed, 39583, None)
	24/04/25 16:28:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 15dedf891bed, 39583, None)
	24/04/25 16:28:17 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 15dedf891bed, 39583, None)
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:18.149 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.82 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:19.028 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	
	
	 /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/676-20240425162748.json 
	
	
	
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:19.172 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9138576779026217 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:20.052 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:28:19 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 345.7 KiB, free 366.0 MiB)
	24/04/25 16:28:19 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 365.9 MiB)
	24/04/25 16:28:19 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 15dedf891bed:39583 (size: 32.6 KiB, free: 366.3 MiB)
	24/04/25 16:28:19 INFO SparkContext: Created broadcast 0 from wholeTextFiles at NativeMethodAccessorImpl.java:0
	24/04/25 16:28:19 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/04/25 16:28:19 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/677/1837/spark-warehouse'.
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:20.229 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7055214723926381 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:21.233 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8663793103448276 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:22.235 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8208955223880596 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:23.237 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7723342939481268 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:24.239 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9086021505376345 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:26.094 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:28:25 INFO CodeGenerator: Code generated in 333.058051 ms
	24/04/25 16:28:25 INFO FileInputFormat: Total input files to process : 1
	24/04/25 16:28:25 INFO FileInputFormat: Total input files to process : 1
	24/04/25 16:28:25 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
	24/04/25 16:28:25 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/25 16:28:25 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
	24/04/25 16:28:25 INFO DAGScheduler: Parents of final stage: List()
	24/04/25 16:28:25 INFO DAGScheduler: Missing parents: List()
	24/04/25 16:28:25 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/25 16:28:26 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 28.6 KiB, free 365.9 MiB)
	24/04/25 16:28:26 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 365.9 MiB)
	24/04/25 16:28:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 15dedf891bed:39583 (size: 12.5 KiB, free: 366.3 MiB)
	24/04/25 16:28:26 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
	24/04/25 16:28:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/25 16:28:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:26.253 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8047337278106509 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:27.096 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:28:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (15dedf891bed, executor driver, partition 0, PROCESS_LOCAL, 10096 bytes) 
	24/04/25 16:28:26 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
	24/04/25 16:28:26 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/userFiles-e110ee56-ef03-46fd-b70a-570896baf917/676-20240425162748.json:0+186591
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:27.268 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8976608187134503 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:28.100 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:28:27 INFO CodeGenerator: Code generated in 142.941738 ms
	24/04/25 16:28:27 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
	org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
	
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator.isEmpty(Iterator.scala:387)
		at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
		at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)
		at scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)
		at scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)
		at scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)
		at scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	24/04/25 16:28:27 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (15dedf891bed executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
	
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator.isEmpty(Iterator.scala:387)
		at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
		at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)
		at scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)
		at scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)
		at scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)
		at scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	
	24/04/25 16:28:27 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
	24/04/25 16:28:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
	24/04/25 16:28:27 INFO TaskSchedulerImpl: Cancelling stage 0
	24/04/25 16:28:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (15dedf891bed executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
	
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator.isEmpty(Iterator.scala:387)
		at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
		at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)
		at scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)
		at scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)
		at scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)
		at scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	
	Driver stacktrace:
	24/04/25 16:28:27 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) failed in 2.015 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (15dedf891bed executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
	
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator.isEmpty(Iterator.scala:387)
		at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
		at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)
		at scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)
		at scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)
		at scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)
		at scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	
	Driver stacktrace:
	24/04/25 16:28:27 INFO DAGScheduler: Job 0 failed: json at NativeMethodAccessorImpl.java:0, took 2.084489 s
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:28.274 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8852941176470588 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:29.104 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:28:28 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 15dedf891bed:39583 in memory (size: 12.5 KiB, free: 366.3 MiB)
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/677/1837/spark_reddit_preprocessing.py", line 50, in <module>
	    raw_df = spark.read.json(raw_df)
	             ^^^^^^^^^^^^^^^^^^^^^^^
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 440, in json
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
	py4j.protocol.Py4JJavaError: An error occurred while calling o38.json.
	: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (15dedf891bed executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
	
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator.isEmpty(Iterator.scala:387)
		at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
		at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)
		at scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)
		at scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)
		at scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)
		at scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	
	Driver stacktrace:
		at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
		at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
		at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
		at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
		at scala.Option.foreach(Option.scala:407)
		at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
		at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.infer(JsonInferSchema.scala:120)
		at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$5(JsonDataSource.scala:109)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:109)
		at org.apache.spark.sql.DataFrameReader.$anonfun$json$4(DataFrameReader.scala:416)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:416)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:391)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:377)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:282)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
	
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator.isEmpty(Iterator.scala:387)
		at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
		at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)
		at scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)
		at scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)
		at scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)
		at scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)
		at scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
		at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		... 1 more
	
	24/04/25 16:28:28 INFO SparkContext: Invoking stop() from shutdown hook
	24/04/25 16:28:28 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/04/25 16:28:28 INFO SparkUI: Stopped Spark web UI at http://15dedf891bed:4040
	24/04/25 16:28:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/04/25 16:28:28 INFO MemoryStore: MemoryStore cleared
	24/04/25 16:28:28 INFO BlockManager: BlockManager stopped
	24/04/25 16:28:28 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/04/25 16:28:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/04/25 16:28:28 INFO SparkContext: Successfully stopped SparkContext
	24/04/25 16:28:28 INFO ShutdownHookManager: Shutdown hook called
	24/04/25 16:28:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-e23b66db-a00c-4ed2-8e89-63e140dd42f6
	24/04/25 16:28:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd
	24/04/25 16:28:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-55af6747-b515-4cd6-86f4-6c899a8f53dd/pyspark-1b3c61cc-9ccd-4cd4-a1d3-c39621b51639
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:29.109 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/677/1837, processId:6925 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:29.110 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:29.110 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:29.111 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:29.111 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:29.116 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:29.116 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:29.116 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/677/1837
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:29.117 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_114/677/1837
[WI-677][TI-1837] - [INFO] 2024-04-25 16:28:29.118 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:29.275 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8296089385474861 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1837] - [INFO] 2024-04-25 16:28:29.867 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1837, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:30.288 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.747093023255814 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:31.289 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7499999999999999 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [WARN] 2024-04-25 16:28:38.361 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[147] - [id: 0x0557cfab, L:/172.18.1.1:1234 - R:/172.18.0.11:53034] is not writable, over high water level : 65536
[WI-0][TI-0] - [WARN] 2024-04-25 16:28:38.362 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[154] - [id: 0x0557cfab, L:/172.18.1.1:1234 - R:/172.18.0.11:53034] is writable, to low water : 32768
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:41.162 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1838, taskName=extract from preprocessed queue, firstSubmitTime=1714033721157, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=13, appIds=null, processInstanceId=678, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"message","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n    --bootstrap-server kafka:9092 \\\n    --topic reddit_preprocessed \\\n    --group preprocessed_consumers \\\n    --max-messages 1 \\\n    --timeout-ms 10000)\n\necho \"#{setValue(message=${message})}\"","resourceList":[]}, environmentConfig=export JAVA_HOME=/opt/java/openjdk, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract from preprocessed queue'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1838'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13375898458848'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425162841'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='678'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_data'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.163 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract from preprocessed queue to wait queue success
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.164 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.165 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.165 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.165 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.165 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714033721165
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.165 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 678_1838
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.165 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1838,
  "taskName" : "extract from preprocessed queue",
  "firstSubmitTime" : 1714033721157,
  "startTime" : 1714033721165,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13377949373536/13/678/1838.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 13,
  "processInstanceId" : 678,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"message\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\\\\n    --bootstrap-server kafka:9092 \\\\\\n    --topic reddit_preprocessed \\\\\\n    --group preprocessed_consumers \\\\\\n    --max-messages 1 \\\\\\n    --timeout-ms 10000)\\n\\necho \\\"#{setValue(message=${message})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export JAVA_HOME=/opt/java/openjdk",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract from preprocessed queue"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1838"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13375898458848"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425162841"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "678"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_data"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "678_1838",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.166 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.166 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.166 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.168 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.168 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.168 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1838 check successfully
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.168 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.169 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.169 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.169 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.169 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "message",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n    --bootstrap-server kafka:9092 \\\n    --topic reddit_preprocessed \\\n    --group preprocessed_consumers \\\n    --max-messages 1 \\\n    --timeout-ms 10000)\n\necho \"#{setValue(message=${message})}\"",
  "resourceList" : [ ]
}
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.169 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.169 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.169 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.169 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.169 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.170 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.170 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.170 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export JAVA_HOME=/opt/java/openjdk
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

message=$(${KAFKA_DIR}/kafka-console-consumer.sh \
    --bootstrap-server kafka:9092 \
    --topic reddit_preprocessed \
    --group preprocessed_consumers \
    --max-messages 1 \
    --timeout-ms 10000)

echo "#{setValue(message=${message})}"
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.170 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.170 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1838/678_1838.sh
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:41.173 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 7128
[WI-0][TI-1838] - [INFO] 2024-04-25 16:28:41.886 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1838, success=true)
[WI-0][TI-1838] - [INFO] 2024-04-25 16:28:41.891 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1838)
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:42.178 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1838/678_1838.sh: line 12: /opt/kafka_2.13-3.7.0/bin/kafka-console-consumer.sh: No such file or directory
	#{setValue(message=)}
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:42.191 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1838, processId:7128 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-678][TI-1838] - [WARN] 2024-04-25 16:28:42.194 +0800 o.a.d.p.t.a.p.AbstractParameters:[152] - Cannot find the output parameter message in the task output parameters
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:42.197 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:42.197 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:42.197 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:42.197 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:42.231 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:42.232 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:42.232 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1838
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:42.233 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1838
[WI-678][TI-1838] - [INFO] 2024-04-25 16:28:42.233 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1838] - [INFO] 2024-04-25 16:28:42.881 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1838, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:42.957 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1839, taskName=keyword filtering, firstSubmitTime=1714033722940, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=13, appIds=null, processInstanceId=678, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1839'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425162842'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='678'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_data'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.964 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.967 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.970 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.972 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.974 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.974 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714033722974
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.975 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 678_1839
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.975 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1839,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1714033722940,
  "startTime" : 1714033722974,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13377949373536/13/678/1839.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 13,
  "processInstanceId" : 678,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1839"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425162842"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "678"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_data"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "678_1839",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.975 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.976 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.976 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.978 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.979 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.981 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1839 check successfully
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.981 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.981 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.982 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.982 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.982 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.982 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.982 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.982 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.982 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.983 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.983 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.983 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1839
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.983 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1839/py_678_1839.py
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.983 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.984 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.984 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.984 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1839/py_678_1839.py
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.984 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.984 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1839/678_1839.sh
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:42.987 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 7139
[WI-0][TI-1839] - [INFO] 2024-04-25 16:28:43.135 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1839, processInstanceId=678, startTime=1714033722974, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/678/1839.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1839] - [INFO] 2024-04-25 16:28:43.137 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1839, processInstanceId=678, startTime=1714033722974, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/678/1839.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714033723134)
[WI-0][TI-1839] - [INFO] 2024-04-25 16:28:43.137 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1839, processInstanceId=678, startTime=1714033722974, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1839] - [INFO] 2024-04-25 16:28:43.139 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1839, processInstanceId=678, startTime=1714033722974, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714033723134)
[WI-0][TI-1839] - [INFO] 2024-04-25 16:28:43.949 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1839, success=true)
[WI-0][TI-1839] - [INFO] 2024-04-25 16:28:43.955 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1839)
[WI-0][TI-1839] - [INFO] 2024-04-25 16:28:43.960 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1839, success=true)
[WI-0][TI-1839] - [INFO] 2024-04-25 16:28:43.966 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1839)
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:43.993 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1839/py_678_1839.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:44.003 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1839, processId:7139 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:44.003 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:44.003 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:44.003 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:44.004 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:44.005 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:44.006 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:44.006 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1839
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:44.006 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1839
[WI-678][TI-1839] - [INFO] 2024-04-25 16:28:44.006 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1839] - [INFO] 2024-04-25 16:28:44.139 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1839, processInstanceId=678, status=6, startTime=1714033722974, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/678/1839.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1839, endTime=1714033724004, processId=7139, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1839] - [INFO] 2024-04-25 16:28:44.142 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1839, processInstanceId=678, status=6, startTime=1714033722974, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/678/1839.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1839, endTime=1714033724004, processId=7139, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714033724139)
[WI-0][TI-1839] - [INFO] 2024-04-25 16:28:44.958 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1839, success=true)
[WI-0][TI-1839] - [INFO] 2024-04-25 16:28:44.960 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1839, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:45.015 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1840, taskName=keyword filtering, firstSubmitTime=1714033725005, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=13, appIds=null, processInstanceId=678, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1840'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425162845'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='678'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_data'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.017 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.018 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.019 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.019 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.019 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.020 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714033725020
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.020 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 678_1840
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.020 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1840,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1714033725005,
  "startTime" : 1714033725020,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13377949373536/13/678/1840.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 13,
  "processInstanceId" : 678,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1840"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425162845"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "678"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_data"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "678_1840",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.020 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.020 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.021 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.023 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.024 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.024 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1840 check successfully
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.024 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.024 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.024 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.024 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.024 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.025 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.025 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.025 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.025 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.025 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.025 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.025 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1840
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.026 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1840/py_678_1840.py
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.026 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.026 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.026 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.026 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1840/py_678_1840.py
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.026 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.026 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1840/678_1840.sh
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:45.029 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 7150
[WI-0][TI-1840] - [INFO] 2024-04-25 16:28:45.144 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1840, processInstanceId=678, startTime=1714033725020, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/678/1840.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1840] - [INFO] 2024-04-25 16:28:45.147 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1840, processInstanceId=678, startTime=1714033725020, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/678/1840.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714033725142)
[WI-0][TI-1840] - [INFO] 2024-04-25 16:28:45.148 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1840, processInstanceId=678, startTime=1714033725020, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1840] - [INFO] 2024-04-25 16:28:45.150 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1840, processInstanceId=678, startTime=1714033725020, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714033725142)
[WI-0][TI-1840] - [INFO] 2024-04-25 16:28:45.961 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1840, success=true)
[WI-0][TI-1840] - [INFO] 2024-04-25 16:28:45.966 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1840)
[WI-0][TI-1840] - [INFO] 2024-04-25 16:28:45.970 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1840, success=true)
[WI-0][TI-1840] - [INFO] 2024-04-25 16:28:45.974 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1840)
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:46.032 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1840/py_678_1840.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:46.070 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1840, processId:7150 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:46.071 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:46.071 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:46.071 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:46.071 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:46.073 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:46.073 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:46.073 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1840
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:46.074 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1840
[WI-678][TI-1840] - [INFO] 2024-04-25 16:28:46.075 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1840] - [INFO] 2024-04-25 16:28:46.151 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1840, processInstanceId=678, status=6, startTime=1714033725020, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/678/1840.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1840, endTime=1714033726071, processId=7150, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1840] - [INFO] 2024-04-25 16:28:46.161 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1840, processInstanceId=678, status=6, startTime=1714033725020, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/678/1840.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1840, endTime=1714033726071, processId=7150, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714033726151)
[WI-0][TI-1840] - [INFO] 2024-04-25 16:28:46.962 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1840, success=true)
[WI-0][TI-1840] - [INFO] 2024-04-25 16:28:46.964 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1840, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:46.985 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1841, taskName=keyword filtering, firstSubmitTime=1714033726977, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=13, appIds=null, processInstanceId=678, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1841'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425162846'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='678'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_data'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:46.986 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:46.986 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:46.987 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:46.992 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:46.992 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:46.992 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714033726992
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:46.992 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 678_1841
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:46.993 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1841,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1714033726977,
  "startTime" : 1714033726992,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13377949373536/13/678/1841.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 13,
  "processInstanceId" : 678,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1841"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425162846"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "678"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_data"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "678_1841",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:46.993 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:46.994 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:46.994 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:47.001 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:47.001 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:47.001 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1841 check successfully
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:47.002 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:47.002 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:47.002 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:47.002 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:47.003 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:47.003 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:47.003 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:47.003 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:47.004 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:47.004 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:47.004 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:47.004 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1841
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:47.005 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1841/py_678_1841.py
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:47.005 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:47.006 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:47.006 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:47.006 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1841/py_678_1841.py
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:47.006 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:47.006 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1841/678_1841.sh
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:47.009 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 7161
[WI-0][TI-1841] - [INFO] 2024-04-25 16:28:47.171 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1841, processInstanceId=678, startTime=1714033726992, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/678/1841.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1841] - [INFO] 2024-04-25 16:28:47.174 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1841, processInstanceId=678, startTime=1714033726992, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/678/1841.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714033727164)
[WI-0][TI-1841] - [INFO] 2024-04-25 16:28:47.174 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1841, processInstanceId=678, startTime=1714033726992, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1841] - [INFO] 2024-04-25 16:28:47.176 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1841, processInstanceId=678, startTime=1714033726992, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714033727164)
[WI-0][TI-1841] - [INFO] 2024-04-25 16:28:47.962 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1841, success=true)
[WI-0][TI-1841] - [INFO] 2024-04-25 16:28:47.969 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1841)
[WI-0][TI-1841] - [INFO] 2024-04-25 16:28:47.975 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1841, success=true)
[WI-0][TI-1841] - [INFO] 2024-04-25 16:28:47.982 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1841)
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:48.014 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1841/py_678_1841.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:48.017 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1841, processId:7161 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:48.018 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:48.019 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:48.019 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:48.019 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:48.021 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:48.022 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:48.022 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1841
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:48.022 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1841
[WI-678][TI-1841] - [INFO] 2024-04-25 16:28:48.022 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1841] - [INFO] 2024-04-25 16:28:48.185 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1841, processInstanceId=678, status=6, startTime=1714033726992, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/678/1841.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1841, endTime=1714033728019, processId=7161, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1841] - [INFO] 2024-04-25 16:28:48.190 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1841, processInstanceId=678, status=6, startTime=1714033726992, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/678/1841.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1841, endTime=1714033728019, processId=7161, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714033728185)
[WI-0][TI-1841] - [INFO] 2024-04-25 16:28:48.975 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1841, success=true)
[WI-0][TI-1841] - [INFO] 2024-04-25 16:28:48.978 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1841, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:49.049 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1842, taskName=keyword filtering, firstSubmitTime=1714033729037, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=13, appIds=null, processInstanceId=678, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1842'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425162849'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='678'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_data'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.050 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.050 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.052 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.052 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.052 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.052 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714033729052
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.052 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 678_1842
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.052 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1842,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1714033729037,
  "startTime" : 1714033729052,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13377949373536/13/678/1842.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 13,
  "processInstanceId" : 678,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1842"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425162849"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "678"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_data"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "678_1842",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.053 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.053 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.053 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.055 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.055 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.056 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1842 check successfully
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.056 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.056 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.056 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.056 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.056 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.056 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.056 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.056 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.056 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.057 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.057 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.057 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1842
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.057 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1842/py_678_1842.py
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.057 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.058 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.058 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.058 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1842/py_678_1842.py
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.058 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.058 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1842/678_1842.sh
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:49.061 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 7172
[WI-0][TI-1842] - [INFO] 2024-04-25 16:28:49.191 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1842, processInstanceId=678, startTime=1714033729052, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/678/1842.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1842] - [INFO] 2024-04-25 16:28:49.193 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1842, processInstanceId=678, startTime=1714033729052, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/678/1842.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714033729190)
[WI-0][TI-1842] - [INFO] 2024-04-25 16:28:49.193 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1842, processInstanceId=678, startTime=1714033729052, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1842] - [INFO] 2024-04-25 16:28:49.196 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1842, processInstanceId=678, startTime=1714033729052, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714033729190)
[WI-0][TI-1842] - [INFO] 2024-04-25 16:28:49.978 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1842, success=true)
[WI-0][TI-1842] - [INFO] 2024-04-25 16:28:49.983 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1842)
[WI-0][TI-1842] - [INFO] 2024-04-25 16:28:49.989 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1842, success=true)
[WI-0][TI-1842] - [INFO] 2024-04-25 16:28:49.999 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1842)
[WI-0][TI-0] - [INFO] 2024-04-25 16:28:50.063 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1842/py_678_1842.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:50.065 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1842, processId:7172 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:50.065 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:50.066 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:50.067 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:50.068 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:50.070 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:50.070 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:50.070 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1842
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:50.071 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1842
[WI-678][TI-1842] - [INFO] 2024-04-25 16:28:50.071 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1842] - [INFO] 2024-04-25 16:28:50.197 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1842, processInstanceId=678, status=6, startTime=1714033729052, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/678/1842.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1842, endTime=1714033730067, processId=7172, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1842] - [INFO] 2024-04-25 16:28:50.200 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1842, processInstanceId=678, status=6, startTime=1714033729052, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/13/678/1842.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_13/678/1842, endTime=1714033730067, processId=7172, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714033730197)
[WI-0][TI-1842] - [INFO] 2024-04-25 16:28:50.984 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1842, success=true)
[WI-0][TI-1842] - [INFO] 2024-04-25 16:28:50.987 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1842, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:30:22.773 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.71671388101983 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:30:27.822 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1843, taskName=search for intake JSON files, firstSubmitTime=1714033827814, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=115, appIds=null, processInstanceId=679, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1843'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340113777664'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425163027'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='679'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.823 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake JSON files to wait queue success
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.823 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.852 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.854 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.855 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.855 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714033827855
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.855 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 679_1843
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.855 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1843,
  "taskName" : "search for intake JSON files",
  "firstSubmitTime" : 1714033827814,
  "startTime" : 1714033827855,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13201021801792/115/679/1843.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 115,
  "processInstanceId" : 679,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set raw_file_dir to null\\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1843"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340113777664"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425163027"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "679"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "679_1843",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.857 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.857 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.857 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.863 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.864 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.865 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/679/1843 check successfully
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.865 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.866 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.867 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.867 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.867 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"",
  "resourceList" : [ ]
}
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.867 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.867 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.867 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.867 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.867 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.868 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.868 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.869 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/reddit/in -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set raw_file_dir to null
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
fi

echo "#{setValue(raw_file_dir=${raw_file_dir})}"
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.869 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.869 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/679/1843/679_1843.sh
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:27.890 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 7200
[WI-0][TI-0] - [INFO] 2024-04-25 16:30:27.894 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1843] - [INFO] 2024-04-25 16:30:28.251 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1843, success=true)
[WI-0][TI-1843] - [INFO] 2024-04-25 16:30:28.256 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1843)
[WI-0][TI-0] - [INFO] 2024-04-25 16:30:28.897 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	#{setValue(raw_file_dir=null)}
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:28.899 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/679/1843, processId:7200 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:28.900 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:28.900 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:28.901 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:28.902 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:28.905 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:28.907 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:28.908 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/679/1843
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:28.908 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/679/1843
[WI-679][TI-1843] - [INFO] 2024-04-25 16:30:28.909 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1843] - [INFO] 2024-04-25 16:30:29.247 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1843, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:30:30.330 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1845, taskName=end workflow, firstSubmitTime=1714033830325, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=115, appIds=null, processInstanceId=679, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"echo No Files Detected - End Workflow","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='end workflow'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1845'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13348985859488'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425163030'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='679'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='null'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"null"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.333 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: end workflow to wait queue success
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.333 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.334 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.336 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.336 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.336 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714033830336
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.336 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 679_1845
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.336 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1845,
  "taskName" : "end workflow",
  "firstSubmitTime" : 1714033830325,
  "startTime" : 1714033830336,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13201021801792/115/679/1845.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 115,
  "processInstanceId" : 679,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"echo No Files Detected - End Workflow\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "end workflow"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1845"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13348985859488"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425163030"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "679"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "null"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "679_1845",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"null\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.337 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.337 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.337 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.339 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.340 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.340 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/679/1845 check successfully
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.340 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.340 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.341 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.341 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.341 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "echo No Files Detected - End Workflow",
  "resourceList" : [ ]
}
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.341 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.341 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"null"}] successfully
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.341 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.341 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.341 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.342 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.342 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.342 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
echo No Files Detected - End Workflow
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.342 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.342 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/679/1845/679_1845.sh
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:30.357 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 7214
[WI-0][TI-0] - [INFO] 2024-04-25 16:30:30.358 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1845] - [INFO] 2024-04-25 16:30:30.758 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1845, processInstanceId=679, startTime=1714033830336, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/115/679/1845.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1845] - [INFO] 2024-04-25 16:30:30.763 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1845, processInstanceId=679, startTime=1714033830336, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/115/679/1845.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714033830757)
[WI-0][TI-1845] - [INFO] 2024-04-25 16:30:30.766 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1845, processInstanceId=679, startTime=1714033830336, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1845] - [INFO] 2024-04-25 16:30:30.768 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1845, processInstanceId=679, startTime=1714033830336, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714033830757)
[WI-0][TI-1845] - [INFO] 2024-04-25 16:30:31.249 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1845, success=true)
[WI-0][TI-1845] - [INFO] 2024-04-25 16:30:31.259 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1845)
[WI-0][TI-1845] - [INFO] 2024-04-25 16:30:31.267 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1845, success=true)
[WI-0][TI-1845] - [INFO] 2024-04-25 16:30:31.285 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1845)
[WI-0][TI-0] - [INFO] 2024-04-25 16:30:31.369 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	No Files Detected - End Workflow
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:31.371 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/679/1845, processId:7214 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:31.372 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:31.373 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:31.373 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:31.373 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:31.375 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:31.376 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:31.376 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/679/1845
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:31.376 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/679/1845
[WI-679][TI-1845] - [INFO] 2024-04-25 16:30:31.376 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1845] - [INFO] 2024-04-25 16:30:31.769 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1845, processInstanceId=679, status=7, startTime=1714033830336, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/115/679/1845.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/679/1845, endTime=1714033831373, processId=7214, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"null"}], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1845] - [INFO] 2024-04-25 16:30:31.772 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1845, processInstanceId=679, status=7, startTime=1714033830336, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/115/679/1845.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/679/1845, endTime=1714033831373, processId=7214, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"null"}], eventCreateTime=0, eventSendTime=1714033831769)
[WI-0][TI-1845] - [INFO] 2024-04-25 16:30:32.249 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1845, success=true)
[WI-0][TI-1845] - [INFO] 2024-04-25 16:30:32.252 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1845, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:30:53.001 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1846, taskName=search for intake JSON files, firstSubmitTime=1714033852979, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=115, appIds=null, processInstanceId=680, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1846'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340113777664'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425163052'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='680'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.012 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake JSON files to wait queue success
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.013 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.018 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.019 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.019 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.019 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714033853019
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.019 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 680_1846
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.020 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1846,
  "taskName" : "search for intake JSON files",
  "firstSubmitTime" : 1714033852979,
  "startTime" : 1714033853019,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13201021801792/115/680/1846.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 115,
  "processInstanceId" : 680,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set raw_file_dir to null\\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1846"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340113777664"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425163052"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "680"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "680_1846",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.020 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.020 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.020 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.024 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.025 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.025 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1846 check successfully
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.025 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.027 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.027 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.027 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.027 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"",
  "resourceList" : [ ]
}
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.028 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.028 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.028 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.028 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.028 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.028 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.029 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.029 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/reddit/processing -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set raw_file_dir to null
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
fi

echo "#{setValue(raw_file_dir=${raw_file_dir})}"
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.029 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.029 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1846/680_1846.sh
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:53.038 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 7230
[WI-0][TI-1846] - [INFO] 2024-04-25 16:30:53.297 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1846, success=true)
[WI-0][TI-1846] - [INFO] 2024-04-25 16:30:53.304 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1846)
[WI-0][TI-0] - [INFO] 2024-04-25 16:30:54.040 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	#{setValue(raw_file_dir=/local_storage/reddit/processing/676-20240425162748.json)}
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:54.044 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1846, processId:7230 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:54.047 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:54.052 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:54.052 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:54.053 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:54.077 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:54.078 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:54.078 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1846
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:54.083 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1846
[WI-680][TI-1846] - [INFO] 2024-04-25 16:30:54.084 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1846] - [INFO] 2024-04-25 16:30:54.278 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1846, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:30:55.352 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1848, taskName=move to processing, firstSubmitTime=1714033855329, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=115, appIds=null, processInstanceId=680, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to processing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1848'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340390094208'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425163055'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='680'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/676-20240425162748.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.354 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to processing to wait queue success
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.354 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.355 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.355 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.355 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.355 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714033855355
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.355 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 680_1848
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.356 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1848,
  "taskName" : "move to processing",
  "firstSubmitTime" : 1714033855329,
  "startTime" : 1714033855355,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13201021801792/115/680/1848.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 115,
  "processInstanceId" : 680,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the reddit processing folder\\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\\nfilename=$(basename \\\"${raw_file_dir}\\\")\\nif ! grep -q \\\"${REDDIT_HOME}/processing\\\" <<< \\\"${raw_file_dir}\\\"; then\\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\\nelse\\n    echo JSON is already in processing\\nfi\\n\\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\\necho \\\"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to processing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1848"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340390094208"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425163055"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "680"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/676-20240425162748.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "680_1848",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/676-20240425162748.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.356 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.356 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.356 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.359 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.360 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.360 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1848 check successfully
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.360 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.361 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.361 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.361 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.361 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"",
  "resourceList" : [ ]
}
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.361 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.361 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}] successfully
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.361 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.361 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.361 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.362 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.362 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.362 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the reddit processing folder
# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error
filename=$(basename "/local_storage/reddit/processing/676-20240425162748.json")
if ! grep -q "/local_storage/reddit/processing" <<< "/local_storage/reddit/processing/676-20240425162748.json"; then
    mv /local_storage/reddit/processing/676-20240425162748.json /local_storage/reddit/processing
else
    echo JSON is already in processing
fi

# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing
echo "#{setValue(raw_file_dir=/local_storage/reddit/processing/${filename})}"
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.362 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.363 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1848/680_1848.sh
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:55.367 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 7243
[WI-0][TI-1848] - [INFO] 2024-04-25 16:30:55.808 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1848, processInstanceId=680, startTime=1714033855355, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/115/680/1848.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1848] - [INFO] 2024-04-25 16:30:55.812 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1848, processInstanceId=680, startTime=1714033855355, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/115/680/1848.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714033855807)
[WI-0][TI-1848] - [INFO] 2024-04-25 16:30:55.812 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1848, processInstanceId=680, startTime=1714033855355, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1848] - [INFO] 2024-04-25 16:30:55.814 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1848, processInstanceId=680, startTime=1714033855355, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714033855807)
[WI-0][TI-1848] - [INFO] 2024-04-25 16:30:56.289 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1848, success=true)
[WI-0][TI-1848] - [INFO] 2024-04-25 16:30:56.295 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1848)
[WI-0][TI-1848] - [INFO] 2024-04-25 16:30:56.302 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1848, success=true)
[WI-0][TI-1848] - [INFO] 2024-04-25 16:30:56.318 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1848)
[WI-0][TI-0] - [INFO] 2024-04-25 16:30:56.371 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	JSON is already in processing
	#{setValue(raw_file_dir=/local_storage/reddit/processing/676-20240425162748.json)}
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:56.372 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1848, processId:7243 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:56.373 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:56.373 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:56.373 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:56.375 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:56.377 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:56.377 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:56.378 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1848
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:56.379 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1848
[WI-680][TI-1848] - [INFO] 2024-04-25 16:30:56.379 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1848] - [INFO] 2024-04-25 16:30:56.815 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1848, processInstanceId=680, status=7, startTime=1714033855355, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/115/680/1848.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1848, endTime=1714033856374, processId=7243, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1848] - [INFO] 2024-04-25 16:30:56.819 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1848, processInstanceId=680, status=7, startTime=1714033855355, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/115/680/1848.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1848, endTime=1714033856374, processId=7243, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], eventCreateTime=0, eventSendTime=1714033856815)
[WI-0][TI-1848] - [INFO] 2024-04-25 16:30:57.282 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1848, success=true)
[WI-0][TI-1848] - [INFO] 2024-04-25 16:30:57.285 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1848, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:30:57.365 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1849, taskName=preprocessing and kafka, firstSubmitTime=1714033857360, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=115, appIds=null, processInstanceId=680, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_reddit_preprocessing.py\n\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='preprocessing and kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1849'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13210058002752'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425163057'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='680'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/676-20240425162748.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.366 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: preprocessing and kafka to wait queue success
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.367 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.368 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.368 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.368 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.368 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714033857368
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.368 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 680_1849
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.369 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1849,
  "taskName" : "preprocessing and kafka",
  "firstSubmitTime" : 1714033857360,
  "startTime" : 1714033857368,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13201021801792/115/680/1849.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 115,
  "processInstanceId" : 680,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name reddit_preprocessing \\\\\\n    --files ${raw_file_dir} \\\\\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\\\\n    --conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n    spark_reddit_preprocessing.py\\n\\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\\\\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocessing and kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1849"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13210058002752"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425163057"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "680"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/676-20240425162748.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "680_1849",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/676-20240425162748.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.369 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.369 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.369 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.371 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.371 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.372 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1849 check successfully
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.372 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.400 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py, resourceRelativePath=spark_reddit_preprocessing.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1849/spark_reddit_preprocessing.py)})
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.400 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.401 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_reddit_preprocessing.py\n\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py",
    "res" : null
  } ]
}
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}] successfully
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.401 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.401 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.401 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.402 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.402 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.402 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master local \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name reddit_preprocessing \
    --files /local_storage/reddit/processing/676-20240425162748.json \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \
    --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
    spark_reddit_preprocessing.py

#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.402 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.402 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1849/680_1849.sh
[WI-680][TI-1849] - [INFO] 2024-04-25 16:30:57.406 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 7255
[WI-0][TI-1849] - [INFO] 2024-04-25 16:30:57.821 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1849, processInstanceId=680, startTime=1714033857368, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/115/680/1849.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1849] - [INFO] 2024-04-25 16:30:57.827 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1849, processInstanceId=680, startTime=1714033857368, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/115/680/1849.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714033857821)
[WI-0][TI-1849] - [INFO] 2024-04-25 16:30:57.827 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1849, processInstanceId=680, startTime=1714033857368, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1849] - [INFO] 2024-04-25 16:30:57.829 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1849, processInstanceId=680, startTime=1714033857368, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714033857821)
[WI-0][TI-1849] - [INFO] 2024-04-25 16:30:58.295 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1849, success=true)
[WI-0][TI-1849] - [INFO] 2024-04-25 16:30:58.309 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1849)
[WI-0][TI-1849] - [INFO] 2024-04-25 16:30:58.338 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1849, success=true)
[WI-0][TI-1849] - [INFO] 2024-04-25 16:30:58.355 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1849)
[WI-0][TI-0] - [INFO] 2024-04-25 16:30:58.407 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 16:30:58.986 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7777777777777777 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:00.001 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8950276243093923 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:00.416 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-64746250-eea1-41d9-91e4-af9f2456ca4c;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:01.018 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9149560117302054 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:01.420 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 815ms :: artifacts dl 21ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-64746250-eea1-41d9-91e4-af9f2456ca4c
		confs: [default]
		0 artifacts copied, 12 already retrieved (0kB/39ms)
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:02.020 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8694267515923566 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:02.422 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:31:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:03.423 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:31:03 INFO SparkContext: Running Spark version 3.5.1
	24/04/25 16:31:03 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/04/25 16:31:03 INFO SparkContext: Java version 1.8.0_402
	24/04/25 16:31:03 INFO ResourceUtils: ==============================================================
	24/04/25 16:31:03 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/04/25 16:31:03 INFO ResourceUtils: ==============================================================
	24/04/25 16:31:03 INFO SparkContext: Submitted application: reddit_preprocessing
	24/04/25 16:31:03 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/04/25 16:31:03 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
	24/04/25 16:31:03 INFO ResourceProfileManager: Added ResourceProfile id: 0
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:04.039 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8138888888888889 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:04.424 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:31:03 INFO SecurityManager: Changing view acls to: default
	24/04/25 16:31:03 INFO SecurityManager: Changing modify acls to: default
	24/04/25 16:31:03 INFO SecurityManager: Changing view acls groups to: 
	24/04/25 16:31:03 INFO SecurityManager: Changing modify acls groups to: 
	24/04/25 16:31:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
	24/04/25 16:31:04 INFO Utils: Successfully started service 'sparkDriver' on port 36147.
	24/04/25 16:31:04 INFO SparkEnv: Registering MapOutputTracker
	24/04/25 16:31:04 INFO SparkEnv: Registering BlockManagerMaster
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:05.051 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9108635097493036 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:05.430 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:31:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/04/25 16:31:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/04/25 16:31:04 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/04/25 16:31:04 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-60401efc-1da1-4c0c-97f3-ae566a0a7190
	24/04/25 16:31:04 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
	24/04/25 16:31:04 INFO SparkEnv: Registering OutputCommitCoordinator
	24/04/25 16:31:05 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/04/25 16:31:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:06.076 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8563619002148484 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:06.439 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:31:05 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar at spark://15dedf891bed:36147/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1714033863052
	24/04/25 16:31:05 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar at spark://15dedf891bed:36147/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1714033863052
	24/04/25 16:31:05 INFO SparkContext: Added JAR file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar at spark://15dedf891bed:36147/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1714033863052
	24/04/25 16:31:05 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://15dedf891bed:36147/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714033863052
	24/04/25 16:31:05 INFO SparkContext: Added JAR file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://15dedf891bed:36147/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714033863052
	24/04/25 16:31:05 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://15dedf891bed:36147/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714033863052
	24/04/25 16:31:05 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://15dedf891bed:36147/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714033863052
	24/04/25 16:31:05 INFO SparkContext: Added JAR file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at spark://15dedf891bed:36147/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714033863052
	24/04/25 16:31:05 INFO SparkContext: Added JAR file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://15dedf891bed:36147/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714033863052
	24/04/25 16:31:05 INFO SparkContext: Added JAR file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://15dedf891bed:36147/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714033863052
	24/04/25 16:31:05 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://15dedf891bed:36147/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714033863052
	24/04/25 16:31:05 INFO SparkContext: Added JAR file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at spark://15dedf891bed:36147/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714033863052
	24/04/25 16:31:05 INFO SparkContext: Added file file:///local_storage/reddit/processing/676-20240425162748.json at file:///local_storage/reddit/processing/676-20240425162748.json with timestamp 1714033863052
	24/04/25 16:31:05 INFO Utils: Copying /local_storage/reddit/processing/676-20240425162748.json to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/676-20240425162748.json
	24/04/25 16:31:05 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1714033863052
	24/04/25 16:31:05 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar
	24/04/25 16:31:05 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1714033863052
	24/04/25 16:31:05 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar
	24/04/25 16:31:05 INFO SparkContext: Added file file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar at file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1714033863052
	24/04/25 16:31:05 INFO Utils: Copying /tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar
	24/04/25 16:31:05 INFO SparkContext: Added file file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714033863052
	24/04/25 16:31:05 INFO Utils: Copying /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/25 16:31:05 INFO SparkContext: Added file file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714033863052
	24/04/25 16:31:05 INFO Utils: Copying /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/25 16:31:05 INFO SparkContext: Added file file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714033863052
	24/04/25 16:31:05 INFO Utils: Copying /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/25 16:31:05 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714033863052
	24/04/25 16:31:05 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/25 16:31:05 INFO SparkContext: Added file file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714033863052
	24/04/25 16:31:05 INFO Utils: Copying /tmp/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.lz4_lz4-java-1.8.0.jar
	24/04/25 16:31:05 INFO SparkContext: Added file file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714033863052
	24/04/25 16:31:05 INFO Utils: Copying /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/25 16:31:06 INFO SparkContext: Added file file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714033863052
	24/04/25 16:31:06 INFO Utils: Copying /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.slf4j_slf4j-api-2.0.7.jar
	24/04/25 16:31:06 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714033863052
	24/04/25 16:31:06 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/25 16:31:06 INFO SparkContext: Added file file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714033863052
	24/04/25 16:31:06 INFO Utils: Copying /tmp/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/commons-logging_commons-logging-1.1.3.jar
	24/04/25 16:31:06 INFO Executor: Starting executor ID driver on host 15dedf891bed
	24/04/25 16:31:06 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/04/25 16:31:06 INFO Executor: Java version 1.8.0_402
	24/04/25 16:31:06 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/04/25 16:31:06 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1e9d4c3c for default.
	24/04/25 16:31:06 INFO Executor: Fetching file:///local_storage/reddit/processing/676-20240425162748.json with timestamp 1714033863052
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:07.080 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8547008547008548 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:07.442 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:31:06 INFO Utils: /local_storage/reddit/processing/676-20240425162748.json has been previously copied to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/676-20240425162748.json
	24/04/25 16:31:06 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1714033863052
	24/04/25 16:31:06 INFO Utils: /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar has been previously copied to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar
	24/04/25 16:31:06 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714033863052
	24/04/25 16:31:06 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/25 16:31:06 INFO Executor: Fetching file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714033863052
	24/04/25 16:31:06 INFO Utils: /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/25 16:31:06 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1714033863052
	24/04/25 16:31:06 INFO Utils: /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar has been previously copied to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar
	24/04/25 16:31:06 INFO Executor: Fetching file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714033863052
	24/04/25 16:31:06 INFO Utils: /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/25 16:31:06 INFO Executor: Fetching file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714033863052
	24/04/25 16:31:06 INFO Utils: /tmp/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.lz4_lz4-java-1.8.0.jar
	24/04/25 16:31:06 INFO Executor: Fetching file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714033863052
	24/04/25 16:31:06 INFO Utils: /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/25 16:31:06 INFO Executor: Fetching file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714033863052
	24/04/25 16:31:06 INFO Utils: /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.slf4j_slf4j-api-2.0.7.jar
	24/04/25 16:31:06 INFO Executor: Fetching file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714033863052
	24/04/25 16:31:06 INFO Utils: /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/25 16:31:06 INFO Executor: Fetching file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714033863052
	24/04/25 16:31:06 INFO Utils: /tmp/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/commons-logging_commons-logging-1.1.3.jar
	24/04/25 16:31:06 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714033863052
	24/04/25 16:31:06 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/25 16:31:06 INFO Executor: Fetching file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1714033863052
	24/04/25 16:31:06 INFO Utils: /tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar has been previously copied to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar
	24/04/25 16:31:06 INFO Executor: Fetching spark://15dedf891bed:36147/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714033863052
	24/04/25 16:31:06 INFO TransportClientFactory: Successfully created connection to 15dedf891bed/172.18.1.1:36147 after 64 ms (0 ms spent in bootstraps)
	24/04/25 16:31:06 INFO Utils: Fetching spark://15dedf891bed:36147/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/fetchFileTemp7042172791051608352.tmp
	24/04/25 16:31:06 INFO Utils: /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/fetchFileTemp7042172791051608352.tmp has been previously copied to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/25 16:31:06 INFO Executor: Adding file:/tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
	24/04/25 16:31:06 INFO Executor: Fetching spark://15dedf891bed:36147/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714033863052
	24/04/25 16:31:06 INFO Utils: Fetching spark://15dedf891bed:36147/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/fetchFileTemp247339645034142222.tmp
	24/04/25 16:31:06 INFO Utils: /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/fetchFileTemp247339645034142222.tmp has been previously copied to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.lz4_lz4-java-1.8.0.jar
	24/04/25 16:31:06 INFO Executor: Adding file:/tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.lz4_lz4-java-1.8.0.jar to class loader default
	24/04/25 16:31:06 INFO Executor: Fetching spark://15dedf891bed:36147/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714033863052
	24/04/25 16:31:06 INFO Utils: Fetching spark://15dedf891bed:36147/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/fetchFileTemp1253152252574825686.tmp
	24/04/25 16:31:07 INFO Utils: /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/fetchFileTemp1253152252574825686.tmp has been previously copied to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/25 16:31:07 INFO Executor: Adding file:/tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
	24/04/25 16:31:07 INFO Executor: Fetching spark://15dedf891bed:36147/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714033863052
	24/04/25 16:31:07 INFO Utils: Fetching spark://15dedf891bed:36147/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/fetchFileTemp5489625697622324954.tmp
	24/04/25 16:31:07 INFO Utils: /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/fetchFileTemp5489625697622324954.tmp has been previously copied to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.slf4j_slf4j-api-2.0.7.jar
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:08.081 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.809116809116809 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:08.444 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:31:07 INFO Executor: Adding file:/tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.slf4j_slf4j-api-2.0.7.jar to class loader default
	24/04/25 16:31:07 INFO Executor: Fetching spark://15dedf891bed:36147/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714033863052
	24/04/25 16:31:07 INFO Utils: Fetching spark://15dedf891bed:36147/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/fetchFileTemp8445074023129867380.tmp
	24/04/25 16:31:07 INFO Utils: /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/fetchFileTemp8445074023129867380.tmp has been previously copied to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/25 16:31:07 INFO Executor: Adding file:/tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
	24/04/25 16:31:07 INFO Executor: Fetching spark://15dedf891bed:36147/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714033863052
	24/04/25 16:31:07 INFO Utils: Fetching spark://15dedf891bed:36147/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/fetchFileTemp6455741214340783723.tmp
	24/04/25 16:31:07 INFO Utils: /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/fetchFileTemp6455741214340783723.tmp has been previously copied to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/commons-logging_commons-logging-1.1.3.jar
	24/04/25 16:31:07 INFO Executor: Adding file:/tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/commons-logging_commons-logging-1.1.3.jar to class loader default
	24/04/25 16:31:07 INFO Executor: Fetching spark://15dedf891bed:36147/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1714033863052
	24/04/25 16:31:07 INFO Utils: Fetching spark://15dedf891bed:36147/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/fetchFileTemp2312396647444686625.tmp
	24/04/25 16:31:07 INFO Utils: /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/fetchFileTemp2312396647444686625.tmp has been previously copied to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar
	24/04/25 16:31:07 INFO Executor: Adding file:/tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar to class loader default
	24/04/25 16:31:07 INFO Executor: Fetching spark://15dedf891bed:36147/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714033863052
	24/04/25 16:31:07 INFO Utils: Fetching spark://15dedf891bed:36147/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/fetchFileTemp5497101039108882233.tmp
	24/04/25 16:31:07 INFO Utils: /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/fetchFileTemp5497101039108882233.tmp has been previously copied to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/25 16:31:07 INFO Executor: Adding file:/tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
	24/04/25 16:31:07 INFO Executor: Fetching spark://15dedf891bed:36147/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714033863052
	24/04/25 16:31:07 INFO Utils: Fetching spark://15dedf891bed:36147/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/fetchFileTemp3063942634206302619.tmp
	24/04/25 16:31:07 INFO Utils: /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/fetchFileTemp3063942634206302619.tmp has been previously copied to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/25 16:31:07 INFO Executor: Adding file:/tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
	24/04/25 16:31:07 INFO Executor: Fetching spark://15dedf891bed:36147/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1714033863052
	24/04/25 16:31:07 INFO Utils: Fetching spark://15dedf891bed:36147/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/fetchFileTemp4615367561702137156.tmp
	24/04/25 16:31:07 INFO Utils: /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/fetchFileTemp4615367561702137156.tmp has been previously copied to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar
	24/04/25 16:31:07 INFO Executor: Adding file:/tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar to class loader default
	24/04/25 16:31:07 INFO Executor: Fetching spark://15dedf891bed:36147/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1714033863052
	24/04/25 16:31:07 INFO Utils: Fetching spark://15dedf891bed:36147/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/fetchFileTemp2729107733640648007.tmp
	24/04/25 16:31:07 INFO Utils: /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/fetchFileTemp2729107733640648007.tmp has been previously copied to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar
	24/04/25 16:31:07 INFO Executor: Adding file:/tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar to class loader default
	24/04/25 16:31:07 INFO Executor: Fetching spark://15dedf891bed:36147/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714033863052
	24/04/25 16:31:07 INFO Utils: Fetching spark://15dedf891bed:36147/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/fetchFileTemp7162653670049567501.tmp
	24/04/25 16:31:07 INFO Utils: /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/fetchFileTemp7162653670049567501.tmp has been previously copied to /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/25 16:31:07 INFO Executor: Adding file:/tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
	24/04/25 16:31:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33033.
	24/04/25 16:31:07 INFO NettyBlockTransferService: Server created on 15dedf891bed:33033
	24/04/25 16:31:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/04/25 16:31:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 15dedf891bed, 33033, None)
	24/04/25 16:31:07 INFO BlockManagerMasterEndpoint: Registering block manager 15dedf891bed:33033 with 366.3 MiB RAM, BlockManagerId(driver, 15dedf891bed, 33033, None)
	24/04/25 16:31:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 15dedf891bed, 33033, None)
	24/04/25 16:31:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 15dedf891bed, 33033, None)
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:09.103 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8146863108198856 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:09.447 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	
	
	 /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/676-20240425162748.json 
	
	
	
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:10.106 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8492782721944866 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:10.451 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:31:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 345.7 KiB, free 366.0 MiB)
	24/04/25 16:31:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 365.9 MiB)
	24/04/25 16:31:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 15dedf891bed:33033 (size: 32.6 KiB, free: 366.3 MiB)
	24/04/25 16:31:09 INFO SparkContext: Created broadcast 0 from wholeTextFiles at NativeMethodAccessorImpl.java:0
	24/04/25 16:31:09 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/04/25 16:31:09 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1849/spark-warehouse'.
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:11.110 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8333333333333333 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:12.114 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7983193277310924 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:13.116 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.940625 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:16.129 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7307692307692308 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:17.138 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7452054794520547 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:17.596 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:31:17 INFO CodeGenerator: Code generated in 400.132387 ms
	24/04/25 16:31:17 INFO FileInputFormat: Total input files to process : 1
	24/04/25 16:31:17 INFO FileInputFormat: Total input files to process : 1
	24/04/25 16:31:17 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
	24/04/25 16:31:17 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/25 16:31:17 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
	24/04/25 16:31:17 INFO DAGScheduler: Parents of final stage: List()
	24/04/25 16:31:17 INFO DAGScheduler: Missing parents: List()
	24/04/25 16:31:17 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/25 16:31:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 28.6 KiB, free 365.9 MiB)
	24/04/25 16:31:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 365.9 MiB)
	24/04/25 16:31:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 15dedf891bed:33033 (size: 12.5 KiB, free: 366.3 MiB)
	24/04/25 16:31:17 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
	24/04/25 16:31:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/25 16:31:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:18.145 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7362204724409449 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:18.598 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:31:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (15dedf891bed, executor driver, partition 0, PROCESS_LOCAL, 10096 bytes) 
	24/04/25 16:31:17 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
	24/04/25 16:31:18 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/676-20240425162748.json:0+186591
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:19.600 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:31:18 INFO CodeGenerator: Code generated in 128.0342 ms
	24/04/25 16:31:19 INFO PythonRunner: Times: total = 819, boot = 604, init = 185, finish = 30
	24/04/25 16:31:19 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2751 bytes result sent to driver
	24/04/25 16:31:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1473 ms on 15dedf891bed (executor driver) (1/1)
	24/04/25 16:31:19 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
	24/04/25 16:31:19 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 56315
	24/04/25 16:31:19 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1.810 s
	24/04/25 16:31:19 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/25 16:31:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
	24/04/25 16:31:19 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1.983732 s
	24/04/25 16:31:19 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 15dedf891bed:33033 in memory (size: 12.5 KiB, free: 366.3 MiB)
	
	
	
	 1 
	
	
	
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:20.180 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7999925144097612 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:20.605 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	
	
	 2 
	
	
	
	
	
	
	 3 
	
	
	
	
	
	
	 4 
	
	
	
	24/04/25 16:31:20 INFO CodeGenerator: Code generated in 48.010833 ms
	24/04/25 16:31:20 INFO CodeGenerator: Code generated in 47.618013 ms
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:21.264 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7326951238170175 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:21.608 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:31:20 INFO SparkContext: Starting job: collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1849/spark_reddit_preprocessing.py:84
	24/04/25 16:31:20 INFO DAGScheduler: Got job 1 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1849/spark_reddit_preprocessing.py:84) with 1 output partitions
	24/04/25 16:31:20 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1849/spark_reddit_preprocessing.py:84)
	24/04/25 16:31:20 INFO DAGScheduler: Parents of final stage: List()
	24/04/25 16:31:20 INFO DAGScheduler: Missing parents: List()
	24/04/25 16:31:20 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[19] at toJavaRDD at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/25 16:31:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 72.6 KiB, free 365.9 MiB)
	24/04/25 16:31:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 365.8 MiB)
	24/04/25 16:31:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 15dedf891bed:33033 (size: 28.7 KiB, free: 366.2 MiB)
	24/04/25 16:31:20 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
	24/04/25 16:31:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[19] at toJavaRDD at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/25 16:31:20 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
	24/04/25 16:31:20 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (15dedf891bed, executor driver, partition 0, PROCESS_LOCAL, 10096 bytes) 
	24/04/25 16:31:20 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
	24/04/25 16:31:20 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/userFiles-8d25b4f4-1fc8-4e7d-a365-4156de769d9e/676-20240425162748.json:0+186591
	24/04/25 16:31:21 INFO CodeGenerator: Code generated in 45.287979 ms
	24/04/25 16:31:21 INFO CodeGenerator: Code generated in 143.495653 ms
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:22.614 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:31:21 INFO CodeGenerator: Code generated in 49.135458 ms
	24/04/25 16:31:21 INFO PythonRunner: Times: total = 165, boot = -1904, init = 2057, finish = 12
	24/04/25 16:31:21 INFO CodeGenerator: Code generated in 69.860722 ms
	24/04/25 16:31:22 INFO CodeGenerator: Code generated in 67.4226 ms
	24/04/25 16:31:22 INFO CodeGenerator: Code generated in 118.744263 ms
	24/04/25 16:31:22 INFO PythonUDFRunner: Times: total = 776, boot = 563, init = 212, finish = 1
	24/04/25 16:31:22 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 172332 bytes result sent to driver
	24/04/25 16:31:22 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1673 ms on 15dedf891bed (executor driver) (1/1)
	24/04/25 16:31:22 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
	24/04/25 16:31:22 INFO DAGScheduler: ResultStage 1 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1849/spark_reddit_preprocessing.py:84) finished in 1.693 s
	24/04/25 16:31:22 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/25 16:31:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
	24/04/25 16:31:22 INFO DAGScheduler: Job 1 finished: collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1849/spark_reddit_preprocessing.py:84, took 1.710798 s
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:23.624 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1849/spark_reddit_preprocessing.py", line 99, in <module>
	    .save()
	     ^^^^^^
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1461, in save
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
	py4j.protocol.Py4JJavaError: An error occurred while calling o111.save.
	: java.lang.NoClassDefFoundError: scala/$less$colon$less
		at org.apache.spark.sql.kafka010.KafkaSourceProvider.createRelation(KafkaSourceProvider.scala:180)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
		at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
		at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
		at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
		at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
		at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:282)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: java.lang.ClassNotFoundException: scala.$less$colon$less
		at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
		at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
		at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
		... 42 more
	
	24/04/25 16:31:23 INFO SparkContext: Invoking stop() from shutdown hook
	24/04/25 16:31:23 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/04/25 16:31:23 INFO SparkUI: Stopped Spark web UI at http://15dedf891bed:4040
	24/04/25 16:31:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/04/25 16:31:23 INFO MemoryStore: MemoryStore cleared
	24/04/25 16:31:23 INFO BlockManager: BlockManager stopped
	24/04/25 16:31:23 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/04/25 16:31:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/04/25 16:31:23 INFO SparkContext: Successfully stopped SparkContext
	24/04/25 16:31:23 INFO ShutdownHookManager: Shutdown hook called
	24/04/25 16:31:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-bfc75e29-e599-4f66-8dfc-1cfdd444c22c
	24/04/25 16:31:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036
	24/04/25 16:31:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-a921383e-09b2-4d43-a46c-136c4899d036/pyspark-fd9ac6e0-f262-4756-9dcd-c6019253df0a
[WI-680][TI-1849] - [INFO] 2024-04-25 16:31:23.628 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1849, processId:7255 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-680][TI-1849] - [INFO] 2024-04-25 16:31:23.629 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-680][TI-1849] - [INFO] 2024-04-25 16:31:23.630 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-680][TI-1849] - [INFO] 2024-04-25 16:31:23.630 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-680][TI-1849] - [INFO] 2024-04-25 16:31:23.631 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-680][TI-1849] - [INFO] 2024-04-25 16:31:23.635 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-680][TI-1849] - [INFO] 2024-04-25 16:31:23.635 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-680][TI-1849] - [INFO] 2024-04-25 16:31:23.636 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1849
[WI-680][TI-1849] - [INFO] 2024-04-25 16:31:23.638 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1849
[WI-680][TI-1849] - [INFO] 2024-04-25 16:31:23.638 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1849] - [INFO] 2024-04-25 16:31:23.946 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1849, processInstanceId=680, status=6, startTime=1714033857368, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/115/680/1849.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1849, endTime=1714033883630, processId=7255, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1849] - [INFO] 2024-04-25 16:31:23.951 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1849, processInstanceId=680, status=6, startTime=1714033857368, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/115/680/1849.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_115/680/1849, endTime=1714033883630, processId=7255, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], eventCreateTime=0, eventSendTime=1714033883944)
[WI-0][TI-1849] - [INFO] 2024-04-25 16:31:24.347 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1849, success=true)
[WI-0][TI-1849] - [INFO] 2024-04-25 16:31:24.350 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1849, success=true)
[WI-0][TI-0] - [WARN] 2024-04-25 16:31:32.518 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[147] - [id: 0x0557cfab, L:/172.18.1.1:1234 - R:/172.18.0.11:53034] is not writable, over high water level : 65536
[WI-0][TI-0] - [WARN] 2024-04-25 16:31:32.521 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[154] - [id: 0x0557cfab, L:/172.18.1.1:1234 - R:/172.18.0.11:53034] is writable, to low water : 32768
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:34.316 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7513812154696133 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:35.355 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7423822714681441 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:31:36.357 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7880597014925372 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1792] - [INFO] 2024-04-25 16:33:10.843 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1792, processInstanceId=661, status=9, startTime=1714031949850, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/661/1792.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792, endTime=1714032332757, processId=4479, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714033690771)
[WI-0][TI-1792] - [INFO] 2024-04-25 16:33:10.846 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1792, processInstanceId=661, status=9, startTime=1714031949850, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/661/1792.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792, endTime=1714032332757, processId=4479, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714033990843)
[WI-0][TI-1790] - [INFO] 2024-04-25 16:33:10.847 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1790, processInstanceId=660, status=9, startTime=1714031469392, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1790.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790, endTime=1714032269172, processId=4321, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714033690771)
[WI-0][TI-1790] - [INFO] 2024-04-25 16:33:10.849 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1790, processInstanceId=660, status=9, startTime=1714031469392, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1790.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790, endTime=1714032269172, processId=4321, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714033990843)
[WI-0][TI-0] - [INFO] 2024-04-25 16:35:29.320 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7210682492581602 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:35:30.361 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8900523560209425 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1792] - [INFO] 2024-04-25 16:38:11.683 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1792, processInstanceId=661, status=9, startTime=1714031949850, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/661/1792.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792, endTime=1714032332757, processId=4479, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714033990843)
[WI-0][TI-1792] - [INFO] 2024-04-25 16:38:11.691 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1792, processInstanceId=661, status=9, startTime=1714031949850, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/661/1792.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792, endTime=1714032332757, processId=4479, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714034291683)
[WI-0][TI-1790] - [INFO] 2024-04-25 16:38:11.691 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1790, processInstanceId=660, status=9, startTime=1714031469392, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1790.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790, endTime=1714032269172, processId=4321, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714033990843)
[WI-0][TI-1790] - [INFO] 2024-04-25 16:38:11.699 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1790, processInstanceId=660, status=9, startTime=1714031469392, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1790.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790, endTime=1714032269172, processId=4321, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714034291683)
[WI-0][TI-0] - [INFO] 2024-04-25 16:38:22.953 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7028571428571428 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:40:03.701 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.824022346368715 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:40:36.846 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8587570621468926 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:09.933 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1850, taskName=search for intake JSON files, firstSubmitTime=1714034469926, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=116, appIds=null, processInstanceId=681, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1850'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340113777664'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425164109'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='681'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:09.937 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake JSON files to wait queue success
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:09.984 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.016 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.016 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.016 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.017 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714034470016
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.017 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 681_1850
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.018 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1850,
  "taskName" : "search for intake JSON files",
  "firstSubmitTime" : 1714034469926,
  "startTime" : 1714034470016,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1850.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 116,
  "processInstanceId" : 681,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set raw_file_dir to null\\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1850"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340113777664"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425164109"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "681"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "681_1850",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.019 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.019 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.020 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.035 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.035 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.036 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1850 check successfully
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.036 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.038 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.038 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.038 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.039 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"",
  "resourceList" : [ ]
}
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.039 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.039 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.039 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.039 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.039 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.040 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.041 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.041 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/reddit/in -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set raw_file_dir to null
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
fi

echo "#{setValue(raw_file_dir=${raw_file_dir})}"
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.041 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.041 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1850/681_1850.sh
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:10.082 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 7594
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:10.082 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1850] - [INFO] 2024-04-25 16:41:10.806 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1850, processInstanceId=681, startTime=1714034470016, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1850.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1850] - [INFO] 2024-04-25 16:41:10.809 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1850, processInstanceId=681, startTime=1714034470016, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1850.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714034470805)
[WI-0][TI-1850] - [INFO] 2024-04-25 16:41:10.809 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1850, processInstanceId=681, startTime=1714034470016, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1850] - [INFO] 2024-04-25 16:41:10.811 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1850, processInstanceId=681, startTime=1714034470016, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714034470805)
[WI-0][TI-1850] - [INFO] 2024-04-25 16:41:10.880 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1850, success=true)
[WI-0][TI-1850] - [INFO] 2024-04-25 16:41:10.884 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1850)
[WI-0][TI-1850] - [INFO] 2024-04-25 16:41:10.890 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1850, success=true)
[WI-0][TI-1850] - [INFO] 2024-04-25 16:41:10.897 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1850)
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:11.102 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	#{setValue(raw_file_dir=/local_storage/reddit/in/676-20240425162748.json)}
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:11.104 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1850, processId:7594 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:11.107 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:11.107 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:11.107 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:11.107 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:11.113 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:11.116 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:11.116 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1850
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:11.117 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1850
[WI-681][TI-1850] - [INFO] 2024-04-25 16:41:11.118 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1850] - [INFO] 2024-04-25 16:41:11.812 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1850, processInstanceId=681, status=7, startTime=1714034470016, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1850.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1850, endTime=1714034471107, processId=7594, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":"/local_storage/reddit/in/676-20240425162748.json"}], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1850] - [INFO] 2024-04-25 16:41:11.820 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1850, processInstanceId=681, status=7, startTime=1714034470016, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1850.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1850, endTime=1714034471107, processId=7594, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":"/local_storage/reddit/in/676-20240425162748.json"}], eventCreateTime=0, eventSendTime=1714034471812)
[WI-0][TI-1850] - [INFO] 2024-04-25 16:41:11.857 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1850, success=true)
[WI-0][TI-1850] - [INFO] 2024-04-25 16:41:11.860 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1850, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:12.992 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1852, taskName=move to processing, firstSubmitTime=1714034472968, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=116, appIds=null, processInstanceId=681, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to processing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1852'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340390094208'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425164112'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='681'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/in/676-20240425162748.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/in/676-20240425162748.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.005 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to processing to wait queue success
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.006 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.007 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.007 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.007 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.007 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714034473007
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.008 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 681_1852
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.008 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1852,
  "taskName" : "move to processing",
  "firstSubmitTime" : 1714034472968,
  "startTime" : 1714034473007,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1852.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 116,
  "processInstanceId" : 681,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the reddit processing folder\\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\\nfilename=$(basename \\\"${raw_file_dir}\\\")\\nif ! grep -q \\\"${REDDIT_HOME}/processing\\\" <<< \\\"${raw_file_dir}\\\"; then\\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\\nelse\\n    echo JSON is already in processing\\nfi\\n\\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\\necho \\\"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to processing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1852"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340390094208"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425164112"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "681"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/in/676-20240425162748.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "681_1852",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/in/676-20240425162748.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.009 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.009 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.009 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.017 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.018 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.018 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1852 check successfully
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.018 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.018 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.020 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.020 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.020 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"",
  "resourceList" : [ ]
}
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.020 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.021 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/in/676-20240425162748.json"}] successfully
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.021 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.021 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.021 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.022 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.022 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.022 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the reddit processing folder
# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error
filename=$(basename "/local_storage/reddit/in/676-20240425162748.json")
if ! grep -q "/local_storage/reddit/processing" <<< "/local_storage/reddit/in/676-20240425162748.json"; then
    mv /local_storage/reddit/in/676-20240425162748.json /local_storage/reddit/processing
else
    echo JSON is already in processing
fi

# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing
echo "#{setValue(raw_file_dir=/local_storage/reddit/processing/${filename})}"
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.022 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.022 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1852/681_1852.sh
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:13.049 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 7608
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:13.050 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1852] - [INFO] 2024-04-25 16:41:13.823 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1852, processInstanceId=681, startTime=1714034473007, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1852.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1852] - [INFO] 2024-04-25 16:41:13.826 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1852, processInstanceId=681, startTime=1714034473007, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1852.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714034473821)
[WI-0][TI-1852] - [INFO] 2024-04-25 16:41:13.827 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1852, processInstanceId=681, startTime=1714034473007, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1852] - [INFO] 2024-04-25 16:41:13.829 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1852, processInstanceId=681, startTime=1714034473007, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714034473821)
[WI-0][TI-1852] - [INFO] 2024-04-25 16:41:13.861 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1852, success=true)
[WI-0][TI-1852] - [INFO] 2024-04-25 16:41:13.868 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1852)
[WI-0][TI-1852] - [INFO] 2024-04-25 16:41:13.877 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1852, success=true)
[WI-0][TI-1852] - [INFO] 2024-04-25 16:41:13.883 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1852)
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:14.058 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	#{setValue(raw_file_dir=/local_storage/reddit/processing/676-20240425162748.json)}
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:14.060 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1852, processId:7608 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:14.062 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:14.062 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:14.062 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:14.062 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:14.066 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:14.066 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:14.066 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1852
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:14.067 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1852
[WI-681][TI-1852] - [INFO] 2024-04-25 16:41:14.067 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1852] - [INFO] 2024-04-25 16:41:14.831 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1852, processInstanceId=681, status=7, startTime=1714034473007, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1852.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1852, endTime=1714034474062, processId=7608, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1852] - [INFO] 2024-04-25 16:41:14.833 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1852, processInstanceId=681, status=7, startTime=1714034473007, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1852.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1852, endTime=1714034474062, processId=7608, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], eventCreateTime=0, eventSendTime=1714034474831)
[WI-0][TI-1852] - [INFO] 2024-04-25 16:41:14.862 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1852, success=true)
[WI-0][TI-1852] - [INFO] 2024-04-25 16:41:14.868 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1852, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:14.948 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1853, taskName=preprocessing and kafka, firstSubmitTime=1714034474935, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=116, appIds=null, processInstanceId=681, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_reddit_preprocessing.py\n\n#     --jars spark_packages/spark-sql-kafka-0-10_2.12-3.5.1.jar \\","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='preprocessing and kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1853'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13210058002752'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425164114'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='681'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/676-20240425162748.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.949 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: preprocessing and kafka to wait queue success
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.957 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.959 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.959 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.959 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.959 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714034474959
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.959 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 681_1853
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.959 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1853,
  "taskName" : "preprocessing and kafka",
  "firstSubmitTime" : 1714034474935,
  "startTime" : 1714034474959,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1853.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 116,
  "processInstanceId" : 681,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name reddit_preprocessing \\\\\\n    --files ${raw_file_dir} \\\\\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\\\\n    --conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n    spark_reddit_preprocessing.py\\n\\n#     --jars spark_packages/spark-sql-kafka-0-10_2.12-3.5.1.jar \\\\\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocessing and kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1853"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13210058002752"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425164114"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "681"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/676-20240425162748.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "681_1853",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/676-20240425162748.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.960 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.960 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.960 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.961 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.961 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.962 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1853 check successfully
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.962 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.964 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py, resourceRelativePath=spark_reddit_preprocessing.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1853/spark_reddit_preprocessing.py)})
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.965 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.965 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.965 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_reddit_preprocessing.py\n\n#     --jars spark_packages/spark-sql-kafka-0-10_2.12-3.5.1.jar \\",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py",
    "res" : null
  } ]
}
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.965 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.965 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}] successfully
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.965 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.965 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.965 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.965 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.965 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.965 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master local \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name reddit_preprocessing \
    --files /local_storage/reddit/processing/676-20240425162748.json \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
    --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
    spark_reddit_preprocessing.py

#     --jars spark_packages/spark-sql-kafka-0-10_2.12-3.5.1.jar \
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.966 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.966 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1853/681_1853.sh
[WI-681][TI-1853] - [INFO] 2024-04-25 16:41:14.970 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 7621
[WI-0][TI-1853] - [INFO] 2024-04-25 16:41:15.838 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1853, processInstanceId=681, startTime=1714034474959, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1853.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1853] - [INFO] 2024-04-25 16:41:15.840 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1853, processInstanceId=681, startTime=1714034474959, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1853.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714034475837)
[WI-0][TI-1853] - [INFO] 2024-04-25 16:41:15.840 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1853, processInstanceId=681, startTime=1714034474959, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1853] - [INFO] 2024-04-25 16:41:15.842 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1853, processInstanceId=681, startTime=1714034474959, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714034475837)
[WI-0][TI-1853] - [INFO] 2024-04-25 16:41:15.864 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1853, success=true)
[WI-0][TI-1853] - [INFO] 2024-04-25 16:41:15.868 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1853)
[WI-0][TI-1853] - [INFO] 2024-04-25 16:41:15.876 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1853, success=true)
[WI-0][TI-1853] - [INFO] 2024-04-25 16:41:15.881 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1853)
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:15.971 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:16.979 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:17.988 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-d56ca159-ffd7-46c2-964a-2915833f4cf2;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:23.994 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:24.997 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.jar ...
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:25.059 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8518518518518519 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:26.070 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8023952095808383 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:34.010 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1!spark-sql-kafka-0-10_2.12.jar (8818ms)
	downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.1/spark-token-provider-kafka-0-10_2.12-3.5.1.jar ...
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:36.015 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1!spark-token-provider-kafka-0-10_2.12.jar (2267ms)
	:: resolution report :: resolve 7658ms :: artifacts dl 11117ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   2   |   2   |   0   ||   11  |   2   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-d56ca159-ffd7-46c2-964a-2915833f4cf2
		confs: [default]
		2 artifacts copied, 9 already retrieved (477kB/39ms)
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:37.018 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:41:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:37.116 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8652037617554859 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:38.019 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:41:37 INFO SparkContext: Running Spark version 3.5.1
	24/04/25 16:41:37 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/04/25 16:41:37 INFO SparkContext: Java version 1.8.0_402
	24/04/25 16:41:37 INFO ResourceUtils: ==============================================================
	24/04/25 16:41:37 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/04/25 16:41:37 INFO ResourceUtils: ==============================================================
	24/04/25 16:41:37 INFO SparkContext: Submitted application: reddit_preprocessing
	24/04/25 16:41:37 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/04/25 16:41:38 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:38.127 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7509727626459144 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:39.025 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:41:38 INFO ResourceProfileManager: Added ResourceProfile id: 0
	24/04/25 16:41:38 INFO SecurityManager: Changing view acls to: default
	24/04/25 16:41:38 INFO SecurityManager: Changing modify acls to: default
	24/04/25 16:41:38 INFO SecurityManager: Changing view acls groups to: 
	24/04/25 16:41:38 INFO SecurityManager: Changing modify acls groups to: 
	24/04/25 16:41:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
	24/04/25 16:41:38 INFO Utils: Successfully started service 'sparkDriver' on port 35561.
	24/04/25 16:41:38 INFO SparkEnv: Registering MapOutputTracker
	24/04/25 16:41:38 INFO SparkEnv: Registering BlockManagerMaster
	24/04/25 16:41:38 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/04/25 16:41:38 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/04/25 16:41:38 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/04/25 16:41:38 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-65f67a90-ef5e-42ec-8863-ce202928f4b3
	24/04/25 16:41:38 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:39.144 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9130434782608695 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:40.026 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:41:39 INFO SparkEnv: Registering OutputCommitCoordinator
	24/04/25 16:41:39 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/04/25 16:41:39 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/04/25 16:41:39 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://15dedf891bed:35561/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714034497808
	24/04/25 16:41:39 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://15dedf891bed:35561/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714034497808
	24/04/25 16:41:39 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://15dedf891bed:35561/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714034497808
	24/04/25 16:41:39 INFO SparkContext: Added JAR file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://15dedf891bed:35561/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714034497808
	24/04/25 16:41:39 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://15dedf891bed:35561/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714034497808
	24/04/25 16:41:39 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://15dedf891bed:35561/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714034497808
	24/04/25 16:41:39 INFO SparkContext: Added JAR file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at spark://15dedf891bed:35561/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714034497808
	24/04/25 16:41:39 INFO SparkContext: Added JAR file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://15dedf891bed:35561/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714034497808
	24/04/25 16:41:39 INFO SparkContext: Added JAR file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://15dedf891bed:35561/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714034497808
	24/04/25 16:41:39 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://15dedf891bed:35561/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714034497808
	24/04/25 16:41:39 INFO SparkContext: Added JAR file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at spark://15dedf891bed:35561/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714034497808
	24/04/25 16:41:39 INFO SparkContext: Added file file:///local_storage/reddit/processing/676-20240425162748.json at file:///local_storage/reddit/processing/676-20240425162748.json with timestamp 1714034497808
	24/04/25 16:41:39 INFO Utils: Copying /local_storage/reddit/processing/676-20240425162748.json to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/676-20240425162748.json
	24/04/25 16:41:39 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714034497808
	24/04/25 16:41:39 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/04/25 16:41:39 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714034497808
	24/04/25 16:41:39 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/04/25 16:41:39 INFO SparkContext: Added file file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714034497808
	24/04/25 16:41:39 INFO Utils: Copying /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/25 16:41:39 INFO SparkContext: Added file file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714034497808
	24/04/25 16:41:39 INFO Utils: Copying /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/25 16:41:39 INFO SparkContext: Added file file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714034497808
	24/04/25 16:41:39 INFO Utils: Copying /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/25 16:41:39 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714034497808
	24/04/25 16:41:39 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:40.148 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9256756756756757 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:41.028 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:41:40 INFO SparkContext: Added file file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714034497808
	24/04/25 16:41:40 INFO Utils: Copying /tmp/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.lz4_lz4-java-1.8.0.jar
	24/04/25 16:41:40 INFO SparkContext: Added file file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714034497808
	24/04/25 16:41:40 INFO Utils: Copying /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/25 16:41:40 INFO SparkContext: Added file file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714034497808
	24/04/25 16:41:40 INFO Utils: Copying /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.slf4j_slf4j-api-2.0.7.jar
	24/04/25 16:41:40 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714034497808
	24/04/25 16:41:40 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/25 16:41:40 INFO SparkContext: Added file file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714034497808
	24/04/25 16:41:40 INFO Utils: Copying /tmp/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/commons-logging_commons-logging-1.1.3.jar
	24/04/25 16:41:40 INFO Executor: Starting executor ID driver on host 15dedf891bed
	24/04/25 16:41:40 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/04/25 16:41:40 INFO Executor: Java version 1.8.0_402
	24/04/25 16:41:40 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/04/25 16:41:40 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@230c0d51 for default.
	24/04/25 16:41:40 INFO Executor: Fetching file:///local_storage/reddit/processing/676-20240425162748.json with timestamp 1714034497808
	24/04/25 16:41:40 INFO Utils: /local_storage/reddit/processing/676-20240425162748.json has been previously copied to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/676-20240425162748.json
	24/04/25 16:41:40 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714034497808
	24/04/25 16:41:40 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/25 16:41:40 INFO Executor: Fetching file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714034497808
	24/04/25 16:41:40 INFO Utils: /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/25 16:41:40 INFO Executor: Fetching file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714034497808
	24/04/25 16:41:40 INFO Utils: /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/25 16:41:40 INFO Executor: Fetching file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714034497808
	24/04/25 16:41:40 INFO Utils: /tmp/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.lz4_lz4-java-1.8.0.jar
	24/04/25 16:41:40 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714034497808
	24/04/25 16:41:40 INFO Utils: /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/04/25 16:41:40 INFO Executor: Fetching file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714034497808
	24/04/25 16:41:40 INFO Utils: /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/25 16:41:40 INFO Executor: Fetching file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714034497808
	24/04/25 16:41:40 INFO Utils: /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.slf4j_slf4j-api-2.0.7.jar
	24/04/25 16:41:40 INFO Executor: Fetching file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714034497808
	24/04/25 16:41:40 INFO Utils: /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/25 16:41:40 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714034497808
	24/04/25 16:41:40 INFO Utils: /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/04/25 16:41:40 INFO Executor: Fetching file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714034497808
	24/04/25 16:41:40 INFO Utils: /tmp/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/commons-logging_commons-logging-1.1.3.jar
	24/04/25 16:41:40 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714034497808
	24/04/25 16:41:40 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/25 16:41:40 INFO Executor: Fetching spark://15dedf891bed:35561/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714034497808
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:41.150 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9327217125382263 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:42.029 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:41:41 INFO TransportClientFactory: Successfully created connection to 15dedf891bed/172.18.1.1:35561 after 80 ms (0 ms spent in bootstraps)
	24/04/25 16:41:41 INFO Utils: Fetching spark://15dedf891bed:35561/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/fetchFileTemp26187770558672843.tmp
	24/04/25 16:41:41 INFO Utils: /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/fetchFileTemp26187770558672843.tmp has been previously copied to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/25 16:41:41 INFO Executor: Adding file:/tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
	24/04/25 16:41:41 INFO Executor: Fetching spark://15dedf891bed:35561/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714034497808
	24/04/25 16:41:41 INFO Utils: Fetching spark://15dedf891bed:35561/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/fetchFileTemp7782344015115836119.tmp
	24/04/25 16:41:41 INFO Utils: /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/fetchFileTemp7782344015115836119.tmp has been previously copied to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/25 16:41:41 INFO Executor: Adding file:/tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
	24/04/25 16:41:41 INFO Executor: Fetching spark://15dedf891bed:35561/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714034497808
	24/04/25 16:41:41 INFO Utils: Fetching spark://15dedf891bed:35561/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/fetchFileTemp5625374920659206980.tmp
	24/04/25 16:41:41 INFO Utils: /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/fetchFileTemp5625374920659206980.tmp has been previously copied to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.slf4j_slf4j-api-2.0.7.jar
	24/04/25 16:41:41 INFO Executor: Adding file:/tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.slf4j_slf4j-api-2.0.7.jar to class loader default
	24/04/25 16:41:41 INFO Executor: Fetching spark://15dedf891bed:35561/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714034497808
	24/04/25 16:41:41 INFO Utils: Fetching spark://15dedf891bed:35561/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/fetchFileTemp8341737969725546919.tmp
	24/04/25 16:41:41 INFO Utils: /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/fetchFileTemp8341737969725546919.tmp has been previously copied to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/25 16:41:41 INFO Executor: Adding file:/tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
	24/04/25 16:41:41 INFO Executor: Fetching spark://15dedf891bed:35561/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714034497808
	24/04/25 16:41:41 INFO Utils: Fetching spark://15dedf891bed:35561/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/fetchFileTemp1045752622239022585.tmp
	24/04/25 16:41:41 INFO Utils: /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/fetchFileTemp1045752622239022585.tmp has been previously copied to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.lz4_lz4-java-1.8.0.jar
	24/04/25 16:41:41 INFO Executor: Adding file:/tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.lz4_lz4-java-1.8.0.jar to class loader default
	24/04/25 16:41:41 INFO Executor: Fetching spark://15dedf891bed:35561/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714034497808
	24/04/25 16:41:41 INFO Utils: Fetching spark://15dedf891bed:35561/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/fetchFileTemp3806280936149345018.tmp
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:42.159 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9098591549295775 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:43.032 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:41:42 INFO Utils: /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/fetchFileTemp3806280936149345018.tmp has been previously copied to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/25 16:41:42 INFO Executor: Adding file:/tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
	24/04/25 16:41:42 INFO Executor: Fetching spark://15dedf891bed:35561/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714034497808
	24/04/25 16:41:42 INFO Utils: Fetching spark://15dedf891bed:35561/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/fetchFileTemp2712740089498415006.tmp
	24/04/25 16:41:42 INFO Utils: /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/fetchFileTemp2712740089498415006.tmp has been previously copied to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/25 16:41:42 INFO Executor: Adding file:/tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
	24/04/25 16:41:42 INFO Executor: Fetching spark://15dedf891bed:35561/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714034497808
	24/04/25 16:41:42 INFO Utils: Fetching spark://15dedf891bed:35561/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/fetchFileTemp284527892359662924.tmp
	24/04/25 16:41:42 INFO Utils: /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/fetchFileTemp284527892359662924.tmp has been previously copied to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/04/25 16:41:42 INFO Executor: Adding file:/tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
	24/04/25 16:41:42 INFO Executor: Fetching spark://15dedf891bed:35561/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714034497808
	24/04/25 16:41:42 INFO Utils: Fetching spark://15dedf891bed:35561/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/fetchFileTemp2900091844424177553.tmp
	24/04/25 16:41:42 INFO Utils: /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/fetchFileTemp2900091844424177553.tmp has been previously copied to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/commons-logging_commons-logging-1.1.3.jar
	24/04/25 16:41:42 INFO Executor: Adding file:/tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/commons-logging_commons-logging-1.1.3.jar to class loader default
	24/04/25 16:41:42 INFO Executor: Fetching spark://15dedf891bed:35561/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714034497808
	24/04/25 16:41:42 INFO Utils: Fetching spark://15dedf891bed:35561/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/fetchFileTemp3687151664725646024.tmp
	24/04/25 16:41:42 INFO Utils: /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/fetchFileTemp3687151664725646024.tmp has been previously copied to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/04/25 16:41:42 INFO Executor: Adding file:/tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
	24/04/25 16:41:42 INFO Executor: Fetching spark://15dedf891bed:35561/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714034497808
	24/04/25 16:41:42 INFO Utils: Fetching spark://15dedf891bed:35561/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/fetchFileTemp2214776664189514279.tmp
	24/04/25 16:41:42 INFO Utils: /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/fetchFileTemp2214776664189514279.tmp has been previously copied to /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/25 16:41:42 INFO Executor: Adding file:/tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
	24/04/25 16:41:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39985.
	24/04/25 16:41:42 INFO NettyBlockTransferService: Server created on 15dedf891bed:39985
	24/04/25 16:41:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/04/25 16:41:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 15dedf891bed, 39985, None)
	24/04/25 16:41:42 INFO BlockManagerMasterEndpoint: Registering block manager 15dedf891bed:39985 with 366.3 MiB RAM, BlockManagerId(driver, 15dedf891bed, 39985, None)
	24/04/25 16:41:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 15dedf891bed, 39985, None)
	24/04/25 16:41:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 15dedf891bed, 39985, None)
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:43.160 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8233695652173912 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:44.035 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	
	
	 /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/676-20240425162748.json 
	
	
	
	24/04/25 16:41:43 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 345.7 KiB, free 366.0 MiB)
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:44.162 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7945945945945947 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:45.036 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:41:44 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 365.9 MiB)
	24/04/25 16:41:44 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 15dedf891bed:39985 (size: 32.6 KiB, free: 366.3 MiB)
	24/04/25 16:41:44 INFO SparkContext: Created broadcast 0 from wholeTextFiles at NativeMethodAccessorImpl.java:0
	24/04/25 16:41:44 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/04/25 16:41:44 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1853/spark-warehouse'.
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:45.166 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8390501319261214 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:46.168 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8804347826086956 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:47.169 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9226666666666667 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:48.181 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8179419525065963 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:49.188 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7128378378378377 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:50.189 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7268518518518519 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:51.086 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:41:50 INFO CodeGenerator: Code generated in 622.944652 ms
	24/04/25 16:41:51 INFO FileInputFormat: Total input files to process : 1
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:51.220 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8591065292096219 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:52.091 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:41:51 INFO FileInputFormat: Total input files to process : 1
	24/04/25 16:41:51 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
	24/04/25 16:41:51 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/25 16:41:51 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
	24/04/25 16:41:51 INFO DAGScheduler: Parents of final stage: List()
	24/04/25 16:41:51 INFO DAGScheduler: Missing parents: List()
	24/04/25 16:41:51 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/25 16:41:51 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 28.1 KiB, free 365.9 MiB)
	24/04/25 16:41:51 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.4 KiB, free 365.9 MiB)
	24/04/25 16:41:51 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 15dedf891bed:39985 (size: 12.4 KiB, free: 366.3 MiB)
	24/04/25 16:41:51 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
	24/04/25 16:41:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/25 16:41:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
	24/04/25 16:41:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (15dedf891bed, executor driver, partition 0, PROCESS_LOCAL, 9885 bytes) 
	24/04/25 16:41:51 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:52.223 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.91699604743083 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:53.095 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:41:52 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/676-20240425162748.json:0+186591
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:53.226 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8903654485049834 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:54.096 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:41:53 INFO CodeGenerator: Code generated in 125.361837 ms
	24/04/25 16:41:53 INFO PythonRunner: Times: total = 872, boot = 615, init = 229, finish = 28
	24/04/25 16:41:53 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2794 bytes result sent to driver
	24/04/25 16:41:53 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1916 ms on 15dedf891bed (executor driver) (1/1)
	24/04/25 16:41:53 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
	24/04/25 16:41:53 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 57199
	24/04/25 16:41:53 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 2.320 s
	24/04/25 16:41:53 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/25 16:41:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
	24/04/25 16:41:53 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 2.671823 s
	
	
	
	 1 
	
	
	
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:54.228 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8400000000000001 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:55.097 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	
	
	 2 
	
	
	
	
	
	
	 3 
	
	
	
	
	
	
	 4 
	
	
	
	24/04/25 16:41:54 INFO CodeGenerator: Code generated in 84.702501 ms
	24/04/25 16:41:54 INFO CodeGenerator: Code generated in 15.872793 ms
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:55.230 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8614958448753464 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:56.105 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:41:55 INFO SparkContext: Starting job: collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1853/spark_reddit_preprocessing.py:84
	24/04/25 16:41:55 INFO DAGScheduler: Got job 1 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1853/spark_reddit_preprocessing.py:84) with 1 output partitions
	24/04/25 16:41:55 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1853/spark_reddit_preprocessing.py:84)
	24/04/25 16:41:55 INFO DAGScheduler: Parents of final stage: List()
	24/04/25 16:41:55 INFO DAGScheduler: Missing parents: List()
	24/04/25 16:41:55 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[19] at toJavaRDD at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/25 16:41:55 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 72.0 KiB, free 365.8 MiB)
	24/04/25 16:41:55 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.6 KiB, free 365.8 MiB)
	24/04/25 16:41:55 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 15dedf891bed:39985 (size: 28.6 KiB, free: 366.2 MiB)
	24/04/25 16:41:55 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
	24/04/25 16:41:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[19] at toJavaRDD at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/25 16:41:55 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
	24/04/25 16:41:55 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (15dedf891bed, executor driver, partition 0, PROCESS_LOCAL, 9885 bytes) 
	24/04/25 16:41:55 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
	24/04/25 16:41:55 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/userFiles-fe3e3f3c-ae26-40f3-a2ac-96a2a34c39ff/676-20240425162748.json:0+186591
	24/04/25 16:41:55 INFO CodeGenerator: Code generated in 92.832997 ms
	24/04/25 16:41:55 INFO CodeGenerator: Code generated in 28.164468 ms
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:56.461 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8856209150326797 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:57.129 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:41:56 INFO CodeGenerator: Code generated in 124.667294 ms
	24/04/25 16:41:56 INFO CodeGenerator: Code generated in 17.665222 ms
	24/04/25 16:41:56 INFO PythonRunner: Times: total = 189, boot = -1849, init = 2013, finish = 25
	24/04/25 16:41:56 INFO CodeGenerator: Code generated in 22.527572 ms
	24/04/25 16:41:57 INFO CodeGenerator: Code generated in 96.622308 ms
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:57.470 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9498207885304659 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:58.133 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:41:57 INFO PythonUDFRunner: Times: total = 1046, boot = 678, init = 367, finish = 1
	24/04/25 16:41:57 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 172332 bytes result sent to driver
	24/04/25 16:41:57 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2002 ms on 15dedf891bed (executor driver) (1/1)
	24/04/25 16:41:57 INFO DAGScheduler: ResultStage 1 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1853/spark_reddit_preprocessing.py:84) finished in 2.070 s
	24/04/25 16:41:57 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/25 16:41:57 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
	24/04/25 16:41:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
	24/04/25 16:41:57 INFO DAGScheduler: Job 1 finished: collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1853/spark_reddit_preprocessing.py:84, took 2.099789 s
	24/04/25 16:41:57 INFO CodeGenerator: Code generated in 7.467614 ms
	24/04/25 16:41:57 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/25 16:41:57 INFO DAGScheduler: Got job 2 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/25 16:41:57 INFO DAGScheduler: Final stage: ResultStage 2 (save at NativeMethodAccessorImpl.java:0)
	24/04/25 16:41:57 INFO DAGScheduler: Parents of final stage: List()
	24/04/25 16:41:58 INFO DAGScheduler: Missing parents: List()
	24/04/25 16:41:58 INFO DAGScheduler: Submitting ResultStage 2 (SQLExecutionRDD[26] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/25 16:41:58 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 20.6 KiB, free 365.8 MiB)
	24/04/25 16:41:58 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 365.8 MiB)
	24/04/25 16:41:58 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 15dedf891bed:39985 (size: 9.0 KiB, free: 366.2 MiB)
	24/04/25 16:41:58 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
	24/04/25 16:41:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (SQLExecutionRDD[26] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/25 16:41:58 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
	24/04/25 16:41:58 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (15dedf891bed, executor driver, partition 0, PROCESS_LOCAL, 10438 bytes) 
	24/04/25 16:41:58 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:58.485 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9550719456609169 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:59.139 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:41:58 INFO CodeGenerator: Code generated in 21.679699 ms
	24/04/25 16:41:58 INFO CodeGenerator: Code generated in 52.555337 ms
	24/04/25 16:41:58 INFO ProducerConfig: ProducerConfig values: 
		acks = -1
		auto.include.jmx.reporter = true
		batch.size = 16384
		bootstrap.servers = [kafka:9092]
		buffer.memory = 33554432
		client.dns.lookup = use_all_dns_ips
		client.id = producer-1
		compression.type = none
		connections.max.idle.ms = 540000
		delivery.timeout.ms = 120000
		enable.idempotence = true
		interceptor.classes = []
		key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
		linger.ms = 0
		max.block.ms = 60000
		max.in.flight.requests.per.connection = 5
		max.request.size = 1048576
		metadata.max.age.ms = 300000
		metadata.max.idle.ms = 300000
		metric.reporters = []
		metrics.num.samples = 2
		metrics.recording.level = INFO
		metrics.sample.window.ms = 30000
		partitioner.adaptive.partitioning.enable = true
		partitioner.availability.timeout.ms = 0
		partitioner.class = null
		partitioner.ignore.keys = false
		receive.buffer.bytes = 32768
		reconnect.backoff.max.ms = 1000
		reconnect.backoff.ms = 50
		request.timeout.ms = 30000
		retries = 2147483647
		retry.backoff.ms = 100
		sasl.client.callback.handler.class = null
		sasl.jaas.config = null
		sasl.kerberos.kinit.cmd = /usr/bin/kinit
		sasl.kerberos.min.time.before.relogin = 60000
		sasl.kerberos.service.name = null
		sasl.kerberos.ticket.renew.jitter = 0.05
		sasl.kerberos.ticket.renew.window.factor = 0.8
		sasl.login.callback.handler.class = null
		sasl.login.class = null
		sasl.login.connect.timeout.ms = null
		sasl.login.read.timeout.ms = null
		sasl.login.refresh.buffer.seconds = 300
		sasl.login.refresh.min.period.seconds = 60
		sasl.login.refresh.window.factor = 0.8
		sasl.login.refresh.window.jitter = 0.05
		sasl.login.retry.backoff.max.ms = 10000
		sasl.login.retry.backoff.ms = 100
		sasl.mechanism = GSSAPI
		sasl.oauthbearer.clock.skew.seconds = 30
		sasl.oauthbearer.expected.audience = null
		sasl.oauthbearer.expected.issuer = null
		sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
		sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
		sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
		sasl.oauthbearer.jwks.endpoint.url = null
		sasl.oauthbearer.scope.claim.name = scope
		sasl.oauthbearer.sub.claim.name = sub
		sasl.oauthbearer.token.endpoint.url = null
		security.protocol = PLAINTEXT
		security.providers = null
		send.buffer.bytes = 131072
		socket.connection.setup.timeout.max.ms = 30000
		socket.connection.setup.timeout.ms = 10000
		ssl.cipher.suites = null
		ssl.enabled.protocols = [TLSv1.2]
		ssl.endpoint.identification.algorithm = https
		ssl.engine.factory.class = null
		ssl.key.password = null
		ssl.keymanager.algorithm = SunX509
		ssl.keystore.certificate.chain = null
		ssl.keystore.key = null
		ssl.keystore.location = null
		ssl.keystore.password = null
		ssl.keystore.type = JKS
		ssl.protocol = TLSv1.2
		ssl.provider = null
		ssl.secure.random.implementation = null
		ssl.trustmanager.algorithm = PKIX
		ssl.truststore.certificates = null
		ssl.truststore.location = null
		ssl.truststore.password = null
		ssl.truststore.type = JKS
		transaction.timeout.ms = 60000
		transactional.id = null
		value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	
	24/04/25 16:41:58 INFO KafkaProducer: [Producer clientId=producer-1] Instantiated an idempotent producer.
	24/04/25 16:41:59 INFO AppInfoParser: Kafka version: 3.4.1
	24/04/25 16:41:59 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
	24/04/25 16:41:59 INFO AppInfoParser: Kafka startTimeMs: 1714034519016
	24/04/25 16:41:59 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 15dedf891bed:39985 in memory (size: 12.4 KiB, free: 366.2 MiB)
	24/04/25 16:41:59 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 15dedf891bed:39985 in memory (size: 28.6 KiB, free: 366.3 MiB)
[WI-0][TI-0] - [INFO] 2024-04-25 16:41:59.508 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.864578301211041 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:00.145 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:41:59 WARN NetworkClient: [Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {reddit_preprocessed=LEADER_NOT_AVAILABLE}
	24/04/25 16:41:59 INFO Metadata: [Producer clientId=producer-1] Cluster ID: 48nq0XTNRoqMTNfMjfKvtg
	24/04/25 16:41:59 INFO TransactionManager: [Producer clientId=producer-1] ProducerId set to 0 with epoch 0
	24/04/25 16:41:59 INFO Metadata: [Producer clientId=producer-1] Resetting the last seen epoch of partition reddit_preprocessed-0 to 0 since the associated topicId changed from null to tSxLTfcgQe6KuiaQjcDezw
	24/04/25 16:41:59 INFO PythonRunner: Times: total = 237, boot = -2420, init = 2657, finish = 0
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:01.148 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:42:00 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1664 bytes result sent to driver
	24/04/25 16:42:00 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2130 ms on 15dedf891bed (executor driver) (1/1)
	24/04/25 16:42:00 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
	24/04/25 16:42:00 INFO DAGScheduler: ResultStage 2 (save at NativeMethodAccessorImpl.java:0) finished in 2.156 s
	24/04/25 16:42:00 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/25 16:42:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
	24/04/25 16:42:00 INFO DAGScheduler: Job 2 finished: save at NativeMethodAccessorImpl.java:0, took 2.208903 s
	24/04/25 16:42:00 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/25 16:42:00 INFO DAGScheduler: Got job 3 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/25 16:42:00 INFO DAGScheduler: Final stage: ResultStage 3 (save at NativeMethodAccessorImpl.java:0)
	24/04/25 16:42:00 INFO DAGScheduler: Parents of final stage: List()
	24/04/25 16:42:00 INFO DAGScheduler: Missing parents: List()
	24/04/25 16:42:00 INFO DAGScheduler: Submitting ResultStage 3 (SQLExecutionRDD[33] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/25 16:42:00 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 20.6 KiB, free 365.9 MiB)
	24/04/25 16:42:00 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 365.9 MiB)
	24/04/25 16:42:00 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 15dedf891bed:39985 (size: 9.0 KiB, free: 366.3 MiB)
	24/04/25 16:42:00 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
	24/04/25 16:42:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (SQLExecutionRDD[33] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/25 16:42:00 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
	24/04/25 16:42:00 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (15dedf891bed, executor driver, partition 0, PROCESS_LOCAL, 10715 bytes) 
	24/04/25 16:42:00 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
	24/04/25 16:42:00 INFO PythonRunner: Times: total = 61, boot = -1898, init = 1959, finish = 0
	24/04/25 16:42:00 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1664 bytes result sent to driver
	24/04/25 16:42:00 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 134 ms on 15dedf891bed (executor driver) (1/1)
	24/04/25 16:42:00 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
	24/04/25 16:42:00 INFO DAGScheduler: ResultStage 3 (save at NativeMethodAccessorImpl.java:0) finished in 0.150 s
	24/04/25 16:42:00 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/25 16:42:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
	24/04/25 16:42:00 INFO DAGScheduler: Job 3 finished: save at NativeMethodAccessorImpl.java:0, took 0.171701 s
	24/04/25 16:42:00 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/25 16:42:00 INFO DAGScheduler: Got job 4 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/25 16:42:00 INFO DAGScheduler: Final stage: ResultStage 4 (save at NativeMethodAccessorImpl.java:0)
	24/04/25 16:42:00 INFO DAGScheduler: Parents of final stage: List()
	24/04/25 16:42:00 INFO DAGScheduler: Missing parents: List()
	24/04/25 16:42:00 INFO DAGScheduler: Submitting ResultStage 4 (SQLExecutionRDD[40] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/25 16:42:00 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 20.6 KiB, free 365.9 MiB)
	24/04/25 16:42:00 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 365.8 MiB)
	24/04/25 16:42:00 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 15dedf891bed:39985 (size: 9.0 KiB, free: 366.2 MiB)
	24/04/25 16:42:00 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
	24/04/25 16:42:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (SQLExecutionRDD[40] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/25 16:42:00 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
	24/04/25 16:42:00 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (15dedf891bed, executor driver, partition 0, PROCESS_LOCAL, 10129 bytes) 
	24/04/25 16:42:00 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
	24/04/25 16:42:00 INFO PythonRunner: Times: total = 140, boot = -251, init = 391, finish = 0
	24/04/25 16:42:00 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1621 bytes result sent to driver
	24/04/25 16:42:00 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 163 ms on 15dedf891bed (executor driver) (1/1)
	24/04/25 16:42:00 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
	24/04/25 16:42:00 INFO DAGScheduler: ResultStage 4 (save at NativeMethodAccessorImpl.java:0) finished in 0.178 s
	24/04/25 16:42:00 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/25 16:42:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
	24/04/25 16:42:00 INFO DAGScheduler: Job 4 finished: save at NativeMethodAccessorImpl.java:0, took 0.200024 s
	24/04/25 16:42:00 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/25 16:42:00 INFO DAGScheduler: Got job 5 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/25 16:42:00 INFO DAGScheduler: Final stage: ResultStage 5 (save at NativeMethodAccessorImpl.java:0)
	24/04/25 16:42:00 INFO DAGScheduler: Parents of final stage: List()
	24/04/25 16:42:00 INFO DAGScheduler: Missing parents: List()
	24/04/25 16:42:00 INFO DAGScheduler: Submitting ResultStage 5 (SQLExecutionRDD[47] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/25 16:42:00 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 20.6 KiB, free 365.8 MiB)
	24/04/25 16:42:00 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 365.8 MiB)
	24/04/25 16:42:00 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 15dedf891bed:39985 (size: 9.0 KiB, free: 366.2 MiB)
	24/04/25 16:42:00 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
	24/04/25 16:42:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (SQLExecutionRDD[47] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/25 16:42:00 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
	24/04/25 16:42:00 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (15dedf891bed, executor driver, partition 0, PROCESS_LOCAL, 10184 bytes) 
	24/04/25 16:42:00 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
	24/04/25 16:42:00 INFO PythonRunner: Times: total = 57, boot = -85, init = 142, finish = 0
	24/04/25 16:42:00 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1621 bytes result sent to driver
	24/04/25 16:42:00 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 76 ms on 15dedf891bed (executor driver) (1/1)
	24/04/25 16:42:00 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
	24/04/25 16:42:00 INFO DAGScheduler: ResultStage 5 (save at NativeMethodAccessorImpl.java:0) finished in 0.088 s
	24/04/25 16:42:00 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/25 16:42:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
	24/04/25 16:42:00 INFO DAGScheduler: Job 5 finished: save at NativeMethodAccessorImpl.java:0, took 0.096513 s
	24/04/25 16:42:01 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/25 16:42:01 INFO DAGScheduler: Got job 6 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/25 16:42:01 INFO DAGScheduler: Final stage: ResultStage 6 (save at NativeMethodAccessorImpl.java:0)
	24/04/25 16:42:01 INFO DAGScheduler: Parents of final stage: List()
	24/04/25 16:42:01 INFO DAGScheduler: Missing parents: List()
	24/04/25 16:42:01 INFO DAGScheduler: Submitting ResultStage 6 (SQLExecutionRDD[54] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/25 16:42:01 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 20.6 KiB, free 365.8 MiB)
	24/04/25 16:42:01 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 365.8 MiB)
	24/04/25 16:42:01 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 15dedf891bed:39985 (size: 9.0 KiB, free: 366.2 MiB)
	24/04/25 16:42:01 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
	24/04/25 16:42:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (SQLExecutionRDD[54] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/25 16:42:01 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
	24/04/25 16:42:01 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (15dedf891bed, executor driver, partition 0, PROCESS_LOCAL, 10081 bytes) 
	24/04/25 16:42:01 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:02.151 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:42:01 INFO PythonRunner: Times: total = 57, boot = -109, init = 166, finish = 0
	24/04/25 16:42:01 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1621 bytes result sent to driver
	24/04/25 16:42:01 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 69 ms on 15dedf891bed (executor driver) (1/1)
	24/04/25 16:42:01 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
	24/04/25 16:42:01 INFO DAGScheduler: ResultStage 6 (save at NativeMethodAccessorImpl.java:0) finished in 0.080 s
	24/04/25 16:42:01 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/25 16:42:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
	24/04/25 16:42:01 INFO DAGScheduler: Job 6 finished: save at NativeMethodAccessorImpl.java:0, took 0.088093 s
	24/04/25 16:42:01 INFO SparkContext: Invoking stop() from shutdown hook
	24/04/25 16:42:01 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/04/25 16:42:01 INFO SparkUI: Stopped Spark web UI at http://15dedf891bed:4040
	24/04/25 16:42:01 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/04/25 16:42:01 INFO MemoryStore: MemoryStore cleared
	24/04/25 16:42:01 INFO BlockManager: BlockManager stopped
	24/04/25 16:42:01 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/04/25 16:42:01 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/04/25 16:42:01 INFO SparkContext: Successfully stopped SparkContext
	24/04/25 16:42:01 INFO ShutdownHookManager: Shutdown hook called
	24/04/25 16:42:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b
	24/04/25 16:42:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-06c47718-b10b-4715-9a70-2e8798120b4b/pyspark-3fd25c27-c05c-4bdc-bcf3-934b0a22877b
	24/04/25 16:42:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-57e09285-6d95-4eb6-ae08-9f15f3a4f759
[WI-681][TI-1853] - [INFO] 2024-04-25 16:42:02.152 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1853, processId:7621 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-681][TI-1853] - [INFO] 2024-04-25 16:42:02.153 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1853] - [INFO] 2024-04-25 16:42:02.154 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-681][TI-1853] - [INFO] 2024-04-25 16:42:02.155 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1853] - [INFO] 2024-04-25 16:42:02.155 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-681][TI-1853] - [INFO] 2024-04-25 16:42:02.157 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-681][TI-1853] - [INFO] 2024-04-25 16:42:02.160 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-681][TI-1853] - [INFO] 2024-04-25 16:42:02.160 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1853
[WI-681][TI-1853] - [INFO] 2024-04-25 16:42:02.161 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1853
[WI-681][TI-1853] - [INFO] 2024-04-25 16:42:02.161 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:02.563 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7584269662921348 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1853] - [INFO] 2024-04-25 16:42:03.098 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1853, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:03.590 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7267904509283819 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:04.594 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7084468664850138 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:05.603 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7647058823529411 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:06.609 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7811550151975684 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:07.611 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8517520215633424 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:16.676 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7556818181818181 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:17.683 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7108753315649867 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:43.295 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1855, taskName=search for intake JSON files, firstSubmitTime=1714034563286, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=116, appIds=null, processInstanceId=681, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1855'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340113777664'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425164243'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='681'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/676-20240425162748.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.298 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.311 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.311 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.297 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake JSON files to wait queue success
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.311 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.311 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714034563311
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.312 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 681_1855
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.313 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1855,
  "taskName" : "search for intake JSON files",
  "firstSubmitTime" : 1714034563286,
  "startTime" : 1714034563311,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1855.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 116,
  "processInstanceId" : 681,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set raw_file_dir to null\\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1855"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340113777664"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425164243"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "681"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/676-20240425162748.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "681_1855",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/676-20240425162748.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.315 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.315 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.315 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.317 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.318 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.319 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1855 check successfully
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.319 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.319 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.319 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.320 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.320 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"",
  "resourceList" : [ ]
}
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.320 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.320 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}] successfully
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.320 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.321 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.321 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.321 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.322 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.322 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/reddit/in -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set raw_file_dir to null
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
fi

echo "#{setValue(raw_file_dir=/local_storage/reddit/processing/676-20240425162748.json)}"
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.322 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.322 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1855/681_1855.sh
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:43.325 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 7875
[WI-0][TI-1855] - [INFO] 2024-04-25 16:42:44.153 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1855, processInstanceId=681, startTime=1714034563311, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1855.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1855] - [INFO] 2024-04-25 16:42:44.156 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1855, processInstanceId=681, startTime=1714034563311, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1855.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714034564153)
[WI-0][TI-1855] - [INFO] 2024-04-25 16:42:44.156 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1855, processInstanceId=681, startTime=1714034563311, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1855] - [INFO] 2024-04-25 16:42:44.158 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1855, processInstanceId=681, startTime=1714034563311, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714034564153)
[WI-0][TI-1855] - [INFO] 2024-04-25 16:42:44.170 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1855, success=true)
[WI-0][TI-1855] - [INFO] 2024-04-25 16:42:44.198 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1855)
[WI-0][TI-1855] - [INFO] 2024-04-25 16:42:44.210 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1855, success=true)
[WI-0][TI-1855] - [INFO] 2024-04-25 16:42:44.217 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1855)
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:44.329 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	#{setValue(raw_file_dir=/local_storage/reddit/processing/676-20240425162748.json)}
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:44.338 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1855, processId:7875 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:44.339 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:44.340 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:44.340 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:44.341 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:44.345 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:44.346 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:44.346 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1855
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:44.346 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1855
[WI-681][TI-1855] - [INFO] 2024-04-25 16:42:44.347 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1855] - [INFO] 2024-04-25 16:42:45.159 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1855, processInstanceId=681, status=7, startTime=1714034563311, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1855.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1855, endTime=1714034564341, processId=7875, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1855] - [INFO] 2024-04-25 16:42:45.161 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1855, processInstanceId=681, status=7, startTime=1714034563311, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1855.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1855, endTime=1714034564341, processId=7875, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], eventCreateTime=0, eventSendTime=1714034565159)
[WI-0][TI-1855] - [INFO] 2024-04-25 16:42:45.172 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1855, success=true)
[WI-0][TI-1855] - [INFO] 2024-04-25 16:42:45.176 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1855, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:46.236 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1857, taskName=move to processing, firstSubmitTime=1714034566231, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=116, appIds=null, processInstanceId=681, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to processing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1857'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340390094208'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425164246'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='681'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/676-20240425162748.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.237 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to processing to wait queue success
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.238 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.239 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.239 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.239 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.239 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714034566239
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.239 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 681_1857
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.239 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1857,
  "taskName" : "move to processing",
  "firstSubmitTime" : 1714034566231,
  "startTime" : 1714034566239,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1857.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 116,
  "processInstanceId" : 681,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the reddit processing folder\\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\\nfilename=$(basename \\\"${raw_file_dir}\\\")\\nif ! grep -q \\\"${REDDIT_HOME}/processing\\\" <<< \\\"${raw_file_dir}\\\"; then\\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\\nelse\\n    echo JSON is already in processing\\nfi\\n\\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\\necho \\\"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to processing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1857"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340390094208"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425164246"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "681"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/676-20240425162748.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "681_1857",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/676-20240425162748.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.246 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.246 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.246 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.248 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.248 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.249 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1857 check successfully
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.249 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.249 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.249 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.249 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.249 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"",
  "resourceList" : [ ]
}
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.250 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.250 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}] successfully
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.250 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.250 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.251 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.252 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.252 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.252 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the reddit processing folder
# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error
filename=$(basename "/local_storage/reddit/processing/676-20240425162748.json")
if ! grep -q "/local_storage/reddit/processing" <<< "/local_storage/reddit/processing/676-20240425162748.json"; then
    mv /local_storage/reddit/processing/676-20240425162748.json /local_storage/reddit/processing
else
    echo JSON is already in processing
fi

# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing
echo "#{setValue(raw_file_dir=/local_storage/reddit/processing/${filename})}"
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.252 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.252 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1857/681_1857.sh
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:46.255 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 7888
[WI-0][TI-1857] - [INFO] 2024-04-25 16:42:47.168 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1857, processInstanceId=681, startTime=1714034566239, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1857.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1857] - [INFO] 2024-04-25 16:42:47.171 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1857, processInstanceId=681, startTime=1714034566239, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1857.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714034567167)
[WI-0][TI-1857] - [INFO] 2024-04-25 16:42:47.171 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1857, processInstanceId=681, startTime=1714034566239, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1857] - [INFO] 2024-04-25 16:42:47.173 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1857, processInstanceId=681, startTime=1714034566239, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714034567167)
[WI-0][TI-1857] - [INFO] 2024-04-25 16:42:47.187 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1857, success=true)
[WI-0][TI-1857] - [INFO] 2024-04-25 16:42:47.194 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1857)
[WI-0][TI-1857] - [INFO] 2024-04-25 16:42:47.201 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1857, success=true)
[WI-0][TI-1857] - [INFO] 2024-04-25 16:42:47.220 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1857)
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:47.256 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	JSON is already in processing
	#{setValue(raw_file_dir=/local_storage/reddit/processing/676-20240425162748.json)}
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:47.263 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1857, processId:7888 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:47.263 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:47.263 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:47.263 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:47.263 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:47.265 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:47.265 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:47.265 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1857
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:47.266 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1857
[WI-681][TI-1857] - [INFO] 2024-04-25 16:42:47.267 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1857] - [INFO] 2024-04-25 16:42:48.175 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1857, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:48.287 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1858, taskName=preprocessing and kafka, firstSubmitTime=1714034568280, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=116, appIds=null, processInstanceId=681, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_reddit_preprocessing.py\n\n#     --jars spark_packages/spark-sql-kafka-0-10_2.12-3.5.1.jar \\","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='preprocessing and kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1858'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13210058002752'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425164248'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='681'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/676-20240425162748.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.288 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: preprocessing and kafka to wait queue success
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.288 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.289 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.290 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.290 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.290 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714034568290
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.290 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 681_1858
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.290 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1858,
  "taskName" : "preprocessing and kafka",
  "firstSubmitTime" : 1714034568280,
  "startTime" : 1714034568290,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1858.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 116,
  "processInstanceId" : 681,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name reddit_preprocessing \\\\\\n    --files ${raw_file_dir} \\\\\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\\\\n    --conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n    spark_reddit_preprocessing.py\\n\\n#     --jars spark_packages/spark-sql-kafka-0-10_2.12-3.5.1.jar \\\\\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocessing and kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1858"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13210058002752"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425164248"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "681"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/676-20240425162748.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "681_1858",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/676-20240425162748.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.290 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.291 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.291 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.293 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.294 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.294 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1858 check successfully
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.295 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.303 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py, resourceRelativePath=spark_reddit_preprocessing.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1858/spark_reddit_preprocessing.py)})
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.304 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.305 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.305 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_reddit_preprocessing.py\n\n#     --jars spark_packages/spark-sql-kafka-0-10_2.12-3.5.1.jar \\",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py",
    "res" : null
  } ]
}
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.306 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.306 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}] successfully
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.306 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.307 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.307 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.308 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.308 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.308 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master local \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name reddit_preprocessing \
    --files /local_storage/reddit/processing/676-20240425162748.json \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
    --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
    spark_reddit_preprocessing.py

#     --jars spark_packages/spark-sql-kafka-0-10_2.12-3.5.1.jar \
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.308 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.309 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1858/681_1858.sh
[WI-681][TI-1858] - [INFO] 2024-04-25 16:42:48.312 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 7900
[WI-0][TI-1858] - [INFO] 2024-04-25 16:42:49.179 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1858, processInstanceId=681, startTime=1714034568290, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1858.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1858] - [INFO] 2024-04-25 16:42:49.181 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1858, processInstanceId=681, startTime=1714034568290, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1858.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714034569177)
[WI-0][TI-1858] - [INFO] 2024-04-25 16:42:49.181 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1858, success=true)
[WI-0][TI-1858] - [INFO] 2024-04-25 16:42:49.182 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1858, processInstanceId=681, startTime=1714034568290, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1858] - [INFO] 2024-04-25 16:42:49.185 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1858, processInstanceId=681, startTime=1714034568290, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714034569177)
[WI-0][TI-1858] - [WARN] 2024-04-25 16:42:49.187 +0800 o.a.d.s.w.m.MessageRetryRunner:[139] - Retry send message to master error
java.util.ConcurrentModificationException: null
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:911)
	at java.util.ArrayList$Itr.next(ArrayList.java:861)
	at org.apache.dolphinscheduler.server.worker.message.MessageRetryRunner.run(MessageRetryRunner.java:127)
[WI-0][TI-1858] - [INFO] 2024-04-25 16:42:49.187 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1858)
[WI-0][TI-1858] - [INFO] 2024-04-25 16:42:49.192 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1858, success=true)
[WI-0][TI-1858] - [INFO] 2024-04-25 16:42:49.196 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1858)
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:49.312 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:50.828 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8338028169014085 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:51.318 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-59ccf7b2-d9a4-429d-80d8-406235355ad2;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 606ms :: artifacts dl 33ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-59ccf7b2-d9a4-429d-80d8-406235355ad2
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/13ms)
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:52.320 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:42:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:53.265 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8282548476454293 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:53.321 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:42:52 INFO SparkContext: Running Spark version 3.5.1
	24/04/25 16:42:52 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/04/25 16:42:52 INFO SparkContext: Java version 1.8.0_402
	24/04/25 16:42:52 INFO ResourceUtils: ==============================================================
	24/04/25 16:42:52 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/04/25 16:42:52 INFO ResourceUtils: ==============================================================
	24/04/25 16:42:52 INFO SparkContext: Submitted application: reddit_preprocessing
	24/04/25 16:42:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/04/25 16:42:52 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
	24/04/25 16:42:52 INFO ResourceProfileManager: Added ResourceProfile id: 0
	24/04/25 16:42:52 INFO SecurityManager: Changing view acls to: default
	24/04/25 16:42:52 INFO SecurityManager: Changing modify acls to: default
	24/04/25 16:42:52 INFO SecurityManager: Changing view acls groups to: 
	24/04/25 16:42:52 INFO SecurityManager: Changing modify acls groups to: 
	24/04/25 16:42:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
	24/04/25 16:42:53 INFO Utils: Successfully started service 'sparkDriver' on port 40599.
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:54.327 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:42:53 INFO SparkEnv: Registering MapOutputTracker
	24/04/25 16:42:53 INFO SparkEnv: Registering BlockManagerMaster
	24/04/25 16:42:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/04/25 16:42:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/04/25 16:42:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/04/25 16:42:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3eb58eb3-ba40-40df-9c99-bf63ff56399f
	24/04/25 16:42:53 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
	24/04/25 16:42:53 INFO SparkEnv: Registering OutputCommitCoordinator
	24/04/25 16:42:53 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/04/25 16:42:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/04/25 16:42:53 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://15dedf891bed:40599/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714034572460
	24/04/25 16:42:53 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://15dedf891bed:40599/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714034572460
	24/04/25 16:42:53 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://15dedf891bed:40599/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714034572460
	24/04/25 16:42:53 INFO SparkContext: Added JAR file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://15dedf891bed:40599/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714034572460
	24/04/25 16:42:53 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://15dedf891bed:40599/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714034572460
	24/04/25 16:42:53 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://15dedf891bed:40599/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714034572460
	24/04/25 16:42:53 INFO SparkContext: Added JAR file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at spark://15dedf891bed:40599/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714034572460
	24/04/25 16:42:53 INFO SparkContext: Added JAR file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://15dedf891bed:40599/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714034572460
	24/04/25 16:42:53 INFO SparkContext: Added JAR file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://15dedf891bed:40599/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714034572460
	24/04/25 16:42:53 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://15dedf891bed:40599/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714034572460
	24/04/25 16:42:53 INFO SparkContext: Added JAR file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at spark://15dedf891bed:40599/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714034572460
	24/04/25 16:42:53 INFO SparkContext: Added file file:///local_storage/reddit/processing/676-20240425162748.json at file:///local_storage/reddit/processing/676-20240425162748.json with timestamp 1714034572460
	24/04/25 16:42:53 INFO Utils: Copying /local_storage/reddit/processing/676-20240425162748.json to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/676-20240425162748.json
	24/04/25 16:42:53 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714034572460
	24/04/25 16:42:53 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/04/25 16:42:54 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714034572460
	24/04/25 16:42:54 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/04/25 16:42:54 INFO SparkContext: Added file file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714034572460
	24/04/25 16:42:54 INFO Utils: Copying /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/25 16:42:54 INFO SparkContext: Added file file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714034572460
	24/04/25 16:42:54 INFO Utils: Copying /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/25 16:42:54 INFO SparkContext: Added file file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714034572460
	24/04/25 16:42:54 INFO Utils: Copying /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/25 16:42:54 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714034572460
	24/04/25 16:42:54 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/25 16:42:54 INFO SparkContext: Added file file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714034572460
	24/04/25 16:42:54 INFO Utils: Copying /tmp/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.lz4_lz4-java-1.8.0.jar
	24/04/25 16:42:54 INFO SparkContext: Added file file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714034572460
	24/04/25 16:42:54 INFO Utils: Copying /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/25 16:42:54 INFO SparkContext: Added file file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714034572460
	24/04/25 16:42:54 INFO Utils: Copying /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.slf4j_slf4j-api-2.0.7.jar
	24/04/25 16:42:54 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714034572460
	24/04/25 16:42:54 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/25 16:42:54 INFO SparkContext: Added file file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714034572460
	24/04/25 16:42:54 INFO Utils: Copying /tmp/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/commons-logging_commons-logging-1.1.3.jar
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:55.305 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7653958944281525 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:55.339 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:42:54 INFO Executor: Starting executor ID driver on host 15dedf891bed
	24/04/25 16:42:54 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/04/25 16:42:54 INFO Executor: Java version 1.8.0_402
	24/04/25 16:42:54 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/04/25 16:42:54 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@390d3040 for default.
	24/04/25 16:42:54 INFO Executor: Fetching file:///local_storage/reddit/processing/676-20240425162748.json with timestamp 1714034572460
	24/04/25 16:42:54 INFO Utils: /local_storage/reddit/processing/676-20240425162748.json has been previously copied to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/676-20240425162748.json
	24/04/25 16:42:54 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714034572460
	24/04/25 16:42:54 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/25 16:42:54 INFO Executor: Fetching file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714034572460
	24/04/25 16:42:54 INFO Utils: /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/25 16:42:54 INFO Executor: Fetching file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714034572460
	24/04/25 16:42:54 INFO Utils: /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/25 16:42:54 INFO Executor: Fetching file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714034572460
	24/04/25 16:42:54 INFO Utils: /tmp/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.lz4_lz4-java-1.8.0.jar
	24/04/25 16:42:54 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714034572460
	24/04/25 16:42:54 INFO Utils: /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/04/25 16:42:54 INFO Executor: Fetching file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714034572460
	24/04/25 16:42:54 INFO Utils: /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/25 16:42:54 INFO Executor: Fetching file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714034572460
	24/04/25 16:42:54 INFO Utils: /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.slf4j_slf4j-api-2.0.7.jar
	24/04/25 16:42:54 INFO Executor: Fetching file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714034572460
	24/04/25 16:42:54 INFO Utils: /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/25 16:42:54 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714034572460
	24/04/25 16:42:54 INFO Utils: /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/04/25 16:42:54 INFO Executor: Fetching file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714034572460
	24/04/25 16:42:54 INFO Utils: /tmp/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/commons-logging_commons-logging-1.1.3.jar
	24/04/25 16:42:54 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714034572460
	24/04/25 16:42:54 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/25 16:42:54 INFO Executor: Fetching spark://15dedf891bed:40599/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714034572460
	24/04/25 16:42:54 INFO TransportClientFactory: Successfully created connection to 15dedf891bed/172.18.1.1:40599 after 58 ms (0 ms spent in bootstraps)
	24/04/25 16:42:54 INFO Utils: Fetching spark://15dedf891bed:40599/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/fetchFileTemp8737712159300938750.tmp
	24/04/25 16:42:54 INFO Utils: /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/fetchFileTemp8737712159300938750.tmp has been previously copied to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.slf4j_slf4j-api-2.0.7.jar
	24/04/25 16:42:54 INFO Executor: Adding file:/tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.slf4j_slf4j-api-2.0.7.jar to class loader default
	24/04/25 16:42:54 INFO Executor: Fetching spark://15dedf891bed:40599/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714034572460
	24/04/25 16:42:54 INFO Utils: Fetching spark://15dedf891bed:40599/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/fetchFileTemp791806465445071236.tmp
	24/04/25 16:42:55 INFO Utils: /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/fetchFileTemp791806465445071236.tmp has been previously copied to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/25 16:42:55 INFO Executor: Adding file:/tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
	24/04/25 16:42:55 INFO Executor: Fetching spark://15dedf891bed:40599/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714034572460
	24/04/25 16:42:55 INFO Utils: Fetching spark://15dedf891bed:40599/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/fetchFileTemp422909999552712739.tmp
	24/04/25 16:42:55 INFO Utils: /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/fetchFileTemp422909999552712739.tmp has been previously copied to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/25 16:42:55 INFO Executor: Adding file:/tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
	24/04/25 16:42:55 INFO Executor: Fetching spark://15dedf891bed:40599/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714034572460
	24/04/25 16:42:55 INFO Utils: Fetching spark://15dedf891bed:40599/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/fetchFileTemp5128075262776901182.tmp
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:56.343 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:42:55 INFO Utils: /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/fetchFileTemp5128075262776901182.tmp has been previously copied to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/commons-logging_commons-logging-1.1.3.jar
	24/04/25 16:42:55 INFO Executor: Adding file:/tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/commons-logging_commons-logging-1.1.3.jar to class loader default
	24/04/25 16:42:55 INFO Executor: Fetching spark://15dedf891bed:40599/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714034572460
	24/04/25 16:42:55 INFO Utils: Fetching spark://15dedf891bed:40599/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/fetchFileTemp2426872277428244173.tmp
	24/04/25 16:42:55 INFO Utils: /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/fetchFileTemp2426872277428244173.tmp has been previously copied to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/25 16:42:55 INFO Executor: Adding file:/tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
	24/04/25 16:42:55 INFO Executor: Fetching spark://15dedf891bed:40599/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714034572460
	24/04/25 16:42:55 INFO Utils: Fetching spark://15dedf891bed:40599/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/fetchFileTemp8774253309296819523.tmp
	24/04/25 16:42:55 INFO Utils: /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/fetchFileTemp8774253309296819523.tmp has been previously copied to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/04/25 16:42:55 INFO Executor: Adding file:/tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
	24/04/25 16:42:55 INFO Executor: Fetching spark://15dedf891bed:40599/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714034572460
	24/04/25 16:42:55 INFO Utils: Fetching spark://15dedf891bed:40599/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/fetchFileTemp8038575342377572816.tmp
	24/04/25 16:42:55 INFO Utils: /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/fetchFileTemp8038575342377572816.tmp has been previously copied to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/25 16:42:55 INFO Executor: Adding file:/tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
	24/04/25 16:42:55 INFO Executor: Fetching spark://15dedf891bed:40599/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714034572460
	24/04/25 16:42:55 INFO Utils: Fetching spark://15dedf891bed:40599/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/fetchFileTemp1366753114243021809.tmp
	24/04/25 16:42:55 INFO Utils: /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/fetchFileTemp1366753114243021809.tmp has been previously copied to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/25 16:42:55 INFO Executor: Adding file:/tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
	24/04/25 16:42:55 INFO Executor: Fetching spark://15dedf891bed:40599/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714034572460
	24/04/25 16:42:55 INFO Utils: Fetching spark://15dedf891bed:40599/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/fetchFileTemp6355612027371573665.tmp
	24/04/25 16:42:55 INFO Utils: /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/fetchFileTemp6355612027371573665.tmp has been previously copied to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.lz4_lz4-java-1.8.0.jar
	24/04/25 16:42:55 INFO Executor: Adding file:/tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.lz4_lz4-java-1.8.0.jar to class loader default
	24/04/25 16:42:55 INFO Executor: Fetching spark://15dedf891bed:40599/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714034572460
	24/04/25 16:42:55 INFO Utils: Fetching spark://15dedf891bed:40599/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/fetchFileTemp5980137925953098060.tmp
	24/04/25 16:42:55 INFO Utils: /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/fetchFileTemp5980137925953098060.tmp has been previously copied to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/25 16:42:55 INFO Executor: Adding file:/tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
	24/04/25 16:42:55 INFO Executor: Fetching spark://15dedf891bed:40599/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714034572460
	24/04/25 16:42:55 INFO Utils: Fetching spark://15dedf891bed:40599/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/fetchFileTemp37985396853455506.tmp
	24/04/25 16:42:55 INFO Utils: /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/fetchFileTemp37985396853455506.tmp has been previously copied to /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/04/25 16:42:55 INFO Executor: Adding file:/tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
	24/04/25 16:42:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41533.
	24/04/25 16:42:55 INFO NettyBlockTransferService: Server created on 15dedf891bed:41533
	24/04/25 16:42:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/04/25 16:42:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 15dedf891bed, 41533, None)
	24/04/25 16:42:55 INFO BlockManagerMasterEndpoint: Registering block manager 15dedf891bed:41533 with 366.3 MiB RAM, BlockManagerId(driver, 15dedf891bed, 41533, None)
	24/04/25 16:42:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 15dedf891bed, 41533, None)
	24/04/25 16:42:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 15dedf891bed, 41533, None)
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:56.345 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8141592920353983 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:57.345 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	
	
	 /tmp/spark-79616cce-1e66-4ea6-bdb8-158527b1a4af/userFiles-c5b0f594-4e79-4cd0-8e23-299c4ce783a0/676-20240425162748.json 
	
	
	
	24/04/25 16:42:57 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 345.7 KiB, free 366.0 MiB)
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:57.353 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1859, taskName=search for intake JSON files, firstSubmitTime=1714034577344, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=116, appIds=null, processInstanceId=682, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1859'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340113777664'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425164257'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='682'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.356 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.355 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake JSON files to wait queue success
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.359 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.359 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.365 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.365 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714034577365
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.365 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 682_1859
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.365 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1859,
  "taskName" : "search for intake JSON files",
  "firstSubmitTime" : 1714034577344,
  "startTime" : 1714034577365,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13201021801792/116/682/1859.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 116,
  "processInstanceId" : 682,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set raw_file_dir to null\\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1859"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340113777664"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425164257"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "682"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "682_1859",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.366 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.366 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.366 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.371 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.372 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.372 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1859 check successfully
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.372 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.372 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.373 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.373 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.373 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"",
  "resourceList" : [ ]
}
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.373 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.373 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.373 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.373 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.373 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.374 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.374 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.374 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/reddit/processing -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set raw_file_dir to null
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
fi

echo "#{setValue(raw_file_dir=${raw_file_dir})}"
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.374 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.375 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1859/682_1859.sh
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:57.385 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 8064
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:57.386 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1859] - [INFO] 2024-04-25 16:42:58.250 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1859, processInstanceId=682, startTime=1714034577365, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/682/1859.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1859] - [INFO] 2024-04-25 16:42:58.256 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1859, processInstanceId=682, startTime=1714034577365, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/682/1859.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714034578246)
[WI-0][TI-1859] - [INFO] 2024-04-25 16:42:58.256 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1859, processInstanceId=682, startTime=1714034577365, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1859] - [INFO] 2024-04-25 16:42:58.259 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1859, success=true)
[WI-0][TI-1859] - [INFO] 2024-04-25 16:42:58.261 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1859, processInstanceId=682, startTime=1714034577365, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714034578246)
[WI-0][TI-1859] - [WARN] 2024-04-25 16:42:58.262 +0800 o.a.d.s.w.m.MessageRetryRunner:[139] - Retry send message to master error
java.util.ConcurrentModificationException: null
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:911)
	at java.util.ArrayList$Itr.next(ArrayList.java:861)
	at org.apache.dolphinscheduler.server.worker.message.MessageRetryRunner.run(MessageRetryRunner.java:127)
[WI-0][TI-1859] - [INFO] 2024-04-25 16:42:58.267 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1859)
[WI-0][TI-1859] - [INFO] 2024-04-25 16:42:58.306 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1859, success=true)
[WI-0][TI-1859] - [INFO] 2024-04-25 16:42:58.336 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1859)
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:58.351 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:42:57 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 365.9 MiB)
	24/04/25 16:42:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 15dedf891bed:41533 (size: 32.6 KiB, free: 366.3 MiB)
	24/04/25 16:42:57 INFO SparkContext: Created broadcast 0 from wholeTextFiles at NativeMethodAccessorImpl.java:0
	24/04/25 16:42:57 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/04/25 16:42:57 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1858/spark-warehouse'.
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:58.357 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9077809798270893 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:58.392 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	#{setValue(raw_file_dir=/local_storage/reddit/processing/676-20240425162748.json)}
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:58.395 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1859, processId:8064 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:58.395 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:58.395 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:58.397 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:58.397 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:58.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:58.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:58.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1859
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:58.402 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1859
[WI-682][TI-1859] - [INFO] 2024-04-25 16:42:58.402 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1859] - [INFO] 2024-04-25 16:42:59.250 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1859, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:42:59.378 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8356164383561644 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:00.352 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1861, taskName=move to processing, firstSubmitTime=1714034580323, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=116, appIds=null, processInstanceId=682, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to processing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1861'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340390094208'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425164300'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='682'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/676-20240425162748.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.353 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to processing to wait queue success
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.354 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.355 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.355 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.355 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.355 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714034580355
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.355 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 682_1861
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.355 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1861,
  "taskName" : "move to processing",
  "firstSubmitTime" : 1714034580323,
  "startTime" : 1714034580355,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13201021801792/116/682/1861.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 116,
  "processInstanceId" : 682,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the reddit processing folder\\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\\nfilename=$(basename \\\"${raw_file_dir}\\\")\\nif ! grep -q \\\"${REDDIT_HOME}/processing\\\" <<< \\\"${raw_file_dir}\\\"; then\\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\\nelse\\n    echo JSON is already in processing\\nfi\\n\\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\\necho \\\"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to processing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1861"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340390094208"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425164300"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "682"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/676-20240425162748.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "682_1861",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/676-20240425162748.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.356 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.356 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.357 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.363 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.363 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.364 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1861 check successfully
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.364 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.364 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.364 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.364 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.364 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"",
  "resourceList" : [ ]
}
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.364 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.364 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}] successfully
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.364 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.365 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.365 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.365 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.365 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.365 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the reddit processing folder
# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error
filename=$(basename "/local_storage/reddit/processing/676-20240425162748.json")
if ! grep -q "/local_storage/reddit/processing" <<< "/local_storage/reddit/processing/676-20240425162748.json"; then
    mv /local_storage/reddit/processing/676-20240425162748.json /local_storage/reddit/processing
else
    echo JSON is already in processing
fi

# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing
echo "#{setValue(raw_file_dir=/local_storage/reddit/processing/${filename})}"
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.365 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.365 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1861/682_1861.sh
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:00.381 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 8078
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:00.383 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7922437673130194 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:00.736 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[55] - Receive TaskInstanceKillRequest: TaskInstanceKillRequest(taskInstanceId=1858)
[WI-0][TI-1858] - [ERROR] 2024-04-25 16:43:00.744 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[139] - kill task error
java.io.IOException: Cannot run program "pstree": error=2, No such file or directory
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.runCommand(AbstractShell.java:138)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.run(AbstractShell.java:118)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execute(ShellExecutor.java:125)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:103)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:86)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeShell(OSUtils.java:345)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeCmd(OSUtils.java:334)
	at org.apache.dolphinscheduler.plugin.task.api.utils.ProcessUtils.getPidsStr(ProcessUtils.java:129)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.killProcess(TaskInstanceKillOperationFunction.java:130)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.doKill(TaskInstanceKillOperationFunction.java:96)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.operate(TaskInstanceKillOperationFunction.java:69)
	at org.apache.dolphinscheduler.server.worker.rpc.TaskInstanceOperatorImpl.killTask(TaskInstanceOperatorImpl.java:49)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.dolphinscheduler.extract.base.server.ServerMethodInvokerImpl.invoke(ServerMethodInvokerImpl.java:41)
	at org.apache.dolphinscheduler.extract.base.server.JdkDynamicServerHandler.lambda$processReceived$0(JdkDynamicServerHandler.java:108)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: error=2, No such file or directory
	at java.lang.UNIXProcess.forkAndExec(Native Method)
	at java.lang.UNIXProcess.<init>(UNIXProcess.java:247)
	at java.lang.ProcessImpl.start(ProcessImpl.java:134)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	... 21 common frames omitted
[WI-0][TI-1858] - [INFO] 2024-04-25 16:43:00.751 +0800 o.a.d.p.t.a.u.ProcessUtils:[182] - Get appIds from worker 172.18.1.1:1234, taskLogPath: /opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1858.log
[WI-0][TI-1858] - [INFO] 2024-04-25 16:43:00.752 +0800 o.a.d.p.t.a.u.LogUtils:[79] - Start finding appId in /opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1858.log, fetch way: log 
[WI-0][TI-1858] - [INFO] 2024-04-25 16:43:00.753 +0800 o.a.d.p.t.a.u.ProcessUtils:[188] - The appId is empty
[WI-0][TI-1858] - [INFO] 2024-04-25 16:43:00.753 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[220] - Begin to kill process process, pid is : 7900
[WI-0][TI-1861] - [INFO] 2024-04-25 16:43:01.284 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1861, processInstanceId=682, startTime=1714034580355, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/682/1861.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1861] - [INFO] 2024-04-25 16:43:01.293 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1861, success=true)
[WI-0][TI-1861] - [INFO] 2024-04-25 16:43:01.306 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1861, processInstanceId=682, startTime=1714034580355, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/682/1861.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714034581269)
[WI-0][TI-1861] - [INFO] 2024-04-25 16:43:01.310 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1861)
[WI-0][TI-1861] - [INFO] 2024-04-25 16:43:01.323 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1861, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:01.385 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	JSON is already in processing
	#{setValue(raw_file_dir=/local_storage/reddit/processing/676-20240425162748.json)}
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:01.413 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1861, processId:8078 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:01.391 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8969359331476323 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:01.413 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:01.414 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:01.415 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:01.416 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:01.418 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:01.418 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:01.418 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1861
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:01.419 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1861
[WI-682][TI-1861] - [INFO] 2024-04-25 16:43:01.419 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1861] - [INFO] 2024-04-25 16:43:02.255 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1861, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:02.389 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1862, taskName=preprocessing and kafka, firstSubmitTime=1714034582369, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=116, appIds=null, processInstanceId=682, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_reddit_preprocessing.py\n\n#     --jars spark_packages/spark-sql-kafka-0-10_2.12-3.5.1.jar \\","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='preprocessing and kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1862'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13210058002752'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425164302'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='682'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/676-20240425162748.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.397 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: preprocessing and kafka to wait queue success
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.397 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.400 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.400 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.400 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714034582401
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 682_1862
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1862,
  "taskName" : "preprocessing and kafka",
  "firstSubmitTime" : 1714034582369,
  "startTime" : 1714034582401,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13201021801792/116/682/1862.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 116,
  "processInstanceId" : 682,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name reddit_preprocessing \\\\\\n    --files ${raw_file_dir} \\\\\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\\\\n    --conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n    spark_reddit_preprocessing.py\\n\\n#     --jars spark_packages/spark-sql-kafka-0-10_2.12-3.5.1.jar \\\\\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocessing and kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1862"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13210058002752"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425164302"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "682"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/676-20240425162748.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "682_1862",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/676-20240425162748.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.402 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.402 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.402 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:02.415 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8319783197831978 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.423 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.424 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.424 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1862 check successfully
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.424 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.440 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py, resourceRelativePath=spark_reddit_preprocessing.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1862/spark_reddit_preprocessing.py)})
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.441 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.442 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.450 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_reddit_preprocessing.py\n\n#     --jars spark_packages/spark-sql-kafka-0-10_2.12-3.5.1.jar \\",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py",
    "res" : null
  } ]
}
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.450 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.452 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}] successfully
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.453 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.453 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.453 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.454 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.459 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.465 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master local \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name reddit_preprocessing \
    --files /local_storage/reddit/processing/676-20240425162748.json \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
    --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
    spark_reddit_preprocessing.py

#     --jars spark_packages/spark-sql-kafka-0-10_2.12-3.5.1.jar \
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.465 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.466 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1862/682_1862.sh
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:02.507 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 8091
[WI-0][TI-1862] - [INFO] 2024-04-25 16:43:03.266 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1862, success=true)
[WI-0][TI-1862] - [INFO] 2024-04-25 16:43:03.275 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1862)
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:03.416 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.872463768115942 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:03.508 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:04.417 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8808139534883721 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [ERROR] 2024-04-25 16:43:05.341 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[279] - Parse var pool error
java.io.IOException: Stream closed
	at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:170)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:283)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.io.BufferedReader.readLine(BufferedReader.java:324)
	at java.io.BufferedReader.readLine(BufferedReader.java:389)
	at org.apache.dolphinscheduler.plugin.task.api.AbstractCommandExecutor.lambda$parseProcessOutput$1(AbstractCommandExecutor.java:273)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:05.427 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8840579710144928 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1858] - [INFO] 2024-04-25 16:43:05.760 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[225] - Success kill task: 681_1858, pid: 7900
[WI-0][TI-1858] - [INFO] 2024-04-25 16:43:05.760 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[119] - kill task by cancelApplication, taskInstanceId: 1858
[WI-681][TI-1858] - [INFO] 2024-04-25 16:43:05.761 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has killed. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1858, processId:7900 ,exitStatusCode:137 ,processWaitForStatus:true ,processExitValue:137
[WI-681][TI-1858] - [INFO] 2024-04-25 16:43:05.763 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1858] - [INFO] 2024-04-25 16:43:05.763 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-681][TI-1858] - [INFO] 2024-04-25 16:43:05.763 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-681][TI-1858] - [INFO] 2024-04-25 16:43:05.763 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-681][TI-1858] - [INFO] 2024-04-25 16:43:05.780 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: KILL to master : 172.18.1.1:1234
[WI-681][TI-1858] - [INFO] 2024-04-25 16:43:05.781 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-681][TI-1858] - [INFO] 2024-04-25 16:43:05.781 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1858
[WI-681][TI-1858] - [INFO] 2024-04-25 16:43:05.782 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1858
[WI-681][TI-1858] - [INFO] 2024-04-25 16:43:05.782 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1858] - [INFO] 2024-04-25 16:43:06.330 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1858, processInstanceId=681, status=9, startTime=1714034568290, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1858.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1858, endTime=1714034585763, processId=7900, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1858] - [INFO] 2024-04-25 16:43:06.348 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1858, processInstanceId=681, status=9, startTime=1714034568290, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1858.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1858, endTime=1714034585763, processId=7900, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], eventCreateTime=0, eventSendTime=1714034586330)
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:06.461 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.88339222614841 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:07.465 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9337979094076655 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:07.549 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-d55cd0d4-bfa8-460e-a440-5ff81bf2286a;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:08.469 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.903448275862069 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:08.554 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 1275ms :: artifacts dl 76ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-d55cd0d4-bfa8-460e-a440-5ff81bf2286a
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/50ms)
	24/04/25 16:43:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:09.482 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8480243161094225 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:10.489 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7652173913043478 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:10.562 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:43:10 INFO SparkContext: Running Spark version 3.5.1
	24/04/25 16:43:10 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/04/25 16:43:10 INFO SparkContext: Java version 1.8.0_402
	24/04/25 16:43:10 INFO ResourceUtils: ==============================================================
	24/04/25 16:43:10 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/04/25 16:43:10 INFO ResourceUtils: ==============================================================
	24/04/25 16:43:10 INFO SparkContext: Submitted application: reddit_preprocessing
	24/04/25 16:43:10 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/04/25 16:43:10 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
	24/04/25 16:43:10 INFO ResourceProfileManager: Added ResourceProfile id: 0
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:11.527 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.887406290297904 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:11.963 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:43:10 INFO SecurityManager: Changing view acls to: default
	24/04/25 16:43:10 INFO SecurityManager: Changing modify acls to: default
	24/04/25 16:43:10 INFO SecurityManager: Changing view acls groups to: 
	24/04/25 16:43:10 INFO SecurityManager: Changing modify acls groups to: 
	24/04/25 16:43:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
	24/04/25 16:43:11 INFO Utils: Successfully started service 'sparkDriver' on port 44019.
	24/04/25 16:43:11 INFO SparkEnv: Registering MapOutputTracker
	24/04/25 16:43:11 INFO SparkEnv: Registering BlockManagerMaster
	24/04/25 16:43:11 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/04/25 16:43:11 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/04/25 16:43:11 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/04/25 16:43:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6be54b61-9f87-4297-9a49-dd251f2506b8
	24/04/25 16:43:11 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
	24/04/25 16:43:11 INFO SparkEnv: Registering OutputCommitCoordinator
	24/04/25 16:43:11 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/04/25 16:43:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/04/25 16:43:11 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[WI-0][TI-1792] - [INFO] 2024-04-25 16:43:12.360 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1792, processInstanceId=661, status=9, startTime=1714031949850, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/661/1792.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792, endTime=1714032332757, processId=4479, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714034291683)
[WI-0][TI-1792] - [INFO] 2024-04-25 16:43:12.365 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1792, processInstanceId=661, status=9, startTime=1714031949850, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/661/1792.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792, endTime=1714032332757, processId=4479, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714034592360)
[WI-0][TI-1790] - [INFO] 2024-04-25 16:43:12.365 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1790, processInstanceId=660, status=9, startTime=1714031469392, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1790.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790, endTime=1714032269172, processId=4321, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714034291683)
[WI-0][TI-1790] - [INFO] 2024-04-25 16:43:12.372 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1790, processInstanceId=660, status=9, startTime=1714031469392, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1790.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790, endTime=1714032269172, processId=4321, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714034592360)
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:12.976 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:43:11 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://15dedf891bed:44019/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714034590202
	24/04/25 16:43:11 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://15dedf891bed:44019/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714034590202
	24/04/25 16:43:11 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://15dedf891bed:44019/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714034590202
	24/04/25 16:43:11 INFO SparkContext: Added JAR file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://15dedf891bed:44019/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714034590202
	24/04/25 16:43:11 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://15dedf891bed:44019/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714034590202
	24/04/25 16:43:11 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://15dedf891bed:44019/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714034590202
	24/04/25 16:43:11 INFO SparkContext: Added JAR file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at spark://15dedf891bed:44019/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714034590202
	24/04/25 16:43:11 INFO SparkContext: Added JAR file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://15dedf891bed:44019/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714034590202
	24/04/25 16:43:11 INFO SparkContext: Added JAR file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://15dedf891bed:44019/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714034590202
	24/04/25 16:43:11 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://15dedf891bed:44019/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714034590202
	24/04/25 16:43:11 INFO SparkContext: Added JAR file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at spark://15dedf891bed:44019/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714034590202
	24/04/25 16:43:12 INFO SparkContext: Added file file:///local_storage/reddit/processing/676-20240425162748.json at file:///local_storage/reddit/processing/676-20240425162748.json with timestamp 1714034590202
	24/04/25 16:43:12 INFO Utils: Copying /local_storage/reddit/processing/676-20240425162748.json to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/676-20240425162748.json
	24/04/25 16:43:12 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714034590202
	24/04/25 16:43:12 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/04/25 16:43:12 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714034590202
	24/04/25 16:43:12 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/04/25 16:43:12 INFO SparkContext: Added file file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714034590202
	24/04/25 16:43:12 INFO Utils: Copying /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/25 16:43:12 INFO SparkContext: Added file file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714034590202
	24/04/25 16:43:12 INFO Utils: Copying /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/25 16:43:12 INFO SparkContext: Added file file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714034590202
	24/04/25 16:43:12 INFO Utils: Copying /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/25 16:43:12 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714034590202
	24/04/25 16:43:12 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/25 16:43:12 INFO SparkContext: Added file file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714034590202
	24/04/25 16:43:12 INFO Utils: Copying /tmp/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.lz4_lz4-java-1.8.0.jar
	24/04/25 16:43:12 INFO SparkContext: Added file file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714034590202
	24/04/25 16:43:12 INFO Utils: Copying /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/25 16:43:12 INFO SparkContext: Added file file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714034590202
	24/04/25 16:43:12 INFO Utils: Copying /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.slf4j_slf4j-api-2.0.7.jar
	24/04/25 16:43:12 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714034590202
	24/04/25 16:43:12 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/25 16:43:12 INFO SparkContext: Added file file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714034590202
	24/04/25 16:43:12 INFO Utils: Copying /tmp/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/commons-logging_commons-logging-1.1.3.jar
	24/04/25 16:43:12 INFO Executor: Starting executor ID driver on host 15dedf891bed
	24/04/25 16:43:12 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/04/25 16:43:12 INFO Executor: Java version 1.8.0_402
	24/04/25 16:43:12 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/04/25 16:43:12 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@27a64e9e for default.
	24/04/25 16:43:12 INFO Executor: Fetching file:///local_storage/reddit/processing/676-20240425162748.json with timestamp 1714034590202
	24/04/25 16:43:12 INFO Utils: /local_storage/reddit/processing/676-20240425162748.json has been previously copied to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/676-20240425162748.json
	24/04/25 16:43:12 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714034590202
	24/04/25 16:43:12 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/25 16:43:12 INFO Executor: Fetching file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714034590202
	24/04/25 16:43:12 INFO Utils: /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/25 16:43:12 INFO Executor: Fetching file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714034590202
	24/04/25 16:43:12 INFO Utils: /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/25 16:43:12 INFO Executor: Fetching file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714034590202
	24/04/25 16:43:12 INFO Utils: /tmp/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.lz4_lz4-java-1.8.0.jar
	24/04/25 16:43:12 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714034590202
	24/04/25 16:43:12 INFO Utils: /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/04/25 16:43:12 INFO Executor: Fetching file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714034590202
	24/04/25 16:43:12 INFO Utils: /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/25 16:43:12 INFO Executor: Fetching file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714034590202
	24/04/25 16:43:12 INFO Utils: /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.slf4j_slf4j-api-2.0.7.jar
	24/04/25 16:43:12 INFO Executor: Fetching file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714034590202
	24/04/25 16:43:12 INFO Utils: /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/25 16:43:12 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714034590202
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:13.559 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8125 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:13.982 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:43:12 INFO Executor: Fetching file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714034590202
	24/04/25 16:43:12 INFO Utils: /tmp/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/commons-logging_commons-logging-1.1.3.jar
	24/04/25 16:43:12 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714034590202
	24/04/25 16:43:13 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/25 16:43:13 INFO Executor: Fetching spark://15dedf891bed:44019/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714034590202
	24/04/25 16:43:13 INFO TransportClientFactory: Successfully created connection to 15dedf891bed/172.18.1.1:44019 after 207 ms (0 ms spent in bootstraps)
	24/04/25 16:43:13 INFO Utils: Fetching spark://15dedf891bed:44019/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/fetchFileTemp6307111514937346333.tmp
	24/04/25 16:43:13 INFO Utils: /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/fetchFileTemp6307111514937346333.tmp has been previously copied to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/25 16:43:13 INFO Executor: Adding file:/tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
	24/04/25 16:43:13 INFO Executor: Fetching spark://15dedf891bed:44019/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714034590202
	24/04/25 16:43:13 INFO Utils: Fetching spark://15dedf891bed:44019/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/fetchFileTemp6450869940556405514.tmp
	24/04/25 16:43:13 INFO Utils: /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/fetchFileTemp6450869940556405514.tmp has been previously copied to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.lz4_lz4-java-1.8.0.jar
	24/04/25 16:43:13 INFO Executor: Adding file:/tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.lz4_lz4-java-1.8.0.jar to class loader default
	24/04/25 16:43:13 INFO Executor: Fetching spark://15dedf891bed:44019/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714034590202
	24/04/25 16:43:13 INFO Utils: Fetching spark://15dedf891bed:44019/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/fetchFileTemp5628620416302798261.tmp
	24/04/25 16:43:13 INFO Utils: /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/fetchFileTemp5628620416302798261.tmp has been previously copied to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/25 16:43:13 INFO Executor: Adding file:/tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
	24/04/25 16:43:13 INFO Executor: Fetching spark://15dedf891bed:44019/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714034590202
	24/04/25 16:43:13 INFO Utils: Fetching spark://15dedf891bed:44019/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/fetchFileTemp4677599409609578479.tmp
	24/04/25 16:43:13 INFO Utils: /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/fetchFileTemp4677599409609578479.tmp has been previously copied to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/25 16:43:13 INFO Executor: Adding file:/tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
	24/04/25 16:43:13 INFO Executor: Fetching spark://15dedf891bed:44019/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714034590202
	24/04/25 16:43:13 INFO Utils: Fetching spark://15dedf891bed:44019/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/fetchFileTemp7688417239598818375.tmp
	24/04/25 16:43:13 INFO Utils: /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/fetchFileTemp7688417239598818375.tmp has been previously copied to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.slf4j_slf4j-api-2.0.7.jar
	24/04/25 16:43:13 INFO Executor: Adding file:/tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.slf4j_slf4j-api-2.0.7.jar to class loader default
	24/04/25 16:43:13 INFO Executor: Fetching spark://15dedf891bed:44019/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714034590202
	24/04/25 16:43:13 INFO Utils: Fetching spark://15dedf891bed:44019/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/fetchFileTemp4914520139499164094.tmp
	24/04/25 16:43:13 INFO Utils: /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/fetchFileTemp4914520139499164094.tmp has been previously copied to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/25 16:43:13 INFO Executor: Adding file:/tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
	24/04/25 16:43:13 INFO Executor: Fetching spark://15dedf891bed:44019/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714034590202
	24/04/25 16:43:13 INFO Utils: Fetching spark://15dedf891bed:44019/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/fetchFileTemp7921648211469298140.tmp
	24/04/25 16:43:13 INFO Utils: /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/fetchFileTemp7921648211469298140.tmp has been previously copied to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/commons-logging_commons-logging-1.1.3.jar
	24/04/25 16:43:13 INFO Executor: Adding file:/tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/commons-logging_commons-logging-1.1.3.jar to class loader default
	24/04/25 16:43:13 INFO Executor: Fetching spark://15dedf891bed:44019/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714034590202
	24/04/25 16:43:13 INFO Utils: Fetching spark://15dedf891bed:44019/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/fetchFileTemp2213543832644584440.tmp
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:14.592 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9376835584984247 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:14.986 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:43:14 INFO Utils: /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/fetchFileTemp2213543832644584440.tmp has been previously copied to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/25 16:43:14 INFO Executor: Adding file:/tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
	24/04/25 16:43:14 INFO Executor: Fetching spark://15dedf891bed:44019/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714034590202
	24/04/25 16:43:14 INFO Utils: Fetching spark://15dedf891bed:44019/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/fetchFileTemp3451400400698617280.tmp
	24/04/25 16:43:14 INFO Utils: /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/fetchFileTemp3451400400698617280.tmp has been previously copied to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/25 16:43:14 INFO Executor: Adding file:/tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
	24/04/25 16:43:14 INFO Executor: Fetching spark://15dedf891bed:44019/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714034590202
	24/04/25 16:43:14 INFO Utils: Fetching spark://15dedf891bed:44019/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/fetchFileTemp2297791210834355722.tmp
	24/04/25 16:43:14 INFO Utils: /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/fetchFileTemp2297791210834355722.tmp has been previously copied to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/04/25 16:43:14 INFO Executor: Adding file:/tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
	24/04/25 16:43:14 INFO Executor: Fetching spark://15dedf891bed:44019/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714034590202
	24/04/25 16:43:14 INFO Utils: Fetching spark://15dedf891bed:44019/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/fetchFileTemp6642918726585890108.tmp
	24/04/25 16:43:14 INFO Utils: /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/fetchFileTemp6642918726585890108.tmp has been previously copied to /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/04/25 16:43:14 INFO Executor: Adding file:/tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
	24/04/25 16:43:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37063.
	24/04/25 16:43:14 INFO NettyBlockTransferService: Server created on 15dedf891bed:37063
	24/04/25 16:43:14 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/04/25 16:43:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 15dedf891bed, 37063, None)
	24/04/25 16:43:14 INFO BlockManagerMasterEndpoint: Registering block manager 15dedf891bed:37063 with 366.3 MiB RAM, BlockManagerId(driver, 15dedf891bed, 37063, None)
	24/04/25 16:43:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 15dedf891bed, 37063, None)
	24/04/25 16:43:14 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 15dedf891bed, 37063, None)
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:15.620 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8943661971830985 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:16.623 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7521008403361344 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:17.031 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	
	
	 /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/676-20240425162748.json 
	
	
	
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:17.632 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.852760736196319 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:18.035 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:43:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 345.7 KiB, free 366.0 MiB)
	24/04/25 16:43:17 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 365.9 MiB)
	24/04/25 16:43:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 15dedf891bed:37063 (size: 32.6 KiB, free: 366.3 MiB)
	24/04/25 16:43:17 INFO SparkContext: Created broadcast 0 from wholeTextFiles at NativeMethodAccessorImpl.java:0
	24/04/25 16:43:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/04/25 16:43:17 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1862/spark-warehouse'.
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:18.641 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7398373983739838 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:19.648 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7575757575757576 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:20.655 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9269005847953216 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:21.657 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9047619047619048 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:23.679 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7264957264957266 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:24.247 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:43:24 INFO CodeGenerator: Code generated in 570.706627 ms
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:24.719 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8782608695652174 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:25.248 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:43:24 INFO FileInputFormat: Total input files to process : 1
	24/04/25 16:43:24 INFO FileInputFormat: Total input files to process : 1
	24/04/25 16:43:24 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
	24/04/25 16:43:24 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/25 16:43:24 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
	24/04/25 16:43:24 INFO DAGScheduler: Parents of final stage: List()
	24/04/25 16:43:24 INFO DAGScheduler: Missing parents: List()
	24/04/25 16:43:24 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/25 16:43:24 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 28.1 KiB, free 365.9 MiB)
	24/04/25 16:43:24 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.4 KiB, free 365.9 MiB)
	24/04/25 16:43:24 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 15dedf891bed:37063 (size: 12.4 KiB, free: 366.3 MiB)
	24/04/25 16:43:24 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
	24/04/25 16:43:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/25 16:43:24 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
	24/04/25 16:43:25 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (15dedf891bed, executor driver, partition 0, PROCESS_LOCAL, 9885 bytes) 
	24/04/25 16:43:25 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:25.721 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.86 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:26.252 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:43:25 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/676-20240425162748.json:0+186591
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:27.259 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:43:26 INFO CodeGenerator: Code generated in 193.873417 ms
	24/04/25 16:43:26 INFO PythonRunner: Times: total = 852, boot = 492, init = 336, finish = 24
	24/04/25 16:43:26 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2751 bytes result sent to driver
	24/04/25 16:43:26 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1768 ms on 15dedf891bed (executor driver) (1/1)
	24/04/25 16:43:26 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
	24/04/25 16:43:26 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 51473
	24/04/25 16:43:26 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 2.315 s
	24/04/25 16:43:26 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/25 16:43:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
	24/04/25 16:43:26 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 2.554180 s
	
	
	
	 1 
	
	
	
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:27.755 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.890625 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:28.262 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:43:27 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 15dedf891bed:37063 in memory (size: 12.4 KiB, free: 366.3 MiB)
	
	
	
	 2 
	
	
	
	
	
	
	 3 
	
	
	
	
	
	
	 4 
	
	
	
	24/04/25 16:43:28 INFO CodeGenerator: Code generated in 51.397375 ms
	24/04/25 16:43:28 INFO CodeGenerator: Code generated in 22.724326 ms
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:28.795 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.735632183908046 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:29.422 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:43:28 INFO SparkContext: Starting job: collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1862/spark_reddit_preprocessing.py:84
	24/04/25 16:43:28 INFO DAGScheduler: Got job 1 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1862/spark_reddit_preprocessing.py:84) with 1 output partitions
	24/04/25 16:43:28 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1862/spark_reddit_preprocessing.py:84)
	24/04/25 16:43:28 INFO DAGScheduler: Parents of final stage: List()
	24/04/25 16:43:28 INFO DAGScheduler: Missing parents: List()
	24/04/25 16:43:28 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[19] at toJavaRDD at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/25 16:43:28 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 72.0 KiB, free 365.9 MiB)
	24/04/25 16:43:28 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.6 KiB, free 365.8 MiB)
	24/04/25 16:43:28 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 15dedf891bed:37063 (size: 28.6 KiB, free: 366.2 MiB)
	24/04/25 16:43:28 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
	24/04/25 16:43:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[19] at toJavaRDD at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/25 16:43:28 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
	24/04/25 16:43:28 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (15dedf891bed, executor driver, partition 0, PROCESS_LOCAL, 9885 bytes) 
	24/04/25 16:43:28 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
	24/04/25 16:43:28 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/userFiles-8fc0a71a-5676-4366-b3ed-eac77d7e7032/676-20240425162748.json:0+186591
	24/04/25 16:43:28 INFO CodeGenerator: Code generated in 14.598046 ms
	24/04/25 16:43:28 INFO CodeGenerator: Code generated in 13.486393 ms
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:30.445 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:43:29 INFO CodeGenerator: Code generated in 57.989477 ms
	24/04/25 16:43:29 INFO CodeGenerator: Code generated in 93.530441 ms
	24/04/25 16:43:29 INFO PythonRunner: Times: total = 275, boot = -2118, init = 2379, finish = 14
	24/04/25 16:43:29 INFO CodeGenerator: Code generated in 28.570334 ms
	24/04/25 16:43:30 INFO CodeGenerator: Code generated in 112.4079 ms
	24/04/25 16:43:30 INFO PythonUDFRunner: Times: total = 888, boot = 573, init = 313, finish = 2
	24/04/25 16:43:30 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 172332 bytes result sent to driver
	24/04/25 16:43:30 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1596 ms on 15dedf891bed (executor driver) (1/1)
	24/04/25 16:43:30 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
	24/04/25 16:43:30 INFO DAGScheduler: ResultStage 1 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1862/spark_reddit_preprocessing.py:84) finished in 1.626 s
	24/04/25 16:43:30 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/25 16:43:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
	24/04/25 16:43:30 INFO DAGScheduler: Job 1 finished: collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1862/spark_reddit_preprocessing.py:84, took 1.640260 s
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:30.872 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.736 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:31.446 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:43:30 INFO CodeGenerator: Code generated in 7.414035 ms
	24/04/25 16:43:30 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/25 16:43:30 INFO DAGScheduler: Got job 2 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/25 16:43:30 INFO DAGScheduler: Final stage: ResultStage 2 (save at NativeMethodAccessorImpl.java:0)
	24/04/25 16:43:30 INFO DAGScheduler: Parents of final stage: List()
	24/04/25 16:43:30 INFO DAGScheduler: Missing parents: List()
	24/04/25 16:43:30 INFO DAGScheduler: Submitting ResultStage 2 (SQLExecutionRDD[26] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/25 16:43:30 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 20.6 KiB, free 365.8 MiB)
	24/04/25 16:43:30 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 365.8 MiB)
	24/04/25 16:43:30 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 15dedf891bed:37063 (size: 9.0 KiB, free: 366.2 MiB)
	24/04/25 16:43:30 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
	24/04/25 16:43:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (SQLExecutionRDD[26] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/25 16:43:30 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
	24/04/25 16:43:30 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (15dedf891bed, executor driver, partition 0, PROCESS_LOCAL, 10438 bytes) 
	24/04/25 16:43:30 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
	24/04/25 16:43:30 INFO CodeGenerator: Code generated in 11.507957 ms
	24/04/25 16:43:30 INFO CodeGenerator: Code generated in 30.181055 ms
	24/04/25 16:43:31 INFO ProducerConfig: ProducerConfig values: 
		acks = -1
		auto.include.jmx.reporter = true
		batch.size = 16384
		bootstrap.servers = [kafka:9092]
		buffer.memory = 33554432
		client.dns.lookup = use_all_dns_ips
		client.id = producer-1
		compression.type = none
		connections.max.idle.ms = 540000
		delivery.timeout.ms = 120000
		enable.idempotence = true
		interceptor.classes = []
		key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
		linger.ms = 0
		max.block.ms = 60000
		max.in.flight.requests.per.connection = 5
		max.request.size = 1048576
		metadata.max.age.ms = 300000
		metadata.max.idle.ms = 300000
		metric.reporters = []
		metrics.num.samples = 2
		metrics.recording.level = INFO
		metrics.sample.window.ms = 30000
		partitioner.adaptive.partitioning.enable = true
		partitioner.availability.timeout.ms = 0
		partitioner.class = null
		partitioner.ignore.keys = false
		receive.buffer.bytes = 32768
		reconnect.backoff.max.ms = 1000
		reconnect.backoff.ms = 50
		request.timeout.ms = 30000
		retries = 2147483647
		retry.backoff.ms = 100
		sasl.client.callback.handler.class = null
		sasl.jaas.config = null
		sasl.kerberos.kinit.cmd = /usr/bin/kinit
		sasl.kerberos.min.time.before.relogin = 60000
		sasl.kerberos.service.name = null
		sasl.kerberos.ticket.renew.jitter = 0.05
		sasl.kerberos.ticket.renew.window.factor = 0.8
		sasl.login.callback.handler.class = null
		sasl.login.class = null
		sasl.login.connect.timeout.ms = null
		sasl.login.read.timeout.ms = null
		sasl.login.refresh.buffer.seconds = 300
		sasl.login.refresh.min.period.seconds = 60
		sasl.login.refresh.window.factor = 0.8
		sasl.login.refresh.window.jitter = 0.05
		sasl.login.retry.backoff.max.ms = 10000
		sasl.login.retry.backoff.ms = 100
		sasl.mechanism = GSSAPI
		sasl.oauthbearer.clock.skew.seconds = 30
		sasl.oauthbearer.expected.audience = null
		sasl.oauthbearer.expected.issuer = null
		sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
		sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
		sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
		sasl.oauthbearer.jwks.endpoint.url = null
		sasl.oauthbearer.scope.claim.name = scope
		sasl.oauthbearer.sub.claim.name = sub
		sasl.oauthbearer.token.endpoint.url = null
		security.protocol = PLAINTEXT
		security.providers = null
		send.buffer.bytes = 131072
		socket.connection.setup.timeout.max.ms = 30000
		socket.connection.setup.timeout.ms = 10000
		ssl.cipher.suites = null
		ssl.enabled.protocols = [TLSv1.2]
		ssl.endpoint.identification.algorithm = https
		ssl.engine.factory.class = null
		ssl.key.password = null
		ssl.keymanager.algorithm = SunX509
		ssl.keystore.certificate.chain = null
		ssl.keystore.key = null
		ssl.keystore.location = null
		ssl.keystore.password = null
		ssl.keystore.type = JKS
		ssl.protocol = TLSv1.2
		ssl.provider = null
		ssl.secure.random.implementation = null
		ssl.trustmanager.algorithm = PKIX
		ssl.truststore.certificates = null
		ssl.truststore.location = null
		ssl.truststore.password = null
		ssl.truststore.type = JKS
		transaction.timeout.ms = 60000
		transactional.id = null
		value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	
	24/04/25 16:43:31 INFO KafkaProducer: [Producer clientId=producer-1] Instantiated an idempotent producer.
	24/04/25 16:43:31 INFO AppInfoParser: Kafka version: 3.4.1
	24/04/25 16:43:31 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
	24/04/25 16:43:31 INFO AppInfoParser: Kafka startTimeMs: 1714034611152
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:32.448 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:43:31 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 15dedf891bed:37063 in memory (size: 28.6 KiB, free: 366.3 MiB)
	24/04/25 16:43:31 INFO Metadata: [Producer clientId=producer-1] Resetting the last seen epoch of partition reddit_preprocessed-0 to 0 since the associated topicId changed from null to tSxLTfcgQe6KuiaQjcDezw
	24/04/25 16:43:31 INFO Metadata: [Producer clientId=producer-1] Cluster ID: 48nq0XTNRoqMTNfMjfKvtg
	24/04/25 16:43:31 INFO TransactionManager: [Producer clientId=producer-1] ProducerId set to 2 with epoch 0
	24/04/25 16:43:32 INFO PythonRunner: Times: total = 89, boot = -1809, init = 1898, finish = 0
	24/04/25 16:43:32 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1707 bytes result sent to driver
	24/04/25 16:43:32 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1338 ms on 15dedf891bed (executor driver) (1/1)
	24/04/25 16:43:32 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
	24/04/25 16:43:32 INFO DAGScheduler: ResultStage 2 (save at NativeMethodAccessorImpl.java:0) finished in 1.358 s
	24/04/25 16:43:32 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/25 16:43:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
	24/04/25 16:43:32 INFO DAGScheduler: Job 2 finished: save at NativeMethodAccessorImpl.java:0, took 1.385287 s
	24/04/25 16:43:32 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/25 16:43:32 INFO DAGScheduler: Got job 3 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/25 16:43:32 INFO DAGScheduler: Final stage: ResultStage 3 (save at NativeMethodAccessorImpl.java:0)
	24/04/25 16:43:32 INFO DAGScheduler: Parents of final stage: List()
	24/04/25 16:43:32 INFO DAGScheduler: Missing parents: List()
	24/04/25 16:43:32 INFO DAGScheduler: Submitting ResultStage 3 (SQLExecutionRDD[33] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/25 16:43:32 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 20.6 KiB, free 365.9 MiB)
	24/04/25 16:43:32 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 365.9 MiB)
	24/04/25 16:43:32 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 15dedf891bed:37063 (size: 9.0 KiB, free: 366.3 MiB)
	24/04/25 16:43:32 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
	24/04/25 16:43:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (SQLExecutionRDD[33] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/25 16:43:32 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
	24/04/25 16:43:32 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (15dedf891bed, executor driver, partition 0, PROCESS_LOCAL, 10715 bytes) 
	24/04/25 16:43:32 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
	24/04/25 16:43:32 INFO PythonRunner: Times: total = 93, boot = -1364, init = 1456, finish = 1
	24/04/25 16:43:32 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1621 bytes result sent to driver
	24/04/25 16:43:32 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 110 ms on 15dedf891bed (executor driver) (1/1)
	24/04/25 16:43:32 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
	24/04/25 16:43:32 INFO DAGScheduler: ResultStage 3 (save at NativeMethodAccessorImpl.java:0) finished in 0.132 s
	24/04/25 16:43:32 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/25 16:43:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
	24/04/25 16:43:32 INFO DAGScheduler: Job 3 finished: save at NativeMethodAccessorImpl.java:0, took 0.164027 s
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:32.903 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8192771084337349 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:33.450 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:43:32 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/25 16:43:32 INFO DAGScheduler: Got job 4 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/25 16:43:32 INFO DAGScheduler: Final stage: ResultStage 4 (save at NativeMethodAccessorImpl.java:0)
	24/04/25 16:43:32 INFO DAGScheduler: Parents of final stage: List()
	24/04/25 16:43:32 INFO DAGScheduler: Missing parents: List()
	24/04/25 16:43:32 INFO DAGScheduler: Submitting ResultStage 4 (SQLExecutionRDD[40] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/25 16:43:32 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 20.6 KiB, free 365.9 MiB)
	24/04/25 16:43:32 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 365.8 MiB)
	24/04/25 16:43:32 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 15dedf891bed:37063 (size: 9.0 KiB, free: 366.2 MiB)
	24/04/25 16:43:32 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
	24/04/25 16:43:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (SQLExecutionRDD[40] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/25 16:43:32 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
	24/04/25 16:43:32 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (15dedf891bed, executor driver, partition 0, PROCESS_LOCAL, 10129 bytes) 
	24/04/25 16:43:32 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
	24/04/25 16:43:32 INFO PythonRunner: Times: total = 219, boot = -278, init = 496, finish = 1
	24/04/25 16:43:32 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1621 bytes result sent to driver
	24/04/25 16:43:32 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 247 ms on 15dedf891bed (executor driver) (1/1)
	24/04/25 16:43:32 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
	24/04/25 16:43:32 INFO DAGScheduler: ResultStage 4 (save at NativeMethodAccessorImpl.java:0) finished in 0.308 s
	24/04/25 16:43:32 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/25 16:43:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
	24/04/25 16:43:32 INFO DAGScheduler: Job 4 finished: save at NativeMethodAccessorImpl.java:0, took 0.349964 s
	24/04/25 16:43:33 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/25 16:43:33 INFO DAGScheduler: Got job 5 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/25 16:43:33 INFO DAGScheduler: Final stage: ResultStage 5 (save at NativeMethodAccessorImpl.java:0)
	24/04/25 16:43:33 INFO DAGScheduler: Parents of final stage: List()
	24/04/25 16:43:33 INFO DAGScheduler: Missing parents: List()
	24/04/25 16:43:33 INFO DAGScheduler: Submitting ResultStage 5 (SQLExecutionRDD[47] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/25 16:43:33 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 20.6 KiB, free 365.8 MiB)
	24/04/25 16:43:33 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 365.8 MiB)
	24/04/25 16:43:33 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 15dedf891bed:37063 (size: 9.0 KiB, free: 366.2 MiB)
	24/04/25 16:43:33 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
	24/04/25 16:43:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (SQLExecutionRDD[47] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/25 16:43:33 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
	24/04/25 16:43:33 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (15dedf891bed, executor driver, partition 0, PROCESS_LOCAL, 10184 bytes) 
	24/04/25 16:43:33 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
	24/04/25 16:43:33 INFO PythonRunner: Times: total = 107, boot = -210, init = 317, finish = 0
	24/04/25 16:43:33 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1621 bytes result sent to driver
	24/04/25 16:43:33 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 144 ms on 15dedf891bed (executor driver) (1/1)
	24/04/25 16:43:33 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
	24/04/25 16:43:33 INFO DAGScheduler: ResultStage 5 (save at NativeMethodAccessorImpl.java:0) finished in 0.170 s
	24/04/25 16:43:33 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/25 16:43:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
	24/04/25 16:43:33 INFO DAGScheduler: Job 5 finished: save at NativeMethodAccessorImpl.java:0, took 0.193183 s
	24/04/25 16:43:33 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/25 16:43:33 INFO DAGScheduler: Got job 6 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/25 16:43:33 INFO DAGScheduler: Final stage: ResultStage 6 (save at NativeMethodAccessorImpl.java:0)
	24/04/25 16:43:33 INFO DAGScheduler: Parents of final stage: List()
	24/04/25 16:43:33 INFO DAGScheduler: Missing parents: List()
	24/04/25 16:43:33 INFO DAGScheduler: Submitting ResultStage 6 (SQLExecutionRDD[54] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/25 16:43:33 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 20.6 KiB, free 365.8 MiB)
	24/04/25 16:43:33 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 365.8 MiB)
	24/04/25 16:43:33 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 15dedf891bed:37063 (size: 9.0 KiB, free: 366.2 MiB)
	24/04/25 16:43:33 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
	24/04/25 16:43:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (SQLExecutionRDD[54] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/25 16:43:33 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
	24/04/25 16:43:33 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (15dedf891bed, executor driver, partition 0, PROCESS_LOCAL, 10081 bytes) 
	24/04/25 16:43:33 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
	24/04/25 16:43:33 INFO PythonRunner: Times: total = 72, boot = -142, init = 214, finish = 0
	24/04/25 16:43:33 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1621 bytes result sent to driver
	24/04/25 16:43:33 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 91 ms on 15dedf891bed (executor driver) (1/1)
	24/04/25 16:43:33 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
	24/04/25 16:43:33 INFO DAGScheduler: ResultStage 6 (save at NativeMethodAccessorImpl.java:0) finished in 0.110 s
	24/04/25 16:43:33 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/25 16:43:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
	24/04/25 16:43:33 INFO DAGScheduler: Job 6 finished: save at NativeMethodAccessorImpl.java:0, took 0.125743 s
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:33.917 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7325905292479109 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:34.453 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/25 16:43:33 INFO SparkContext: Invoking stop() from shutdown hook
	24/04/25 16:43:33 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/04/25 16:43:33 INFO SparkUI: Stopped Spark web UI at http://15dedf891bed:4041
	24/04/25 16:43:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/04/25 16:43:33 INFO MemoryStore: MemoryStore cleared
	24/04/25 16:43:33 INFO BlockManager: BlockManager stopped
	24/04/25 16:43:33 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/04/25 16:43:33 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/04/25 16:43:33 INFO SparkContext: Successfully stopped SparkContext
	24/04/25 16:43:33 INFO ShutdownHookManager: Shutdown hook called
	24/04/25 16:43:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4/pyspark-39b926d2-0e50-4624-88e9-2adea8c93ad9
	24/04/25 16:43:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-517b2677-aad8-4700-8b07-938c0c6de223
	24/04/25 16:43:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-64aca201-0dd7-4204-b93a-d2d51b7a13c4
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:34.454 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1862, processId:8091 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:34.454 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:34.455 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:34.455 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:34.456 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:34.457 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:34.458 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:34.458 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1862
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:34.459 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1862
[WI-682][TI-1862] - [INFO] 2024-04-25 16:43:34.459 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1862] - [INFO] 2024-04-25 16:43:34.613 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1862, processInstanceId=682, status=7, startTime=1714034582401, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/682/1862.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1862, endTime=1714034614455, processId=8091, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1862] - [INFO] 2024-04-25 16:43:34.624 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1862, processInstanceId=682, status=7, startTime=1714034582401, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/682/1862.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1862, endTime=1714034614455, processId=8091, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], eventCreateTime=0, eventSendTime=1714034614613)
[WI-0][TI-1862] - [INFO] 2024-04-25 16:43:35.436 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1862, success=true)
[WI-0][TI-1862] - [INFO] 2024-04-25 16:43:35.439 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1862, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:35.468 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1863, taskName=move to archives, firstSubmitTime=1714034615455, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=116, appIds=null, processInstanceId=682, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the reddit archive folder\nmv ${raw_file_dir} ${REDDIT_HOME}/archive\n","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to archives'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1863'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13375839065312'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425164335'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='682'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/676-20240425162748.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.469 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to archives to wait queue success
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.470 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.475 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.476 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.476 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.476 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714034615476
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.476 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 682_1863
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.477 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1863,
  "taskName" : "move to archives",
  "firstSubmitTime" : 1714034615455,
  "startTime" : 1714034615476,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13201021801792/116/682/1863.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 116,
  "processInstanceId" : 682,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the reddit archive folder\\nmv ${raw_file_dir} ${REDDIT_HOME}/archive\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to archives"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1863"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13375839065312"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425164335"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "682"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/676-20240425162748.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "682_1863",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/676-20240425162748.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.478 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.479 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.479 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.483 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.483 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.488 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1863 check successfully
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.489 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.490 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.490 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.490 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.490 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the reddit archive folder\nmv ${raw_file_dir} ${REDDIT_HOME}/archive\n",
  "resourceList" : [ ]
}
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.491 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.491 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}] successfully
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.491 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.491 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.491 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.492 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.492 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.492 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the reddit archive folder
mv /local_storage/reddit/processing/676-20240425162748.json /local_storage/reddit/archive

[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.492 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.492 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1863/682_1863.sh
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:35.498 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 8395
[WI-0][TI-1863] - [INFO] 2024-04-25 16:43:35.630 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1863, processInstanceId=682, startTime=1714034615476, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/682/1863.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1863] - [INFO] 2024-04-25 16:43:35.634 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1863, processInstanceId=682, startTime=1714034615476, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/682/1863.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714034615628)
[WI-0][TI-1863] - [INFO] 2024-04-25 16:43:35.634 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1863, processInstanceId=682, startTime=1714034615476, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1863] - [INFO] 2024-04-25 16:43:35.636 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1863, processInstanceId=682, startTime=1714034615476, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714034615628)
[WI-0][TI-1863] - [INFO] 2024-04-25 16:43:36.442 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1863, success=true)
[WI-0][TI-1863] - [INFO] 2024-04-25 16:43:36.447 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1863)
[WI-0][TI-1863] - [INFO] 2024-04-25 16:43:36.452 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1863, success=true)
[WI-0][TI-1863] - [INFO] 2024-04-25 16:43:36.457 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1863)
[WI-0][TI-0] - [INFO] 2024-04-25 16:43:36.499 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:36.502 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1863, processId:8395 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:36.502 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:36.502 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:36.502 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:36.503 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:36.512 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:36.512 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:36.513 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1863
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:36.513 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1863
[WI-682][TI-1863] - [INFO] 2024-04-25 16:43:36.514 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1863] - [INFO] 2024-04-25 16:43:36.638 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1863, processInstanceId=682, status=7, startTime=1714034615476, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/682/1863.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1863, endTime=1714034616502, processId=8395, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1863] - [INFO] 2024-04-25 16:43:36.641 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1863, processInstanceId=682, status=7, startTime=1714034615476, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/682/1863.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/682/1863, endTime=1714034616502, processId=8395, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}], eventCreateTime=0, eventSendTime=1714034616638)
[WI-0][TI-1863] - [INFO] 2024-04-25 16:43:37.439 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1863, success=true)
[WI-0][TI-1863] - [INFO] 2024-04-25 16:43:37.443 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1863, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:44:23.232 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7731092436974789 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:44:28.277 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7005813953488372 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:44:52.578 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7715877437325905 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:46:03.960 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1864, taskName=extract from preprocessed queue, firstSubmitTime=1714034763951, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=15, appIds=null, processInstanceId=683, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"message","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n    --bootstrap-server kafka:9092 \\\n    --topic reddit_preprocessed \\\n    --group preprocessed_consumers \\\n    --max-messages 1 \\\n    --timeout-ms 10000)\n\necho \"#{setValue(message=${message})}\"","resourceList":[]}, environmentConfig=export JAVA_HOME=/opt/java/openjdk, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract from preprocessed queue'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1864'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13375898458848'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425164603'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='683'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_data'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:03.962 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract from preprocessed queue to wait queue success
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:03.966 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.017 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.022 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.022 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.022 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714034764022
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.022 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 683_1864
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.022 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1864,
  "taskName" : "extract from preprocessed queue",
  "firstSubmitTime" : 1714034763951,
  "startTime" : 1714034764022,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13377949373536/15/683/1864.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 15,
  "processInstanceId" : 683,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"message\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\\\\n    --bootstrap-server kafka:9092 \\\\\\n    --topic reddit_preprocessed \\\\\\n    --group preprocessed_consumers \\\\\\n    --max-messages 1 \\\\\\n    --timeout-ms 10000)\\n\\necho \\\"#{setValue(message=${message})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export JAVA_HOME=/opt/java/openjdk",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract from preprocessed queue"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1864"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13375898458848"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425164603"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "683"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_data"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "683_1864",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.022 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.022 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.023 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.048 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.048 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.049 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1864 check successfully
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.049 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.049 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.049 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.050 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.050 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "message",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n    --bootstrap-server kafka:9092 \\\n    --topic reddit_preprocessed \\\n    --group preprocessed_consumers \\\n    --max-messages 1 \\\n    --timeout-ms 10000)\n\necho \"#{setValue(message=${message})}\"",
  "resourceList" : [ ]
}
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.050 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.050 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.050 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.050 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.050 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.050 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.050 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.050 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export JAVA_HOME=/opt/java/openjdk
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

message=$(${KAFKA_DIR}/kafka-console-consumer.sh \
    --bootstrap-server kafka:9092 \
    --topic reddit_preprocessed \
    --group preprocessed_consumers \
    --max-messages 1 \
    --timeout-ms 10000)

echo "#{setValue(message=${message})}"
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.051 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.051 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1864/683_1864.sh
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:04.069 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 8438
[WI-0][TI-0] - [INFO] 2024-04-25 16:46:04.077 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1864] - [INFO] 2024-04-25 16:46:04.872 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1864, success=true)
[WI-0][TI-1864] - [INFO] 2024-04-25 16:46:04.880 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1864)
[WI-0][TI-0] - [INFO] 2024-04-25 16:46:05.005 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7050938337801609 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:46:05.093 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1864/683_1864.sh: line 12: /opt/kafka_2.13-3.7.0/bin/kafka-console-consumer.sh: No such file or directory
	#{setValue(message=)}
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:05.095 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1864, processId:8438 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-683][TI-1864] - [WARN] 2024-04-25 16:46:05.096 +0800 o.a.d.p.t.a.p.AbstractParameters:[152] - Cannot find the output parameter message in the task output parameters
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:05.096 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:05.096 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:05.096 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:05.097 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:05.100 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:05.100 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:05.100 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1864
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:05.101 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1864
[WI-683][TI-1864] - [INFO] 2024-04-25 16:46:05.101 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1864] - [INFO] 2024-04-25 16:46:05.879 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1864, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:46:06.029 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1865, taskName=keyword filtering, firstSubmitTime=1714034766017, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=15, appIds=null, processInstanceId=683, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [${keywords}]\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1865'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425164606'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='683'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_data'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.031 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.031 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.032 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.032 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.032 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.032 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714034766032
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.032 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 683_1865
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.033 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1865,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1714034766017,
  "startTime" : 1714034766032,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13377949373536/15/683/1865.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 15,
  "processInstanceId" : 683,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [${keywords}]\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1865"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425164606"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "683"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_data"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "683_1865",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.033 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.033 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.034 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.068 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.068 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.069 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1865 check successfully
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.069 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.069 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.069 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.069 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.069 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [${keywords}]\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.070 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.070 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.070 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.070 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.070 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.070 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [${keywords}]

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.070 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1865
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.071 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1865/py_683_1865.py
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.071 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = ["singapore", "lol"]

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.071 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.072 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.072 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1865/py_683_1865.py
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.072 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.072 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1865/683_1865.sh
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:06.089 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 8450
[WI-0][TI-0] - [INFO] 2024-04-25 16:46:06.093 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1865] - [INFO] 2024-04-25 16:46:06.885 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1865, success=true)
[WI-0][TI-1865] - [INFO] 2024-04-25 16:46:06.895 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1865)
[WI-0][TI-0] - [INFO] 2024-04-25 16:46:07.065 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7197802197802198 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:46:07.107 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1865/py_683_1865.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:07.107 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1865, processId:8450 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:07.109 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:07.109 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:07.109 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:07.109 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:07.113 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:07.114 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:07.114 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1865
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:07.114 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1865
[WI-683][TI-1865] - [INFO] 2024-04-25 16:46:07.115 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1865] - [INFO] 2024-04-25 16:46:07.879 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1865, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:46:07.936 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1866, taskName=keyword filtering, firstSubmitTime=1714034767927, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=15, appIds=null, processInstanceId=683, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [${keywords}]\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1866'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425164607'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='683'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_data'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.937 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.937 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.938 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.938 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.938 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.938 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714034767938
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.938 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 683_1866
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.938 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1866,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1714034767927,
  "startTime" : 1714034767938,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13377949373536/15/683/1866.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 15,
  "processInstanceId" : 683,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [${keywords}]\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1866"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425164607"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "683"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_data"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "683_1866",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.939 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.939 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.939 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.942 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.943 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.943 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1866 check successfully
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.943 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.943 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.943 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.943 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.944 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [${keywords}]\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.944 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.944 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.944 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.944 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.944 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.944 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [${keywords}]

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.945 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1866
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.945 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1866/py_683_1866.py
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.945 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = ["singapore", "lol"]

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.945 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.946 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.946 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1866/py_683_1866.py
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.946 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.946 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1866/683_1866.sh
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:07.950 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 8461
[WI-0][TI-1866] - [INFO] 2024-04-25 16:46:08.017 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1866, processInstanceId=683, startTime=1714034767938, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/15/683/1866.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1866] - [INFO] 2024-04-25 16:46:08.020 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1866, processInstanceId=683, startTime=1714034767938, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/15/683/1866.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714034768016)
[WI-0][TI-1866] - [INFO] 2024-04-25 16:46:08.021 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1866, processInstanceId=683, startTime=1714034767938, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1866] - [INFO] 2024-04-25 16:46:08.022 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1866, processInstanceId=683, startTime=1714034767938, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714034768016)
[WI-0][TI-1866] - [INFO] 2024-04-25 16:46:08.886 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1866, success=true)
[WI-0][TI-1866] - [INFO] 2024-04-25 16:46:08.891 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1866)
[WI-0][TI-1866] - [INFO] 2024-04-25 16:46:08.896 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1866, success=true)
[WI-0][TI-1866] - [INFO] 2024-04-25 16:46:08.903 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1866)
[WI-0][TI-0] - [INFO] 2024-04-25 16:46:08.950 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1866/py_683_1866.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:08.952 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1866, processId:8461 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:08.952 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:08.953 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:08.953 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:08.953 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:08.955 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:08.955 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:08.955 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1866
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:08.956 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1866
[WI-683][TI-1866] - [INFO] 2024-04-25 16:46:08.956 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1866] - [INFO] 2024-04-25 16:46:09.031 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1866, processInstanceId=683, status=6, startTime=1714034767938, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/15/683/1866.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1866, endTime=1714034768953, processId=8461, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1866] - [INFO] 2024-04-25 16:46:09.034 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1866, processInstanceId=683, status=6, startTime=1714034767938, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/15/683/1866.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1866, endTime=1714034768953, processId=8461, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714034769031)
[WI-0][TI-1866] - [INFO] 2024-04-25 16:46:09.887 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1866, success=true)
[WI-0][TI-1866] - [INFO] 2024-04-25 16:46:09.890 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1866, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:46:10.009 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1867, taskName=keyword filtering, firstSubmitTime=1714034769970, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=15, appIds=null, processInstanceId=683, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [${keywords}]\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1867'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425164609'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='683'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_data'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.012 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.013 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.013 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.013 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.013 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.013 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714034770013
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.014 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 683_1867
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.014 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1867,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1714034769970,
  "startTime" : 1714034770013,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13377949373536/15/683/1867.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 15,
  "processInstanceId" : 683,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [${keywords}]\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1867"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425164609"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "683"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_data"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "683_1867",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.014 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.015 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.015 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.017 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.017 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.017 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1867 check successfully
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.017 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.017 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.018 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.018 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.018 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [${keywords}]\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.018 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.018 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.018 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.018 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.019 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.019 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [${keywords}]

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.019 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1867
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.020 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1867/py_683_1867.py
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.020 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = ["singapore", "lol"]

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.020 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.021 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.021 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1867/py_683_1867.py
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.021 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.021 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1867/683_1867.sh
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:10.032 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 8478
[WI-0][TI-1867] - [INFO] 2024-04-25 16:46:10.037 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1867, processInstanceId=683, startTime=1714034770013, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/15/683/1867.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-0] - [INFO] 2024-04-25 16:46:10.033 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1867] - [INFO] 2024-04-25 16:46:10.040 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1867, processInstanceId=683, startTime=1714034770013, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/15/683/1867.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714034770035)
[WI-0][TI-1867] - [INFO] 2024-04-25 16:46:10.040 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1867, processInstanceId=683, startTime=1714034770013, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1867] - [INFO] 2024-04-25 16:46:10.041 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1867, processInstanceId=683, startTime=1714034770013, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714034770035)
[WI-0][TI-1867] - [INFO] 2024-04-25 16:46:10.901 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1867, success=true)
[WI-0][TI-1867] - [INFO] 2024-04-25 16:46:10.913 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1867)
[WI-0][TI-1867] - [INFO] 2024-04-25 16:46:10.924 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1867, success=true)
[WI-0][TI-1867] - [INFO] 2024-04-25 16:46:10.936 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1867)
[WI-0][TI-0] - [INFO] 2024-04-25 16:46:11.039 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1867/py_683_1867.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:11.041 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1867, processId:8478 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:11.042 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:11.044 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:11.044 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:11.044 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:11.052 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:11.052 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:11.052 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1867
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:11.053 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1867
[WI-683][TI-1867] - [INFO] 2024-04-25 16:46:11.054 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1867] - [INFO] 2024-04-25 16:46:11.897 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1867, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:46:11.924 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1868, taskName=keyword filtering, firstSubmitTime=1714034771914, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=15, appIds=null, processInstanceId=683, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [${keywords}]\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1868'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425164611'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='683'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_data'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.926 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.926 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.927 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.927 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.927 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.928 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714034771928
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.928 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 683_1868
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.928 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1868,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1714034771914,
  "startTime" : 1714034771928,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13377949373536/15/683/1868.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 15,
  "processInstanceId" : 683,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [${keywords}]\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1868"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425164611"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "683"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_data"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "683_1868",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.929 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.929 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.929 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.933 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.933 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.934 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1868 check successfully
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.935 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.935 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.935 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.936 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.936 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [${keywords}]\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.937 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.937 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.939 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.939 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.940 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.940 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [${keywords}]

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.941 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1868
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.942 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1868/py_683_1868.py
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.942 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = ["singapore", "lol"]

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.945 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.945 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.946 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1868/py_683_1868.py
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.946 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.946 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1868/683_1868.sh
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:11.950 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 8489
[WI-0][TI-1868] - [INFO] 2024-04-25 16:46:12.041 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1868, processInstanceId=683, startTime=1714034771928, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/15/683/1868.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1868] - [INFO] 2024-04-25 16:46:12.045 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1868, processInstanceId=683, startTime=1714034771928, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/15/683/1868.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714034772041)
[WI-0][TI-1868] - [INFO] 2024-04-25 16:46:12.045 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1868, processInstanceId=683, startTime=1714034771928, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1868] - [INFO] 2024-04-25 16:46:12.047 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1868, processInstanceId=683, startTime=1714034771928, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714034772041)
[WI-0][TI-1868] - [INFO] 2024-04-25 16:46:12.899 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1868, success=true)
[WI-0][TI-1868] - [INFO] 2024-04-25 16:46:12.918 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1868)
[WI-0][TI-1868] - [INFO] 2024-04-25 16:46:12.925 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1868, success=true)
[WI-0][TI-1868] - [INFO] 2024-04-25 16:46:12.948 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1868)
[WI-0][TI-0] - [INFO] 2024-04-25 16:46:12.956 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1868/py_683_1868.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:12.985 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1868, processId:8489 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:12.986 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:12.986 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:12.986 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:12.987 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:12.997 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:12.997 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:12.997 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1868
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:12.997 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1868
[WI-683][TI-1868] - [INFO] 2024-04-25 16:46:12.998 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1868] - [INFO] 2024-04-25 16:46:13.049 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1868, processInstanceId=683, status=6, startTime=1714034771928, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/15/683/1868.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1868, endTime=1714034772987, processId=8489, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1868] - [INFO] 2024-04-25 16:46:13.052 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1868, processInstanceId=683, status=6, startTime=1714034771928, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13377949373536/15/683/1868.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_15/683/1868, endTime=1714034772987, processId=8489, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714034773049)
[WI-0][TI-1868] - [INFO] 2024-04-25 16:46:13.891 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1868, success=true)
[WI-0][TI-1868] - [INFO] 2024-04-25 16:46:13.893 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1868, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 16:46:14.096 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.785342744194235 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 16:46:21.126 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7138810198300283 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1858] - [INFO] 2024-04-25 16:48:06.710 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1858, processInstanceId=681, status=9, startTime=1714034568290, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1858.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1858, endTime=1714034585763, processId=7900, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], eventCreateTime=0, eventSendTime=1714034586330)
[WI-0][TI-1858] - [INFO] 2024-04-25 16:48:06.713 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1858, processInstanceId=681, status=9, startTime=1714034568290, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1858.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1858, endTime=1714034585763, processId=7900, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], eventCreateTime=0, eventSendTime=1714034886710)
[WI-0][TI-1792] - [INFO] 2024-04-25 16:48:12.742 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1792, processInstanceId=661, status=9, startTime=1714031949850, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/661/1792.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792, endTime=1714032332757, processId=4479, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714034592360)
[WI-0][TI-1792] - [INFO] 2024-04-25 16:48:12.745 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1792, processInstanceId=661, status=9, startTime=1714031949850, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/661/1792.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792, endTime=1714032332757, processId=4479, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714034892742)
[WI-0][TI-1790] - [INFO] 2024-04-25 16:48:12.745 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1790, processInstanceId=660, status=9, startTime=1714031469392, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1790.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790, endTime=1714032269172, processId=4321, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714034592360)
[WI-0][TI-1790] - [INFO] 2024-04-25 16:48:12.747 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1790, processInstanceId=660, status=9, startTime=1714031469392, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1790.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790, endTime=1714032269172, processId=4321, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714034892742)
[WI-0][TI-0] - [INFO] 2024-04-25 16:52:32.309 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8245125348189415 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1858] - [INFO] 2024-04-25 16:53:07.662 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1858, processInstanceId=681, status=9, startTime=1714034568290, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1858.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1858, endTime=1714034585763, processId=7900, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], eventCreateTime=0, eventSendTime=1714034886710)
[WI-0][TI-1858] - [INFO] 2024-04-25 16:53:07.665 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1858, processInstanceId=681, status=9, startTime=1714034568290, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1858.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1858, endTime=1714034585763, processId=7900, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], eventCreateTime=0, eventSendTime=1714035187662)
[WI-0][TI-1792] - [INFO] 2024-04-25 16:53:13.690 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1792, processInstanceId=661, status=9, startTime=1714031949850, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/661/1792.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792, endTime=1714032332757, processId=4479, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714034892742)
[WI-0][TI-1792] - [INFO] 2024-04-25 16:53:13.693 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1792, processInstanceId=661, status=9, startTime=1714031949850, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/661/1792.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792, endTime=1714032332757, processId=4479, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714035193690)
[WI-0][TI-1790] - [INFO] 2024-04-25 16:53:13.693 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1790, processInstanceId=660, status=9, startTime=1714031469392, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1790.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790, endTime=1714032269172, processId=4321, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714034892742)
[WI-0][TI-1790] - [INFO] 2024-04-25 16:53:13.694 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1790, processInstanceId=660, status=9, startTime=1714031469392, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1790.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790, endTime=1714032269172, processId=4321, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714035193690)
[WI-0][TI-1858] - [INFO] 2024-04-25 16:58:08.082 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1858, processInstanceId=681, status=9, startTime=1714034568290, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1858.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1858, endTime=1714034585763, processId=7900, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], eventCreateTime=0, eventSendTime=1714035187662)
[WI-0][TI-1858] - [INFO] 2024-04-25 16:58:08.088 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1858, processInstanceId=681, status=9, startTime=1714034568290, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13201021801792/116/681/1858.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_116/681/1858, endTime=1714034585763, processId=7900, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/676-20240425162748.json"}], eventCreateTime=0, eventSendTime=1714035488082)
[WI-0][TI-1792] - [INFO] 2024-04-25 16:58:14.644 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1792, processInstanceId=661, status=9, startTime=1714031949850, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/661/1792.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792, endTime=1714032332757, processId=4479, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714035193690)
[WI-0][TI-1792] - [INFO] 2024-04-25 16:58:14.690 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1792, processInstanceId=661, status=9, startTime=1714031949850, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/661/1792.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/661/1792, endTime=1714032332757, processId=4479, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714035494643)
[WI-0][TI-1790] - [INFO] 2024-04-25 16:58:14.691 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1790, processInstanceId=660, status=9, startTime=1714031469392, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1790.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790, endTime=1714032269172, processId=4321, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714035193690)
[WI-0][TI-1790] - [INFO] 2024-04-25 16:58:14.704 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1790, processInstanceId=660, status=9, startTime=1714031469392, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240425/13386218435520/10/660/1790.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_10/660/1790, endTime=1714032269172, processId=4321, appIds=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], eventCreateTime=0, eventSendTime=1714035494643)
