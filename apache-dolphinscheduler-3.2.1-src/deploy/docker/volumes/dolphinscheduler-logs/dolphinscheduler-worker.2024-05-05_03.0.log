[WI-0][TI-0] - [INFO] 2024-05-05 03:00:00.192 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7641334477828196 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:01.197 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7737658155927682 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:02.203 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7752278501173028 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:03.208 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7754312584293657 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:04.212 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7754330479453077 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:05.221 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7758804269307833 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3262] - [INFO] 2024-05-05 03:00:05.281 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3262, processInstanceId=942, status=9, startTime=1714842212466, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262, endTime=1714842296579, processId=18774, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714848905038)
[WI-0][TI-3262] - [INFO] 2024-05-05 03:00:05.288 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3262, processInstanceId=942, status=9, startTime=1714842212466, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262, endTime=1714842296579, processId=18774, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714849205281)
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:06.233 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7753670346905619 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:07.236 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.77399010159082 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:08.244 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.774788623371118 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:09.252 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7747884245360133 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:10.262 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7753012202709207 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:11.266 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7753010214358161 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:12.271 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7752733833562689 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:13.278 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7752847169572343 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:14.281 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7752111479685116 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:15.286 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7755320678274262 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:16.302 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7747035219463253 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:17.306 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7748035360039671 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:18.309 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7754288724081099 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:19.312 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7754286735730052 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:20.315 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7753620638129455 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:21.320 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7753372094248635 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:22.339 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7757209611768493 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:23.342 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7754308607591565 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:24.346 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.775849408654457 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:25.352 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7755883381620439 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:26.363 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.776021003349775 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:27.366 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7754742068119714 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:28.369 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7755861509758927 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:29.374 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7758645201224108 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:30.378 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7760392961794034 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:31.380 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7757472074106638 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:32.383 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7757595351871526 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:33.384 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7757505876074431 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:34.389 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.775825150771689 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:35.397 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7762100955343028 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:36.411 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7762373359436406 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:37.418 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7766638372431274 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:38.420 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7761526321890572 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:39.428 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7761639657900226 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:40.432 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7761639657900226 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:41.441 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7764162875378309 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:42.443 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7763681694425042 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:43.446 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7764383582344476 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:44.452 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7765320095687406 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:45.459 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7763375488363872 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:46.474 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.776531810733636 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:47.478 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7766739778334648 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:48.483 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7766729836579416 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:49.488 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7768734094434347 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:50.493 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7767574885774203 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:51.495 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7765477175420084 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:52.503 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7763327767938754 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:53.507 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7767572897423156 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:54.519 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7765091435317052 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:55.522 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.776535787435729 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:56.542 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7766256609030335 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:57.551 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7757734536444785 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:58.554 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7756971009642906 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:00:59.563 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7753849298499809 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:00.566 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7748399228281192 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:01.567 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7753127527069908 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:02.569 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7758635259468876 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:03.570 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7758625317713643 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:04.573 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7762462835233501 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:05.575 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7764870728350883 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:06.588 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7766117424457076 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:07.589 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7764685811703553 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:08.590 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7762212303001634 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:09.591 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7762240139916287 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:10.592 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7765986193288003 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:11.593 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7766771591951394 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:12.595 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7766336143072198 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3263] - [INFO] 2024-05-05 03:01:13.351 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3263, processInstanceId=943, status=9, startTime=1714842505551, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/943/3263.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263, endTime=1714842665300, processId=20088, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714848972831)
[WI-0][TI-3263] - [INFO] 2024-05-05 03:01:13.362 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3263, processInstanceId=943, status=9, startTime=1714842505551, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/943/3263.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263, endTime=1714842665300, processId=20088, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714849273351)
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:13.596 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7764252351175404 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:14.598 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7764250362824358 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:15.599 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7766014030202655 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:16.609 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7768022264759679 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:17.612 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7766723871526277 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:18.615 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7766785510408719 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:19.615 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7765483140473224 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:20.618 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7767043996044772 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:21.628 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.776917749671773 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:22.637 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7767008205725934 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:23.642 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7765691917333112 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:24.643 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7765817183449045 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:25.644 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7766006076798468 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:26.654 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7767224935990009 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:27.654 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7767898986994792 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:28.655 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7767813487899791 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:29.656 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7770557412344041 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:30.658 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7768207181407009 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:31.659 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7768205193055963 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:32.661 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7771656970472788 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:33.663 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7771647028717555 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:34.664 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7770972977712772 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:35.665 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7771090290424519 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:36.674 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7770114010060659 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:37.674 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7774609671776928 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:38.676 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.777181206185442 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:39.677 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7771350764411618 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:40.679 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.777106046515882 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:41.680 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7771185731274753 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:42.681 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7773504148595041 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:43.682 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7772173941744893 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:44.686 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7771688784089533 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:45.692 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7771730539461511 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:46.698 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7791291937057555 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:47.698 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7787832206236542 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:48.701 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7789667454252516 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:49.703 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9232954545454546 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:50.716 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9244712990936557 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:51.732 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8292682926829268 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:52.744 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9201101928374655 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:53.748 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8928571428571428 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:54.754 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8066298342541437 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:55.764 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7810467594550566 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:56.776 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7809372013123912 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:57.778 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7811674523635828 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:58.784 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7451523545706371 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:01:59.786 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7809900914502297 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:00.789 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7810553093645568 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:01.793 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7810254840988584 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:02.797 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7158469945355191 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:03.799 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7806389486554074 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:04.801 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7805180569117767 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:05.803 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7808690008714942 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:06.820 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7811853475230017 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:07.823 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7810149458383117 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:08.824 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7808254559835747 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:09.825 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.779731465237758 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:10.827 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7805633913156382 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:11.829 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7805944095919645 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:12.831 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7806906457826179 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:13.837 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7808695973768083 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:14.838 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7808025899465392 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:15.841 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7804540320080775 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:16.851 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7807111257983975 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:17.855 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7805391334328702 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:18.857 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7799446164699492 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:19.862 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7809710032801828 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:20.867 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7807586473884103 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:21.871 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.781168446539106 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:22.873 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7808914692383204 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:23.880 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7245179063360881 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:24.884 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.778688524590164 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:25.887 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7130681818181819 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:26.938 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7472527472527473 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:27.946 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7808204851059583 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:28.951 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.829971181556196 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:29.962 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9273255813953488 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:30.964 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9174311926605505 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:31.998 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9106628242074928 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:33.027 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8262032085561497 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:34.037 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7762039660056658 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:35.040 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8310626702997275 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:36.044 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8626373626373627 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:37.068 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7834757834757835 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:38.083 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8235294117647058 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:39.084 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9100817438692099 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:40.092 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8852459016393442 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:41.094 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7838048011917381 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:42.157 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7465181058495822 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:43.160 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7842505894963765 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:44.164 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8860398860398861 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:45.170 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9278074866310161 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:46.180 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8333333333333334 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:47.191 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8235294117647058 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:48.197 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.78935329478716 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:49.203 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.754197768492872 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:50.210 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.919889502762431 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:51.362 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7400990099009901 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:52.365 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8633879781420766 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:53.368 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7927716679064035 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:54.369 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7737430167597765 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:55.408 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.836111111111111 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:56.410 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9514970890948803 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:57.440 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8282548476454293 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:58.448 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7365269461077844 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:02:59.452 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8662952646239555 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:00.455 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8212290502793296 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:01.464 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9206349206349207 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:02.482 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.861671469740634 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:03.485 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8650137741046832 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:04.489 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7465940054495912 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:05.490 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7889238109611033 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:06.492 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7887530116062039 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:07.507 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7893628388721834 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:08.526 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.921195652173913 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:09.544 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8328690807799443 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:10.548 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.793010752688172 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:11.556 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7344632768361582 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:12.561 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7859142428170321 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:13.563 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7861526461075146 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:14.565 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7082152974504249 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:15.567 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8760806916426513 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:16.572 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9186046511627908 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:17.621 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8608247422680413 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:18.624 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7873931783254624 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:19.630 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8693101339170745 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:20.638 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8361581920903955 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3258] - [INFO] 2024-05-05 03:03:21.607 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3258, processInstanceId=938, status=9, startTime=1714841447434, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, endTime=1714842191466, processId=18118, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714849101162)
[WI-0][TI-3258] - [INFO] 2024-05-05 03:03:21.614 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3258, processInstanceId=938, status=9, startTime=1714841447434, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, endTime=1714842191466, processId=18118, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714849401607)
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:21.642 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8910614525139665 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:22.666 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9166666666666666 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:23.670 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8080229226361031 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:24.685 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9059139784946237 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:25.689 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7988588530913397 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:26.690 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8248587570621468 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:27.723 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8487394957983193 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:28.724 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.706043956043956 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:29.726 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8686567164179104 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:30.731 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7877375607267264 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:31.840 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7267834336799854 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:32.917 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.812856986369967 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:33.920 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7880597014925373 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:34.923 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7890353574548152 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:35.924 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7878787878787878 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:36.927 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9113573407202216 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:38.024 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8525 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:39.026 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8588588588588588 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:40.027 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7408450704225352 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:41.028 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7937865222805671 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:42.029 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7939660703800713 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:43.037 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7935180948892816 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:44.043 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.79299615273956 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:45.043 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7928253533846606 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:46.045 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7928253533846606 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:47.047 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8635097493036211 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:48.065 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8391812865497076 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:49.065 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7926813967688898 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:50.066 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.792567464253922 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:51.069 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7925481772487704 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:52.070 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7880849256545602 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:53.076 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7872935619380299 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:54.207 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7906602379300629 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3261] - [INFO] 2024-05-05 03:03:55.012 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3261, processInstanceId=941, status=9, startTime=1714842212366, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, endTime=1714842223865, processId=18773, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714849134192)
[WI-0][TI-3261] - [INFO] 2024-05-05 03:03:55.019 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3261, processInstanceId=941, status=9, startTime=1714842212366, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, endTime=1714842223865, processId=18773, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714849435012)
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:55.216 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7854441966296257 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:56.300 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7776124795274405 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:57.305 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7769346506556687 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3260] - [INFO] 2024-05-05 03:03:58.025 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3260, processInstanceId=940, status=9, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, endTime=1714842227024, processId=18772, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714849137199)
[WI-0][TI-3260] - [INFO] 2024-05-05 03:03:58.033 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3260, processInstanceId=940, status=9, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, endTime=1714842227024, processId=18772, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714849438025)
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:58.321 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7576973542802139 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:59.323 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.834710743801653 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:59.953 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 03:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
	24/05/05 03:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
	24/05/05 03:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
	24/05/05 03:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:59.963 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 03:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
	24/05/05 03:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
	24/05/05 03:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
	24/05/05 03:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
[WI-0][TI-0] - [INFO] 2024-05-05 03:03:59.962 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 03:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
	24/05/05 03:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
	24/05/05 03:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
	24/05/05 03:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
[WI-0][TI-0] - [INFO] 2024-05-05 03:04:00.016 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 03:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
	24/05/05 03:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
	24/05/05 03:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
	24/05/05 03:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
[WI-0][TI-0] - [INFO] 2024-05-05 03:04:00.017 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 03:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
	24/05/05 03:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
	24/05/05 03:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
[WI-0][TI-0] - [INFO] 2024-05-05 03:04:00.040 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 03:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
	24/05/05 03:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
	24/05/05 03:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
	24/05/05 03:04:00 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
[WI-0][TI-0] - [INFO] 2024-05-05 03:04:00.046 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 03:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
	24/05/05 03:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
	24/05/05 03:03:59 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
	24/05/05 03:04:00 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
[WI-0][TI-0] - [INFO] 2024-05-05 03:04:00.325 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7033726211616623 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:04:01.025 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 03:04:00 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1 (kafka/172.18.0.3:9092) could not be established. Broker may not be available.
[WI-0][TI-3259] - [INFO] 2024-05-05 03:04:01.041 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3259, processInstanceId=939, status=9, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, endTime=1714842229451, processId=18762, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714849140202)
[WI-0][TI-3259] - [INFO] 2024-05-05 03:04:01.044 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3259, processInstanceId=939, status=9, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, endTime=1714842229451, processId=18762, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714849441041)
[WI-0][TI-3191] - [INFO] 2024-05-05 03:04:06.053 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3191, processInstanceId=936, status=6, startTime=1714839942174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13377949373536/25/936/3191.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/936/3191, endTime=1714840005766, processId=14716, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714849145212)
[WI-0][TI-3191] - [INFO] 2024-05-05 03:04:06.067 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3191, processInstanceId=936, status=6, startTime=1714839942174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13377949373536/25/936/3191.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/936/3191, endTime=1714840005766, processId=14716, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714849446052)
[WI-0][TI-0] - [WARN] 2024-05-05 03:05:01.657 +0800 o.s.c.k.c.p.AbstractKubernetesProfileEnvironmentPostProcessor:[258] - Not running inside kubernetes. Skipping 'kubernetes' profile activation.
[WI-0][TI-0] - [WARN] 2024-05-05 03:05:08.349 +0800 o.s.c.k.c.p.AbstractKubernetesProfileEnvironmentPostProcessor:[258] - Not running inside kubernetes. Skipping 'kubernetes' profile activation.
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:08.383 +0800 o.a.d.s.w.WorkerServer:[634] - No active profile set, falling back to 1 default profile: "default"
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:22.369 +0800 o.s.c.c.s.GenericScope:[283] - BeanFactory id=7e7bf7c6-2cb3-34d2-a0d5-a56161c67d0e
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:25.529 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'org.springframework.cloud.commons.config.CommonsConfigAutoConfiguration' of type [org.springframework.cloud.commons.config.CommonsConfigAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:25.535 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'org.springframework.cloud.client.loadbalancer.LoadBalancerDefaultMappingsProviderAutoConfiguration' of type [org.springframework.cloud.client.loadbalancer.LoadBalancerDefaultMappingsProviderAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:25.602 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'loadBalancerClientsDefaultsMappingsProvider' of type [org.springframework.cloud.client.loadbalancer.LoadBalancerDefaultMappingsProviderAutoConfiguration$$Lambda$392/1872158052] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:25.644 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'defaultsBindHandlerAdvisor' of type [org.springframework.cloud.commons.config.DefaultsBindHandlerAdvisor] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:25.776 +0800 o.a.d.c.u.NetUtils:[312] - Get all NetworkInterfaces: [name:eth0 (eth0), name:lo (lo)]
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:26.065 +0800 o.a.d.s.w.c.WorkerConfig:[98] - 
****************************Worker Configuration**************************************
  listen-port -> 1234
  exec-threads -> 100
  max-heartbeat-interval -> PT10S
  host-weight -> 100
  tenantConfig -> TenantConfig(autoCreateTenantEnabled=true, distributedTenantEnabled=false, defaultTenantEnabled=false)
  server-load-protection -> WorkerServerLoadProtection(enabled=true, maxCpuUsagePercentageThresholds=0.7, maxJVMMemoryUsagePercentageThresholds=0.7, maxSystemMemoryUsagePercentageThresholds=0.7, maxDiskUsagePercentageThresholds=0.7)
  registry-disconnect-strategy -> ConnectStrategyProperties(strategy=WAITING, maxWaitingTime=PT1M40S)
  task-execute-threads-full-policy: REJECT
  address -> 172.18.1.1:1234
  registry-path: /nodes/worker/172.18.1.1:1234
****************************Worker Configuration**************************************
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:26.066 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'workerConfig' of type [org.apache.dolphinscheduler.server.worker.config.WorkerConfig$$EnhancerBySpringCGLIB$$39bca86c] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:27.069 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'io.kubernetes.client.spring.extended.manifests.config.KubernetesManifestsAutoConfiguration' of type [io.kubernetes.client.spring.extended.manifests.config.KubernetesManifestsAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:27.309 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'kubernetes.manifests-io.kubernetes.client.spring.extended.manifests.config.KubernetesManifestsProperties' of type [io.kubernetes.client.spring.extended.manifests.config.KubernetesManifestsProperties] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:27.425 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'io.kubernetes.client.spring.extended.controller.config.KubernetesInformerAutoConfiguration' of type [io.kubernetes.client.spring.extended.controller.config.KubernetesInformerAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:27.506 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'defaultApiClient' of type [io.kubernetes.client.openapi.ApiClient] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:28.407 +0800 o.e.j.u.log:[170] - Logging initialized @45524ms to org.eclipse.jetty.util.log.Slf4jLog
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:30.434 +0800 o.s.b.w.e.j.JettyServletWebServerFactory:[166] - Server initialized with port: 1235
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:30.440 +0800 o.e.j.s.Server:[375] - jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 1.8.0_402-b06
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:30.519 +0800 o.e.j.s.h.C.application:[2368] - Initializing Spring embedded WebApplicationContext
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:30.520 +0800 o.s.b.w.s.c.ServletWebServerApplicationContext:[292] - Root WebApplicationContext: initialization completed in 21363 ms
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:35.420 +0800 o.e.j.s.session:[334] - DefaultSessionIdManager workerName=node0
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:35.421 +0800 o.e.j.s.session:[339] - No SessionScavenger set, using defaults
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:35.427 +0800 o.e.j.s.session:[132] - node0 Scavenging every 660000ms
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:35.464 +0800 o.e.j.s.h.ContextHandler:[921] - Started o.s.b.w.e.j.JettyEmbeddedWebAppContext@5a6195b8{application,/,[file:///tmp/jetty-docbase.1235.3465798185968849070/],AVAILABLE}
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:35.478 +0800 o.e.j.s.Server:[415] - Started @52595ms
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.578 +0800 o.a.c.f.i.CuratorFrameworkImpl:[338] - Starting
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.645 +0800 o.a.z.ZooKeeper:[98] - Client environment:zookeeper.version=3.8.0-5a02a05eddb59aee6ac762f7ea82e92a68eb9c0f, built on 2022-02-25 08:49 UTC
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.646 +0800 o.a.z.ZooKeeper:[98] - Client environment:host.name=bee82f6f8cde
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.646 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.version=1.8.0_402
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.647 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.vendor=Temurin
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.647 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.home=/opt/java/openjdk
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.649 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.class.path=/opt/dolphinscheduler/conf:/opt/dolphinscheduler/libs/kerb-client-1.0.1.jar:/opt/dolphinscheduler/libs/spring-cloud-starter-kubernetes-client-config-2.1.3.jar:/opt/dolphinscheduler/libs/oauth2-oidc-sdk-9.35.jar:/opt/dolphinscheduler/libs/jsqlparser-4.4.jar:/opt/dolphinscheduler/libs/reactive-streams-1.0.4.jar:/opt/dolphinscheduler/libs/kubernetes-model-extensions-5.10.2.jar:/opt/dolphinscheduler/libs/zookeeper-3.8.0.jar:/opt/dolphinscheduler/libs/kubernetes-model-apps-5.10.2.jar:/opt/dolphinscheduler/libs/grpc-api-1.41.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-pigeon-3.2.1.jar:/opt/dolphinscheduler/libs/client-java-api-13.0.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-obs-3.2.1.jar:/opt/dolphinscheduler/libs/spring-web-5.3.22.jar:/opt/dolphinscheduler/libs/aws-java-sdk-emr-1.12.300.jar:/opt/dolphinscheduler/libs/spring-context-5.3.22.jar:/opt/dolphinscheduler/libs/gapic-google-cloud-storage-v2-2.18.0-alpha.jar:/opt/dolphinscheduler/libs/spring-cloud-kubernetes-client-config-2.1.3.jar:/opt/dolphinscheduler/libs/jetty-io-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/annotations-13.0.jar:/opt/dolphinscheduler/libs/jpam-1.1.jar:/opt/dolphinscheduler/libs/joda-time-2.10.13.jar:/opt/dolphinscheduler/libs/spring-cloud-kubernetes-client-autoconfig-2.1.3.jar:/opt/dolphinscheduler/libs/kerb-identity-1.0.1.jar:/opt/dolphinscheduler/libs/vertica-jdbc-12.0.4-0.jar:/opt/dolphinscheduler/libs/grpc-googleapis-1.52.1.jar:/opt/dolphinscheduler/libs/aliyun-java-sdk-kms-2.11.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-dvc-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-hana-3.2.1.jar:/opt/dolphinscheduler/libs/kubernetes-httpclient-okhttp-6.0.0.jar:/opt/dolphinscheduler/libs/jline-2.12.jar:/opt/dolphinscheduler/libs/sshd-core-2.8.0.jar:/opt/dolphinscheduler/libs/jna-platform-5.10.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-sqlserver-3.2.1.jar:/opt/dolphinscheduler/libs/commons-beanutils-1.9.4.jar:/opt/dolphinscheduler/libs/grpc-services-1.41.0.jar:/opt/dolphinscheduler/libs/jmespath-java-1.12.300.jar:/opt/dolphinscheduler/libs/curator-client-5.3.0.jar:/opt/dolphinscheduler/libs/proto-google-common-protos-2.0.1.jar:/opt/dolphinscheduler/libs/mybatis-3.5.10.jar:/opt/dolphinscheduler/libs/jackson-annotations-2.13.4.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-all-3.2.1.jar:/opt/dolphinscheduler/libs/jakarta.xml.bind-api-2.3.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-jupyter-3.2.1.jar:/opt/dolphinscheduler/libs/mybatis-plus-extension-3.5.2.jar:/opt/dolphinscheduler/libs/j2objc-annotations-1.3.jar:/opt/dolphinscheduler/libs/Java-WebSocket-1.5.1.jar:/opt/dolphinscheduler/libs/azure-storage-common-12.20.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-sagemaker-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-databend-3.2.1.jar:/opt/dolphinscheduler/libs/google-auth-library-oauth2-http-1.15.0.jar:/opt/dolphinscheduler/libs/jetty-security-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-python-3.2.1.jar:/opt/dolphinscheduler/libs/snappy-java-1.1.10.1.jar:/opt/dolphinscheduler/libs/kubernetes-model-batch-5.10.2.jar:/opt/dolphinscheduler/libs/netty-transport-native-epoll-4.1.53.Final-linux-x86_64.jar:/opt/dolphinscheduler/libs/jackson-core-asl-1.9.13.jar:/opt/dolphinscheduler/libs/gson-fire-1.8.5.jar:/opt/dolphinscheduler/libs/clickhouse-jdbc-0.4.6.jar:/opt/dolphinscheduler/libs/grpc-netty-shaded-1.41.0.jar:/opt/dolphinscheduler/libs/caffeine-2.9.3.jar:/opt/dolphinscheduler/libs/aliyun-java-sdk-core-4.5.10.jar:/opt/dolphinscheduler/libs/jackson-module-jaxb-annotations-2.13.3.jar:/opt/dolphinscheduler/libs/jetty-http-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/jersey-servlet-1.19.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-dinky-3.2.1.jar:/opt/dolphinscheduler/libs/simpleclient_tracer_common-0.15.0.jar:/opt/dolphinscheduler/libs/netty-handler-4.1.53.Final.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-datafactory-3.2.1.jar:/opt/dolphinscheduler/libs/json-path-2.7.0.jar:/opt/dolphinscheduler/libs/mybatis-plus-annotation-3.5.2.jar:/opt/dolphinscheduler/libs/azure-resourcemanager-datafactory-1.0.0-beta.19.jar:/opt/dolphinscheduler/libs/commons-codec-1.11.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-master-3.2.1.jar:/opt/dolphinscheduler/libs/spring-aop-5.3.22.jar:/opt/dolphinscheduler/libs/kubernetes-model-core-5.10.2.jar:/opt/dolphinscheduler/libs/kubernetes-model-networking-5.10.2.jar:/opt/dolphinscheduler/libs/kotlin-stdlib-jdk7-1.6.21.jar:/opt/dolphinscheduler/libs/kerby-xdr-1.0.1.jar:/opt/dolphinscheduler/libs/aliyun-java-sdk-ram-3.1.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-k8s-3.2.1.jar:/opt/dolphinscheduler/libs/guava-31.1-jre.jar:/opt/dolphinscheduler/libs/netty-codec-dns-4.1.53.Final.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-kubeflow-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-azure-sql-3.2.1.jar:/opt/dolphinscheduler/libs/HikariCP-4.0.3.jar:/opt/dolphinscheduler/libs/hive-metastore-2.3.9.jar:/opt/dolphinscheduler/libs/msal4j-1.13.3.jar:/opt/dolphinscheduler/libs/jackson-core-2.13.4.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-hive-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-mr-3.2.1.jar:/opt/dolphinscheduler/libs/kubernetes-model-node-5.10.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-common-3.2.1.jar:/opt/dolphinscheduler/libs/jose4j-0.7.8.jar:/opt/dolphinscheduler/libs/jul-to-slf4j-1.7.36.jar:/opt/dolphinscheduler/libs/azure-identity-1.7.1.jar:/opt/dolphinscheduler/libs/spring-boot-autoconfigure-2.7.3.jar:/opt/dolphinscheduler/libs/commons-math3-3.1.1.jar:/opt/dolphinscheduler/libs/jackson-jaxrs-json-provider-2.13.3.jar:/opt/dolphinscheduler/libs/perfmark-api-0.23.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-doris-3.2.1.jar:/opt/dolphinscheduler/libs/httpclient-4.5.13.jar:/opt/dolphinscheduler/libs/hive-service-2.3.9.jar:/opt/dolphinscheduler/libs/kerby-util-1.0.1.jar:/opt/dolphinscheduler/libs/commons-collections-3.2.2.jar:/opt/dolphinscheduler/libs/hadoop-yarn-client-3.2.4.jar:/opt/dolphinscheduler/libs/commons-text-1.8.jar:/opt/dolphinscheduler/libs/dolphinscheduler-worker-3.2.1.jar:/opt/dolphinscheduler/libs/hadoop-yarn-common-3.2.4.jar:/opt/dolphinscheduler/libs/zeppelin-client-0.10.1.jar:/opt/dolphinscheduler/libs/azure-core-management-1.10.1.jar:/opt/dolphinscheduler/libs/hadoop-mapreduce-client-jobclient-3.2.4.jar:/opt/dolphinscheduler/libs/jta-1.1.jar:/opt/dolphinscheduler/libs/annotations-4.1.1.4.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-alert-3.2.1.jar:/opt/dolphinscheduler/libs/curator-framework-5.3.0.jar:/opt/dolphinscheduler/libs/hive-jdbc-2.3.9.jar:/opt/dolphinscheduler/libs/kyuubi-hive-jdbc-shaded-1.7.0.jar:/opt/dolphinscheduler/libs/client-java-proto-13.0.2.jar:/opt/dolphinscheduler/libs/checker-qual-3.19.0.jar:/opt/dolphinscheduler/libs/jetty-servlet-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/google-cloud-core-grpc-2.10.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-shell-3.2.1.jar:/opt/dolphinscheduler/libs/spring-security-rsa-1.0.10.RELEASE.jar:/opt/dolphinscheduler/libs/avro-1.7.7.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-postgresql-3.2.1.jar:/opt/dolphinscheduler/libs/netty-nio-client-2.17.282.jar:/opt/dolphinscheduler/libs/spring-webmvc-5.3.22.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-mysql-3.2.1.jar:/opt/dolphinscheduler/libs/opentracing-util-0.33.0.jar:/opt/dolphinscheduler/libs/auto-value-1.10.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-all-3.2.1.jar:/opt/dolphinscheduler/libs/jetcd-core-0.5.11.jar:/opt/dolphinscheduler/libs/grpc-context-1.41.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-datasync-3.2.1.jar:/opt/dolphinscheduler/libs/jackson-dataformat-cbor-2.13.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-gcs-3.2.1.jar:/opt/dolphinscheduler/libs/client-java-extended-13.0.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-redshift-3.2.1.jar:/opt/dolphinscheduler/libs/opencsv-2.3.jar:/opt/dolphinscheduler/libs/jcip-annotations-1.0-1.jar:/opt/dolphinscheduler/libs/accessors-smart-2.4.8.jar:/opt/dolphinscheduler/libs/hadoop-client-3.2.4.jar:/opt/dolphinscheduler/libs/commons-collections4-4.3.jar:/opt/dolphinscheduler/libs/metrics-core-4.2.11.jar:/opt/dolphinscheduler/libs/netty-resolver-4.1.53.Final.jar:/opt/dolphinscheduler/libs/parquet-hadoop-bundle-1.8.1.jar:/opt/dolphinscheduler/libs/bucket4j-core-6.2.0.jar:/opt/dolphinscheduler/libs/spring-cloud-starter-3.1.3.jar:/opt/dolphinscheduler/libs/mssql-jdbc-11.2.1.jre8.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/dolphinscheduler/libs/kubernetes-model-coordination-5.10.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-linkis-3.2.1.jar:/opt/dolphinscheduler/libs/commons-net-3.6.jar:/opt/dolphinscheduler/libs/netty-transport-native-kqueue-4.1.53.Final-osx-x86_64.jar:/opt/dolphinscheduler/libs/dolphinscheduler-meter-3.2.1.jar:/opt/dolphinscheduler/libs/kubernetes-client-api-6.0.0.jar:/opt/dolphinscheduler/libs/kubernetes-model-certificates-5.10.2.jar:/opt/dolphinscheduler/libs/opencensus-api-0.31.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-mlflow-3.2.1.jar:/opt/dolphinscheduler/libs/kotlin-stdlib-jdk8-1.6.21.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-jdbc-3.2.1.jar:/opt/dolphinscheduler/libs/audience-annotations-0.12.0.jar:/opt/dolphinscheduler/libs/simpleclient_httpserver-0.15.0.jar:/opt/dolphinscheduler/libs/jetty-xml-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/kerby-asn1-1.0.1.jar:/opt/dolphinscheduler/libs/lang-tag-1.6.jar:/opt/dolphinscheduler/libs/api-common-2.6.0.jar:/opt/dolphinscheduler/libs/HdrHistogram-2.1.12.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-abs-3.2.1.jar:/opt/dolphinscheduler/libs/failsafe-2.4.4.jar:/opt/dolphinscheduler/libs/hbase-noop-htrace-4.1.1.jar:/opt/dolphinscheduler/libs/jamon-runtime-2.3.1.jar:/opt/dolphinscheduler/libs/jetty-util-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/threetenbp-1.6.5.jar:/opt/dolphinscheduler/libs/jakarta.activation-api-1.2.2.jar:/opt/dolphinscheduler/libs/kubernetes-model-common-5.10.2.jar:/opt/dolphinscheduler/libs/okhttp-4.9.3.jar:/opt/dolphinscheduler/libs/profiles-2.17.282.jar:/opt/dolphinscheduler/libs/hadoop-auth-3.2.4.jar:/opt/dolphinscheduler/libs/grpc-netty-1.41.0.jar:/opt/dolphinscheduler/libs/httpcore-nio-4.4.15.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-seatunnel-3.2.1.jar:/opt/dolphinscheduler/libs/aws-java-sdk-sagemaker-1.12.300.jar:/opt/dolphinscheduler/libs/oshi-core-6.1.1.jar:/opt/dolphinscheduler/libs/bonecp-0.8.0.RELEASE.jar:/opt/dolphinscheduler/libs/jackson-module-parameter-names-2.13.3.jar:/opt/dolphinscheduler/libs/bcutil-jdk15on-1.69.jar:/opt/dolphinscheduler/libs/aws-json-protocol-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-base-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-all-3.2.1.jar:/opt/dolphinscheduler/libs/derby-10.14.2.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-snowflake-3.2.1.jar:/opt/dolphinscheduler/libs/netty-codec-http2-4.1.53.Final.jar:/opt/dolphinscheduler/libs/jetty-continuation-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/jackson-mapper-asl-1.9.13.jar:/opt/dolphinscheduler/libs/datanucleus-core-4.1.17.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/dolphinscheduler/libs/failureaccess-1.0.1.jar:/opt/dolphinscheduler/libs/error_prone_annotations-2.5.1.jar:/opt/dolphinscheduler/libs/kerb-admin-1.0.1.jar:/opt/dolphinscheduler/libs/token-provider-1.0.1.jar:/opt/dolphinscheduler/libs/reactor-netty-core-1.0.22.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-all-3.2.1.jar:/opt/dolphinscheduler/libs/jakarta.servlet-api-4.0.4.jar:/opt/dolphinscheduler/libs/http-client-spi-2.17.282.jar:/opt/dolphinscheduler/libs/regions-2.17.282.jar:/opt/dolphinscheduler/libs/logback-core-1.2.11.jar:/opt/dolphinscheduler/libs/json-1.8.jar:/opt/dolphinscheduler/libs/gax-grpc-2.23.0.jar:/opt/dolphinscheduler/libs/google-http-client-1.42.3.jar:/opt/dolphinscheduler/libs/hive-serde-2.3.9.jar:/opt/dolphinscheduler/libs/jettison-1.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-http-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-zookeeper-3.2.1.jar:/opt/dolphinscheduler/libs/spring-security-crypto-5.7.3.jar:/opt/dolphinscheduler/libs/auth-2.17.282.jar:/opt/dolphinscheduler/libs/proto-google-cloud-storage-v2-2.18.0-alpha.jar:/opt/dolphinscheduler/libs/jetty-server-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/javax.annotation-api-1.3.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-starrocks-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-dataquality-3.2.1.jar:/opt/dolphinscheduler/libs/jcl-over-slf4j-1.7.36.jar:/opt/dolphinscheduler/libs/commons-lang3-3.12.0.jar:/opt/dolphinscheduler/libs/tomcat-embed-el-9.0.65.jar:/opt/dolphinscheduler/libs/opentracing-noop-0.33.0.jar:/opt/dolphinscheduler/libs/client-java-13.0.2.jar:/opt/dolphinscheduler/libs/spring-beans-5.3.22.jar:/opt/dolphinscheduler/libs/snakeyaml-1.33.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-ssh-3.2.1.jar:/opt/dolphinscheduler/libs/commons-pool-1.6.jar:/opt/dolphinscheduler/libs/javax.servlet-api-3.1.0.jar:/opt/dolphinscheduler/libs/hadoop-common-3.2.4.jar:/opt/dolphinscheduler/libs/kubernetes-model-apiextensions-5.10.2.jar:/opt/dolphinscheduler/libs/spring-boot-2.7.3.jar:/opt/dolphinscheduler/libs/bcprov-ext-jdk15on-1.69.jar:/opt/dolphinscheduler/libs/spring-boot-starter-2.7.3.jar:/opt/dolphinscheduler/libs/paranamer-2.3.jar:/opt/dolphinscheduler/libs/httpmime-4.5.13.jar:/opt/dolphinscheduler/libs/reactor-core-3.4.22.jar:/opt/dolphinscheduler/libs/azure-core-1.36.0.jar:/opt/dolphinscheduler/libs/bcpkix-jdk15on-1.69.jar:/opt/dolphinscheduler/libs/kubernetes-model-scheduling-5.10.2.jar:/opt/dolphinscheduler/libs/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/dolphinscheduler/libs/websocket-client-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/unirest-java-3.7.04-standalone.jar:/opt/dolphinscheduler/libs/hadoop-mapreduce-client-core-3.2.4.jar:/opt/dolphinscheduler/libs/google-auth-library-credentials-1.15.0.jar:/opt/dolphinscheduler/libs/azure-storage-internal-avro-12.6.0.jar:/opt/dolphinscheduler/libs/jackson-datatype-jdk8-2.13.3.jar:/opt/dolphinscheduler/libs/spring-expression-5.3.22.jar:/opt/dolphinscheduler/libs/aspectjweaver-1.9.7.jar:/opt/dolphinscheduler/libs/websocket-common-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/client-java-api-fluent-13.0.2.jar:/opt/dolphinscheduler/libs/mybatis-plus-3.5.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-athena-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-oss-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-openmldb-3.2.1.jar:/opt/dolphinscheduler/libs/curator-recipes-5.3.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-api-3.2.1.jar:/opt/dolphinscheduler/libs/netty-all-4.1.53.Final.jar:/opt/dolphinscheduler/libs/netty-resolver-dns-4.1.53.Final.jar:/opt/dolphinscheduler/libs/kubernetes-model-autoscaling-5.10.2.jar:/opt/dolphinscheduler/libs/kerb-util-1.0.1.jar:/opt/dolphinscheduler/libs/grpc-alts-1.41.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-presto-3.2.1.jar:/opt/dolphinscheduler/libs/kerb-simplekdc-1.0.1.jar:/opt/dolphinscheduler/libs/protobuf-java-util-3.17.2.jar:/opt/dolphinscheduler/libs/azure-core-http-netty-1.13.0.jar:/opt/dolphinscheduler/libs/transaction-api-1.1.jar:/opt/dolphinscheduler/libs/datanucleus-api-jdo-4.2.4.jar:/opt/dolphinscheduler/libs/zeppelin-common-0.10.1.jar:/opt/dolphinscheduler/libs/zt-zip-1.15.jar:/opt/dolphinscheduler/libs/animal-sniffer-annotations-1.19.jar:/opt/dolphinscheduler/libs/google-http-client-jackson2-1.42.3.jar:/opt/dolphinscheduler/libs/netty-buffer-4.1.53.Final.jar:/opt/dolphinscheduler/libs/jsr305-3.0.0.jar:/opt/dolphinscheduler/libs/netty-transport-classes-epoll-4.1.79.Final.jar:/opt/dolphinscheduler/libs/spring-boot-actuator-autoconfigure-2.7.3.jar:/opt/dolphinscheduler/libs/simpleclient-0.15.0.jar:/opt/dolphinscheduler/libs/aliyun-sdk-oss-3.15.1.jar:/opt/dolphinscheduler/libs/json-utils-2.17.282.jar:/opt/dolphinscheduler/libs/tephra-api-0.6.0.jar:/opt/dolphinscheduler/libs/spring-jdbc-5.3.22.jar:/opt/dolphinscheduler/libs/grpc-xds-1.41.0.jar:/opt/dolphinscheduler/libs/logback-classic-1.2.11.jar:/opt/dolphinscheduler/libs/google-cloud-storage-2.18.0.jar:/opt/dolphinscheduler/libs/micrometer-registry-prometheus-1.9.3.jar:/opt/dolphinscheduler/libs/grpc-core-1.41.0.jar:/opt/dolphinscheduler/libs/aws-java-sdk-kms-1.12.300.jar:/opt/dolphinscheduler/libs/kubernetes-model-rbac-5.10.2.jar:/opt/dolphinscheduler/libs/ini4j-0.5.4.jar:/opt/dolphinscheduler/libs/spring-boot-starter-web-2.7.3.jar:/opt/dolphinscheduler/libs/spring-cloud-commons-3.1.3.jar:/opt/dolphinscheduler/libs/netty-codec-http-4.1.53.Final.jar:/opt/dolphinscheduler/libs/jetty-client-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/jetcd-common-0.5.11.jar:/opt/dolphinscheduler/libs/metrics-spi-2.17.282.jar:/opt/dolphinscheduler/libs/utils-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-emr-3.2.1.jar:/opt/dolphinscheduler/libs/protocol-core-2.17.282.jar:/opt/dolphinscheduler/libs/micrometer-core-1.9.3.jar:/opt/dolphinscheduler/libs/netty-transport-native-unix-common-4.1.53.Final.jar:/opt/dolphinscheduler/libs/zjsonpatch-0.4.11.jar:/opt/dolphinscheduler/libs/annotations-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-sql-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-common-3.2.1.jar:/opt/dolphinscheduler/libs/apache-client-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-oceanbase-3.2.1.jar:/opt/dolphinscheduler/libs/jdo-api-3.0.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-datax-3.2.1.jar:/opt/dolphinscheduler/libs/postgresql-42.4.1.jar:/opt/dolphinscheduler/libs/jetty-util-ajax-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/gax-2.23.0.jar:/opt/dolphinscheduler/libs/grpc-stub-1.41.0.jar:/opt/dolphinscheduler/libs/libfb303-0.9.3.jar:/opt/dolphinscheduler/libs/spring-core-5.3.22.jar:/opt/dolphinscheduler/libs/jaxb-api-2.3.1.jar:/opt/dolphinscheduler/libs/hive-service-rpc-2.3.9.jar:/opt/dolphinscheduler/libs/kubernetes-model-flowcontrol-5.10.2.jar:/opt/dolphinscheduler/libs/commons-logging-1.1.1.jar:/opt/dolphinscheduler/libs/mybatis-spring-2.0.7.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-oracle-3.2.1.jar:/opt/dolphinscheduler/libs/opencensus-proto-0.2.0.jar:/opt/dolphinscheduler/libs/grpc-protobuf-1.41.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-clickhouse-3.2.1.jar:/opt/dolphinscheduler/libs/spring-cloud-starter-bootstrap-3.1.3.jar:/opt/dolphinscheduler/libs/kubernetes-model-admissionregistration-5.10.2.jar:/opt/dolphinscheduler/libs/grpc-google-cloud-storage-v2-2.18.0-alpha.jar:/opt/dolphinscheduler/libs/esdk-obs-java-bundle-3.23.3.jar:/opt/dolphinscheduler/libs/dnsjava-2.1.7.jar:/opt/dolphinscheduler/libs/asm-9.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-spark-3.2.1.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/dolphinscheduler/libs/re2j-1.6.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-vertica-3.2.1.jar:/opt/dolphinscheduler/libs/json-smart-2.4.8.jar:/opt/dolphinscheduler/libs/reactor-netty-http-1.0.22.jar:/opt/dolphinscheduler/libs/google-api-services-storage-v1-rev20220705-2.0.0.jar:/opt/dolphinscheduler/libs/kubernetes-client-6.0.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-kyuubi-3.2.1.jar:/opt/dolphinscheduler/libs/auto-value-annotations-1.10.1.jar:/opt/dolphinscheduler/libs/commons-cli-1.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-data-quality-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-chunjun-3.2.1.jar:/opt/dolphinscheduler/libs/kerb-server-1.0.1.jar:/opt/dolphinscheduler/libs/content-type-2.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-remoteshell-3.2.1.jar:/opt/dolphinscheduler/libs/opencensus-contrib-http-util-0.31.1.jar:/opt/dolphinscheduler/libs/druid-1.2.20.jar:/opt/dolphinscheduler/libs/javolution-5.5.1.jar:/opt/dolphinscheduler/libs/protobuf-java-3.17.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-db2-3.2.1.jar:/opt/dolphinscheduler/libs/aws-java-sdk-core-1.12.300.jar:/opt/dolphinscheduler/libs/spring-boot-starter-logging-2.7.3.jar:/opt/dolphinscheduler/libs/kerby-pkix-1.0.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-procedure-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-etcd-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-sqoop-3.2.1.jar:/opt/dolphinscheduler/libs/zookeeper-jute-3.8.0.jar:/opt/dolphinscheduler/libs/netty-resolver-dns-native-macos-4.1.53.Final-osx-x86_64.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-sagemaker-3.2.1.jar:/opt/dolphinscheduler/libs/spring-boot-configuration-processor-2.6.1.jar:/opt/dolphinscheduler/libs/hive-common-2.3.9.jar:/opt/dolphinscheduler/libs/kerb-common-1.0.1.jar:/opt/dolphinscheduler/libs/stax-api-1.0.1.jar:/opt/dolphinscheduler/libs/kubernetes-model-storageclass-5.10.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-s3-3.2.1.jar:/opt/dolphinscheduler/libs/spring-boot-starter-jetty-2.7.3.jar:/opt/dolphinscheduler/libs/okio-2.8.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-dms-3.2.1.jar:/opt/dolphinscheduler/libs/simpleclient_common-0.15.0.jar:/opt/dolphinscheduler/libs/sdk-core-2.17.282.jar:/opt/dolphinscheduler/libs/javax.activation-api-1.2.0.jar:/opt/dolphinscheduler/libs/netty-handler-proxy-4.1.53.Final.jar:/opt/dolphinscheduler/libs/kerby-config-1.0.1.jar:/opt/dolphinscheduler/libs/netty-tcnative-classes-2.0.53.Final.jar:/opt/dolphinscheduler/libs/jackson-jaxrs-base-2.13.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-trino-3.2.1.jar:/opt/dolphinscheduler/libs/presto-jdbc-0.238.1.jar:/opt/dolphinscheduler/libs/websocket-api-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-flink-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-api-3.2.1.jar:/opt/dolphinscheduler/libs/kubernetes-model-metrics-5.10.2.jar:/opt/dolphinscheduler/libs/datasync-2.17.282.jar:/opt/dolphinscheduler/libs/hadoop-yarn-api-3.2.4.jar:/opt/dolphinscheduler/libs/google-http-client-apache-v2-1.42.3.jar:/opt/dolphinscheduler/libs/hadoop-annotations-3.2.4.jar:/opt/dolphinscheduler/libs/google-oauth-client-1.34.1.jar:/opt/dolphinscheduler/libs/sshd-common-2.8.0.jar:/opt/dolphinscheduler/libs/LatencyUtils-2.0.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-spark-3.2.1.jar:/opt/dolphinscheduler/libs/slf4j-api-1.7.36.jar:/opt/dolphinscheduler/libs/snowflake-jdbc-3.13.29.jar:/opt/dolphinscheduler/libs/jackson-datatype-jsr310-2.13.3.jar:/opt/dolphinscheduler/libs/trino-jdbc-402.jar:/opt/dolphinscheduler/libs/logging-interceptor-4.9.3.jar:/opt/dolphinscheduler/libs/simpleclient_tracer_otel_agent-0.15.0.jar:/opt/dolphinscheduler/libs/janino-3.0.16.jar:/opt/dolphinscheduler/libs/jackson-dataformat-yaml-2.13.3.jar:/opt/dolphinscheduler/libs/ion-java-1.0.2.jar:/opt/dolphinscheduler/libs/commons-dbcp-1.4.jar:/opt/dolphinscheduler/libs/jsp-api-2.1.jar:/opt/dolphinscheduler/libs/google-http-client-gson-1.42.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-hivecli-3.2.1.jar:/opt/dolphinscheduler/libs/spring-boot-actuator-2.7.3.jar:/opt/dolphinscheduler/libs/google-cloud-core-http-2.10.0.jar:/opt/dolphinscheduler/libs/DmJdbcDriver18-8.1.2.79.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-java-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-dameng-3.2.1.jar:/opt/dolphinscheduler/libs/sshd-sftp-2.8.0.jar:/opt/dolphinscheduler/libs/kerb-crypto-1.0.1.jar:/opt/dolphinscheduler/libs/conscrypt-openjdk-uber-2.5.2.jar:/opt/dolphinscheduler/libs/httpcore-4.4.15.jar:/opt/dolphinscheduler/libs/lz4-java-1.4.0.jar:/opt/dolphinscheduler/libs/guava-retrying-2.0.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-flink-stream-3.2.1.jar:/opt/dolphinscheduler/libs/spring-tx-5.3.22.jar:/opt/dolphinscheduler/libs/grpc-auth-1.41.0.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/dolphinscheduler/libs/databend-jdbc-0.0.7.jar:/opt/dolphinscheduler/libs/bcprov-jdk15on-1.69.jar:/opt/dolphinscheduler/libs/commons-lang-2.6.jar:/opt/dolphinscheduler/libs/kubernetes-model-policy-5.10.2.jar:/opt/dolphinscheduler/libs/commons-io-2.11.0.jar:/opt/dolphinscheduler/libs/grpc-grpclb-1.41.0.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/dolphinscheduler/libs/opentracing-api-0.33.0.jar:/opt/dolphinscheduler/libs/sshd-scp-2.8.0.jar:/opt/dolphinscheduler/libs/reload4j-1.2.18.3.jar:/opt/dolphinscheduler/libs/netty-tcnative-2.0.48.Final.jar:/opt/dolphinscheduler/libs/jdom2-2.0.6.1.jar:/opt/dolphinscheduler/libs/spring-boot-starter-json-2.7.3.jar:/opt/dolphinscheduler/libs/netty-transport-native-epoll-4.1.53.Final.jar:/opt/dolphinscheduler/libs/google-cloud-core-2.10.0.jar:/opt/dolphinscheduler/libs/javax.jdo-3.2.0-m3.jar:/opt/dolphinscheduler/libs/gax-httpjson-0.108.0.jar:/opt/dolphinscheduler/libs/mybatis-plus-core-3.5.2.jar:/opt/dolphinscheduler/libs/auto-service-annotations-1.0.1.jar:/opt/dolphinscheduler/libs/nimbus-jose-jwt-9.22.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-zeppelin-3.2.1.jar:/opt/dolphinscheduler/libs/commons-compress-1.21.jar:/opt/dolphinscheduler/libs/hadoop-hdfs-client-3.2.4.jar:/opt/dolphinscheduler/libs/msal4j-persistence-extension-1.1.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-pytorch-3.2.1.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/dolphinscheduler/libs/jetty-servlets-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/woodstox-core-6.4.0.jar:/opt/dolphinscheduler/libs/spring-cloud-kubernetes-commons-2.1.3.jar:/opt/dolphinscheduler/libs/grpc-protobuf-lite-1.41.0.jar:/opt/dolphinscheduler/libs/jetty-webapp-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/eventstream-1.0.1.jar:/opt/dolphinscheduler/libs/commons-compiler-3.1.7.jar:/opt/dolphinscheduler/libs/netty-transport-4.1.53.Final.jar:/opt/dolphinscheduler/libs/spring-cloud-context-3.1.3.jar:/opt/dolphinscheduler/libs/aws-java-sdk-dms-1.12.300.jar:/opt/dolphinscheduler/libs/hive-storage-api-2.4.0.jar:/opt/dolphinscheduler/libs/client-java-spring-integration-13.0.2.jar:/opt/dolphinscheduler/libs/spring-boot-starter-actuator-2.7.3.jar:/opt/dolphinscheduler/libs/third-party-jackson-core-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-hdfs-3.2.1.jar:/opt/dolphinscheduler/libs/netty-codec-4.1.53.Final.jar:/opt/dolphinscheduler/libs/google-http-client-appengine-1.42.3.jar:/opt/dolphinscheduler/libs/commons-configuration2-2.1.1.jar:/opt/dolphinscheduler/libs/datanucleus-rdbms-4.1.19.jar:/opt/dolphinscheduler/libs/swagger-annotations-1.6.2.jar:/opt/dolphinscheduler/libs/simpleclient_tracer_otel-0.15.0.jar:/opt/dolphinscheduler/libs/kotlin-stdlib-common-1.6.21.jar:/opt/dolphinscheduler/libs/aws-core-2.17.282.jar:/opt/dolphinscheduler/libs/kerb-core-1.0.1.jar:/opt/dolphinscheduler/libs/httpasyncclient-4.1.5.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-worker-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-spi-3.2.1.jar:/opt/dolphinscheduler/libs/okhttp-2.7.5.jar:/opt/dolphinscheduler/libs/google-api-client-2.2.0.jar:/opt/dolphinscheduler/libs/kotlin-stdlib-1.6.21.jar:/opt/dolphinscheduler/libs/jakarta.websocket-api-1.1.2.jar:/opt/dolphinscheduler/libs/libthrift-0.9.3.jar:/opt/dolphinscheduler/libs/spring-boot-starter-aop-2.7.3.jar:/opt/dolphinscheduler/libs/netty-common-4.1.53.Final.jar:/opt/dolphinscheduler/libs/log4j-1.2-api-2.17.2.jar:/opt/dolphinscheduler/libs/jackson-databind-2.13.4.jar:/opt/dolphinscheduler/libs/jackson-dataformat-xml-2.13.3.jar:/opt/dolphinscheduler/libs/kubernetes-model-events-5.10.2.jar:/opt/dolphinscheduler/libs/gson-2.9.1.jar:/opt/dolphinscheduler/libs/proto-google-iam-v1-1.9.0.jar:/opt/dolphinscheduler/libs/netty-codec-socks-4.1.53.Final.jar:/opt/dolphinscheduler/libs/zjsonpatch-0.3.0.jar:/opt/dolphinscheduler/libs/hadoop-mapreduce-client-common-3.2.4.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-api-3.2.1.jar:/opt/dolphinscheduler/libs/azure-storage-blob-12.21.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-zeppelin-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-api-3.2.1.jar:/opt/dolphinscheduler/libs/aws-java-sdk-s3-1.12.300.jar:/opt/dolphinscheduler/libs/stax2-api-4.2.1.jar:/opt/dolphinscheduler/libs/jakarta.annotation-api-1.3.5.jar:/opt/dolphinscheduler/libs/kubernetes-model-discovery-5.10.2.jar:/opt/dolphinscheduler/libs/jna-5.10.0.jar:/opt/dolphinscheduler/libs/spring-jcl-5.3.22.jar
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.651 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.651 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.io.tmpdir=/tmp
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.652 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.compiler=<NA>
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.654 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.name=Linux
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.654 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.arch=amd64
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.678 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.version=6.5.0-28-generic
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.679 +0800 o.a.z.ZooKeeper:[98] - Client environment:user.name=root
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.680 +0800 o.a.z.ZooKeeper:[98] - Client environment:user.home=/root
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.680 +0800 o.a.z.ZooKeeper:[98] - Client environment:user.dir=/opt/dolphinscheduler
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.680 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.memory.free=3210MB
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.681 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.memory.max=3840MB
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.682 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.memory.total=3840MB
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.688 +0800 o.a.z.ZooKeeper:[637] - Initiating client connection, connectString=dolphinscheduler-zookeeper:2181 sessionTimeout=30000 watcher=org.apache.curator.ConnectionState@1cc93da4
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.705 +0800 o.a.z.c.X509Util:[77] - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.714 +0800 o.a.z.ClientCnxnSocket:[239] - jute.maxbuffer value is 1048575 Bytes
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.747 +0800 o.a.z.ClientCnxn:[1732] - zookeeper.request.timeout value is 0. feature enabled=false
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.807 +0800 o.a.c.f.i.CuratorFrameworkImpl:[386] - Default schema
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.848 +0800 o.a.z.ClientCnxn:[1171] - Opening socket connection to server dolphinscheduler-zookeeper/172.18.0.2:2181.
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.904 +0800 o.a.z.ClientCnxn:[1173] - SASL config status: Will not attempt to authenticate using SASL (unknown error)
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.940 +0800 o.a.z.ClientCnxn:[1005] - Socket connection established, initiating session, client: /172.18.1.1:39190, server: dolphinscheduler-zookeeper/172.18.0.2:2181
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:36.997 +0800 o.a.z.ClientCnxn:[1444] - Session establishment complete on server dolphinscheduler-zookeeper/172.18.0.2:2181, session id = 0x10000c4dc030001, negotiated timeout = 30000
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:37.021 +0800 o.a.c.f.s.ConnectionStateManager:[252] - State change: CONNECTED
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:37.393 +0800 o.a.c.f.i.EnsembleTracker:[201] - New config event received: {}
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:37.409 +0800 o.a.c.f.i.EnsembleTracker:[201] - New config event received: {}
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:37.987 +0800 o.a.d.s.w.r.WorkerRpcServer:[41] - WorkerRpcServer starting...
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:39.350 +0800 o.a.d.e.b.NettyRemotingServer:[112] - WorkerRpcServer bind success at port: 1234
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:39.350 +0800 o.a.d.s.w.r.WorkerRpcServer:[43] - WorkerRpcServer started...
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:39.883 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: JAVA - JavaTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:39.899 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: JAVA - JavaTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:39.900 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: JUPYTER - JupyterTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:39.926 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: JUPYTER - JupyterTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:39.927 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SPARK - SparkTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:39.929 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SPARK - SparkTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:39.930 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: FLINK_STREAM - FlinkStreamTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.061 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: FLINK_STREAM - FlinkStreamTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.062 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: PYTHON - PythonTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.063 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: PYTHON - PythonTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.072 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DATASYNC - DatasyncTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.074 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DATASYNC - DatasyncTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.080 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DATA_FACTORY - DatafactoryTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.095 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DATA_FACTORY - DatafactoryTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.095 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: CHUNJUN - ChunJunTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.097 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: CHUNJUN - ChunJunTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.145 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: REMOTESHELL - RemoteShellTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.149 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: REMOTESHELL - RemoteShellTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.149 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: PIGEON - PigeonTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.151 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: PIGEON - PigeonTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.151 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SHELL - ShellTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.152 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SHELL - ShellTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.152 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: PROCEDURE - ProcedureTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.154 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: PROCEDURE - ProcedureTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.154 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: MR - MapReduceTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.155 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: MR - MapReduceTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.156 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SQOOP - SqoopTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.181 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SQOOP - SqoopTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.188 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: PYTORCH - PytorchTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.191 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: PYTORCH - PytorchTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.191 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: K8S - K8sTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.193 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: K8S - K8sTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.193 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SEATUNNEL - SeatunnelTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.221 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SEATUNNEL - SeatunnelTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.222 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SAGEMAKER - SagemakerTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.224 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SAGEMAKER - SagemakerTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.224 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: HTTP - HttpTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.226 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: HTTP - HttpTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.226 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: EMR - EmrTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.229 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: EMR - EmrTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.229 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DMS - DmsTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.231 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DMS - DmsTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.231 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DATA_QUALITY - DataQualityTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.272 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DATA_QUALITY - DataQualityTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.273 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: KUBEFLOW - KubeflowTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.274 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: KUBEFLOW - KubeflowTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.274 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SQL - SqlTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.297 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SQL - SqlTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.305 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DVC - DvcTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.320 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DVC - DvcTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.324 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DATAX - DataxTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.360 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DATAX - DataxTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.362 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: ZEPPELIN - ZeppelinTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.366 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: ZEPPELIN - ZeppelinTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.589 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DINKY - DinkyTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.605 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DINKY - DinkyTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.610 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: MLFLOW - MlflowTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.613 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: MLFLOW - MlflowTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.618 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: OPENMLDB - OpenmldbTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.638 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: OPENMLDB - OpenmldbTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.640 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: LINKIS - LinkisTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.644 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: LINKIS - LinkisTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.648 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: FLINK - FlinkTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.665 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: FLINK - FlinkTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.668 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: HIVECLI - HiveCliTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:40.671 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: HIVECLI - HiveCliTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:41.087 +0800 o.a.d.c.u.JSONUtils:[72] - init timezone: sun.util.calendar.ZoneInfo[id="Asia/Shanghai",offset=28800000,dstSavings=0,useDaylight=false,transitions=31,lastRule=null]
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:41.290 +0800 o.a.d.s.w.r.WorkerRegistryClient:[104] - Worker node: 172.18.1.1:1234 registry to ZK /nodes/worker/172.18.1.1:1234 successfully
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:42.380 +0800 o.a.d.c.m.BaseHeartBeatTask:[48] - Starting WorkerHeartBeatTask...
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:42.399 +0800 o.a.d.c.m.BaseHeartBeatTask:[50] - Started WorkerHeartBeatTask, heartBeatInterval: 10000...
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:42.400 +0800 o.a.d.s.w.r.WorkerRegistryClient:[114] - Worker node: 172.18.1.1:1234 registry finished
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:42.405 +0800 o.a.d.s.w.m.MessageRetryRunner:[69] - Message retry runner staring
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:42.413 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.083969465648855 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:42.556 +0800 o.a.d.s.w.m.MessageRetryRunner:[72] - Injected message sender: TaskInstanceExecutionFinishEventSender
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:42.556 +0800 o.a.d.s.w.m.MessageRetryRunner:[72] - Injected message sender: TaskInstanceExecutionInfoUpdateEventSender
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:42.556 +0800 o.a.d.s.w.m.MessageRetryRunner:[72] - Injected message sender: TaskInstanceExecutionRunningEventSender
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:42.577 +0800 o.a.d.s.w.m.MessageRetryRunner:[75] - Message retry runner started
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:43.638 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.2454212454212454 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:44.683 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.1567796610169492 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:45.686 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.188235294117647 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:46.722 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0477518108514419 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:47.755 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.2233502538071066 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:48.894 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.2458333333333333 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:49.898 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.3442622950819672 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:50.961 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.1968911917098446 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:53.077 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.3076923076923077 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:54.298 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.353658536585366 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:55.302 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.3404255319148937 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:56.305 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.3181818181818181 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:58.110 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.5119617224880382 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:05:59.130 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.3035714285714286 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:00.134 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.2454873646209386 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:01.422 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.3791469194312795 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:02.499 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.2067510548523206 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:03.518 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.3514851485148514 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:05.033 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.3146853146853146 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:06.035 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.471794871794872 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:06.714 +0800 o.s.b.a.e.w.EndpointLinksResolver:[58] - Exposing 3 endpoint(s) beneath base path '/actuator'
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:07.072 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.489655172413793 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:07.328 +0800 o.e.j.s.h.C.application:[2368] - Initializing Spring DispatcherServlet 'dispatcherServlet'
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:07.360 +0800 o.s.w.s.DispatcherServlet:[525] - Initializing Servlet 'dispatcherServlet'
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:07.364 +0800 o.s.w.s.DispatcherServlet:[547] - Completed initialization in 3 ms
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:07.387 +0800 o.e.j.s.AbstractConnector:[333] - Started ServerConnector@2fc7fa6e{HTTP/1.1, (http/1.1)}{0.0.0.0:1235}
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:07.411 +0800 o.s.b.w.e.j.JettyWebServer:[172] - Jetty started on port(s) 1235 (http/1.1) with context path '/'
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:07.531 +0800 o.a.d.s.w.WorkerServer:[61] - Started WorkerServer in 76.077 seconds (JVM running for 84.648)
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:08.074 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.2864077669902914 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:09.091 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.1336405529953917 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:10.134 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.2569832402234637 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:11.309 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9033816425120773 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:12.337 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9141414141414141 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:13.498 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9017658730158731 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:14.506 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9850995024875622 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:15.533 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.2107843137254901 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:16.540 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.2430939226519337 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:17.555 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7826086956521738 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:18.623 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.824858757062147 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:19.665 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9741935483870967 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:20.750 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7815126050420168 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:21.754 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.912037037037037 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:22.756 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7787234042553192 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:23.759 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.799373040752351 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:27.880 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9348534201954397 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-2989] - [INFO] 2024-05-05 03:06:28.587 +0800 o.a.d.s.w.r.o.UpdateWorkflowHostOperationFunction:[49] - Received UpdateWorkflowHostRequest: UpdateWorkflowHostRequest(taskInstanceId=2989, workflowHost=172.18.0.12:5678)
[WI-0][TI-2935] - [INFO] 2024-05-05 03:06:28.587 +0800 o.a.d.s.w.r.o.UpdateWorkflowHostOperationFunction:[49] - Received UpdateWorkflowHostRequest: UpdateWorkflowHostRequest(taskInstanceId=2935, workflowHost=172.18.0.12:5678)
[WI-0][TI-2936] - [INFO] 2024-05-05 03:06:28.590 +0800 o.a.d.s.w.r.o.UpdateWorkflowHostOperationFunction:[49] - Received UpdateWorkflowHostRequest: UpdateWorkflowHostRequest(taskInstanceId=2936, workflowHost=172.18.0.12:5678)
[WI-0][TI-2937] - [INFO] 2024-05-05 03:06:28.591 +0800 o.a.d.s.w.r.o.UpdateWorkflowHostOperationFunction:[49] - Received UpdateWorkflowHostRequest: UpdateWorkflowHostRequest(taskInstanceId=2937, workflowHost=172.18.0.12:5678)
[WI-0][TI-3362] - [INFO] 2024-05-05 03:06:28.592 +0800 o.a.d.s.w.r.o.UpdateWorkflowHostOperationFunction:[49] - Received UpdateWorkflowHostRequest: UpdateWorkflowHostRequest(taskInstanceId=3362, workflowHost=172.18.0.12:5678)
[WI-0][TI-2975] - [INFO] 2024-05-05 03:06:28.593 +0800 o.a.d.s.w.r.o.UpdateWorkflowHostOperationFunction:[49] - Received UpdateWorkflowHostRequest: UpdateWorkflowHostRequest(taskInstanceId=2975, workflowHost=172.18.0.12:5678)
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:28.823 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3363, taskName=extract_gnews_article, firstSubmitTime=1714849587217, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13471700679104, processDefineVersion=3, appIds=null, processInstanceId=950, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_article'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3363'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13471571877568'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505030627'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='950'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_articles_main'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13471700679104'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:28.884 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract_gnews_article to wait queue success
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:28.898 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0658682634730539 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:28.911 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:28.917 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:28.915 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3364, taskName=extract_gnews_article, firstSubmitTime=1714849587285, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13471700679104, processDefineVersion=3, appIds=null, processInstanceId=949, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_article'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3364'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13471571877568'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505030627'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='949'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_articles_main'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13471700679104'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:28.918 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:28.919 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:28.919 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714849588919
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:28.920 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 950_3363
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:28.926 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3363,
  "taskName" : "extract_gnews_article",
  "firstSubmitTime" : 1714849587217,
  "startTime" : 1714849588919,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13471700679104/3/950/3363.log",
  "processId" : 0,
  "processDefineCode" : 13471700679104,
  "processDefineVersion" : 3,
  "processInstanceId" : 950,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"from gnews import GNews\\nimport json\\n\\n\\n# Initialize GNews client\\ngoogle_news = GNews()\\n\\n# Fetch Google News articles for a specific topic, which is singapore related\\nnews_results = google_news.get_news(topic)[:5]\\n\\n# List to store the relevant information about the news articles\\nnews_articles = []\\n\\n# Process each news article\\nfor news in news_results:\\n    # Extract basic information from Google News\\n    title = news.get('title')\\n    datetime = news.get('published date')\\n    url = news.get('url')\\n    description = news.get('description')\\n    \\n    # Extract publisher information\\n    publisher_info = news.get('publisher', {})\\n    publisher_href = publisher_info.get('href')\\n    \\n    # Initialize and handle exceptions in article parsing\\n    try:\\n        article = google_news.get_full_article(url)\\n\\n        # If parsing is successful, extract authors and text\\n        authors = article.authors if article.authors else \\\"Unknown Author\\\"\\n        context = article.text\\n    \\n    except Exception as e:\\n        print(f\\\"Failed to download article: {e}\\\")  # Log the error message\\n        continue\\n    \\n    # store the article data in a dictionary\\n    article_data = {\\n        'title': title,\\n        'datetime': datetime,\\n        'description': description,\\n        'url': url,\\n        'context': context,\\n        'authors': authors,\\n        'publisher_href': publisher_href,\\n    }\\n\\n\\n    # store the article data into the article_data list for JSON serialization\\n    news_articles.append(article_data)\\n\\n\\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\\nnews_articles_string = '|'.join(news_articles)\\n\\n# save the news_articles_string dictionary into a parameter to pass it downstream\\n\\nprint(\\\"#setValue(news_articles_string=%s)\\\" % news_articles_string)\\n\\nprint(news_articles_string) \",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_article"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3363"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471571877568"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505030627"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "950"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_articles_main"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471700679104"
    }
  },
  "taskAppId" : "950_3363",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:28.930 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:28.930 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:28.930 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:28.941 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract_gnews_article to wait queue success
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:28.948 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:28.958 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:28.959 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:28.961 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3365, taskName=hash_article_data, firstSubmitTime=1714849587624, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13471825071552, processDefineVersion=29, appIds=null, processInstanceId=951, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"article_data_hash","direct":"OUT","type":"VARCHAR","value":""},{"prop":"article_data","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"import hashlib\nimport json\n\n# INSERT ${news_article} instead\n# receive the article data from the main gnews extraction workflow\narticle_data = {\n    \"title\": \"Singapore diplomat accused of filming naked boy at Tokyo bathhouse - South China Morning Post\",\n    \"datetime\": \"Thu, 02 May 2024 08:01:58 GMT\",\n    \"description\": \"Singapore diplomat accused of filming naked boy at Tokyo bathhouse  South China Morning Post\",\n    \"url\": \"https://news.google.com/rss/articles/CBMinQFodHRwczovL3d3dy5zY21wLmNvbS9uZXdzL2FzaWEvc291dGhlYXN0LWFzaWEvYXJ0aWNsZS8zMjYxMTc4L2phcGFuLXBvbGljZS1pbnZlc3RpZ2F0ZS1zaW5nYXBvcmUtZGlwbG9tYXQtYWxsZWdlZGx5LWZpbG1pbmctbmFrZWQtc2Nob29sYm95LXRva3lvLXB1YmxpYy1iYXRo0gGdAWh0dHBzOi8vYW1wLnNjbXAuY29tL25ld3MvYXNpYS9zb3V0aGVhc3QtYXNpYS9hcnRpY2xlLzMyNjExNzgvamFwYW4tcG9saWNlLWludmVzdGlnYXRlLXNpbmdhcG9yZS1kaXBsb21hdC1hbGxlZ2VkbHktZmlsbWluZy1uYWtlZC1zY2hvb2xib3ktdG9reW8tcHVibGljLWJhdGg?oc=5&hl=en-SG&gl=SG&ceid=SG:en\",\n    \"context\": \"The Yomiuri newspaper reported that on February 27, the man used his smartphone to secretly film a 13-year-old secondary school student in the changing room of a public bath. The boy was naked.\\n\\nThe diplomat in question is a 55-year-old, who is a \\u201cformer\\u201d counsellor at the embassy, according to NHK. A counsellor is a diplomatic rank for officers serving overseas, such as in an embassy.\\n\\nA diplomat at the Singapore embassy in Tokyo was questioned by police after being suspected of filming a male teenager at a public bath, according to Japanese media reports on Thursday.\\n\\nStaff members at the public bath in Tokyo\\u2019s Minato ward reportedly called the police who, upon arriving, searched the diplomat\\u2019s phone and found \\u201cmultiple naked photos of male customers\\u201d, according to the Asahi newspaper.\\n\\nIt added that the diplomat refused to go to the police station but told officers he had taken such photos in other public baths.\\n\\nWhen asked to delete the photos from his phone, the diplomat \\u201cdeleted them on the spot\\u201d, reported the Japanese news outlet. He allegedly deleted 700 photos from his phone, which he told police he had taken in the six months leading up to the incident.\\n\\nThe Tokyo police are investigating potential violations of child pornography laws and are planning to ask Singapore\\u2019s Ministry of Foreign Affairs (MFA) to have the man turn himself in, said Asahi. Formal charges are also being considered.\\n\\nThe diplomat has been dismissed as a counsellor but is immune to arrest in accordance with the Vienna Convention on Diplomatic Relations, which states that diplomats cannot be arrested or detained in a country they have been dispatched to.\\n\\nWhen asked by the Asahi newspaper on Thursday, the embassy was reportedly unaware of the public bath incident. The embassy also told the paper the diplomat had \\u201ccompleted his assignment as of April 12\\u201d and has returned to Singapore.\\n\\nCNA has reached out to MFA for comment.\\n\\nAdditional reporting by Kyodo\",\n    \"authors\": \"Unknown Author\",\n    \"publisher_href\": \"https://www.scmp.com\"\n} \n\n# convert JSON data to string\narticle_data_json = json.dumps(article_data)\n\n# hash the JSON string\narticle_data_hash = hashlib.sha256(article_data_json.encode()).hexdigest()\n\n# pass the hash and article data to the downstream nodes\nprint(\"${setValue(article_data_hash=%s)}\" % str(article_data_hash))\nprint(\"${setValue(article_data=%s)}\" % article_data)","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='hash_article_data'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3365'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13471727617984'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505030627'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='951'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_articles_hashing_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13471825071552'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:28.963 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:28.964 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714849588964
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:28.965 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 949_3364
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:28.967 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3364,
  "taskName" : "extract_gnews_article",
  "firstSubmitTime" : 1714849587285,
  "startTime" : 1714849588964,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13471700679104/3/949/3364.log",
  "processId" : 0,
  "processDefineCode" : 13471700679104,
  "processDefineVersion" : 3,
  "processInstanceId" : 949,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"from gnews import GNews\\nimport json\\n\\n\\n# Initialize GNews client\\ngoogle_news = GNews()\\n\\n# Fetch Google News articles for a specific topic, which is singapore related\\nnews_results = google_news.get_news(topic)[:5]\\n\\n# List to store the relevant information about the news articles\\nnews_articles = []\\n\\n# Process each news article\\nfor news in news_results:\\n    # Extract basic information from Google News\\n    title = news.get('title')\\n    datetime = news.get('published date')\\n    url = news.get('url')\\n    description = news.get('description')\\n    \\n    # Extract publisher information\\n    publisher_info = news.get('publisher', {})\\n    publisher_href = publisher_info.get('href')\\n    \\n    # Initialize and handle exceptions in article parsing\\n    try:\\n        article = google_news.get_full_article(url)\\n\\n        # If parsing is successful, extract authors and text\\n        authors = article.authors if article.authors else \\\"Unknown Author\\\"\\n        context = article.text\\n    \\n    except Exception as e:\\n        print(f\\\"Failed to download article: {e}\\\")  # Log the error message\\n        continue\\n    \\n    # store the article data in a dictionary\\n    article_data = {\\n        'title': title,\\n        'datetime': datetime,\\n        'description': description,\\n        'url': url,\\n        'context': context,\\n        'authors': authors,\\n        'publisher_href': publisher_href,\\n    }\\n\\n\\n    # store the article data into the article_data list for JSON serialization\\n    news_articles.append(article_data)\\n\\n\\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\\nnews_articles_string = '|'.join(news_articles)\\n\\n# save the news_articles_string dictionary into a parameter to pass it downstream\\n\\nprint(\\\"#setValue(news_articles_string=%s)\\\" % news_articles_string)\\n\\nprint(news_articles_string) \",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_article"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3364"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471571877568"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505030627"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "949"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_articles_main"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471700679104"
    }
  },
  "taskAppId" : "949_3364",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:28.973 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:28.974 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:28.975 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:28.976 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:28.975 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: hash_article_data to wait queue success
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:28.978 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:28.980 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:28.980 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:28.980 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714849588980
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:28.981 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 951_3365
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:28.981 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3365,
  "taskName" : "hash_article_data",
  "firstSubmitTime" : 1714849587624,
  "startTime" : 1714849588980,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13471825071552/29/951/3365.log",
  "processId" : 0,
  "processDefineCode" : 13471825071552,
  "processDefineVersion" : 29,
  "processInstanceId" : 951,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"article_data_hash\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"},{\"prop\":\"article_data\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"import hashlib\\nimport json\\n\\n# INSERT ${news_article} instead\\n# receive the article data from the main gnews extraction workflow\\narticle_data = {\\n    \\\"title\\\": \\\"Singapore diplomat accused of filming naked boy at Tokyo bathhouse - South China Morning Post\\\",\\n    \\\"datetime\\\": \\\"Thu, 02 May 2024 08:01:58 GMT\\\",\\n    \\\"description\\\": \\\"Singapore diplomat accused of filming naked boy at Tokyo bathhouse  South China Morning Post\\\",\\n    \\\"url\\\": \\\"https://news.google.com/rss/articles/CBMinQFodHRwczovL3d3dy5zY21wLmNvbS9uZXdzL2FzaWEvc291dGhlYXN0LWFzaWEvYXJ0aWNsZS8zMjYxMTc4L2phcGFuLXBvbGljZS1pbnZlc3RpZ2F0ZS1zaW5nYXBvcmUtZGlwbG9tYXQtYWxsZWdlZGx5LWZpbG1pbmctbmFrZWQtc2Nob29sYm95LXRva3lvLXB1YmxpYy1iYXRo0gGdAWh0dHBzOi8vYW1wLnNjbXAuY29tL25ld3MvYXNpYS9zb3V0aGVhc3QtYXNpYS9hcnRpY2xlLzMyNjExNzgvamFwYW4tcG9saWNlLWludmVzdGlnYXRlLXNpbmdhcG9yZS1kaXBsb21hdC1hbGxlZ2VkbHktZmlsbWluZy1uYWtlZC1zY2hvb2xib3ktdG9reW8tcHVibGljLWJhdGg?oc=5&hl=en-SG&gl=SG&ceid=SG:en\\\",\\n    \\\"context\\\": \\\"The Yomiuri newspaper reported that on February 27, the man used his smartphone to secretly film a 13-year-old secondary school student in the changing room of a public bath. The boy was naked.\\\\n\\\\nThe diplomat in question is a 55-year-old, who is a \\\\u201cformer\\\\u201d counsellor at the embassy, according to NHK. A counsellor is a diplomatic rank for officers serving overseas, such as in an embassy.\\\\n\\\\nA diplomat at the Singapore embassy in Tokyo was questioned by police after being suspected of filming a male teenager at a public bath, according to Japanese media reports on Thursday.\\\\n\\\\nStaff members at the public bath in Tokyo\\\\u2019s Minato ward reportedly called the police who, upon arriving, searched the diplomat\\\\u2019s phone and found \\\\u201cmultiple naked photos of male customers\\\\u201d, according to the Asahi newspaper.\\\\n\\\\nIt added that the diplomat refused to go to the police station but told officers he had taken such photos in other public baths.\\\\n\\\\nWhen asked to delete the photos from his phone, the diplomat \\\\u201cdeleted them on the spot\\\\u201d, reported the Japanese news outlet. He allegedly deleted 700 photos from his phone, which he told police he had taken in the six months leading up to the incident.\\\\n\\\\nThe Tokyo police are investigating potential violations of child pornography laws and are planning to ask Singapore\\\\u2019s Ministry of Foreign Affairs (MFA) to have the man turn himself in, said Asahi. Formal charges are also being considered.\\\\n\\\\nThe diplomat has been dismissed as a counsellor but is immune to arrest in accordance with the Vienna Convention on Diplomatic Relations, which states that diplomats cannot be arrested or detained in a country they have been dispatched to.\\\\n\\\\nWhen asked by the Asahi newspaper on Thursday, the embassy was reportedly unaware of the public bath incident. The embassy also told the paper the diplomat had \\\\u201ccompleted his assignment as of April 12\\\\u201d and has returned to Singapore.\\\\n\\\\nCNA has reached out to MFA for comment.\\\\n\\\\nAdditional reporting by Kyodo\\\",\\n    \\\"authors\\\": \\\"Unknown Author\\\",\\n    \\\"publisher_href\\\": \\\"https://www.scmp.com\\\"\\n} \\n\\n# convert JSON data to string\\narticle_data_json = json.dumps(article_data)\\n\\n# hash the JSON string\\narticle_data_hash = hashlib.sha256(article_data_json.encode()).hexdigest()\\n\\n# pass the hash and article data to the downstream nodes\\nprint(\\\"${setValue(article_data_hash=%s)}\\\" % str(article_data_hash))\\nprint(\\\"${setValue(article_data=%s)}\\\" % article_data)\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "hash_article_data"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3365"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471727617984"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505030627"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "951"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_articles_hashing_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471825071552"
    }
  },
  "taskAppId" : "951_3365",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:28.988 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:28.989 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:28.990 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:28.993 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3371, taskName=keyword filtering, firstSubmitTime=1714849588774, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=25, appIds=null, processInstanceId=923, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"gnews_preprocessed|gnews_filtered\""},{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"[\"singapore\", \"sg\"]"}], executorId=1, cmdTypeIfComplement=2, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, from_json\nfrom pyspark.sql.types import StringType, StructType, StructField, IntegerType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the topics parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# filter rows containing specific keywords\nkeywords = ${keywords}\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = parsed_df.filter(filter_condition)\n\n# stream the data to kafka\nkafka_write = filtered_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()\n","resourceList":[]}, environmentConfig=export HADOOP_HOME=/opt/hadoop-3.4.0
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='["singapore", "sg"]'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"gnews_preprocessed|gnews_filtered"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3371'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505030628'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='923'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_data_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:29.067 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:29.068 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:29.081 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:29.081 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:29.081 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:29.082 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714849589082
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:29.082 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 923_3371
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:29.087 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3369, taskName=sentiment analysis, firstSubmitTime=1714849588733, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=948, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=2, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3369'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505030628'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='948'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:29.090 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:29.091 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3371,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1714849588774,
  "startTime" : 1714849589082,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13377949373536/25/923/3371.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 25,
  "processInstanceId" : 923,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"gnews_preprocessed|gnews_filtered\\\"\"},{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"[\\\"singapore\\\", \\\"sg\\\"]\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 2,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import col, from_json\\nfrom pyspark.sql.types import StringType, StructType, StructField, IntegerType\\n\\nspark = SparkSession.builder \\\\\\n    .master(\\\"local\\\") \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the topics parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# filter rows containing specific keywords\\nkeywords = ${keywords}\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = parsed_df.filter(filter_condition)\\n\\n# stream the data to kafka\\nkafka_write = filtered_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export HADOOP_HOME=/opt/hadoop-3.4.0\nexport SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "[\"singapore\", \"sg\"]"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"gnews_preprocessed|gnews_filtered\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3371"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505030628"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "923"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_data_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "923_3371",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:29.092 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:29.093 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:29.093 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:29.100 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3368, taskName=sentiment analysis, firstSubmitTime=1714849588719, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=928, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\""}], executorId=1, cmdTypeIfComplement=2, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3368'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505030628'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='928'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:29.095 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:29.107 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:29.108 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:29.108 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:29.108 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714849589108
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:29.109 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 948_3369
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:29.109 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:29.112 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3369,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714849588733,
  "startTime" : 1714849589108,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/948/3369.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 948,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 2,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3369"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505030628"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "948"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "948_3369",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:29.118 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:29.118 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:29.118 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:29.109 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:29.128 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:29.128 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:29.129 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:29.130 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714849589130
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:29.130 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 928_3368
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:29.142 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3368,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714849588719,
  "startTime" : 1714849589130,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/928/3368.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 928,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 2,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3368"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505030628"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "928"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "928_3368",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:29.144 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:29.144 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:29.145 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:29.140 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3366, taskName=sentiment analysis, firstSubmitTime=1714849588711, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=904, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\""}], executorId=1, cmdTypeIfComplement=2, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3366'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505030628'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='904'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:29.147 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:29.149 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:29.150 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:29.150 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:29.151 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:29.151 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714849589151
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:29.151 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 904_3366
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:29.153 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3366,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714849588711,
  "startTime" : 1714849589151,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/904/3366.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 904,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 2,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3366"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505030628"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "904"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "904_3366",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:29.154 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:29.158 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:29.162 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:29.170 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3367, taskName=sentiment analysis, firstSubmitTime=1714849588717, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=909, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\""}], executorId=1, cmdTypeIfComplement=2, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3367'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505030628'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='909'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:29.173 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:29.175 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:29.180 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:29.192 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:29.182 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3370, taskName=sentiment analysis, firstSubmitTime=1714849588753, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=900, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\""}], executorId=1, cmdTypeIfComplement=2, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3370'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505030628'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='900'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:29.192 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:29.194 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714849589194
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:29.194 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:29.195 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:29.207 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:29.208 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:29.208 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:29.208 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714849589208
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:29.209 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 900_3370
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:29.209 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3370,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714849588753,
  "startTime" : 1714849589208,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/900/3370.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 900,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 2,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3370"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505030628"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "900"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "900_3370",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:29.210 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:29.211 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:29.211 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:29.212 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 909_3367
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:29.215 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3367,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714849588717,
  "startTime" : 1714849589194,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/909/3367.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 909,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 2,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3367"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505030628"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "909"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "909_3367",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:29.217 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:29.222 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:29.223 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:29.469 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:29.520 +0800 o.a.d.c.u.OSUtils:[231] - create linux os user: default
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:29.563 +0800 o.a.d.c.u.OSUtils:[233] - execute cmd: sudo useradd -g root
 default
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:29.541 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:29.574 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:29.596 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:29.601 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:29.621 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:29.654 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:29.655 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:29.656 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:29.850 +0800 o.a.d.c.u.OSUtils:[231] - create linux os user: default
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:29.851 +0800 o.a.d.c.u.OSUtils:[233] - execute cmd: sudo useradd -g root
 default
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:29.858 +0800 o.a.d.c.u.OSUtils:[231] - create linux os user: default
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:29.859 +0800 o.a.d.c.u.OSUtils:[233] - execute cmd: sudo useradd -g root
 default
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:29.895 +0800 o.a.d.c.u.OSUtils:[231] - create linux os user: default
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:29.896 +0800 o.a.d.c.u.OSUtils:[233] - execute cmd: sudo useradd -g root
 default
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:29.896 +0800 o.a.d.c.u.OSUtils:[231] - create linux os user: default
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:29.896 +0800 o.a.d.c.u.OSUtils:[231] - create linux os user: default
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:29.897 +0800 o.a.d.c.u.OSUtils:[233] - execute cmd: sudo useradd -g root
 default
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:29.897 +0800 o.a.d.c.u.OSUtils:[231] - create linux os user: default
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:29.897 +0800 o.a.d.c.u.OSUtils:[233] - execute cmd: sudo useradd -g root
 default
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:29.897 +0800 o.a.d.c.u.OSUtils:[233] - execute cmd: sudo useradd -g root
 default
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:29.898 +0800 o.a.d.c.u.OSUtils:[231] - create linux os user: default
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:29.898 +0800 o.a.d.c.u.OSUtils:[233] - execute cmd: sudo useradd -g root
 default
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:29.860 +0800 o.a.d.c.u.OSUtils:[231] - create linux os user: default
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:29.899 +0800 o.a.d.c.u.OSUtils:[233] - execute cmd: sudo useradd -g root
 default
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:29.901 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.1935483870967742 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3366] - [INFO] 2024-05-05 03:06:30.170 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3366, success=true)
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:30.171 +0800 o.a.d.c.u.OSUtils:[190] - create user default success
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:30.172 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-0][TI-3369] - [INFO] 2024-05-05 03:06:30.173 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3369, success=true)
[WI-0][TI-3370] - [INFO] 2024-05-05 03:06:30.173 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3370, success=true)
[WI-0][TI-3364] - [INFO] 2024-05-05 03:06:30.173 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3364, success=true)
[WI-0][TI-3365] - [INFO] 2024-05-05 03:06:30.174 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3365, success=true)
[WI-0][TI-3368] - [INFO] 2024-05-05 03:06:30.291 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3368, success=true)
[WI-0][TI-3363] - [INFO] 2024-05-05 03:06:30.293 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3363, success=true)
[WI-0][TI-3371] - [INFO] 2024-05-05 03:06:30.297 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3371, success=true)
[WI-0][TI-3367] - [INFO] 2024-05-05 03:06:30.321 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3367, success=true)
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:30.334 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471825071552_29/951/3365 check successfully
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:30.361 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-923][TI-3371] - [ERROR] 2024-05-05 03:06:30.369 +0800 o.a.d.c.u.OSUtils:[217] - useradd: user 'default' already exists

org.apache.dolphinscheduler.common.shell.AbstractShell$ExitCodeException: useradd: user 'default' already exists

	at org.apache.dolphinscheduler.common.shell.AbstractShell.runCommand(AbstractShell.java:205)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.run(AbstractShell.java:118)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execute(ShellExecutor.java:125)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:103)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:86)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeShell(OSUtils.java:345)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeCmd(OSUtils.java:334)
	at org.apache.dolphinscheduler.common.utils.OSUtils.createLinuxUser(OSUtils.java:234)
	at org.apache.dolphinscheduler.common.utils.OSUtils.createUser(OSUtils.java:213)
	at org.apache.dolphinscheduler.common.utils.OSUtils.createUserIfAbsent(OSUtils.java:189)
	at org.apache.dolphinscheduler.server.worker.utils.TaskExecutionContextUtils.getOrCreateTenant(TaskExecutionContextUtils.java:67)
	at org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecutor.beforeExecute(WorkerTaskExecutor.java:215)
	at org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecutor.run(WorkerTaskExecutor.java:167)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:30.370 +0800 o.a.d.c.u.OSUtils:[190] - create user default fail
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:30.371 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:30.373 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:30.378 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/923/3371 check successfully
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:30.379 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:30.380 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-928][TI-3368] - [ERROR] 2024-05-05 03:06:30.385 +0800 o.a.d.c.u.OSUtils:[217] - useradd: user 'default' already exists

org.apache.dolphinscheduler.common.shell.AbstractShell$ExitCodeException: useradd: user 'default' already exists

	at org.apache.dolphinscheduler.common.shell.AbstractShell.runCommand(AbstractShell.java:205)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.run(AbstractShell.java:118)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execute(ShellExecutor.java:125)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:103)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:86)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeShell(OSUtils.java:345)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeCmd(OSUtils.java:334)
	at org.apache.dolphinscheduler.common.utils.OSUtils.createLinuxUser(OSUtils.java:234)
	at org.apache.dolphinscheduler.common.utils.OSUtils.createUser(OSUtils.java:213)
	at org.apache.dolphinscheduler.common.utils.OSUtils.createUserIfAbsent(OSUtils.java:189)
	at org.apache.dolphinscheduler.server.worker.utils.TaskExecutionContextUtils.getOrCreateTenant(TaskExecutionContextUtils.java:67)
	at org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecutor.beforeExecute(WorkerTaskExecutor.java:215)
	at org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecutor.run(WorkerTaskExecutor.java:167)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[WI-909][TI-3367] - [ERROR] 2024-05-05 03:06:30.386 +0800 o.a.d.c.u.OSUtils:[217] - useradd: user 'default' already exists

org.apache.dolphinscheduler.common.shell.AbstractShell$ExitCodeException: useradd: user 'default' already exists

	at org.apache.dolphinscheduler.common.shell.AbstractShell.runCommand(AbstractShell.java:205)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.run(AbstractShell.java:118)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execute(ShellExecutor.java:125)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:103)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:86)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeShell(OSUtils.java:345)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeCmd(OSUtils.java:334)
	at org.apache.dolphinscheduler.common.utils.OSUtils.createLinuxUser(OSUtils.java:234)
	at org.apache.dolphinscheduler.common.utils.OSUtils.createUser(OSUtils.java:213)
	at org.apache.dolphinscheduler.common.utils.OSUtils.createUserIfAbsent(OSUtils.java:189)
	at org.apache.dolphinscheduler.server.worker.utils.TaskExecutionContextUtils.getOrCreateTenant(TaskExecutionContextUtils.java:67)
	at org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecutor.beforeExecute(WorkerTaskExecutor.java:215)
	at org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecutor.run(WorkerTaskExecutor.java:167)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:30.398 +0800 o.a.d.c.u.OSUtils:[190] - create user default fail
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:30.420 +0800 o.a.d.c.u.OSUtils:[190] - create user default fail
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:30.421 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:30.421 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:30.424 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/909/3367 check successfully
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:30.424 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-949][TI-3364] - [ERROR] 2024-05-05 03:06:30.424 +0800 o.a.d.c.u.OSUtils:[217] - useradd: user 'default' already exists

org.apache.dolphinscheduler.common.shell.AbstractShell$ExitCodeException: useradd: user 'default' already exists

	at org.apache.dolphinscheduler.common.shell.AbstractShell.runCommand(AbstractShell.java:205)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.run(AbstractShell.java:118)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execute(ShellExecutor.java:125)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:103)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:86)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeShell(OSUtils.java:345)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeCmd(OSUtils.java:334)
	at org.apache.dolphinscheduler.common.utils.OSUtils.createLinuxUser(OSUtils.java:234)
	at org.apache.dolphinscheduler.common.utils.OSUtils.createUser(OSUtils.java:213)
	at org.apache.dolphinscheduler.common.utils.OSUtils.createUserIfAbsent(OSUtils.java:189)
	at org.apache.dolphinscheduler.server.worker.utils.TaskExecutionContextUtils.getOrCreateTenant(TaskExecutionContextUtils.java:67)
	at org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecutor.beforeExecute(WorkerTaskExecutor.java:215)
	at org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecutor.run(WorkerTaskExecutor.java:167)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:30.424 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-904][TI-3366] - [ERROR] 2024-05-05 03:06:30.424 +0800 o.a.d.c.u.OSUtils:[217] - useradd: user 'default' already exists

org.apache.dolphinscheduler.common.shell.AbstractShell$ExitCodeException: useradd: user 'default' already exists

	at org.apache.dolphinscheduler.common.shell.AbstractShell.runCommand(AbstractShell.java:205)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.run(AbstractShell.java:118)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execute(ShellExecutor.java:125)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:103)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:86)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeShell(OSUtils.java:345)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeCmd(OSUtils.java:334)
	at org.apache.dolphinscheduler.common.utils.OSUtils.createLinuxUser(OSUtils.java:234)
	at org.apache.dolphinscheduler.common.utils.OSUtils.createUser(OSUtils.java:213)
	at org.apache.dolphinscheduler.common.utils.OSUtils.createUserIfAbsent(OSUtils.java:189)
	at org.apache.dolphinscheduler.server.worker.utils.TaskExecutionContextUtils.getOrCreateTenant(TaskExecutionContextUtils.java:67)
	at org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecutor.beforeExecute(WorkerTaskExecutor.java:215)
	at org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecutor.run(WorkerTaskExecutor.java:167)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:30.425 +0800 o.a.d.c.u.OSUtils:[190] - create user default fail
[WI-900][TI-3370] - [ERROR] 2024-05-05 03:06:30.425 +0800 o.a.d.c.u.OSUtils:[217] - useradd: user 'default' already exists

org.apache.dolphinscheduler.common.shell.AbstractShell$ExitCodeException: useradd: user 'default' already exists

	at org.apache.dolphinscheduler.common.shell.AbstractShell.runCommand(AbstractShell.java:205)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.run(AbstractShell.java:118)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execute(ShellExecutor.java:125)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:103)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:86)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeShell(OSUtils.java:345)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeCmd(OSUtils.java:334)
	at org.apache.dolphinscheduler.common.utils.OSUtils.createLinuxUser(OSUtils.java:234)
	at org.apache.dolphinscheduler.common.utils.OSUtils.createUser(OSUtils.java:213)
	at org.apache.dolphinscheduler.common.utils.OSUtils.createUserIfAbsent(OSUtils.java:189)
	at org.apache.dolphinscheduler.server.worker.utils.TaskExecutionContextUtils.getOrCreateTenant(TaskExecutionContextUtils.java:67)
	at org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecutor.beforeExecute(WorkerTaskExecutor.java:215)
	at org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecutor.run(WorkerTaskExecutor.java:167)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:30.441 +0800 o.a.d.c.u.OSUtils:[190] - create user default fail
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:30.442 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/928/3368 check successfully
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:30.442 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:30.443 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:30.424 +0800 o.a.d.c.u.OSUtils:[190] - create user default fail
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:30.450 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:30.444 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-950][TI-3363] - [ERROR] 2024-05-05 03:06:30.438 +0800 o.a.d.c.u.OSUtils:[217] - useradd: user 'default' already exists

org.apache.dolphinscheduler.common.shell.AbstractShell$ExitCodeException: useradd: user 'default' already exists

	at org.apache.dolphinscheduler.common.shell.AbstractShell.runCommand(AbstractShell.java:205)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.run(AbstractShell.java:118)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execute(ShellExecutor.java:125)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:103)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:86)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeShell(OSUtils.java:345)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeCmd(OSUtils.java:334)
	at org.apache.dolphinscheduler.common.utils.OSUtils.createLinuxUser(OSUtils.java:234)
	at org.apache.dolphinscheduler.common.utils.OSUtils.createUser(OSUtils.java:213)
	at org.apache.dolphinscheduler.common.utils.OSUtils.createUserIfAbsent(OSUtils.java:189)
	at org.apache.dolphinscheduler.server.worker.utils.TaskExecutionContextUtils.getOrCreateTenant(TaskExecutionContextUtils.java:67)
	at org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecutor.beforeExecute(WorkerTaskExecutor.java:215)
	at org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecutor.run(WorkerTaskExecutor.java:167)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:30.452 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:30.454 +0800 o.a.d.c.u.OSUtils:[190] - create user default fail
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:30.455 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:30.456 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/950/3363 check successfully
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:30.457 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:30.457 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-948][TI-3369] - [ERROR] 2024-05-05 03:06:30.424 +0800 o.a.d.c.u.OSUtils:[217] - useradd: user 'default' already exists

org.apache.dolphinscheduler.common.shell.AbstractShell$ExitCodeException: useradd: user 'default' already exists

	at org.apache.dolphinscheduler.common.shell.AbstractShell.runCommand(AbstractShell.java:205)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.run(AbstractShell.java:118)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execute(ShellExecutor.java:125)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:103)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:86)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeShell(OSUtils.java:345)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeCmd(OSUtils.java:334)
	at org.apache.dolphinscheduler.common.utils.OSUtils.createLinuxUser(OSUtils.java:234)
	at org.apache.dolphinscheduler.common.utils.OSUtils.createUser(OSUtils.java:213)
	at org.apache.dolphinscheduler.common.utils.OSUtils.createUserIfAbsent(OSUtils.java:189)
	at org.apache.dolphinscheduler.server.worker.utils.TaskExecutionContextUtils.getOrCreateTenant(TaskExecutionContextUtils.java:67)
	at org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecutor.beforeExecute(WorkerTaskExecutor.java:215)
	at org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecutor.run(WorkerTaskExecutor.java:167)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:30.458 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:30.424 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:30.425 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:30.449 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:30.458 +0800 o.a.d.c.u.OSUtils:[190] - create user default fail
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:30.461 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:30.425 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:30.463 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:30.461 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/900/3370 check successfully
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:30.460 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/904/3366 check successfully
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:30.463 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:30.464 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:30.484 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:30.464 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/948/3369 check successfully
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:30.488 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:30.489 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:30.489 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:30.458 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:30.458 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:30.454 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/949/3364 check successfully
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:30.452 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:30.496 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:30.497 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:30.488 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:30.485 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:30.514 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:30.488 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, from_json\nfrom pyspark.sql.types import StringType, StructType, StructField, IntegerType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the topics parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# filter rows containing specific keywords\nkeywords = ${keywords}\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = parsed_df.filter(filter_condition)\n\n# stream the data to kafka\nkafka_write = filtered_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()\n",
  "resourceList" : [ ]
}
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:30.514 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:30.515 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:30.515 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:30.466 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:30.517 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:30.519 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:30.515 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:30.512 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:30.497 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:30.523 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:30.523 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:30.525 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "news_articles_string",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ",
  "resourceList" : [ ]
}
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:30.511 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "article_data_hash",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  }, {
    "prop" : "article_data",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "import hashlib\nimport json\n\n# INSERT ${news_article} instead\n# receive the article data from the main gnews extraction workflow\narticle_data = {\n    \"title\": \"Singapore diplomat accused of filming naked boy at Tokyo bathhouse - South China Morning Post\",\n    \"datetime\": \"Thu, 02 May 2024 08:01:58 GMT\",\n    \"description\": \"Singapore diplomat accused of filming naked boy at Tokyo bathhouse  South China Morning Post\",\n    \"url\": \"https://news.google.com/rss/articles/CBMinQFodHRwczovL3d3dy5zY21wLmNvbS9uZXdzL2FzaWEvc291dGhlYXN0LWFzaWEvYXJ0aWNsZS8zMjYxMTc4L2phcGFuLXBvbGljZS1pbnZlc3RpZ2F0ZS1zaW5nYXBvcmUtZGlwbG9tYXQtYWxsZWdlZGx5LWZpbG1pbmctbmFrZWQtc2Nob29sYm95LXRva3lvLXB1YmxpYy1iYXRo0gGdAWh0dHBzOi8vYW1wLnNjbXAuY29tL25ld3MvYXNpYS9zb3V0aGVhc3QtYXNpYS9hcnRpY2xlLzMyNjExNzgvamFwYW4tcG9saWNlLWludmVzdGlnYXRlLXNpbmdhcG9yZS1kaXBsb21hdC1hbGxlZ2VkbHktZmlsbWluZy1uYWtlZC1zY2hvb2xib3ktdG9reW8tcHVibGljLWJhdGg?oc=5&hl=en-SG&gl=SG&ceid=SG:en\",\n    \"context\": \"The Yomiuri newspaper reported that on February 27, the man used his smartphone to secretly film a 13-year-old secondary school student in the changing room of a public bath. The boy was naked.\\n\\nThe diplomat in question is a 55-year-old, who is a \\u201cformer\\u201d counsellor at the embassy, according to NHK. A counsellor is a diplomatic rank for officers serving overseas, such as in an embassy.\\n\\nA diplomat at the Singapore embassy in Tokyo was questioned by police after being suspected of filming a male teenager at a public bath, according to Japanese media reports on Thursday.\\n\\nStaff members at the public bath in Tokyo\\u2019s Minato ward reportedly called the police who, upon arriving, searched the diplomat\\u2019s phone and found \\u201cmultiple naked photos of male customers\\u201d, according to the Asahi newspaper.\\n\\nIt added that the diplomat refused to go to the police station but told officers he had taken such photos in other public baths.\\n\\nWhen asked to delete the photos from his phone, the diplomat \\u201cdeleted them on the spot\\u201d, reported the Japanese news outlet. He allegedly deleted 700 photos from his phone, which he told police he had taken in the six months leading up to the incident.\\n\\nThe Tokyo police are investigating potential violations of child pornography laws and are planning to ask Singapore\\u2019s Ministry of Foreign Affairs (MFA) to have the man turn himself in, said Asahi. Formal charges are also being considered.\\n\\nThe diplomat has been dismissed as a counsellor but is immune to arrest in accordance with the Vienna Convention on Diplomatic Relations, which states that diplomats cannot be arrested or detained in a country they have been dispatched to.\\n\\nWhen asked by the Asahi newspaper on Thursday, the embassy was reportedly unaware of the public bath incident. The embassy also told the paper the diplomat had \\u201ccompleted his assignment as of April 12\\u201d and has returned to Singapore.\\n\\nCNA has reached out to MFA for comment.\\n\\nAdditional reporting by Kyodo\",\n    \"authors\": \"Unknown Author\",\n    \"publisher_href\": \"https://www.scmp.com\"\n} \n\n# convert JSON data to string\narticle_data_json = json.dumps(article_data)\n\n# hash the JSON string\narticle_data_hash = hashlib.sha256(article_data_json.encode()).hexdigest()\n\n# pass the hash and article data to the downstream nodes\nprint(\"${setValue(article_data_hash=%s)}\" % str(article_data_hash))\nprint(\"${setValue(article_data=%s)}\" % article_data)",
  "resourceList" : [ ]
}
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:30.497 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "news_articles_string",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ",
  "resourceList" : [ ]
}
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:30.526 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:30.520 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:30.529 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:30.520 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:30.517 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:30.528 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:30.527 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:30.534 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:30.537 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:30.527 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:30.537 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:30.539 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:30.533 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:30.541 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:30.539 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:30.542 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:30.530 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:30.541 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:30.542 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:30.543 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json
from pyspark.sql.types import StringType, StructType, StructField, IntegerType

spark = SparkSession.builder \
    .master("local") \
    .appName("Reddit Keyword Filtering") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the topics parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# filter rows containing specific keywords
keywords = ${keywords}

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = parsed_df.filter(filter_condition)

# stream the data to kafka
kafka_write = filtered_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()

[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:30.543 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:30.544 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:30.544 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:30.544 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:30.544 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:30.544 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:30.537 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:30.546 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:30.546 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:30.546 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import hashlib
import json

# INSERT ${news_article} instead
# receive the article data from the main gnews extraction workflow
article_data = {
    "title": "Singapore diplomat accused of filming naked boy at Tokyo bathhouse - South China Morning Post",
    "datetime": "Thu, 02 May 2024 08:01:58 GMT",
    "description": "Singapore diplomat accused of filming naked boy at Tokyo bathhouse  South China Morning Post",
    "url": "https://news.google.com/rss/articles/CBMinQFodHRwczovL3d3dy5zY21wLmNvbS9uZXdzL2FzaWEvc291dGhlYXN0LWFzaWEvYXJ0aWNsZS8zMjYxMTc4L2phcGFuLXBvbGljZS1pbnZlc3RpZ2F0ZS1zaW5nYXBvcmUtZGlwbG9tYXQtYWxsZWdlZGx5LWZpbG1pbmctbmFrZWQtc2Nob29sYm95LXRva3lvLXB1YmxpYy1iYXRo0gGdAWh0dHBzOi8vYW1wLnNjbXAuY29tL25ld3MvYXNpYS9zb3V0aGVhc3QtYXNpYS9hcnRpY2xlLzMyNjExNzgvamFwYW4tcG9saWNlLWludmVzdGlnYXRlLXNpbmdhcG9yZS1kaXBsb21hdC1hbGxlZ2VkbHktZmlsbWluZy1uYWtlZC1zY2hvb2xib3ktdG9reW8tcHVibGljLWJhdGg?oc=5&hl=en-SG&gl=SG&ceid=SG:en",
    "context": "The Yomiuri newspaper reported that on February 27, the man used his smartphone to secretly film a 13-year-old secondary school student in the changing room of a public bath. The boy was naked.\n\nThe diplomat in question is a 55-year-old, who is a \u201cformer\u201d counsellor at the embassy, according to NHK. A counsellor is a diplomatic rank for officers serving overseas, such as in an embassy.\n\nA diplomat at the Singapore embassy in Tokyo was questioned by police after being suspected of filming a male teenager at a public bath, according to Japanese media reports on Thursday.\n\nStaff members at the public bath in Tokyo\u2019s Minato ward reportedly called the police who, upon arriving, searched the diplomat\u2019s phone and found \u201cmultiple naked photos of male customers\u201d, according to the Asahi newspaper.\n\nIt added that the diplomat refused to go to the police station but told officers he had taken such photos in other public baths.\n\nWhen asked to delete the photos from his phone, the diplomat \u201cdeleted them on the spot\u201d, reported the Japanese news outlet. He allegedly deleted 700 photos from his phone, which he told police he had taken in the six months leading up to the incident.\n\nThe Tokyo police are investigating potential violations of child pornography laws and are planning to ask Singapore\u2019s Ministry of Foreign Affairs (MFA) to have the man turn himself in, said Asahi. Formal charges are also being considered.\n\nThe diplomat has been dismissed as a counsellor but is immune to arrest in accordance with the Vienna Convention on Diplomatic Relations, which states that diplomats cannot be arrested or detained in a country they have been dispatched to.\n\nWhen asked by the Asahi newspaper on Thursday, the embassy was reportedly unaware of the public bath incident. The embassy also told the paper the diplomat had \u201ccompleted his assignment as of April 12\u201d and has returned to Singapore.\n\nCNA has reached out to MFA for comment.\n\nAdditional reporting by Kyodo",
    "authors": "Unknown Author",
    "publisher_href": "https://www.scmp.com"
} 

# convert JSON data to string
article_data_json = json.dumps(article_data)

# hash the JSON string
article_data_hash = hashlib.sha256(article_data_json.encode()).hexdigest()

# pass the hash and article data to the downstream nodes
print("${setValue(article_data_hash=%s)}" % str(article_data_hash))
print("${setValue(article_data=%s)}" % article_data)
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:30.537 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:30.537 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:30.538 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:30.553 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:30.553 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:30.553 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:30.562 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/923/3371
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:30.554 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:30.564 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:30.565 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/900/3370
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:30.565 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/900/3370/py_900_3370.py
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:30.566 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:30.563 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/948/3369
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:30.562 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:30.562 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471825071552_29/951/3365
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:30.578 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:30.560 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/949/3364
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:30.543 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:30.579 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/923/3371/py_923_3371.py
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:30.538 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:30.590 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:30.590 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:30.589 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json
from pyspark.sql.types import StringType, StructType, StructField, IntegerType

spark = SparkSession.builder \
    .master("local") \
    .appName("Reddit Keyword Filtering") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the topics parameter
topics = "gnews_preprocessed|gnews_filtered"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# filter rows containing specific keywords
keywords = ["singapore", "sg"]

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = parsed_df.filter(filter_condition)

# stream the data to kafka
kafka_write = filtered_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()

[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:30.582 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471825071552_29/951/3365/py_951_3365.py
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:30.591 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:30.601 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:30.602 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:30.602 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:30.605 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:30.609 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:30.612 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:30.636 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/904/3366
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:30.588 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/909/3367
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:30.638 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/904/3366/py_904_3366.py
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:30.639 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:30.579 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:30.653 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:30.638 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/909/3367/py_909_3367.py
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:30.593 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import hashlib
import json

# INSERT ${news_article} instead
# receive the article data from the main gnews extraction workflow
article_data = {
    "title": "Singapore diplomat accused of filming naked boy at Tokyo bathhouse - South China Morning Post",
    "datetime": "Thu, 02 May 2024 08:01:58 GMT",
    "description": "Singapore diplomat accused of filming naked boy at Tokyo bathhouse  South China Morning Post",
    "url": "https://news.google.com/rss/articles/CBMinQFodHRwczovL3d3dy5zY21wLmNvbS9uZXdzL2FzaWEvc291dGhlYXN0LWFzaWEvYXJ0aWNsZS8zMjYxMTc4L2phcGFuLXBvbGljZS1pbnZlc3RpZ2F0ZS1zaW5nYXBvcmUtZGlwbG9tYXQtYWxsZWdlZGx5LWZpbG1pbmctbmFrZWQtc2Nob29sYm95LXRva3lvLXB1YmxpYy1iYXRo0gGdAWh0dHBzOi8vYW1wLnNjbXAuY29tL25ld3MvYXNpYS9zb3V0aGVhc3QtYXNpYS9hcnRpY2xlLzMyNjExNzgvamFwYW4tcG9saWNlLWludmVzdGlnYXRlLXNpbmdhcG9yZS1kaXBsb21hdC1hbGxlZ2VkbHktZmlsbWluZy1uYWtlZC1zY2hvb2xib3ktdG9reW8tcHVibGljLWJhdGg?oc=5&hl=en-SG&gl=SG&ceid=SG:en",
    "context": "The Yomiuri newspaper reported that on February 27, the man used his smartphone to secretly film a 13-year-old secondary school student in the changing room of a public bath. The boy was naked.\n\nThe diplomat in question is a 55-year-old, who is a \u201cformer\u201d counsellor at the embassy, according to NHK. A counsellor is a diplomatic rank for officers serving overseas, such as in an embassy.\n\nA diplomat at the Singapore embassy in Tokyo was questioned by police after being suspected of filming a male teenager at a public bath, according to Japanese media reports on Thursday.\n\nStaff members at the public bath in Tokyo\u2019s Minato ward reportedly called the police who, upon arriving, searched the diplomat\u2019s phone and found \u201cmultiple naked photos of male customers\u201d, according to the Asahi newspaper.\n\nIt added that the diplomat refused to go to the police station but told officers he had taken such photos in other public baths.\n\nWhen asked to delete the photos from his phone, the diplomat \u201cdeleted them on the spot\u201d, reported the Japanese news outlet. He allegedly deleted 700 photos from his phone, which he told police he had taken in the six months leading up to the incident.\n\nThe Tokyo police are investigating potential violations of child pornography laws and are planning to ask Singapore\u2019s Ministry of Foreign Affairs (MFA) to have the man turn himself in, said Asahi. Formal charges are also being considered.\n\nThe diplomat has been dismissed as a counsellor but is immune to arrest in accordance with the Vienna Convention on Diplomatic Relations, which states that diplomats cannot be arrested or detained in a country they have been dispatched to.\n\nWhen asked by the Asahi newspaper on Thursday, the embassy was reportedly unaware of the public bath incident. The embassy also told the paper the diplomat had \u201ccompleted his assignment as of April 12\u201d and has returned to Singapore.\n\nCNA has reached out to MFA for comment.\n\nAdditional reporting by Kyodo",
    "authors": "Unknown Author",
    "publisher_href": "https://www.scmp.com"
} 

# convert JSON data to string
article_data_json = json.dumps(article_data)

# hash the JSON string
article_data_hash = hashlib.sha256(article_data_json.encode()).hexdigest()

# pass the hash and article data to the downstream nodes
print("${setValue(article_data_hash=%s)}" % str(article_data_hash))
print("${setValue(article_data=%s)}" % article_data)
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:30.665 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:30.590 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:30.673 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/928/3368
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:30.686 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/928/3368/py_928_3368.py
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:30.686 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:30.583 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/948/3369/py_948_3369.py
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:30.582 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/949/3364/py_949_3364.py
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:30.688 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:30.656 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/950/3363
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:30.687 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:30.700 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/950/3363/py_950_3363.py
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:30.703 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:30.715 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:30.716 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:30.716 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export HADOOP_HOME=/opt/hadoop-3.4.0
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/923/3371/py_923_3371.py
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:30.716 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:30.718 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:30.732 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:30.732 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/928/3368/py_928_3368.py
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:30.733 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:30.733 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:30.734 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/928/3368/928_3368.sh
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:30.735 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:30.736 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/950/3363/py_950_3363.py
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:30.745 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:30.737 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/923/3371/923_3371.sh
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:30.756 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/950/3363/950_3363.sh
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:30.757 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:30.758 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:30.757 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:30.759 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:30.760 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13471825071552_29/951/3365/py_951_3365.py
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:30.761 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:30.758 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/909/3367/py_909_3367.py
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:30.761 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:30.737 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:30.736 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:30.736 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:30.762 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13471825071552_29/951/3365/951_3365.sh
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:30.737 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:30.762 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/909/3367/909_3367.sh
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:30.765 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:30.764 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:30.765 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:30.775 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/904/3366/py_904_3366.py
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:30.775 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:30.775 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/948/3369/py_948_3369.py
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:30.779 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:30.776 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/904/3366/904_3366.sh
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:30.788 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/948/3369/948_3369.sh
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:30.774 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/949/3364/py_949_3364.py
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:30.789 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:30.789 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/949/3364/949_3364.sh
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:30.770 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:30.790 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/900/3370/py_900_3370.py
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:30.792 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:30.804 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/900/3370/900_3370.sh
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:30.835 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 187
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:31.025 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.1940298507462686 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-909][TI-3367] - [INFO] 2024-05-05 03:06:31.046 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 189
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:31.072 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 196
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:31.074 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:31.652 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 193
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:31.657 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:31.582 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:31.539 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:31.549 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 186
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:32.055 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-3365] - [INFO] 2024-05-05 03:06:31.517 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3365)
[WI-948][TI-3369] - [INFO] 2024-05-05 03:06:31.337 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 194
[WI-928][TI-3368] - [INFO] 2024-05-05 03:06:31.247 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 183
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:31.245 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:31.239 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 200
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:31.087 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-923][TI-3371] - [INFO] 2024-05-05 03:06:31.085 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 185
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:32.130 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.1428571428571428 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3367] - [INFO] 2024-05-05 03:06:32.129 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3367)
[WI-0][TI-3366] - [INFO] 2024-05-05 03:06:32.129 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3366)
[WI-0][TI-3364] - [INFO] 2024-05-05 03:06:32.128 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3364)
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:31.877 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	${setValue(article_data_hash=c1890cda263e4d010c5292e71f03fbcca8e13aa3a5e703e6efcc2966dad482df)}
	${setValue(article_data={'title': 'Singapore diplomat accused of filming naked boy at Tokyo bathhouse - South China Morning Post', 'datetime': 'Thu, 02 May 2024 08:01:58 GMT', 'description': 'Singapore diplomat accused of filming naked boy at Tokyo bathhouse  South China Morning Post', 'url': 'https://news.google.com/rss/articles/CBMinQFodHRwczovL3d3dy5zY21wLmNvbS9uZXdzL2FzaWEvc291dGhlYXN0LWFzaWEvYXJ0aWNsZS8zMjYxMTc4L2phcGFuLXBvbGljZS1pbnZlc3RpZ2F0ZS1zaW5nYXBvcmUtZGlwbG9tYXQtYWxsZWdlZGx5LWZpbG1pbmctbmFrZWQtc2Nob29sYm95LXRva3lvLXB1YmxpYy1iYXRo0gGdAWh0dHBzOi8vYW1wLnNjbXAuY29tL25ld3MvYXNpYS9zb3V0aGVhc3QtYXNpYS9hcnRpY2xlLzMyNjExNzgvamFwYW4tcG9saWNlLWludmVzdGlnYXRlLXNpbmdhcG9yZS1kaXBsb21hdC1hbGxlZ2VkbHktZmlsbWluZy1uYWtlZC1zY2hvb2xib3ktdG9reW8tcHVibGljLWJhdGg?oc=5&hl=en-SG&gl=SG&ceid=SG:en', 'context': 'The Yomiuri newspaper reported that on February 27, the man used his smartphone to secretly film a 13-year-old secondary school student in the changing room of a public bath. The boy was naked.\n\nThe diplomat in question is a 55-year-old, who is a “former” counsellor at the embassy, according to NHK. A counsellor is a diplomatic rank for officers serving overseas, such as in an embassy.\n\nA diplomat at the Singapore embassy in Tokyo was questioned by police after being suspected of filming a male teenager at a public bath, according to Japanese media reports on Thursday.\n\nStaff members at the public bath in Tokyo’s Minato ward reportedly called the police who, upon arriving, searched the diplomat’s phone and found “multiple naked photos of male customers”, according to the Asahi newspaper.\n\nIt added that the diplomat refused to go to the police station but told officers he had taken such photos in other public baths.\n\nWhen asked to delete the photos from his phone, the diplomat “deleted them on the spot”, reported the Japanese news outlet. He allegedly deleted 700 photos from his phone, which he told police he had taken in the six months leading up to the incident.\n\nThe Tokyo police are investigating potential violations of child pornography laws and are planning to ask Singapore’s Ministry of Foreign Affairs (MFA) to have the man turn himself in, said Asahi. Formal charges are also being considered.\n\nThe diplomat has been dismissed as a counsellor but is immune to arrest in accordance with the Vienna Convention on Diplomatic Relations, which states that diplomats cannot be arrested or detained in a country they have been dispatched to.\n\nWhen asked by the Asahi newspaper on Thursday, the embassy was reportedly unaware of the public bath incident. The embassy also told the paper the diplomat had “completed his assignment as of April 12” and has returned to Singapore.\n\nCNA has reached out to MFA for comment.\n\nAdditional reporting by Kyodo', 'authors': 'Unknown Author', 'publisher_href': 'https://www.scmp.com'})}
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:31.654 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:32.382 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471825071552_29/951/3365, processId:187 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:32.383 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:32.383 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:32.383 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:32.398 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:32.422 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:32.423 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:32.424 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471825071552_29/951/3365
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:32.514 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471825071552_29/951/3365
[WI-951][TI-3365] - [INFO] 2024-05-05 03:06:32.519 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:33.023 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:33.040 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-3369] - [INFO] 2024-05-05 03:06:33.085 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3369)
[WI-0][TI-3370] - [INFO] 2024-05-05 03:06:33.086 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3370)
[WI-0][TI-3368] - [INFO] 2024-05-05 03:06:33.089 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3368)
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:33.152 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-3365] - [INFO] 2024-05-05 03:06:33.163 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3365, success=true)
[WI-0][TI-3371] - [INFO] 2024-05-05 03:06:33.174 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3371)
[WI-0][TI-3363] - [INFO] 2024-05-05 03:06:33.175 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3363)
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:33.194 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:33.257 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0664451827242525 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:33.599 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3372, taskName=check data existence in db, firstSubmitTime=1714849593325, startTime=0, taskType=SQL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13471825071552, processDefineVersion=29, appIds=null, processInstanceId=951, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"article_hash_in_db","direct":"OUT","type":"LIST","value":""}],"resourceList":[],"type":"POSTGRESQL","datasource":5,"sql":"SELECT hash\nFROM article_hashes\nWHERE hash = 'llll'","sqlType":"0","preStatements":["CREATE TABLE IF NOT EXISTS article_hashes (\n\t    hash TEXT NOT NULL\n\t);"],"postStatements":[],"displayRows":10}, environmentConfig=null, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='check data existence in db'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, article_data=Property{prop='article_data', direct=IN, type=VARCHAR, value='{'title': 'Singapore diplomat accused of filming naked boy at Tokyo bathhouse - South China Morning Post', 'datetime': 'Thu, 02 May 2024 08:01:58 GMT', 'description': 'Singapore diplomat accused of filming naked boy at Tokyo bathhouse  South China Morning Post', 'url': 'https://news.google.com/rss/articles/CBMinQFodHRwczovL3d3dy5zY21wLmNvbS9uZXdzL2FzaWEvc291dGhlYXN0LWFzaWEvYXJ0aWNsZS8zMjYxMTc4L2phcGFuLXBvbGljZS1pbnZlc3RpZ2F0ZS1zaW5nYXBvcmUtZGlwbG9tYXQtYWxsZWdlZGx5LWZpbG1pbmctbmFrZWQtc2Nob29sYm95LXRva3lvLXB1YmxpYy1iYXRo0gGdAWh0dHBzOi8vYW1wLnNjbXAuY29tL25ld3MvYXNpYS9zb3V0aGVhc3QtYXNpYS9hcnRpY2xlLzMyNjExNzgvamFwYW4tcG9saWNlLWludmVzdGlnYXRlLXNpbmdhcG9yZS1kaXBsb21hdC1hbGxlZ2VkbHktZmlsbWluZy1uYWtlZC1zY2hvb2xib3ktdG9reW8tcHVibGljLWJhdGg?oc=5&hl=en-SG&gl=SG&ceid=SG:en', 'context': 'The Yomiuri newspaper reported that on February 27, the man used his smartphone to secretly film a 13-year-old secondary school student in the changing room of a public bath. The boy was naked.\n\nThe diplomat in question is a 55-year-old, who is a “former” counsellor at the embassy, according to NHK. A counsellor is a diplomatic rank for officers serving overseas, such as in an embassy.\n\nA diplomat at the Singapore embassy in Tokyo was questioned by police after being suspected of filming a male teenager at a public bath, according to Japanese media reports on Thursday.\n\nStaff members at the public bath in Tokyo’s Minato ward reportedly called the police who, upon arriving, searched the diplomat’s phone and found “multiple naked photos of male customers”, according to the Asahi newspaper.\n\nIt added that the diplomat refused to go to the police station but told officers he had taken such photos in other public baths.\n\nWhen asked to delete the photos from his phone, the diplomat “deleted them on the spot”, reported the Japanese news outlet. He allegedly deleted 700 photos from his phone, which he told police he had taken in the six months leading up to the incident.\n\nThe Tokyo police are investigating potential violations of child pornography laws and are planning to ask Singapore’s Ministry of Foreign Affairs (MFA) to have the man turn himself in, said Asahi. Formal charges are also being considered.\n\nThe diplomat has been dismissed as a counsellor but is immune to arrest in accordance with the Vienna Convention on Diplomatic Relations, which states that diplomats cannot be arrested or detained in a country they have been dispatched to.\n\nWhen asked by the Asahi newspaper on Thursday, the embassy was reportedly unaware of the public bath incident. The embassy also told the paper the diplomat had “completed his assignment as of April 12” and has returned to Singapore.\n\nCNA has reached out to MFA for comment.\n\nAdditional reporting by Kyodo', 'authors': 'Unknown Author', 'publisher_href': 'https://www.scmp.com'}'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3372'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13472192399424'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505030633'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='951'}, article_data_hash=Property{prop='article_data_hash', direct=IN, type=VARCHAR, value='c1890cda263e4d010c5292e71f03fbcca8e13aa3a5e703e6efcc2966dad482df'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_articles_hashing_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13471825071552'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=org.apache.dolphinscheduler.plugin.task.api.parameters.resource.ResourceParametersHelper@2b445251, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"article_data_hash","direct":"IN","type":"VARCHAR","value":"c1890cda263e4d010c5292e71f03fbcca8e13aa3a5e703e6efcc2966dad482df"},{"prop":"article_data","direct":"IN","type":"VARCHAR","value":"{'title': 'Singapore diplomat accused of filming naked boy at Tokyo bathhouse - South China Morning Post', 'datetime': 'Thu, 02 May 2024 08:01:58 GMT', 'description': 'Singapore diplomat accused of filming naked boy at Tokyo bathhouse  South China Morning Post', 'url': 'https://news.google.com/rss/articles/CBMinQFodHRwczovL3d3dy5zY21wLmNvbS9uZXdzL2FzaWEvc291dGhlYXN0LWFzaWEvYXJ0aWNsZS8zMjYxMTc4L2phcGFuLXBvbGljZS1pbnZlc3RpZ2F0ZS1zaW5nYXBvcmUtZGlwbG9tYXQtYWxsZWdlZGx5LWZpbG1pbmctbmFrZWQtc2Nob29sYm95LXRva3lvLXB1YmxpYy1iYXRo0gGdAWh0dHBzOi8vYW1wLnNjbXAuY29tL25ld3MvYXNpYS9zb3V0aGVhc3QtYXNpYS9hcnRpY2xlLzMyNjExNzgvamFwYW4tcG9saWNlLWludmVzdGlnYXRlLXNpbmdhcG9yZS1kaXBsb21hdC1hbGxlZ2VkbHktZmlsbWluZy1uYWtlZC1zY2hvb2xib3ktdG9reW8tcHVibGljLWJhdGg?oc=5&hl=en-SG&gl=SG&ceid=SG:en', 'context': 'The Yomiuri newspaper reported that on February 27, the man used his smartphone to secretly film a 13-year-old secondary school student in the changing room of a public bath. The boy was naked.\\n\\nThe diplomat in question is a 55-year-old, who is a “former” counsellor at the embassy, according to NHK. A counsellor is a diplomatic rank for officers serving overseas, such as in an embassy.\\n\\nA diplomat at the Singapore embassy in Tokyo was questioned by police after being suspected of filming a male teenager at a public bath, according to Japanese media reports on Thursday.\\n\\nStaff members at the public bath in Tokyo’s Minato ward reportedly called the police who, upon arriving, searched the diplomat’s phone and found “multiple naked photos of male customers”, according to the Asahi newspaper.\\n\\nIt added that the diplomat refused to go to the police station but told officers he had taken such photos in other public baths.\\n\\nWhen asked to delete the photos from his phone, the diplomat “deleted them on the spot”, reported the Japanese news outlet. He allegedly deleted 700 photos from his phone, which he told police he had taken in the six months leading up to the incident.\\n\\nThe Tokyo police are investigating potential violations of child pornography laws and are planning to ask Singapore’s Ministry of Foreign Affairs (MFA) to have the man turn himself in, said Asahi. Formal charges are also being considered.\\n\\nThe diplomat has been dismissed as a counsellor but is immune to arrest in accordance with the Vienna Convention on Diplomatic Relations, which states that diplomats cannot be arrested or detained in a country they have been dispatched to.\\n\\nWhen asked by the Asahi newspaper on Thursday, the embassy was reportedly unaware of the public bath incident. The embassy also told the paper the diplomat had “completed his assignment as of April 12” and has returned to Singapore.\\n\\nCNA has reached out to MFA for comment.\\n\\nAdditional reporting by Kyodo', 'authors': 'Unknown Author', 'publisher_href': 'https://www.scmp.com'}"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.601 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: check data existence in db to wait queue success
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.603 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.609 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.613 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.618 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.619 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714849593619
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.620 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 951_3372
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.628 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3372,
  "taskName" : "check data existence in db",
  "firstSubmitTime" : 1714849593325,
  "startTime" : 1714849593619,
  "taskType" : "SQL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13471825071552/29/951/3372.log",
  "processId" : 0,
  "processDefineCode" : 13471825071552,
  "processDefineVersion" : 29,
  "processInstanceId" : 951,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"article_hash_in_db\",\"direct\":\"OUT\",\"type\":\"LIST\",\"value\":\"\"}],\"resourceList\":[],\"type\":\"POSTGRESQL\",\"datasource\":5,\"sql\":\"SELECT hash\\nFROM article_hashes\\nWHERE hash = 'llll'\",\"sqlType\":\"0\",\"preStatements\":[\"CREATE TABLE IF NOT EXISTS article_hashes (\\n\\t    hash TEXT NOT NULL\\n\\t);\"],\"postStatements\":[],\"displayRows\":10}",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "check data existence in db"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "article_data" : {
      "prop" : "article_data",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "{'title': 'Singapore diplomat accused of filming naked boy at Tokyo bathhouse - South China Morning Post', 'datetime': 'Thu, 02 May 2024 08:01:58 GMT', 'description': 'Singapore diplomat accused of filming naked boy at Tokyo bathhouse  South China Morning Post', 'url': 'https://news.google.com/rss/articles/CBMinQFodHRwczovL3d3dy5zY21wLmNvbS9uZXdzL2FzaWEvc291dGhlYXN0LWFzaWEvYXJ0aWNsZS8zMjYxMTc4L2phcGFuLXBvbGljZS1pbnZlc3RpZ2F0ZS1zaW5nYXBvcmUtZGlwbG9tYXQtYWxsZWdlZGx5LWZpbG1pbmctbmFrZWQtc2Nob29sYm95LXRva3lvLXB1YmxpYy1iYXRo0gGdAWh0dHBzOi8vYW1wLnNjbXAuY29tL25ld3MvYXNpYS9zb3V0aGVhc3QtYXNpYS9hcnRpY2xlLzMyNjExNzgvamFwYW4tcG9saWNlLWludmVzdGlnYXRlLXNpbmdhcG9yZS1kaXBsb21hdC1hbGxlZ2VkbHktZmlsbWluZy1uYWtlZC1zY2hvb2xib3ktdG9reW8tcHVibGljLWJhdGg?oc=5&hl=en-SG&gl=SG&ceid=SG:en', 'context': 'The Yomiuri newspaper reported that on February 27, the man used his smartphone to secretly film a 13-year-old secondary school student in the changing room of a public bath. The boy was naked.\\n\\nThe diplomat in question is a 55-year-old, who is a “former” counsellor at the embassy, according to NHK. A counsellor is a diplomatic rank for officers serving overseas, such as in an embassy.\\n\\nA diplomat at the Singapore embassy in Tokyo was questioned by police after being suspected of filming a male teenager at a public bath, according to Japanese media reports on Thursday.\\n\\nStaff members at the public bath in Tokyo’s Minato ward reportedly called the police who, upon arriving, searched the diplomat’s phone and found “multiple naked photos of male customers”, according to the Asahi newspaper.\\n\\nIt added that the diplomat refused to go to the police station but told officers he had taken such photos in other public baths.\\n\\nWhen asked to delete the photos from his phone, the diplomat “deleted them on the spot”, reported the Japanese news outlet. He allegedly deleted 700 photos from his phone, which he told police he had taken in the six months leading up to the incident.\\n\\nThe Tokyo police are investigating potential violations of child pornography laws and are planning to ask Singapore’s Ministry of Foreign Affairs (MFA) to have the man turn himself in, said Asahi. Formal charges are also being considered.\\n\\nThe diplomat has been dismissed as a counsellor but is immune to arrest in accordance with the Vienna Convention on Diplomatic Relations, which states that diplomats cannot be arrested or detained in a country they have been dispatched to.\\n\\nWhen asked by the Asahi newspaper on Thursday, the embassy was reportedly unaware of the public bath incident. The embassy also told the paper the diplomat had “completed his assignment as of April 12” and has returned to Singapore.\\n\\nCNA has reached out to MFA for comment.\\n\\nAdditional reporting by Kyodo', 'authors': 'Unknown Author', 'publisher_href': 'https://www.scmp.com'}"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3372"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13472192399424"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505030633"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "951"
    },
    "article_data_hash" : {
      "prop" : "article_data_hash",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "c1890cda263e4d010c5292e71f03fbcca8e13aa3a5e703e6efcc2966dad482df"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_articles_hashing_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471825071552"
    }
  },
  "taskAppId" : "951_3372",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "resourceParametersHelper" : {
    "resourceMap" : {
      "DATASOURCE" : {
        "5" : {
          "resourceType" : "DATASOURCE",
          "type" : "POSTGRESQL",
          "connectionParams" : "{\"user\":\"root\",\"password\":\"******\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"gnews_article_hash_db\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/gnews_article_hash_db\",\"driverClassName\":\"org.postgresql.Driver\",\"validationQuery\":\"select version()\",\"other\":{\"password\":\"root\"}}",
          "DATASOURCE" : null
        }
      }
    }
  },
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"article_data_hash\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"c1890cda263e4d010c5292e71f03fbcca8e13aa3a5e703e6efcc2966dad482df\"},{\"prop\":\"article_data\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"{'title': 'Singapore diplomat accused of filming naked boy at Tokyo bathhouse - South China Morning Post', 'datetime': 'Thu, 02 May 2024 08:01:58 GMT', 'description': 'Singapore diplomat accused of filming naked boy at Tokyo bathhouse  South China Morning Post', 'url': 'https://news.google.com/rss/articles/CBMinQFodHRwczovL3d3dy5zY21wLmNvbS9uZXdzL2FzaWEvc291dGhlYXN0LWFzaWEvYXJ0aWNsZS8zMjYxMTc4L2phcGFuLXBvbGljZS1pbnZlc3RpZ2F0ZS1zaW5nYXBvcmUtZGlwbG9tYXQtYWxsZWdlZGx5LWZpbG1pbmctbmFrZWQtc2Nob29sYm95LXRva3lvLXB1YmxpYy1iYXRo0gGdAWh0dHBzOi8vYW1wLnNjbXAuY29tL25ld3MvYXNpYS9zb3V0aGVhc3QtYXNpYS9hcnRpY2xlLzMyNjExNzgvamFwYW4tcG9saWNlLWludmVzdGlnYXRlLXNpbmdhcG9yZS1kaXBsb21hdC1hbGxlZ2VkbHktZmlsbWluZy1uYWtlZC1zY2hvb2xib3ktdG9reW8tcHVibGljLWJhdGg?oc=5&hl=en-SG&gl=SG&ceid=SG:en', 'context': 'The Yomiuri newspaper reported that on February 27, the man used his smartphone to secretly film a 13-year-old secondary school student in the changing room of a public bath. The boy was naked.\\\\n\\\\nThe diplomat in question is a 55-year-old, who is a “former” counsellor at the embassy, according to NHK. A counsellor is a diplomatic rank for officers serving overseas, such as in an embassy.\\\\n\\\\nA diplomat at the Singapore embassy in Tokyo was questioned by police after being suspected of filming a male teenager at a public bath, according to Japanese media reports on Thursday.\\\\n\\\\nStaff members at the public bath in Tokyo’s Minato ward reportedly called the police who, upon arriving, searched the diplomat’s phone and found “multiple naked photos of male customers”, according to the Asahi newspaper.\\\\n\\\\nIt added that the diplomat refused to go to the police station but told officers he had taken such photos in other public baths.\\\\n\\\\nWhen asked to delete the photos from his phone, the diplomat “deleted them on the spot”, reported the Japanese news outlet. He allegedly deleted 700 photos from his phone, which he told police he had taken in the six months leading up to the incident.\\\\n\\\\nThe Tokyo police are investigating potential violations of child pornography laws and are planning to ask Singapore’s Ministry of Foreign Affairs (MFA) to have the man turn himself in, said Asahi. Formal charges are also being considered.\\\\n\\\\nThe diplomat has been dismissed as a counsellor but is immune to arrest in accordance with the Vienna Convention on Diplomatic Relations, which states that diplomats cannot be arrested or detained in a country they have been dispatched to.\\\\n\\\\nWhen asked by the Asahi newspaper on Thursday, the embassy was reportedly unaware of the public bath incident. The embassy also told the paper the diplomat had “completed his assignment as of April 12” and has returned to Singapore.\\\\n\\\\nCNA has reached out to MFA for comment.\\\\n\\\\nAdditional reporting by Kyodo', 'authors': 'Unknown Author', 'publisher_href': 'https://www.scmp.com'}\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.641 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.642 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.642 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.655 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.656 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.658 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471825071552_29/951/3372 check successfully
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.661 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.sql.SqlTaskChannel successfully
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.677 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.697 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.714 +0800 o.a.d.p.t.s.SqlTask:[99] - Initialize sql task parameter {
  "localParams" : [ {
    "prop" : "article_hash_in_db",
    "direct" : "OUT",
    "type" : "LIST",
    "value" : ""
  } ],
  "varPool" : null,
  "type" : "POSTGRESQL",
  "datasource" : 5,
  "sql" : "SELECT hash\nFROM article_hashes\nWHERE hash = 'llll'",
  "sqlType" : 0,
  "sendEmail" : null,
  "displayRows" : 10,
  "udfs" : null,
  "showType" : null,
  "connParams" : null,
  "preStatements" : [ "CREATE TABLE IF NOT EXISTS article_hashes (\n\t    hash TEXT NOT NULL\n\t);" ],
  "postStatements" : [ ],
  "groupId" : 0,
  "title" : null,
  "limit" : 0
}
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.719 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SQL create successfully
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.721 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.721 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"article_data_hash","direct":"IN","type":"VARCHAR","value":"c1890cda263e4d010c5292e71f03fbcca8e13aa3a5e703e6efcc2966dad482df"},{"prop":"article_data","direct":"IN","type":"VARCHAR","value":"{'title': 'Singapore diplomat accused of filming naked boy at Tokyo bathhouse - South China Morning Post', 'datetime': 'Thu, 02 May 2024 08:01:58 GMT', 'description': 'Singapore diplomat accused of filming naked boy at Tokyo bathhouse  South China Morning Post', 'url': 'https://news.google.com/rss/articles/CBMinQFodHRwczovL3d3dy5zY21wLmNvbS9uZXdzL2FzaWEvc291dGhlYXN0LWFzaWEvYXJ0aWNsZS8zMjYxMTc4L2phcGFuLXBvbGljZS1pbnZlc3RpZ2F0ZS1zaW5nYXBvcmUtZGlwbG9tYXQtYWxsZWdlZGx5LWZpbG1pbmctbmFrZWQtc2Nob29sYm95LXRva3lvLXB1YmxpYy1iYXRo0gGdAWh0dHBzOi8vYW1wLnNjbXAuY29tL25ld3MvYXNpYS9zb3V0aGVhc3QtYXNpYS9hcnRpY2xlLzMyNjExNzgvamFwYW4tcG9saWNlLWludmVzdGlnYXRlLXNpbmdhcG9yZS1kaXBsb21hdC1hbGxlZ2VkbHktZmlsbWluZy1uYWtlZC1zY2hvb2xib3ktdG9reW8tcHVibGljLWJhdGg?oc=5&hl=en-SG&gl=SG&ceid=SG:en', 'context': 'The Yomiuri newspaper reported that on February 27, the man used his smartphone to secretly film a 13-year-old secondary school student in the changing room of a public bath. The boy was naked.\\n\\nThe diplomat in question is a 55-year-old, who is a “former” counsellor at the embassy, according to NHK. A counsellor is a diplomatic rank for officers serving overseas, such as in an embassy.\\n\\nA diplomat at the Singapore embassy in Tokyo was questioned by police after being suspected of filming a male teenager at a public bath, according to Japanese media reports on Thursday.\\n\\nStaff members at the public bath in Tokyo’s Minato ward reportedly called the police who, upon arriving, searched the diplomat’s phone and found “multiple naked photos of male customers”, according to the Asahi newspaper.\\n\\nIt added that the diplomat refused to go to the police station but told officers he had taken such photos in other public baths.\\n\\nWhen asked to delete the photos from his phone, the diplomat “deleted them on the spot”, reported the Japanese news outlet. He allegedly deleted 700 photos from his phone, which he told police he had taken in the six months leading up to the incident.\\n\\nThe Tokyo police are investigating potential violations of child pornography laws and are planning to ask Singapore’s Ministry of Foreign Affairs (MFA) to have the man turn himself in, said Asahi. Formal charges are also being considered.\\n\\nThe diplomat has been dismissed as a counsellor but is immune to arrest in accordance with the Vienna Convention on Diplomatic Relations, which states that diplomats cannot be arrested or detained in a country they have been dispatched to.\\n\\nWhen asked by the Asahi newspaper on Thursday, the embassy was reportedly unaware of the public bath incident. The embassy also told the paper the diplomat had “completed his assignment as of April 12” and has returned to Singapore.\\n\\nCNA has reached out to MFA for comment.\\n\\nAdditional reporting by Kyodo', 'authors': 'Unknown Author', 'publisher_href': 'https://www.scmp.com'}"}] successfully
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.722 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.723 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.724 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.724 +0800 o.a.d.p.t.s.SqlTask:[119] - Full sql parameters: SqlParameters{type='POSTGRESQL', datasource=5, sql='SELECT hash
FROM article_hashes
WHERE hash = 'llll'', sqlType=0, sendEmail=null, displayRows=10, limit=0, udfs='null', showType='null', connParams='null', groupId='0', title='null', preStatements=[CREATE TABLE IF NOT EXISTS article_hashes (
	    hash TEXT NOT NULL
	);], postStatements=[]}
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.725 +0800 o.a.d.p.t.s.SqlTask:[120] - sql type : POSTGRESQL, datasource : 5, sql : SELECT hash
FROM article_hashes
WHERE hash = 'llll' , localParams : [Property{prop='article_hash_in_db', direct=OUT, type=LIST, value=''}],udfs : null,showType : null,connParams : null,varPool : [Property{prop='article_data_hash', direct=IN, type=VARCHAR, value='c1890cda263e4d010c5292e71f03fbcca8e13aa3a5e703e6efcc2966dad482df'}, Property{prop='article_data', direct=IN, type=VARCHAR, value='{'title': 'Singapore diplomat accused of filming naked boy at Tokyo bathhouse - South China Morning Post', 'datetime': 'Thu, 02 May 2024 08:01:58 GMT', 'description': 'Singapore diplomat accused of filming naked boy at Tokyo bathhouse  South China Morning Post', 'url': 'https://news.google.com/rss/articles/CBMinQFodHRwczovL3d3dy5zY21wLmNvbS9uZXdzL2FzaWEvc291dGhlYXN0LWFzaWEvYXJ0aWNsZS8zMjYxMTc4L2phcGFuLXBvbGljZS1pbnZlc3RpZ2F0ZS1zaW5nYXBvcmUtZGlwbG9tYXQtYWxsZWdlZGx5LWZpbG1pbmctbmFrZWQtc2Nob29sYm95LXRva3lvLXB1YmxpYy1iYXRo0gGdAWh0dHBzOi8vYW1wLnNjbXAuY29tL25ld3MvYXNpYS9zb3V0aGVhc3QtYXNpYS9hcnRpY2xlLzMyNjExNzgvamFwYW4tcG9saWNlLWludmVzdGlnYXRlLXNpbmdhcG9yZS1kaXBsb21hdC1hbGxlZ2VkbHktZmlsbWluZy1uYWtlZC1zY2hvb2xib3ktdG9reW8tcHVibGljLWJhdGg?oc=5&hl=en-SG&gl=SG&ceid=SG:en', 'context': 'The Yomiuri newspaper reported that on February 27, the man used his smartphone to secretly film a 13-year-old secondary school student in the changing room of a public bath. The boy was naked.\n\nThe diplomat in question is a 55-year-old, who is a “former” counsellor at the embassy, according to NHK. A counsellor is a diplomatic rank for officers serving overseas, such as in an embassy.\n\nA diplomat at the Singapore embassy in Tokyo was questioned by police after being suspected of filming a male teenager at a public bath, according to Japanese media reports on Thursday.\n\nStaff members at the public bath in Tokyo’s Minato ward reportedly called the police who, upon arriving, searched the diplomat’s phone and found “multiple naked photos of male customers”, according to the Asahi newspaper.\n\nIt added that the diplomat refused to go to the police station but told officers he had taken such photos in other public baths.\n\nWhen asked to delete the photos from his phone, the diplomat “deleted them on the spot”, reported the Japanese news outlet. He allegedly deleted 700 photos from his phone, which he told police he had taken in the six months leading up to the incident.\n\nThe Tokyo police are investigating potential violations of child pornography laws and are planning to ask Singapore’s Ministry of Foreign Affairs (MFA) to have the man turn himself in, said Asahi. Formal charges are also being considered.\n\nThe diplomat has been dismissed as a counsellor but is immune to arrest in accordance with the Vienna Convention on Diplomatic Relations, which states that diplomats cannot be arrested or detained in a country they have been dispatched to.\n\nWhen asked by the Asahi newspaper on Thursday, the embassy was reportedly unaware of the public bath incident. The embassy also told the paper the diplomat had “completed his assignment as of April 12” and has returned to Singapore.\n\nCNA has reached out to MFA for comment.\n\nAdditional reporting by Kyodo', 'authors': 'Unknown Author', 'publisher_href': 'https://www.scmp.com'}'}] ,query max result limit  0
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.770 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: HANA
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.776 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: HANA
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.782 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: SQLSERVER
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.784 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: SQLSERVER
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.792 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: SAGEMAKER
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.793 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: SAGEMAKER
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.797 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: DATABEND
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:33.800 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: DATABEND
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.043 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: AZURESQL
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.054 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: AZURESQL
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.096 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: HIVE
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.098 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: HIVE
[WI-0][TI-3372] - [INFO] 2024-05-05 03:06:34.128 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3372, success=true)
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.151 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: DORIS
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.152 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: DORIS
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.155 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: POSTGRESQL
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.162 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: POSTGRESQL
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.165 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: MYSQL
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.167 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: MYSQL
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.172 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: REDSHIFT
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.172 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: REDSHIFT
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.176 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: SNOWFLAKE
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.178 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: SNOWFLAKE
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.183 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: STARROCKS
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.183 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: STARROCKS
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.186 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: SSH
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.186 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: SSH
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.191 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: ATHENA
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.191 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: ATHENA
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.195 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: PRESTO
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.196 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: PRESTO
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.204 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: OCEANBASE
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.204 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: OCEANBASE
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:34.267 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.1337792642140467 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.406 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: ORACLE
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.406 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: ORACLE
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.416 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: CLICKHOUSE
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.417 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: CLICKHOUSE
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.443 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: VERTICA
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.444 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: VERTICA
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.462 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: KYUUBI
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.463 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: KYUUBI
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.481 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: DB2
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.486 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: DB2
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.582 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: TRINO
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.630 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: TRINO
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.647 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: SPARK
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.648 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: SPARK
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.654 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: DAMENG
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.655 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: DAMENG
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.660 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: ZEPPELIN
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:34.663 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: ZEPPELIN
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:34.892 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/949/3364/py_949_3364.py", line 11, in <module>
	    news_results = google_news.get_news(topic)[:5]
	                                        ^^^^^
	NameError: name 'topic' is not defined
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:34.896 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/949/3364, processId:196 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:34.898 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:34.898 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:34.898 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:34.899 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:34.925 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:34.925 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:34.926 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/949/3364
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:34.940 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/949/3364
[WI-949][TI-3364] - [INFO] 2024-05-05 03:06:34.941 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:34.973 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/950/3363/py_950_3363.py", line 11, in <module>
	    news_results = google_news.get_news(topic)[:5]
	                                        ^^^^^
	NameError: name 'topic' is not defined
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:34.990 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/950/3363, processId:186 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:34.991 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:34.991 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:35.131 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:35.140 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-0][TI-3364] - [INFO] 2024-05-05 03:06:35.197 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3364, success=true)
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:35.198 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:35.198 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:35.198 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/950/3363
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:35.199 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/950/3363
[WI-950][TI-3363] - [INFO] 2024-05-05 03:06:35.199 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:35.855 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.1367924528301887 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:35.905 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3373, taskName=extract_gnews_article, firstSubmitTime=1714849595815, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13471700679104, processDefineVersion=3, appIds=null, processInstanceId=949, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_article'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3373'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13471571877568'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505030635'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='949'}, news_articles_string=Property{prop='news_articles_string', direct=OUT, type=VARCHAR, value=''}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_articles_main'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13471700679104'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:35.912 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract_gnews_article to wait queue success
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:35.913 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:35.989 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:35.990 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:35.990 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:35.996 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714849595990
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:35.996 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 949_3373
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:35.997 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3373,
  "taskName" : "extract_gnews_article",
  "firstSubmitTime" : 1714849595815,
  "startTime" : 1714849595990,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13471700679104/3/949/3373.log",
  "processId" : 0,
  "processDefineCode" : 13471700679104,
  "processDefineVersion" : 3,
  "processInstanceId" : 949,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"from gnews import GNews\\nimport json\\n\\n\\n# Initialize GNews client\\ngoogle_news = GNews()\\n\\n# Fetch Google News articles for a specific topic, which is singapore related\\nnews_results = google_news.get_news(topic)[:5]\\n\\n# List to store the relevant information about the news articles\\nnews_articles = []\\n\\n# Process each news article\\nfor news in news_results:\\n    # Extract basic information from Google News\\n    title = news.get('title')\\n    datetime = news.get('published date')\\n    url = news.get('url')\\n    description = news.get('description')\\n    \\n    # Extract publisher information\\n    publisher_info = news.get('publisher', {})\\n    publisher_href = publisher_info.get('href')\\n    \\n    # Initialize and handle exceptions in article parsing\\n    try:\\n        article = google_news.get_full_article(url)\\n\\n        # If parsing is successful, extract authors and text\\n        authors = article.authors if article.authors else \\\"Unknown Author\\\"\\n        context = article.text\\n    \\n    except Exception as e:\\n        print(f\\\"Failed to download article: {e}\\\")  # Log the error message\\n        continue\\n    \\n    # store the article data in a dictionary\\n    article_data = {\\n        'title': title,\\n        'datetime': datetime,\\n        'description': description,\\n        'url': url,\\n        'context': context,\\n        'authors': authors,\\n        'publisher_href': publisher_href,\\n    }\\n\\n\\n    # store the article data into the article_data list for JSON serialization\\n    news_articles.append(article_data)\\n\\n\\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\\nnews_articles_string = '|'.join(news_articles)\\n\\n# save the news_articles_string dictionary into a parameter to pass it downstream\\n\\nprint(\\\"#setValue(news_articles_string=%s)\\\" % news_articles_string)\\n\\nprint(news_articles_string) \",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_article"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3373"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471571877568"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505030635"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "949"
    },
    "news_articles_string" : {
      "prop" : "news_articles_string",
      "direct" : "OUT",
      "type" : "VARCHAR",
      "value" : ""
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_articles_main"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471700679104"
    }
  },
  "taskAppId" : "949_3373",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:35.998 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:35.999 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:35.999 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:36.066 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:36.067 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:36.067 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/949/3373 check successfully
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:36.068 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:36.068 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:36.069 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:36.070 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:36.070 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "news_articles_string",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ",
  "resourceList" : [ ]
}
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:36.071 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:36.071 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}] successfully
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:36.071 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:36.072 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:36.072 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:36.072 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:36.073 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/949/3373
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:36.074 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/949/3373/py_949_3373.py
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:36.074 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:36.075 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:36.076 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:36.076 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/949/3373/py_949_3373.py
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:36.076 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:36.077 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/949/3373/949_3373.sh
[WI-0][TI-3363] - [INFO] 2024-05-05 03:06:36.164 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3363, success=true)
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:36.164 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 452
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:36.167 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-3373] - [INFO] 2024-05-05 03:06:36.463 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3373, success=true)
[WI-0][TI-3373] - [INFO] 2024-05-05 03:06:36.540 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3373)
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:37.055 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0625 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.075 +0800 o.a.d.p.t.s.SqlTask:[410] - after replace sql , preparing : SELECT hash
FROM article_hashes
WHERE hash = 'llll'
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.075 +0800 o.a.d.p.t.s.SqlTask:[420] - Sql Params are replaced sql , parameters:
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.076 +0800 o.a.d.p.t.s.SqlTask:[410] - after replace sql , preparing : CREATE TABLE IF NOT EXISTS article_hashes (
	    hash TEXT NOT NULL
	);
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.076 +0800 o.a.d.p.t.s.SqlTask:[420] - Sql Params are replaced sql , parameters:
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.077 +0800 o.a.d.p.t.s.SqlTask:[489] - can't find udf function resource
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.415 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[49] - Registering datasource plugin: hive
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.418 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[57] - Registered datasource plugin: hive
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.418 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[49] - Registering datasource plugin: vertica
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.419 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[57] - Registered datasource plugin: vertica
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.419 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[49] - Registering datasource plugin: doris
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.420 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[57] - Registered datasource plugin: doris
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.420 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[49] - Registering datasource plugin: ssh
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.420 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[57] - Registered datasource plugin: ssh
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.421 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[49] - Registering datasource plugin: databend
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.421 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[57] - Registered datasource plugin: databend
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.421 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[49] - Registering datasource plugin: sqlserver
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.421 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[57] - Registered datasource plugin: sqlserver
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.422 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[49] - Registering datasource plugin: postgresql
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.422 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[57] - Registered datasource plugin: postgresql
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.422 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[49] - Registering datasource plugin: redshift
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.423 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[57] - Registered datasource plugin: redshift
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.423 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[49] - Registering datasource plugin: spark
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.424 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[57] - Registered datasource plugin: spark
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.424 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[49] - Registering datasource plugin: oceanbase
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.424 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[57] - Registered datasource plugin: oceanbase
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.425 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[49] - Registering datasource plugin: starrocks
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.425 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[57] - Registered datasource plugin: starrocks
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.425 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[49] - Registering datasource plugin: mysql
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.426 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[57] - Registered datasource plugin: mysql
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.426 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[49] - Registering datasource plugin: hana
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.426 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[57] - Registered datasource plugin: hana
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.427 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[49] - Registering datasource plugin: sagemaker
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.434 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[57] - Registered datasource plugin: sagemaker
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.434 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[49] - Registering datasource plugin: dameng
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.449 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[57] - Registered datasource plugin: dameng
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.451 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[49] - Registering datasource plugin: snowflake
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.452 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[57] - Registered datasource plugin: snowflake
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.452 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[49] - Registering datasource plugin: oracle
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.458 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[57] - Registered datasource plugin: oracle
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.458 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[49] - Registering datasource plugin: kyuubi
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.459 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[57] - Registered datasource plugin: kyuubi
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.459 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[49] - Registering datasource plugin: zeppelin
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.460 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[57] - Registered datasource plugin: zeppelin
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.460 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[49] - Registering datasource plugin: trino
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.461 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[57] - Registered datasource plugin: trino
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.461 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[49] - Registering datasource plugin: db2
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.462 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[57] - Registered datasource plugin: db2
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.462 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[49] - Registering datasource plugin: clickhouse
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.463 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[57] - Registered datasource plugin: clickhouse
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.463 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[49] - Registering datasource plugin: azuresql
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.463 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[57] - Registered datasource plugin: azuresql
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.463 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[49] - Registering datasource plugin: athena
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.464 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[57] - Registered datasource plugin: athena
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.464 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[49] - Registering datasource plugin: presto
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:37.465 +0800 o.a.d.p.d.a.p.DataSourcePluginManager:[57] - Registered datasource plugin: presto
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:38.058 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.096774193548387 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:39.149 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0235294117647058 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:40.247 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0284552845528456 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:40.538 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/949/3373/py_949_3373.py", line 11, in <module>
	    news_results = google_news.get_news(topic)[:5]
	                                        ^^^^^
	NameError: name 'topic' is not defined
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:41.278 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0985915492957747 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:41.550 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/949/3373, processId:452 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:41.580 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:41.581 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:41.582 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:41.583 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:41.661 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:41.662 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:41.664 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/949/3373
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:41.736 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/949/3373
[WI-949][TI-3373] - [INFO] 2024-05-05 03:06:41.752 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:42.288 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0481283422459893 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3373] - [INFO] 2024-05-05 03:06:42.387 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3373, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:43.322 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0380724538619275 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:44.413 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9603505976095619 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:45.431 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0474308300395256 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-951][TI-3372] - [WARN] 2024-05-05 03:06:45.566 +0800 n.s.c.j.SnowflakeConnectString:[136] - Connect strings must start with jdbc:snowflake://
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:46.433 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0666666666666667 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:46.835 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-e1529324-1ae9-416c-83e3-12622dfd1778;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:47.470 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.1028806584362139 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:48.485 +0800 o.a.d.p.t.s.SqlTask:[392] - prepare statement replace sql : CREATE TABLE IF NOT EXISTS article_hashes (
	    hash TEXT NOT NULL
	);, sql parameters : {}
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:48.510 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.1439688715953307 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:48.648 +0800 o.a.d.p.t.s.SqlTask:[328] - pre statement execute update result: 0, for sql: CREATE TABLE IF NOT EXISTS article_hashes (
	    hash TEXT NOT NULL
	);
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:48.652 +0800 o.a.d.p.t.s.SqlTask:[392] - prepare statement replace sql : SELECT hash
FROM article_hashes
WHERE hash = 'llll', sql parameters : {}
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:48.666 +0800 o.a.d.p.t.s.SqlTask:[316] - main statement execute query, for sql: SELECT hash
FROM article_hashes
WHERE hash = 'llll'
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:48.746 +0800 o.a.d.p.t.s.SqlTask:[260] - display sql result 0 rows as follows:
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:48.767 +0800 o.a.d.p.t.s.SqlTask:[288] - sql query results is empty
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:48.793 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:48.794 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:48.794 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:48.796 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:48.810 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:48.812 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:48.812 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471825071552_29/951/3372
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:48.812 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471825071552_29/951/3372
[WI-951][TI-3372] - [INFO] 2024-05-05 03:06:48.813 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3372] - [INFO] 2024-05-05 03:06:49.526 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3372, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:49.609 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:49.611 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:49.687 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0422077922077921 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:50.170 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:50.624 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-6925b484-45aa-4a75-b6c0-23af1a641a56;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:50.626 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-c1ad8578-d987-44b0-ab96-37e49fab9204;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:50.693 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9853801169590642 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:50.842 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:51.196 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-82046349-0b35-4baa-8f6c-d1346ca2dad3;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:51.623 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-4f33da16-1264-4cea-a290-52899849563e;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:51.696 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9448051948051948 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:51.847 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-50be5122-8d3c-40aa-9376-ba6ea8f90e90;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:52.740 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9937906674102994 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:53.741 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9857397948590175 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:54.754 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9792899408284024 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:55.756 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.976878612716763 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:55.937 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:56.767 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9936305732484076 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:56.887 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: resolution report :: resolve 5264ms :: artifacts dl 0ms
		:: modules in use:
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   1   |   0   |   0   |   0   ||   0   |   0   |
		---------------------------------------------------------------------
	
	:: problems summary ::
	:::: WARNINGS
		problem while downloading module descriptor: https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.pom: Downloaded file size (0) doesn't match expected Content Length (14867) for https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.pom. Please retry. (355ms)
	
			module not found: org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1
	
		==== local-m2-cache: tried
	
		  file:/home/default/.m2/repository/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.pom
	
		  -- artifact org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1!spark-sql-kafka-0-10_2.12.jar:
	
		  file:/home/default/.m2/repository/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.jar
	
		==== local-ivy-cache: tried
	
		  /tmp/local/org.apache.spark/spark-sql-kafka-0-10_2.12/3.5.1/ivys/ivy.xml
	
		  -- artifact org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1!spark-sql-kafka-0-10_2.12.jar:
	
		  /tmp/local/org.apache.spark/spark-sql-kafka-0-10_2.12/3.5.1/jars/spark-sql-kafka-0-10_2.12.jar
	
		==== central: tried
	
		  https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.pom
	
		==== spark-packages: tried
	
		  https://repos.spark-packages.org/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.pom
	
		  -- artifact org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1!spark-sql-kafka-0-10_2.12.jar:
	
		  https://repos.spark-packages.org/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.jar
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
			::          UNRESOLVED DEPENDENCIES         ::
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
			:: org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1: not found
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
	
	
	:: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS
	Exception in thread "main" java.lang.RuntimeException: [unresolved dependency: org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1: not found]
		at org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:1608)
		at org.apache.spark.util.DependencyUtils$.resolveMavenDependencies(DependencyUtils.scala:185)
		at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:334)
		at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)
		at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
		at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
		at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
		at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
		at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
		at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/900/3370/py_900_3370.py", line 27, in <module>
	    .getOrCreate()
	     ^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py", line 497, in getOrCreate
	    sc = SparkContext.getOrCreate(sparkConf)
	         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 515, in getOrCreate
	    SparkContext(conf=conf or SparkConf())
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 201, in __init__
	    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 436, in _ensure_initialized
	    SparkContext._gateway = gateway or launch_gateway(conf)
	                                       ^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/java_gateway.py", line 107, in launch_gateway
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:57.657 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:57.669 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:57.774 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:57.893 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/900/3370, processId:200 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:57.893 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:57.893 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:57.893 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:57.894 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:57.901 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:57.907 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:57.909 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/900/3370
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:57.911 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/900/3370
[WI-900][TI-3370] - [INFO] 2024-05-05 03:06:57.911 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:57.955 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:58.250 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: resolution report :: resolve 6961ms :: artifacts dl 0ms
		:: modules in use:
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   1   |   0   |   0   |   0   ||   0   |   0   |
		---------------------------------------------------------------------
	
	:: problems summary ::
	:::: WARNINGS
		problem while downloading module descriptor: https://repo1.maven.org/maven2/org/apache/spark/spark-parent_2.12/3.5.1/spark-parent_2.12-3.5.1.pom: Downloaded file size (0) doesn't match expected Content Length (136483) for https://repo1.maven.org/maven2/org/apache/spark/spark-parent_2.12/3.5.1/spark-parent_2.12-3.5.1.pom. Please retry. (686ms)
	
		io problem while parsing ivy file: https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.pom (java.io.IOException: Impossible to load parent for file:/tmp/org.apache.spark/spark-sql-kafka-0-10_2.12/ivy-3.5.1.xml.original. Parent=org.apache.spark#spark-parent_2.12;3.5.1)
	
			module not found: org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1
	
		==== local-m2-cache: tried
	
		  file:/home/default/.m2/repository/org/apache/spark/spark-parent_2.12/3.5.1/spark-parent_2.12-3.5.1.pom
	
		  -- artifact org.apache.spark#spark-parent_2.12;3.5.1!spark-parent_2.12.jar:
	
		  file:/home/default/.m2/repository/org/apache/spark/spark-parent_2.12/3.5.1/spark-parent_2.12-3.5.1.jar
	
		==== local-ivy-cache: tried
	
		  /tmp/local/org.apache.spark/spark-parent_2.12/3.5.1/ivys/ivy.xml
	
		  -- artifact org.apache.spark#spark-parent_2.12;3.5.1!spark-parent_2.12.jar:
	
		  /tmp/local/org.apache.spark/spark-parent_2.12/3.5.1/jars/spark-parent_2.12.jar
	
		==== central: tried
	
		  https://repo1.maven.org/maven2/org/apache/spark/spark-parent_2.12/3.5.1/spark-parent_2.12-3.5.1.pom
	
		==== spark-packages: tried
	
		  https://repos.spark-packages.org/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.pom
	
		  -- artifact org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1!spark-sql-kafka-0-10_2.12.jar:
	
		  https://repos.spark-packages.org/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.jar
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
			::          UNRESOLVED DEPENDENCIES         ::
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
			:: org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1: not found
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
	
	
	:: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS
	Exception in thread "main" java.lang.RuntimeException: [unresolved dependency: org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1: not found]
		at org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:1608)
		at org.apache.spark.util.DependencyUtils$.resolveMavenDependencies(DependencyUtils.scala:185)
		at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:334)
		at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)
		at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
		at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
		at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
		at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
		at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
		at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/904/3366/py_904_3366.py", line 27, in <module>
	    .getOrCreate()
	     ^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py", line 497, in getOrCreate
	    sc = SparkContext.getOrCreate(sparkConf)
	         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 515, in getOrCreate
	    SparkContext(conf=conf or SparkConf())
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 201, in __init__
	    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 436, in _ensure_initialized
	    SparkContext._gateway = gateway or launch_gateway(conf)
	                                       ^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/java_gateway.py", line 107, in launch_gateway
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.
[WI-0][TI-3370] - [INFO] 2024-05-05 03:06:58.638 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3370, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:58.802 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0108695652173914 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:58.963 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.kafka#kafka-clients;3.4.1 in central
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:59.266 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/904/3366, processId:193 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:59.266 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:59.267 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:59.267 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:59.268 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:59.308 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:59.310 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:59.310 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/904/3366
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:59.312 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/904/3366
[WI-904][TI-3366] - [INFO] 2024-05-05 03:06:59.312 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3366] - [INFO] 2024-05-05 03:06:59.718 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3366, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:59.722 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:59.725 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:59.720 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:06:59.992 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.lz4#lz4-java;1.8.0 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:00.037 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0037014633022239 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:00.793 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:00.821 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:00.825 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:01.048 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9966445258818141 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:01.009 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:02.051 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9028571428571428 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:02.940 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.slf4j#slf4j-api;2.0.7 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:03.053 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9340659340659341 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:03.091 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.slf4j#slf4j-api;2.0.7 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:03.830 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.slf4j#slf4j-api;2.0.7 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:03.911 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.slf4j#slf4j-api;2.0.7 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:04.056 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9369627507163324 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:05.073 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9424083769633508 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:05.953 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:06.084 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9223744292237442 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:06.105 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:06.954 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:06.958 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:07.086 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9117647058823529 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:07.894 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found com.google.code.findbugs#jsr305;3.0.0 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:08.093 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9805194805194806 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:09.121 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8719723183391004 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:09.976 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found commons-logging#commons-logging;1.1.3 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:09.977 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found commons-logging#commons-logging;1.1.3 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:10.127 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found commons-logging#commons-logging;1.1.3 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:10.162 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8615384615384615 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:10.983 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found com.google.code.findbugs#jsr305;3.0.0 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:10.983 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found com.google.code.findbugs#jsr305;3.0.0 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:11.130 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found com.google.code.findbugs#jsr305;3.0.0 in central
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:11.202 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9156976744186047 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:12.209 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9625287356321839 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:12.913 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.commons#commons-pool2;2.11.1 in central
	downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.jar ...
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:13.171 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.commons#commons-pool2;2.11.1 in central
	downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.jar ...
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:13.914 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.1/spark-token-provider-kafka-0-10_2.12-3.5.1.jar ...
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:13.993 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.1/spark-token-provider-kafka-0-10_2.12-3.5.1.jar ...
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:13.999 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.1/spark-token-provider-kafka-0-10_2.12-3.5.1.jar ...
		[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1!spark-token-provider-kafka-0-10_2.12.jar (460ms)
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:14.173 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1!spark-sql-kafka-0-10_2.12.jar (694ms)
	downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.1/spark-token-provider-kafka-0-10_2.12-3.5.1.jar ...
		[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1!spark-token-provider-kafka-0-10_2.12.jar (419ms)
	downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:14.915 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1!spark-token-provider-kafka-0-10_2.12.jar (436ms)
	downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:14.999 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:15.009 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:15.229 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8575498575498576 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:16.188 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar (1900ms)
	downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...
		[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (418ms)
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:16.262 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9308510638297871 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:16.933 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...
		[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (428ms)
	downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...
		[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (574ms)
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:17.215 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...
		[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (447ms)
	downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:17.270 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9268292682926829 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:17.934 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...
		[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (680ms)
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:18.014 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:18.029 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:18.298 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.975 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:18.953 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.3/snappy-java-1.1.10.3.jar ...
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:19.314 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9823535429311874 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:19.988 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.3!snappy-java.jar(bundle) (1708ms)
	downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...
		[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (522ms)
	:: resolution report :: resolve 21066ms :: artifacts dl 7084ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   9   |   6   |   4   |   0   ||   8   |   6   |
		---------------------------------------------------------------------
	
	:: problems summary ::
	:::: WARNINGS
		problem while downloading module descriptor: https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-project/3.3.4/hadoop-project-3.3.4.pom: /tmp/org.apache.hadoop/hadoop-project/ivy-3.3.4.xml.original (No such file or directory) (41ms)
	
		io problem while parsing ivy file: https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.pom (java.io.IOException: Impossible to load parent for file:/tmp/org.apache.hadoop/hadoop-client-runtime/ivy-3.3.4.xml.original. Parent=org.apache.hadoop#hadoop-project;3.3.4)
	
			module not found: org.apache.hadoop#hadoop-client-runtime;3.3.4
	
		==== local-m2-cache: tried
	
		  file:/home/default/.m2/repository/org/apache/hadoop/hadoop-project/3.3.4/hadoop-project-3.3.4.pom
	
		  -- artifact org.apache.hadoop#hadoop-project;3.3.4!hadoop-project.jar:
	
		  file:/home/default/.m2/repository/org/apache/hadoop/hadoop-project/3.3.4/hadoop-project-3.3.4.jar
	
		==== local-ivy-cache: tried
	
		  /tmp/local/org.apache.hadoop/hadoop-project/3.3.4/ivys/ivy.xml
	
		  -- artifact org.apache.hadoop#hadoop-project;3.3.4!hadoop-project.jar:
	
		  /tmp/local/org.apache.hadoop/hadoop-project/3.3.4/jars/hadoop-project.jar
	
		==== central: tried
	
		  https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-project/3.3.4/hadoop-project-3.3.4.pom
	
		==== spark-packages: tried
	
		  https://repos.spark-packages.org/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.pom
	
		  -- artifact org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar:
	
		  https://repos.spark-packages.org/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar
	
		impossible to put metadata file in cache: https://repo1.maven.org/maven2/org/apache/commons/commons-parent/52/commons-parent-52.pom (52) (java.io.FileNotFoundException: /tmp/org.apache.commons/commons-parent/ivy-52.xml.original (No such file or directory))
	
			[FAILED     ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1!spark-sql-kafka-0-10_2.12.jar: Downloaded file size (0) doesn't match expected Content Length (432340) for https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.jar. Please retry. (875ms)
	
			[FAILED     ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1!spark-sql-kafka-0-10_2.12.jar: Downloaded file size (0) doesn't match expected Content Length (432340) for https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.jar. Please retry. (875ms)
	
		==== central: tried
	
		  https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.jar
	
			[FAILED     ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar: Downloaded file size (0) doesn't match expected Content Length (5050443) for https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar. Please retry. (1758ms)
	
			[FAILED     ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar: Downloaded file size (0) doesn't match expected Content Length (5050443) for https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar. Please retry. (1758ms)
	
		==== central: tried
	
		  https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
			::          UNRESOLVED DEPENDENCIES         ::
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
			:: org.apache.hadoop#hadoop-client-runtime;3.3.4: not found
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
			::              FAILED DOWNLOADS            ::
	
			:: ^ see resolution messages for details  ^ ::
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
			:: org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1!spark-sql-kafka-0-10_2.12.jar
	
			:: org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
	
	
	:: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS
	Exception in thread "main" java.lang.RuntimeException: [unresolved dependency: org.apache.hadoop#hadoop-client-runtime;3.3.4: not found, download failed: org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1!spark-sql-kafka-0-10_2.12.jar, download failed: org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar]
		at org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:1608)
		at org.apache.spark.util.DependencyUtils$.resolveMavenDependencies(DependencyUtils.scala:185)
		at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:334)
		at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)
		at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
		at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
		at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
		at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
		at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
		at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/948/3369/py_948_3369.py", line 27, in <module>
	    .getOrCreate()
	     ^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py", line 497, in getOrCreate
	    sc = SparkContext.getOrCreate(sparkConf)
	         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 515, in getOrCreate
	    SparkContext(conf=conf or SparkConf())
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 201, in __init__
	    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 436, in _ensure_initialized
	    SparkContext._gateway = gateway or launch_gateway(conf)
	                                       ^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/java_gateway.py", line 107, in launch_gateway
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.
[WI-948][TI-3369] - [INFO] 2024-05-05 03:07:19.992 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/948/3369, processId:194 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-948][TI-3369] - [INFO] 2024-05-05 03:07:19.999 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-948][TI-3369] - [INFO] 2024-05-05 03:07:20.000 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-948][TI-3369] - [INFO] 2024-05-05 03:07:20.000 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-948][TI-3369] - [INFO] 2024-05-05 03:07:20.001 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-948][TI-3369] - [INFO] 2024-05-05 03:07:20.006 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-948][TI-3369] - [INFO] 2024-05-05 03:07:20.006 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-948][TI-3369] - [INFO] 2024-05-05 03:07:20.006 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/948/3369
[WI-948][TI-3369] - [INFO] 2024-05-05 03:07:20.007 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/948/3369
[WI-948][TI-3369] - [INFO] 2024-05-05 03:07:20.009 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:20.319 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8190404797601198 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3369] - [INFO] 2024-05-05 03:07:20.883 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3369, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:21.246 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (3698ms)
	downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...
		[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (424ms)
	:: resolution report :: resolve 26619ms :: artifacts dl 8045ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   9   |   9   |   0   ||   10  |   7   |
		---------------------------------------------------------------------
	
	:: problems summary ::
	:::: WARNINGS
		impossible to put metadata file in cache: https://repo1.maven.org/maven2/org/apache/spark/spark-parent_2.12/3.5.1/spark-parent_2.12-3.5.1.pom (3.5.1) (java.io.FileNotFoundException: /tmp/org.apache.spark/spark-parent_2.12/ivy-3.5.1.xml.original (No such file or directory))
	
		impossible to put metadata file in cache: https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.1/spark-token-provider-kafka-0-10_2.12-3.5.1.pom (3.5.1) (java.io.FileNotFoundException: /tmp/org.apache.spark/spark-token-provider-kafka-0-10_2.12/ivy-3.5.1.xml.original (No such file or directory))
	
		io problem while parsing ivy file: https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.pom (java.io.FileNotFoundException: /tmp/org.apache.hadoop/hadoop-client-api/ivy-3.3.4.xml.original (No such file or directory))
	
			module not found: org.apache.hadoop#hadoop-client-api;3.3.4
	
		==== local-m2-cache: tried
	
		  file:/home/default/.m2/repository/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.pom
	
		  -- artifact org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar:
	
		  file:/home/default/.m2/repository/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar
	
		==== local-ivy-cache: tried
	
		  /tmp/local/org.apache.hadoop/hadoop-client-api/3.3.4/ivys/ivy.xml
	
		  -- artifact org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar:
	
		  /tmp/local/org.apache.hadoop/hadoop-client-api/3.3.4/jars/hadoop-client-api.jar
	
		==== central: tried
	
		  https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.pom
	
		==== spark-packages: tried
	
		  https://repos.spark-packages.org/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.pom
	
		  -- artifact org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar:
	
		  https://repos.spark-packages.org/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
			::          UNRESOLVED DEPENDENCIES         ::
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
			:: org.apache.hadoop#hadoop-client-api;3.3.4: not found
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
	
	
	:: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS
	Exception in thread "main" java.lang.RuntimeException: [unresolved dependency: org.apache.hadoop#hadoop-client-api;3.3.4: not found]
		at org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:1608)
		at org.apache.spark.util.DependencyUtils$.resolveMavenDependencies(DependencyUtils.scala:185)
		at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:334)
		at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)
		at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
		at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
		at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
		at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
		at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
		at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/923/3371/py_923_3371.py", line 13, in <module>
	    .getOrCreate()
	     ^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py", line 497, in getOrCreate
	    sc = SparkContext.getOrCreate(sparkConf)
	         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 515, in getOrCreate
	    SparkContext(conf=conf or SparkConf())
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 201, in __init__
	    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 436, in _ensure_initialized
	    SparkContext._gateway = gateway or launch_gateway(conf)
	                                       ^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/java_gateway.py", line 107, in launch_gateway
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.
[WI-923][TI-3371] - [INFO] 2024-05-05 03:07:21.249 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/923/3371, processId:185 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-923][TI-3371] - [INFO] 2024-05-05 03:07:21.249 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-923][TI-3371] - [INFO] 2024-05-05 03:07:21.251 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-923][TI-3371] - [INFO] 2024-05-05 03:07:21.253 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-923][TI-3371] - [INFO] 2024-05-05 03:07:21.255 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-923][TI-3371] - [INFO] 2024-05-05 03:07:21.263 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-923][TI-3371] - [INFO] 2024-05-05 03:07:21.272 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-923][TI-3371] - [INFO] 2024-05-05 03:07:21.272 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/923/3371
[WI-923][TI-3371] - [INFO] 2024-05-05 03:07:21.273 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/923/3371
[WI-923][TI-3371] - [INFO] 2024-05-05 03:07:21.283 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3371] - [INFO] 2024-05-05 03:07:21.983 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3371, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:22.345 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8548387096774194 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:23.368 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8419618528610354 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:24.369 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7655786350148368 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:25.063 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:25.062 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:25.385 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7853107344632769 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:26.391 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.849112426035503 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:27.069 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (2853ms)
	:: resolution report :: resolve 23496ms :: artifacts dl 13507ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   6   |   6   |   0   ||   10  |   2   |
		---------------------------------------------------------------------
	
	:: problems summary ::
	:::: WARNINGS
		impossible to put metadata file in cache: https://repo1.maven.org/maven2/org/apache/spark/spark-parent_2.12/3.5.1/spark-parent_2.12-3.5.1.pom (3.5.1) (java.io.FileNotFoundException: /tmp/org.apache.spark/spark-parent_2.12/ivy-3.5.1.xml.original (No such file or directory))
	
		problem while downloading module descriptor: https://repo1.maven.org/maven2/org/apache/commons/commons-parent/52/commons-parent-52.pom: Downloaded file size (0) doesn't match expected Content Length (79273) for https://repo1.maven.org/maven2/org/apache/commons/commons-parent/52/commons-parent-52.pom. Please retry. (301ms)
	
		io problem while parsing ivy file: https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.pom (java.io.IOException: Impossible to load parent for file:/tmp/org.apache.commons/commons-pool2/ivy-2.11.1.xml.original. Parent=org.apache.commons#commons-parent;52)
	
			module not found: org.apache.commons#commons-pool2;2.11.1
	
		==== local-m2-cache: tried
	
		  file:/home/default/.m2/repository/org/apache/commons/commons-parent/52/commons-parent-52.pom
	
		  -- artifact org.apache.commons#commons-parent;52!commons-parent.jar:
	
		  file:/home/default/.m2/repository/org/apache/commons/commons-parent/52/commons-parent-52.jar
	
		==== local-ivy-cache: tried
	
		  /tmp/local/org.apache.commons/commons-parent/52/ivys/ivy.xml
	
		  -- artifact org.apache.commons#commons-parent;52!commons-parent.jar:
	
		  /tmp/local/org.apache.commons/commons-parent/52/jars/commons-parent.jar
	
		==== central: tried
	
		  https://repo1.maven.org/maven2/org/apache/commons/commons-parent/52/commons-parent-52.pom
	
		==== spark-packages: tried
	
		  https://repos.spark-packages.org/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.pom
	
		  -- artifact org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar:
	
		  https://repos.spark-packages.org/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar
	
			[FAILED     ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar: Downloaded file size (0) doesn't match expected Content Length (5050443) for https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar. Please retry. (2899ms)
	
			[FAILED     ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar: Downloaded file size (0) doesn't match expected Content Length (5050443) for https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar. Please retry. (2899ms)
	
		==== central: tried
	
		  https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar
	
			[FAILED     ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar: Downloaded file size (0) doesn't match expected Content Length (30085504) for https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar. Please retry. (7269ms)
	
			[FAILED     ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar: Downloaded file size (0) doesn't match expected Content Length (30085504) for https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar. Please retry. (7269ms)
	
		==== central: tried
	
		  https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
			::          UNRESOLVED DEPENDENCIES         ::
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
			:: org.apache.commons#commons-pool2;2.11.1: not found
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
			::              FAILED DOWNLOADS            ::
	
			:: ^ see resolution messages for details  ^ ::
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
			:: org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar
	
			:: org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
	
	
	:: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS
	Exception in thread "main" java.lang.RuntimeException: [unresolved dependency: org.apache.commons#commons-pool2;2.11.1: not found, download failed: org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar, download failed: org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar]
		at org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:1608)
		at org.apache.spark.util.DependencyUtils$.resolveMavenDependencies(DependencyUtils.scala:185)
		at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:334)
		at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)
		at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
		at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
		at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
		at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
		at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
		at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:27.073 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: resolution report :: resolve 23033ms :: artifacts dl 13507ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   5   |   5   |   0   ||   10  |   0   |
		---------------------------------------------------------------------
	
	:: problems summary ::
	:::: WARNINGS
		impossible to put metadata file in cache: https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.pom (3.3.4) (java.io.FileNotFoundException: /tmp/org.apache.hadoop/hadoop-client-api/ivy-3.3.4.xml.original (No such file or directory))
	
		problem while downloading module descriptor: https://repo1.maven.org/maven2/org/apache/commons/commons-parent/52/commons-parent-52.pom: Downloaded file size (0) doesn't match expected Content Length (79273) for https://repo1.maven.org/maven2/org/apache/commons/commons-parent/52/commons-parent-52.pom. Please retry. (266ms)
	
		io problem while parsing ivy file: https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.pom (java.io.IOException: Impossible to load parent for file:/tmp/org.apache.commons/commons-pool2/ivy-2.11.1.xml.original. Parent=org.apache.commons#commons-parent;52)
	
			module not found: org.apache.commons#commons-pool2;2.11.1
	
		==== local-m2-cache: tried
	
		  file:/home/default/.m2/repository/org/apache/commons/commons-parent/52/commons-parent-52.pom
	
		  -- artifact org.apache.commons#commons-parent;52!commons-parent.jar:
	
		  file:/home/default/.m2/repository/org/apache/commons/commons-parent/52/commons-parent-52.jar
	
		==== local-ivy-cache: tried
	
		  /tmp/local/org.apache.commons/commons-parent/52/ivys/ivy.xml
	
		  -- artifact org.apache.commons#commons-parent;52!commons-parent.jar:
	
		  /tmp/local/org.apache.commons/commons-parent/52/jars/commons-parent.jar
	
		==== central: tried
	
		  https://repo1.maven.org/maven2/org/apache/commons/commons-parent/52/commons-parent-52.pom
	
		==== spark-packages: tried
	
		  https://repos.spark-packages.org/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.pom
	
		  -- artifact org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar:
	
		  https://repos.spark-packages.org/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar
	
			[FAILED     ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1!spark-token-provider-kafka-0-10_2.12.jar: Downloaded file size (0) doesn't match expected Content Length (56810) for https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.1/spark-token-provider-kafka-0-10_2.12-3.5.1.jar. Please retry. (461ms)
	
			[FAILED     ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1!spark-token-provider-kafka-0-10_2.12.jar: Downloaded file size (0) doesn't match expected Content Length (56810) for https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.1/spark-token-provider-kafka-0-10_2.12-3.5.1.jar. Please retry. (461ms)
	
		==== central: tried
	
		  https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.1/spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	
			[FAILED     ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar: Downloaded file size (0) doesn't match expected Content Length (5050443) for https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar. Please retry. (2871ms)
	
			[FAILED     ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar: Downloaded file size (0) doesn't match expected Content Length (5050443) for https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar. Please retry. (2871ms)
	
		==== central: tried
	
		  https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar
	
			[FAILED     ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar: Downloaded file size (0) doesn't match expected Content Length (30085504) for https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar. Please retry. (7252ms)
	
			[FAILED     ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar: Downloaded file size (0) doesn't match expected Content Length (30085504) for https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar. Please retry. (7252ms)
	
		==== central: tried
	
		  https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar
	
			[FAILED     ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar: Downloaded file size (0) doesn't match expected Content Length (19458635) for https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar. Please retry. (2848ms)
	
			[FAILED     ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar: Downloaded file size (0) doesn't match expected Content Length (19458635) for https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar. Please retry. (2848ms)
	
		==== central: tried
	
		  https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
			::          UNRESOLVED DEPENDENCIES         ::
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
			:: org.apache.commons#commons-pool2;2.11.1: not found
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
			::              FAILED DOWNLOADS            ::
	
			:: ^ see resolution messages for details  ^ ::
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
			:: org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1!spark-token-provider-kafka-0-10_2.12.jar
	
			:: org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar
	
			:: org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar
	
			:: org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar
	
			::::::::::::::::::::::::::::::::::::::::::::::
	
	
	
	:: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS
	Exception in thread "main" java.lang.RuntimeException: [unresolved dependency: org.apache.commons#commons-pool2;2.11.1: not found, download failed: org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1!spark-token-provider-kafka-0-10_2.12.jar, download failed: org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar, download failed: org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar, download failed: org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar]
		at org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:1608)
		at org.apache.spark.util.DependencyUtils$.resolveMavenDependencies(DependencyUtils.scala:185)
		at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:334)
		at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)
		at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
		at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
		at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
		at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
		at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
		at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/928/3368/py_928_3368.py", line 27, in <module>
	    .getOrCreate()
	     ^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py", line 497, in getOrCreate
	    sc = SparkContext.getOrCreate(sparkConf)
	         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 515, in getOrCreate
	    SparkContext(conf=conf or SparkConf())
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 201, in __init__
	    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 436, in _ensure_initialized
	    SparkContext._gateway = gateway or launch_gateway(conf)
	                                       ^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/java_gateway.py", line 107, in launch_gateway
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:27.397 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9404388714733543 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-928][TI-3368] - [INFO] 2024-05-05 03:07:28.078 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/928/3368, processId:183 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-928][TI-3368] - [INFO] 2024-05-05 03:07:28.079 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-928][TI-3368] - [INFO] 2024-05-05 03:07:28.079 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-928][TI-3368] - [INFO] 2024-05-05 03:07:28.079 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-928][TI-3368] - [INFO] 2024-05-05 03:07:28.079 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:28.085 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/909/3367/py_909_3367.py", line 27, in <module>
	    .getOrCreate()
	     ^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py", line 497, in getOrCreate
	    sc = SparkContext.getOrCreate(sparkConf)
	         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 515, in getOrCreate
	    SparkContext(conf=conf or SparkConf())
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 201, in __init__
	    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 436, in _ensure_initialized
	    SparkContext._gateway = gateway or launch_gateway(conf)
	                                       ^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/java_gateway.py", line 107, in launch_gateway
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.
[WI-928][TI-3368] - [INFO] 2024-05-05 03:07:28.086 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-909][TI-3367] - [INFO] 2024-05-05 03:07:28.088 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/909/3367, processId:189 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-909][TI-3367] - [INFO] 2024-05-05 03:07:28.088 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-909][TI-3367] - [INFO] 2024-05-05 03:07:28.088 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-909][TI-3367] - [INFO] 2024-05-05 03:07:28.088 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-928][TI-3368] - [INFO] 2024-05-05 03:07:28.088 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-928][TI-3368] - [INFO] 2024-05-05 03:07:28.089 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/928/3368
[WI-909][TI-3367] - [INFO] 2024-05-05 03:07:28.090 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-928][TI-3368] - [INFO] 2024-05-05 03:07:28.090 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/928/3368
[WI-928][TI-3368] - [INFO] 2024-05-05 03:07:28.090 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-909][TI-3367] - [INFO] 2024-05-05 03:07:28.096 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-909][TI-3367] - [INFO] 2024-05-05 03:07:28.096 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-909][TI-3367] - [INFO] 2024-05-05 03:07:28.096 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/909/3367
[WI-909][TI-3367] - [INFO] 2024-05-05 03:07:28.097 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/909/3367
[WI-909][TI-3367] - [INFO] 2024-05-05 03:07:28.097 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3367] - [INFO] 2024-05-05 03:07:28.933 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3367, success=true)
[WI-0][TI-3368] - [INFO] 2024-05-05 03:07:28.933 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3368, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:40.490 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7290424985605577 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:41.583 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3402, taskName=extract_gnews_article, firstSubmitTime=1714849661574, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13471700679104, processDefineVersion=3, appIds=null, processInstanceId=952, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_article'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3402'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13471571877568'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505030741'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='952'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_articles_main'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13471700679104'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.593 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract_gnews_article to wait queue success
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.597 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.631 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.631 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.631 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.632 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714849661632
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.632 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 952_3402
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.635 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3402,
  "taskName" : "extract_gnews_article",
  "firstSubmitTime" : 1714849661574,
  "startTime" : 1714849661632,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13471700679104/3/952/3402.log",
  "processId" : 0,
  "processDefineCode" : 13471700679104,
  "processDefineVersion" : 3,
  "processInstanceId" : 952,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"from gnews import GNews\\nimport json\\n\\n\\n# Initialize GNews client\\ngoogle_news = GNews()\\n\\n# Fetch Google News articles for a specific topic, which is singapore related\\nnews_results = google_news.get_news(topic)[:5]\\n\\n# List to store the relevant information about the news articles\\nnews_articles = []\\n\\n# Process each news article\\nfor news in news_results:\\n    # Extract basic information from Google News\\n    title = news.get('title')\\n    datetime = news.get('published date')\\n    url = news.get('url')\\n    description = news.get('description')\\n    \\n    # Extract publisher information\\n    publisher_info = news.get('publisher', {})\\n    publisher_href = publisher_info.get('href')\\n    \\n    # Initialize and handle exceptions in article parsing\\n    try:\\n        article = google_news.get_full_article(url)\\n\\n        # If parsing is successful, extract authors and text\\n        authors = article.authors if article.authors else \\\"Unknown Author\\\"\\n        context = article.text\\n    \\n    except Exception as e:\\n        print(f\\\"Failed to download article: {e}\\\")  # Log the error message\\n        continue\\n    \\n    # store the article data in a dictionary\\n    article_data = {\\n        'title': title,\\n        'datetime': datetime,\\n        'description': description,\\n        'url': url,\\n        'context': context,\\n        'authors': authors,\\n        'publisher_href': publisher_href,\\n    }\\n\\n\\n    # store the article data into the article_data list for JSON serialization\\n    news_articles.append(article_data)\\n\\n\\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\\nnews_articles_string = '|'.join(news_articles)\\n\\n# save the news_articles_string dictionary into a parameter to pass it downstream\\n\\nprint(\\\"#setValue(news_articles_string=%s)\\\" % news_articles_string)\\n\\nprint(news_articles_string) \",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_article"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3402"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471571877568"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505030741"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "952"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_articles_main"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471700679104"
    }
  },
  "taskAppId" : "952_3402",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.637 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.638 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.638 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.665 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.666 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.669 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3402 check successfully
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.669 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.670 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.671 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.673 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.675 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "news_articles_string",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ",
  "resourceList" : [ ]
}
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.676 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.676 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.677 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.677 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.677 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.678 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.679 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3402
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.680 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3402/py_952_3402.py
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.680 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.681 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.682 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.682 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3402/py_952_3402.py
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.683 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.684 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3402/952_3402.sh
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:41.708 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 521
[WI-0][TI-3402] - [INFO] 2024-05-05 03:07:42.070 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3402, success=true)
[WI-0][TI-3402] - [INFO] 2024-05-05 03:07:42.097 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3402)
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:42.541 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.93125 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:42.713 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3402/py_952_3402.py", line 11, in <module>
	    news_results = google_news.get_news(topic)[:5]
	                                        ^^^^^
	NameError: name 'topic' is not defined
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:42.724 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3402, processId:521 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:42.725 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:42.725 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:42.725 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:42.725 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:42.731 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:42.731 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:42.731 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3402
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:42.749 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3402
[WI-952][TI-3402] - [INFO] 2024-05-05 03:07:42.750 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3402] - [INFO] 2024-05-05 03:07:43.048 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3402, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:43.173 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3403, taskName=extract_gnews_article, firstSubmitTime=1714849663125, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13471700679104, processDefineVersion=3, appIds=null, processInstanceId=952, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_article'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3403'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13471571877568'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505030743'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='952'}, news_articles_string=Property{prop='news_articles_string', direct=OUT, type=VARCHAR, value=''}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_articles_main'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13471700679104'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.174 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract_gnews_article to wait queue success
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.175 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.176 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.177 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.177 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.177 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714849663177
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.177 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 952_3403
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.178 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3403,
  "taskName" : "extract_gnews_article",
  "firstSubmitTime" : 1714849663125,
  "startTime" : 1714849663177,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13471700679104/3/952/3403.log",
  "processId" : 0,
  "processDefineCode" : 13471700679104,
  "processDefineVersion" : 3,
  "processInstanceId" : 952,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"from gnews import GNews\\nimport json\\n\\n\\n# Initialize GNews client\\ngoogle_news = GNews()\\n\\n# Fetch Google News articles for a specific topic, which is singapore related\\nnews_results = google_news.get_news(topic)[:5]\\n\\n# List to store the relevant information about the news articles\\nnews_articles = []\\n\\n# Process each news article\\nfor news in news_results:\\n    # Extract basic information from Google News\\n    title = news.get('title')\\n    datetime = news.get('published date')\\n    url = news.get('url')\\n    description = news.get('description')\\n    \\n    # Extract publisher information\\n    publisher_info = news.get('publisher', {})\\n    publisher_href = publisher_info.get('href')\\n    \\n    # Initialize and handle exceptions in article parsing\\n    try:\\n        article = google_news.get_full_article(url)\\n\\n        # If parsing is successful, extract authors and text\\n        authors = article.authors if article.authors else \\\"Unknown Author\\\"\\n        context = article.text\\n    \\n    except Exception as e:\\n        print(f\\\"Failed to download article: {e}\\\")  # Log the error message\\n        continue\\n    \\n    # store the article data in a dictionary\\n    article_data = {\\n        'title': title,\\n        'datetime': datetime,\\n        'description': description,\\n        'url': url,\\n        'context': context,\\n        'authors': authors,\\n        'publisher_href': publisher_href,\\n    }\\n\\n\\n    # store the article data into the article_data list for JSON serialization\\n    news_articles.append(article_data)\\n\\n\\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\\nnews_articles_string = '|'.join(news_articles)\\n\\n# save the news_articles_string dictionary into a parameter to pass it downstream\\n\\nprint(\\\"#setValue(news_articles_string=%s)\\\" % news_articles_string)\\n\\nprint(news_articles_string) \",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_article"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3403"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471571877568"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505030743"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "952"
    },
    "news_articles_string" : {
      "prop" : "news_articles_string",
      "direct" : "OUT",
      "type" : "VARCHAR",
      "value" : ""
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_articles_main"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471700679104"
    }
  },
  "taskAppId" : "952_3403",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.178 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.179 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.179 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.188 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.188 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.189 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3403 check successfully
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.189 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.190 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.190 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.190 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.191 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "news_articles_string",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ",
  "resourceList" : [ ]
}
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.191 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.191 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}] successfully
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.191 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.191 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.192 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.192 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.192 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3403
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.192 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3403/py_952_3403.py
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.193 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.195 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.195 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.196 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3403/py_952_3403.py
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.196 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.196 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3403/952_3403.sh
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:43.201 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 532
[WI-0][TI-3403] - [INFO] 2024-05-05 03:07:44.067 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3403, success=true)
[WI-0][TI-3403] - [INFO] 2024-05-05 03:07:44.083 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3403)
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:44.201 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3403/py_952_3403.py", line 11, in <module>
	    news_results = google_news.get_news(topic)[:5]
	                                        ^^^^^
	NameError: name 'topic' is not defined
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:44.204 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3403, processId:532 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:44.207 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:44.207 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:44.207 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:44.208 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:44.224 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:44.225 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:44.225 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3403
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:44.228 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3403
[WI-952][TI-3403] - [INFO] 2024-05-05 03:07:44.228 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3403] - [INFO] 2024-05-05 03:07:45.057 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3403, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:45.217 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3404, taskName=extract_gnews_article, firstSubmitTime=1714849665168, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13471700679104, processDefineVersion=3, appIds=null, processInstanceId=952, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_article'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3404'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13471571877568'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505030745'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='952'}, news_articles_string=Property{prop='news_articles_string', direct=OUT, type=VARCHAR, value=''}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_articles_main'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13471700679104'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.224 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract_gnews_article to wait queue success
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.224 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.226 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.226 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.226 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.228 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714849665228
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.228 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 952_3404
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.229 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3404,
  "taskName" : "extract_gnews_article",
  "firstSubmitTime" : 1714849665168,
  "startTime" : 1714849665228,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13471700679104/3/952/3404.log",
  "processId" : 0,
  "processDefineCode" : 13471700679104,
  "processDefineVersion" : 3,
  "processInstanceId" : 952,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"from gnews import GNews\\nimport json\\n\\n\\n# Initialize GNews client\\ngoogle_news = GNews()\\n\\n# Fetch Google News articles for a specific topic, which is singapore related\\nnews_results = google_news.get_news(topic)[:5]\\n\\n# List to store the relevant information about the news articles\\nnews_articles = []\\n\\n# Process each news article\\nfor news in news_results:\\n    # Extract basic information from Google News\\n    title = news.get('title')\\n    datetime = news.get('published date')\\n    url = news.get('url')\\n    description = news.get('description')\\n    \\n    # Extract publisher information\\n    publisher_info = news.get('publisher', {})\\n    publisher_href = publisher_info.get('href')\\n    \\n    # Initialize and handle exceptions in article parsing\\n    try:\\n        article = google_news.get_full_article(url)\\n\\n        # If parsing is successful, extract authors and text\\n        authors = article.authors if article.authors else \\\"Unknown Author\\\"\\n        context = article.text\\n    \\n    except Exception as e:\\n        print(f\\\"Failed to download article: {e}\\\")  # Log the error message\\n        continue\\n    \\n    # store the article data in a dictionary\\n    article_data = {\\n        'title': title,\\n        'datetime': datetime,\\n        'description': description,\\n        'url': url,\\n        'context': context,\\n        'authors': authors,\\n        'publisher_href': publisher_href,\\n    }\\n\\n\\n    # store the article data into the article_data list for JSON serialization\\n    news_articles.append(article_data)\\n\\n\\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\\nnews_articles_string = '|'.join(news_articles)\\n\\n# save the news_articles_string dictionary into a parameter to pass it downstream\\n\\nprint(\\\"#setValue(news_articles_string=%s)\\\" % news_articles_string)\\n\\nprint(news_articles_string) \",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_article"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3404"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471571877568"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505030745"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "952"
    },
    "news_articles_string" : {
      "prop" : "news_articles_string",
      "direct" : "OUT",
      "type" : "VARCHAR",
      "value" : ""
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_articles_main"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471700679104"
    }
  },
  "taskAppId" : "952_3404",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.235 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.235 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.235 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.244 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.245 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.245 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3404 check successfully
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.245 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.246 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.246 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.247 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.247 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "news_articles_string",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ",
  "resourceList" : [ ]
}
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.248 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.248 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}] successfully
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.248 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.248 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.248 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.248 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.249 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3404
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.249 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3404/py_952_3404.py
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.249 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.255 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.256 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.256 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3404/py_952_3404.py
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.256 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.257 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3404/952_3404.sh
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:45.279 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 543
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:45.637 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7264957264957264 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3404] - [INFO] 2024-05-05 03:07:46.071 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3404, success=true)
[WI-0][TI-3404] - [INFO] 2024-05-05 03:07:46.086 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3404)
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:46.284 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3404/py_952_3404.py", line 11, in <module>
	    news_results = google_news.get_news(topic)[:5]
	                                        ^^^^^
	NameError: name 'topic' is not defined
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:46.712 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8542857142857142 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:47.286 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3404, processId:543 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:47.287 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:47.287 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:47.288 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:47.292 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:47.296 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:47.297 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:47.297 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3404
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:47.298 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3404
[WI-952][TI-3404] - [INFO] 2024-05-05 03:07:47.298 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:47.714 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8225352112676056 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3404] - [INFO] 2024-05-05 03:07:48.103 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3404, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:55.753 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8213256484149857 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:07:56.846 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7393617021276595 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:08:05.891 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7847025495750708 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:08:11.937 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7721893491124261 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:08:13.035 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7858942065491183 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:08:16.840 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3406, taskName=extract_gnews_article, firstSubmitTime=1714849696826, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13471700679104, processDefineVersion=3, appIds=null, processInstanceId=952, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_article'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3406'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13471571877568'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505030816'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='952'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_articles_main'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13471700679104'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.847 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract_gnews_article to wait queue success
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.854 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.885 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.885 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.885 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.886 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714849696885
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.886 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 952_3406
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.886 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3406,
  "taskName" : "extract_gnews_article",
  "firstSubmitTime" : 1714849696826,
  "startTime" : 1714849696885,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13471700679104/3/952/3406.log",
  "processId" : 0,
  "processDefineCode" : 13471700679104,
  "processDefineVersion" : 3,
  "processInstanceId" : 952,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"from gnews import GNews\\nimport json\\n\\n\\n# Initialize GNews client\\ngoogle_news = GNews()\\n\\n# Fetch Google News articles for a specific topic, which is singapore related\\nnews_results = google_news.get_news(topic)[:5]\\n\\n# List to store the relevant information about the news articles\\nnews_articles = []\\n\\n# Process each news article\\nfor news in news_results:\\n    # Extract basic information from Google News\\n    title = news.get('title')\\n    datetime = news.get('published date')\\n    url = news.get('url')\\n    description = news.get('description')\\n    \\n    # Extract publisher information\\n    publisher_info = news.get('publisher', {})\\n    publisher_href = publisher_info.get('href')\\n    \\n    # Initialize and handle exceptions in article parsing\\n    try:\\n        article = google_news.get_full_article(url)\\n\\n        # If parsing is successful, extract authors and text\\n        authors = article.authors if article.authors else \\\"Unknown Author\\\"\\n        context = article.text\\n    \\n    except Exception as e:\\n        print(f\\\"Failed to download article: {e}\\\")  # Log the error message\\n        continue\\n    \\n    # store the article data in a dictionary\\n    article_data = {\\n        'title': title,\\n        'datetime': datetime,\\n        'description': description,\\n        'url': url,\\n        'context': context,\\n        'authors': authors,\\n        'publisher_href': publisher_href,\\n    }\\n\\n\\n    # store the article data into the article_data list for JSON serialization\\n    news_articles.append(article_data)\\n\\n\\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\\nnews_articles_string = '|'.join(news_articles)\\n\\n# save the news_articles_string dictionary into a parameter to pass it downstream\\n\\nprint(\\\"#setValue(news_articles_string=%s)\\\" % news_articles_string)\\n\\nprint(news_articles_string) \",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_article"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3406"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471571877568"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505030816"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "952"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_articles_main"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471700679104"
    }
  },
  "taskAppId" : "952_3406",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.887 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.887 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.887 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.898 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.899 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.899 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3406 check successfully
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.899 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.900 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.901 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.901 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.902 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "news_articles_string",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ",
  "resourceList" : [ ]
}
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.902 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.902 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.902 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.902 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.903 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.903 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.903 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3406
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.904 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3406/py_952_3406.py
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.904 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.904 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.905 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.905 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3406/py_952_3406.py
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.905 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.905 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3406/952_3406.sh
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:16.916 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 560
[WI-0][TI-3406] - [INFO] 2024-05-05 03:08:17.176 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3406, success=true)
[WI-0][TI-3406] - [INFO] 2024-05-05 03:08:17.236 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3406)
[WI-0][TI-0] - [INFO] 2024-05-05 03:08:17.920 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3406/py_952_3406.py", line 11, in <module>
	    news_results = google_news.get_news(topic)[:5]
	                                        ^^^^^
	NameError: name 'topic' is not defined
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:17.937 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3406, processId:560 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:17.940 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:17.941 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:17.941 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:17.943 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:17.961 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:17.962 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:17.962 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3406
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:17.965 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3406
[WI-952][TI-3406] - [INFO] 2024-05-05 03:08:17.965 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-05-05 03:08:18.066 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9317507418397627 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3406] - [INFO] 2024-05-05 03:08:18.181 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3406, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 03:08:18.320 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3407, taskName=extract_gnews_article, firstSubmitTime=1714849698274, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13471700679104, processDefineVersion=3, appIds=null, processInstanceId=952, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_article'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3407'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13471571877568'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505030818'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='952'}, news_articles_string=Property{prop='news_articles_string', direct=OUT, type=VARCHAR, value=''}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_articles_main'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13471700679104'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.326 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract_gnews_article to wait queue success
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.334 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.342 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.342 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.342 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.343 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714849698343
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.343 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 952_3407
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.390 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3407,
  "taskName" : "extract_gnews_article",
  "firstSubmitTime" : 1714849698274,
  "startTime" : 1714849698343,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13471700679104/3/952/3407.log",
  "processId" : 0,
  "processDefineCode" : 13471700679104,
  "processDefineVersion" : 3,
  "processInstanceId" : 952,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"from gnews import GNews\\nimport json\\n\\n\\n# Initialize GNews client\\ngoogle_news = GNews()\\n\\n# Fetch Google News articles for a specific topic, which is singapore related\\nnews_results = google_news.get_news(topic)[:5]\\n\\n# List to store the relevant information about the news articles\\nnews_articles = []\\n\\n# Process each news article\\nfor news in news_results:\\n    # Extract basic information from Google News\\n    title = news.get('title')\\n    datetime = news.get('published date')\\n    url = news.get('url')\\n    description = news.get('description')\\n    \\n    # Extract publisher information\\n    publisher_info = news.get('publisher', {})\\n    publisher_href = publisher_info.get('href')\\n    \\n    # Initialize and handle exceptions in article parsing\\n    try:\\n        article = google_news.get_full_article(url)\\n\\n        # If parsing is successful, extract authors and text\\n        authors = article.authors if article.authors else \\\"Unknown Author\\\"\\n        context = article.text\\n    \\n    except Exception as e:\\n        print(f\\\"Failed to download article: {e}\\\")  # Log the error message\\n        continue\\n    \\n    # store the article data in a dictionary\\n    article_data = {\\n        'title': title,\\n        'datetime': datetime,\\n        'description': description,\\n        'url': url,\\n        'context': context,\\n        'authors': authors,\\n        'publisher_href': publisher_href,\\n    }\\n\\n\\n    # store the article data into the article_data list for JSON serialization\\n    news_articles.append(article_data)\\n\\n\\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\\nnews_articles_string = '|'.join(news_articles)\\n\\n# save the news_articles_string dictionary into a parameter to pass it downstream\\n\\nprint(\\\"#setValue(news_articles_string=%s)\\\" % news_articles_string)\\n\\nprint(news_articles_string) \",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_article"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3407"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471571877568"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505030818"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "952"
    },
    "news_articles_string" : {
      "prop" : "news_articles_string",
      "direct" : "OUT",
      "type" : "VARCHAR",
      "value" : ""
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_articles_main"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471700679104"
    }
  },
  "taskAppId" : "952_3407",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.403 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.404 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.404 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.427 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.428 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.429 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3407 check successfully
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.429 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.429 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.430 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.431 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.431 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "news_articles_string",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ",
  "resourceList" : [ ]
}
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.432 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.433 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}] successfully
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.433 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.433 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.433 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.434 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.434 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3407
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.435 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3407/py_952_3407.py
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.435 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.437 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.437 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.437 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3407/py_952_3407.py
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.437 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.437 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3407/952_3407.sh
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:18.441 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 577
[WI-0][TI-0] - [INFO] 2024-05-05 03:08:19.098 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9555555555555555 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3407] - [INFO] 2024-05-05 03:08:19.239 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3407, success=true)
[WI-0][TI-3407] - [INFO] 2024-05-05 03:08:19.262 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3407)
[WI-0][TI-0] - [INFO] 2024-05-05 03:08:19.442 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3407/py_952_3407.py", line 11, in <module>
	    news_results = google_news.get_news(topic)[:5]
	                                        ^^^^^
	NameError: name 'topic' is not defined
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:19.445 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3407, processId:577 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:19.446 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:19.446 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:19.446 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:19.447 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:19.469 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:19.480 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:19.480 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3407
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:19.481 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3407
[WI-952][TI-3407] - [INFO] 2024-05-05 03:08:19.481 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3407] - [INFO] 2024-05-05 03:08:20.160 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3407, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 03:08:20.230 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3408, taskName=extract_gnews_article, firstSubmitTime=1714849700215, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13471700679104, processDefineVersion=3, appIds=null, processInstanceId=952, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_article'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3408'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13471571877568'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505030820'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='952'}, news_articles_string=Property{prop='news_articles_string', direct=OUT, type=VARCHAR, value=''}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_articles_main'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13471700679104'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.233 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract_gnews_article to wait queue success
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.233 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.235 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.241 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.241 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.241 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714849700241
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.242 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 952_3408
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.243 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3408,
  "taskName" : "extract_gnews_article",
  "firstSubmitTime" : 1714849700215,
  "startTime" : 1714849700241,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13471700679104/3/952/3408.log",
  "processId" : 0,
  "processDefineCode" : 13471700679104,
  "processDefineVersion" : 3,
  "processInstanceId" : 952,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"from gnews import GNews\\nimport json\\n\\n\\n# Initialize GNews client\\ngoogle_news = GNews()\\n\\n# Fetch Google News articles for a specific topic, which is singapore related\\nnews_results = google_news.get_news(topic)[:5]\\n\\n# List to store the relevant information about the news articles\\nnews_articles = []\\n\\n# Process each news article\\nfor news in news_results:\\n    # Extract basic information from Google News\\n    title = news.get('title')\\n    datetime = news.get('published date')\\n    url = news.get('url')\\n    description = news.get('description')\\n    \\n    # Extract publisher information\\n    publisher_info = news.get('publisher', {})\\n    publisher_href = publisher_info.get('href')\\n    \\n    # Initialize and handle exceptions in article parsing\\n    try:\\n        article = google_news.get_full_article(url)\\n\\n        # If parsing is successful, extract authors and text\\n        authors = article.authors if article.authors else \\\"Unknown Author\\\"\\n        context = article.text\\n    \\n    except Exception as e:\\n        print(f\\\"Failed to download article: {e}\\\")  # Log the error message\\n        continue\\n    \\n    # store the article data in a dictionary\\n    article_data = {\\n        'title': title,\\n        'datetime': datetime,\\n        'description': description,\\n        'url': url,\\n        'context': context,\\n        'authors': authors,\\n        'publisher_href': publisher_href,\\n    }\\n\\n\\n    # store the article data into the article_data list for JSON serialization\\n    news_articles.append(article_data)\\n\\n\\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\\nnews_articles_string = '|'.join(news_articles)\\n\\n# save the news_articles_string dictionary into a parameter to pass it downstream\\n\\nprint(\\\"#setValue(news_articles_string=%s)\\\" % news_articles_string)\\n\\nprint(news_articles_string) \",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_article"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3408"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471571877568"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505030820"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "952"
    },
    "news_articles_string" : {
      "prop" : "news_articles_string",
      "direct" : "OUT",
      "type" : "VARCHAR",
      "value" : ""
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_articles_main"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471700679104"
    }
  },
  "taskAppId" : "952_3408",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.246 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.246 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.246 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.277 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.278 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.278 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3408 check successfully
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.278 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.279 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.280 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.280 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.281 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "news_articles_string",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ",
  "resourceList" : [ ]
}
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.282 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.283 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}] successfully
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.283 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.284 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.284 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.284 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.286 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3408
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.286 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3408/py_952_3408.py
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.286 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.287 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.287 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.288 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3408/py_952_3408.py
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.288 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.289 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3408/952_3408.sh
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:20.293 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 588
[WI-0][TI-3408] - [INFO] 2024-05-05 03:08:21.175 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3408, success=true)
[WI-0][TI-3408] - [INFO] 2024-05-05 03:08:21.187 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3408)
[WI-0][TI-0] - [INFO] 2024-05-05 03:08:21.294 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3408/py_952_3408.py", line 11, in <module>
	    news_results = google_news.get_news(topic)[:5]
	                                        ^^^^^
	NameError: name 'topic' is not defined
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:21.305 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3408, processId:588 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:21.318 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:21.321 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:21.321 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:21.321 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:21.342 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:21.342 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:21.343 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3408
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:21.344 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3408
[WI-952][TI-3408] - [INFO] 2024-05-05 03:08:21.344 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3408] - [INFO] 2024-05-05 03:08:22.187 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3408, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 03:08:22.280 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3409, taskName=extract_gnews_article, firstSubmitTime=1714849702231, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13471700679104, processDefineVersion=3, appIds=null, processInstanceId=952, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_article'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3409'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13471571877568'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505030822'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='952'}, news_articles_string=Property{prop='news_articles_string', direct=OUT, type=VARCHAR, value=''}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_articles_main'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13471700679104'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.288 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract_gnews_article to wait queue success
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.288 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.301 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.302 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.302 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.302 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714849702302
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.302 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 952_3409
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.302 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3409,
  "taskName" : "extract_gnews_article",
  "firstSubmitTime" : 1714849702231,
  "startTime" : 1714849702302,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13471700679104/3/952/3409.log",
  "processId" : 0,
  "processDefineCode" : 13471700679104,
  "processDefineVersion" : 3,
  "processInstanceId" : 952,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"from gnews import GNews\\nimport json\\n\\n\\n# Initialize GNews client\\ngoogle_news = GNews()\\n\\n# Fetch Google News articles for a specific topic, which is singapore related\\nnews_results = google_news.get_news(topic)[:5]\\n\\n# List to store the relevant information about the news articles\\nnews_articles = []\\n\\n# Process each news article\\nfor news in news_results:\\n    # Extract basic information from Google News\\n    title = news.get('title')\\n    datetime = news.get('published date')\\n    url = news.get('url')\\n    description = news.get('description')\\n    \\n    # Extract publisher information\\n    publisher_info = news.get('publisher', {})\\n    publisher_href = publisher_info.get('href')\\n    \\n    # Initialize and handle exceptions in article parsing\\n    try:\\n        article = google_news.get_full_article(url)\\n\\n        # If parsing is successful, extract authors and text\\n        authors = article.authors if article.authors else \\\"Unknown Author\\\"\\n        context = article.text\\n    \\n    except Exception as e:\\n        print(f\\\"Failed to download article: {e}\\\")  # Log the error message\\n        continue\\n    \\n    # store the article data in a dictionary\\n    article_data = {\\n        'title': title,\\n        'datetime': datetime,\\n        'description': description,\\n        'url': url,\\n        'context': context,\\n        'authors': authors,\\n        'publisher_href': publisher_href,\\n    }\\n\\n\\n    # store the article data into the article_data list for JSON serialization\\n    news_articles.append(article_data)\\n\\n\\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\\nnews_articles_string = '|'.join(news_articles)\\n\\n# save the news_articles_string dictionary into a parameter to pass it downstream\\n\\nprint(\\\"#setValue(news_articles_string=%s)\\\" % news_articles_string)\\n\\nprint(news_articles_string) \",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_article"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3409"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471571877568"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505030822"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "952"
    },
    "news_articles_string" : {
      "prop" : "news_articles_string",
      "direct" : "OUT",
      "type" : "VARCHAR",
      "value" : ""
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_articles_main"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471700679104"
    }
  },
  "taskAppId" : "952_3409",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.309 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.310 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.310 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.326 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.329 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.329 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3409 check successfully
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.330 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.330 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.331 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.331 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.331 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "news_articles_string",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ",
  "resourceList" : [ ]
}
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.332 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.332 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}] successfully
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.332 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.332 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.332 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.333 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.333 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3409
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.334 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3409/py_952_3409.py
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.334 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.348 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.348 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.348 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3409/py_952_3409.py
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.348 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.349 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3409/952_3409.sh
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:22.382 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 599
[WI-0][TI-3409] - [INFO] 2024-05-05 03:08:23.216 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3409, success=true)
[WI-0][TI-3409] - [INFO] 2024-05-05 03:08:23.236 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3409)
[WI-0][TI-0] - [INFO] 2024-05-05 03:08:23.387 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3409/py_952_3409.py", line 11, in <module>
	    news_results = google_news.get_news(topic)[:5]
	                                        ^^^^^
	NameError: name 'topic' is not defined
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:23.393 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3409, processId:599 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:23.394 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:23.394 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:23.395 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:23.395 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:23.400 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:23.400 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:23.400 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3409
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:23.402 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/952/3409
[WI-952][TI-3409] - [INFO] 2024-05-05 03:08:23.402 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3409] - [INFO] 2024-05-05 03:08:24.189 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3409, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 03:08:24.916 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3410, taskName=extract_gnews_article, firstSubmitTime=1714849704904, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13471700679104, processDefineVersion=3, appIds=null, processInstanceId=953, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_article'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3410'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13471571877568'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505030824'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='953'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_articles_main'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13471700679104'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.918 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract_gnews_article to wait queue success
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.919 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.921 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.922 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.922 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.922 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714849704922
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.922 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 953_3410
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.923 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3410,
  "taskName" : "extract_gnews_article",
  "firstSubmitTime" : 1714849704904,
  "startTime" : 1714849704922,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13471700679104/3/953/3410.log",
  "processId" : 0,
  "processDefineCode" : 13471700679104,
  "processDefineVersion" : 3,
  "processInstanceId" : 953,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"from gnews import GNews\\nimport json\\n\\n\\n# Initialize GNews client\\ngoogle_news = GNews()\\n\\n# Fetch Google News articles for a specific topic, which is singapore related\\nnews_results = google_news.get_news(topic)[:5]\\n\\n# List to store the relevant information about the news articles\\nnews_articles = []\\n\\n# Process each news article\\nfor news in news_results:\\n    # Extract basic information from Google News\\n    title = news.get('title')\\n    datetime = news.get('published date')\\n    url = news.get('url')\\n    description = news.get('description')\\n    \\n    # Extract publisher information\\n    publisher_info = news.get('publisher', {})\\n    publisher_href = publisher_info.get('href')\\n    \\n    # Initialize and handle exceptions in article parsing\\n    try:\\n        article = google_news.get_full_article(url)\\n\\n        # If parsing is successful, extract authors and text\\n        authors = article.authors if article.authors else \\\"Unknown Author\\\"\\n        context = article.text\\n    \\n    except Exception as e:\\n        print(f\\\"Failed to download article: {e}\\\")  # Log the error message\\n        continue\\n    \\n    # store the article data in a dictionary\\n    article_data = {\\n        'title': title,\\n        'datetime': datetime,\\n        'description': description,\\n        'url': url,\\n        'context': context,\\n        'authors': authors,\\n        'publisher_href': publisher_href,\\n    }\\n\\n\\n    # store the article data into the article_data list for JSON serialization\\n    news_articles.append(article_data)\\n\\n\\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\\nnews_articles_string = '|'.join(news_articles)\\n\\n# save the news_articles_string dictionary into a parameter to pass it downstream\\n\\nprint(\\\"#setValue(news_articles_string=%s)\\\" % news_articles_string)\\n\\nprint(news_articles_string) \",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_article"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3410"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471571877568"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505030824"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "953"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_articles_main"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471700679104"
    }
  },
  "taskAppId" : "953_3410",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.924 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.924 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.924 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.926 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.927 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.927 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3410 check successfully
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.927 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.928 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.928 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.928 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.928 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "news_articles_string",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ",
  "resourceList" : [ ]
}
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.928 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.929 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.929 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.929 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.929 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.929 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.929 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3410
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.930 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3410/py_953_3410.py
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.930 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.930 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.931 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.931 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3410/py_953_3410.py
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.931 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.931 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3410/953_3410.sh
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:24.934 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 610
[WI-0][TI-3410] - [INFO] 2024-05-05 03:08:25.197 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3410, success=true)
[WI-0][TI-3410] - [INFO] 2024-05-05 03:08:25.205 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3410)
[WI-0][TI-0] - [INFO] 2024-05-05 03:08:25.937 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3410/py_953_3410.py", line 11, in <module>
	    news_results = google_news.get_news(topic)[:5]
	                                        ^^^^^
	NameError: name 'topic' is not defined
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:25.941 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3410, processId:610 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:25.944 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:25.944 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:25.944 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:25.945 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:25.950 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:25.951 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:25.951 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3410
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:25.951 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3410
[WI-953][TI-3410] - [INFO] 2024-05-05 03:08:25.952 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3410] - [INFO] 2024-05-05 03:08:26.196 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3410, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 03:08:26.277 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3411, taskName=extract_gnews_article, firstSubmitTime=1714849706262, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13471700679104, processDefineVersion=3, appIds=null, processInstanceId=953, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_article'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3411'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13471571877568'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505030826'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='953'}, news_articles_string=Property{prop='news_articles_string', direct=OUT, type=VARCHAR, value=''}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_articles_main'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13471700679104'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.278 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract_gnews_article to wait queue success
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.278 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.279 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.280 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.280 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.280 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714849706280
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.280 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 953_3411
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.282 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3411,
  "taskName" : "extract_gnews_article",
  "firstSubmitTime" : 1714849706262,
  "startTime" : 1714849706280,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13471700679104/3/953/3411.log",
  "processId" : 0,
  "processDefineCode" : 13471700679104,
  "processDefineVersion" : 3,
  "processInstanceId" : 953,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"from gnews import GNews\\nimport json\\n\\n\\n# Initialize GNews client\\ngoogle_news = GNews()\\n\\n# Fetch Google News articles for a specific topic, which is singapore related\\nnews_results = google_news.get_news(topic)[:5]\\n\\n# List to store the relevant information about the news articles\\nnews_articles = []\\n\\n# Process each news article\\nfor news in news_results:\\n    # Extract basic information from Google News\\n    title = news.get('title')\\n    datetime = news.get('published date')\\n    url = news.get('url')\\n    description = news.get('description')\\n    \\n    # Extract publisher information\\n    publisher_info = news.get('publisher', {})\\n    publisher_href = publisher_info.get('href')\\n    \\n    # Initialize and handle exceptions in article parsing\\n    try:\\n        article = google_news.get_full_article(url)\\n\\n        # If parsing is successful, extract authors and text\\n        authors = article.authors if article.authors else \\\"Unknown Author\\\"\\n        context = article.text\\n    \\n    except Exception as e:\\n        print(f\\\"Failed to download article: {e}\\\")  # Log the error message\\n        continue\\n    \\n    # store the article data in a dictionary\\n    article_data = {\\n        'title': title,\\n        'datetime': datetime,\\n        'description': description,\\n        'url': url,\\n        'context': context,\\n        'authors': authors,\\n        'publisher_href': publisher_href,\\n    }\\n\\n\\n    # store the article data into the article_data list for JSON serialization\\n    news_articles.append(article_data)\\n\\n\\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\\nnews_articles_string = '|'.join(news_articles)\\n\\n# save the news_articles_string dictionary into a parameter to pass it downstream\\n\\nprint(\\\"#setValue(news_articles_string=%s)\\\" % news_articles_string)\\n\\nprint(news_articles_string) \",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_article"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3411"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471571877568"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505030826"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "953"
    },
    "news_articles_string" : {
      "prop" : "news_articles_string",
      "direct" : "OUT",
      "type" : "VARCHAR",
      "value" : ""
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_articles_main"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471700679104"
    }
  },
  "taskAppId" : "953_3411",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.282 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.283 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.283 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.285 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.286 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.286 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3411 check successfully
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.287 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.287 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.287 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.287 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.288 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "news_articles_string",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ",
  "resourceList" : [ ]
}
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.288 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.289 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}] successfully
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.289 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.289 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.289 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.289 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.290 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3411
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.290 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3411/py_953_3411.py
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.290 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.291 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.291 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.291 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3411/py_953_3411.py
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.291 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.292 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3411/953_3411.sh
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:26.295 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 621
[WI-0][TI-3411] - [INFO] 2024-05-05 03:08:27.200 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3411, success=true)
[WI-0][TI-3411] - [INFO] 2024-05-05 03:08:27.209 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3411)
[WI-0][TI-0] - [INFO] 2024-05-05 03:08:27.297 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3411/py_953_3411.py", line 11, in <module>
	    news_results = google_news.get_news(topic)[:5]
	                                        ^^^^^
	NameError: name 'topic' is not defined
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:27.300 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3411, processId:621 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:27.300 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:27.300 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:27.300 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:27.301 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:27.304 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:27.304 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:27.304 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3411
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:27.305 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3411
[WI-953][TI-3411] - [INFO] 2024-05-05 03:08:27.305 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3411] - [INFO] 2024-05-05 03:08:28.200 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3411, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 03:08:28.256 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3412, taskName=extract_gnews_article, firstSubmitTime=1714849708245, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13471700679104, processDefineVersion=3, appIds=null, processInstanceId=953, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_article'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3412'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13471571877568'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505030828'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='953'}, news_articles_string=Property{prop='news_articles_string', direct=OUT, type=VARCHAR, value=''}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_articles_main'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13471700679104'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.257 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract_gnews_article to wait queue success
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.258 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.259 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.259 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.259 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.259 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714849708259
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.259 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 953_3412
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.260 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3412,
  "taskName" : "extract_gnews_article",
  "firstSubmitTime" : 1714849708245,
  "startTime" : 1714849708259,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13471700679104/3/953/3412.log",
  "processId" : 0,
  "processDefineCode" : 13471700679104,
  "processDefineVersion" : 3,
  "processInstanceId" : 953,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"from gnews import GNews\\nimport json\\n\\n\\n# Initialize GNews client\\ngoogle_news = GNews()\\n\\n# Fetch Google News articles for a specific topic, which is singapore related\\nnews_results = google_news.get_news(topic)[:5]\\n\\n# List to store the relevant information about the news articles\\nnews_articles = []\\n\\n# Process each news article\\nfor news in news_results:\\n    # Extract basic information from Google News\\n    title = news.get('title')\\n    datetime = news.get('published date')\\n    url = news.get('url')\\n    description = news.get('description')\\n    \\n    # Extract publisher information\\n    publisher_info = news.get('publisher', {})\\n    publisher_href = publisher_info.get('href')\\n    \\n    # Initialize and handle exceptions in article parsing\\n    try:\\n        article = google_news.get_full_article(url)\\n\\n        # If parsing is successful, extract authors and text\\n        authors = article.authors if article.authors else \\\"Unknown Author\\\"\\n        context = article.text\\n    \\n    except Exception as e:\\n        print(f\\\"Failed to download article: {e}\\\")  # Log the error message\\n        continue\\n    \\n    # store the article data in a dictionary\\n    article_data = {\\n        'title': title,\\n        'datetime': datetime,\\n        'description': description,\\n        'url': url,\\n        'context': context,\\n        'authors': authors,\\n        'publisher_href': publisher_href,\\n    }\\n\\n\\n    # store the article data into the article_data list for JSON serialization\\n    news_articles.append(article_data)\\n\\n\\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\\nnews_articles_string = '|'.join(news_articles)\\n\\n# save the news_articles_string dictionary into a parameter to pass it downstream\\n\\nprint(\\\"#setValue(news_articles_string=%s)\\\" % news_articles_string)\\n\\nprint(news_articles_string) \",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_article"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3412"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471571877568"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505030828"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "953"
    },
    "news_articles_string" : {
      "prop" : "news_articles_string",
      "direct" : "OUT",
      "type" : "VARCHAR",
      "value" : ""
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_articles_main"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471700679104"
    }
  },
  "taskAppId" : "953_3412",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.261 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.261 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.261 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.264 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.265 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.266 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3412 check successfully
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.266 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.266 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.267 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.268 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.268 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "news_articles_string",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ",
  "resourceList" : [ ]
}
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.269 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.269 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}] successfully
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.269 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.269 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.270 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.270 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.271 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3412
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.272 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3412/py_953_3412.py
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.272 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.273 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.273 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.273 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3412/py_953_3412.py
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.273 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.274 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3412/953_3412.sh
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:28.276 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 632
[WI-0][TI-3412] - [INFO] 2024-05-05 03:08:29.206 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3412, success=true)
[WI-0][TI-3412] - [INFO] 2024-05-05 03:08:29.212 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3412)
[WI-0][TI-0] - [INFO] 2024-05-05 03:08:29.280 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3412/py_953_3412.py", line 11, in <module>
	    news_results = google_news.get_news(topic)[:5]
	                                        ^^^^^
	NameError: name 'topic' is not defined
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:29.284 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3412, processId:632 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:29.286 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:29.286 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:29.286 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:29.287 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:29.290 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:29.290 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:29.290 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3412
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:29.291 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3412
[WI-953][TI-3412] - [INFO] 2024-05-05 03:08:29.291 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3412] - [INFO] 2024-05-05 03:08:30.204 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3412, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 03:08:30.301 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3413, taskName=extract_gnews_article, firstSubmitTime=1714849710284, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13471700679104, processDefineVersion=3, appIds=null, processInstanceId=953, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_article'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3413'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13471571877568'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505030830'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='953'}, news_articles_string=Property{prop='news_articles_string', direct=OUT, type=VARCHAR, value=''}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_articles_main'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13471700679104'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.302 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract_gnews_article to wait queue success
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.302 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.304 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.304 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.304 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.304 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714849710304
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.304 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 953_3413
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.305 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3413,
  "taskName" : "extract_gnews_article",
  "firstSubmitTime" : 1714849710284,
  "startTime" : 1714849710304,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13471700679104/3/953/3413.log",
  "processId" : 0,
  "processDefineCode" : 13471700679104,
  "processDefineVersion" : 3,
  "processInstanceId" : 953,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"from gnews import GNews\\nimport json\\n\\n\\n# Initialize GNews client\\ngoogle_news = GNews()\\n\\n# Fetch Google News articles for a specific topic, which is singapore related\\nnews_results = google_news.get_news(topic)[:5]\\n\\n# List to store the relevant information about the news articles\\nnews_articles = []\\n\\n# Process each news article\\nfor news in news_results:\\n    # Extract basic information from Google News\\n    title = news.get('title')\\n    datetime = news.get('published date')\\n    url = news.get('url')\\n    description = news.get('description')\\n    \\n    # Extract publisher information\\n    publisher_info = news.get('publisher', {})\\n    publisher_href = publisher_info.get('href')\\n    \\n    # Initialize and handle exceptions in article parsing\\n    try:\\n        article = google_news.get_full_article(url)\\n\\n        # If parsing is successful, extract authors and text\\n        authors = article.authors if article.authors else \\\"Unknown Author\\\"\\n        context = article.text\\n    \\n    except Exception as e:\\n        print(f\\\"Failed to download article: {e}\\\")  # Log the error message\\n        continue\\n    \\n    # store the article data in a dictionary\\n    article_data = {\\n        'title': title,\\n        'datetime': datetime,\\n        'description': description,\\n        'url': url,\\n        'context': context,\\n        'authors': authors,\\n        'publisher_href': publisher_href,\\n    }\\n\\n\\n    # store the article data into the article_data list for JSON serialization\\n    news_articles.append(article_data)\\n\\n\\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\\nnews_articles_string = '|'.join(news_articles)\\n\\n# save the news_articles_string dictionary into a parameter to pass it downstream\\n\\nprint(\\\"#setValue(news_articles_string=%s)\\\" % news_articles_string)\\n\\nprint(news_articles_string) \",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_article"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3413"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471571877568"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505030830"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "953"
    },
    "news_articles_string" : {
      "prop" : "news_articles_string",
      "direct" : "OUT",
      "type" : "VARCHAR",
      "value" : ""
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_articles_main"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471700679104"
    }
  },
  "taskAppId" : "953_3413",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.305 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.305 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.306 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.310 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.310 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.310 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3413 check successfully
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.311 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.311 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.311 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.311 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.311 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "news_articles_string",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "from gnews import GNews\nimport json\n\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ",
  "resourceList" : [ ]
}
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.312 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.312 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}] successfully
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.312 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.312 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.312 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.312 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.313 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3413
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.313 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3413/py_953_3413.py
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.313 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from gnews import GNews
import json


# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.314 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.314 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.314 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3413/py_953_3413.py
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.314 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.314 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3413/953_3413.sh
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:30.318 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 643
[WI-0][TI-3413] - [INFO] 2024-05-05 03:08:31.216 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3413, success=true)
[WI-0][TI-3413] - [INFO] 2024-05-05 03:08:31.221 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3413)
[WI-0][TI-0] - [INFO] 2024-05-05 03:08:31.319 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3413/py_953_3413.py", line 11, in <module>
	    news_results = google_news.get_news(topic)[:5]
	                                        ^^^^^
	NameError: name 'topic' is not defined
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:32.323 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3413, processId:643 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:32.325 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:32.326 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:32.326 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:32.327 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:32.331 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:32.331 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:32.331 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3413
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:32.332 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_3/953/3413
[WI-953][TI-3413] - [INFO] 2024-05-05 03:08:32.333 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3413] - [INFO] 2024-05-05 03:08:33.219 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3413, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 03:09:06.301 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7321937321937323 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:09:11.349 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.719298245614035 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:09:29.578 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8628571428571429 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:09:32.624 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.736231884057971 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:09:33.657 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7103064066852367 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:09:34.659 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8202247191011236 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:11:17.435 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7070422535211267 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:11:18.472 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7155172413793103 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:11:19.475 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.88 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:11:20.480 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8142857142857143 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:11:33.088 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3414, taskName=extract_gnews_article, firstSubmitTime=1714849893064, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13471700679104, processDefineVersion=4, appIds=null, processInstanceId=954, scheduleTime=0, globalParams=[{"prop":"topic","direct":"IN","type":"VARCHAR","value":"'Singapore'"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"from gnews import GNews\nimport json\n\n# define the topic to search by in Google News\ntopic = ${topic}\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_article'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3414'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13471571877568'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505031133'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='954'}, topic=Property{prop='topic', direct=IN, type=VARCHAR, value=''Singapore''}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_articles_main'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13471700679104'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.091 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract_gnews_article to wait queue success
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.092 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.095 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.095 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.096 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.096 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714849893096
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.096 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 954_3414
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.097 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3414,
  "taskName" : "extract_gnews_article",
  "firstSubmitTime" : 1714849893064,
  "startTime" : 1714849893096,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13471700679104/4/954/3414.log",
  "processId" : 0,
  "processDefineCode" : 13471700679104,
  "processDefineVersion" : 4,
  "processInstanceId" : 954,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"'Singapore'\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"from gnews import GNews\\nimport json\\n\\n# define the topic to search by in Google News\\ntopic = ${topic}\\n\\n# Initialize GNews client\\ngoogle_news = GNews()\\n\\n# Fetch Google News articles for a specific topic, which is singapore related\\nnews_results = google_news.get_news(topic)[:5]\\n\\n# List to store the relevant information about the news articles\\nnews_articles = []\\n\\n# Process each news article\\nfor news in news_results:\\n    # Extract basic information from Google News\\n    title = news.get('title')\\n    datetime = news.get('published date')\\n    url = news.get('url')\\n    description = news.get('description')\\n    \\n    # Extract publisher information\\n    publisher_info = news.get('publisher', {})\\n    publisher_href = publisher_info.get('href')\\n    \\n    # Initialize and handle exceptions in article parsing\\n    try:\\n        article = google_news.get_full_article(url)\\n\\n        # If parsing is successful, extract authors and text\\n        authors = article.authors if article.authors else \\\"Unknown Author\\\"\\n        context = article.text\\n    \\n    except Exception as e:\\n        print(f\\\"Failed to download article: {e}\\\")  # Log the error message\\n        continue\\n    \\n    # store the article data in a dictionary\\n    article_data = {\\n        'title': title,\\n        'datetime': datetime,\\n        'description': description,\\n        'url': url,\\n        'context': context,\\n        'authors': authors,\\n        'publisher_href': publisher_href,\\n    }\\n\\n\\n    # store the article data into the article_data list for JSON serialization\\n    news_articles.append(article_data)\\n\\n\\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\\nnews_articles_string = '|'.join(news_articles)\\n\\n# save the news_articles_string dictionary into a parameter to pass it downstream\\n\\nprint(\\\"#setValue(news_articles_string=%s)\\\" % news_articles_string)\\n\\nprint(news_articles_string) \",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_article"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3414"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471571877568"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505031133"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "954"
    },
    "topic" : {
      "prop" : "topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "'Singapore'"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_articles_main"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471700679104"
    }
  },
  "taskAppId" : "954_3414",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.098 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.098 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.098 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.103 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.104 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.105 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_4/954/3414 check successfully
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.105 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.105 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.106 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.106 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.107 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "news_articles_string",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "from gnews import GNews\nimport json\n\n# define the topic to search by in Google News\ntopic = ${topic}\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = google_news.get_full_article(url)\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n    \n    except Exception as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ",
  "resourceList" : [ ]
}
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.107 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.107 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.108 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.108 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.108 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.108 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from gnews import GNews
import json

# define the topic to search by in Google News
topic = ${topic}

# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.109 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_4/954/3414
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.110 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_4/954/3414/py_954_3414.py
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.110 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from gnews import GNews
import json

# define the topic to search by in Google News
topic = 'Singapore'

# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = google_news.get_full_article(url)

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text
    
    except Exception as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.111 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.111 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.111 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_4/954/3414/py_954_3414.py
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.111 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.112 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_4/954/3414/954_3414.sh
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:33.133 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 692
[WI-0][TI-0] - [INFO] 2024-05-05 03:11:33.565 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8121019108280254 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3414] - [INFO] 2024-05-05 03:11:33.674 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3414, success=true)
[WI-0][TI-3414] - [INFO] 2024-05-05 03:11:33.692 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3414)
[WI-0][TI-0] - [INFO] 2024-05-05 03:11:34.137 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-05-05 03:11:38.661 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.725212464589235 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:11:39.714 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7966101694915254 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:11:40.717 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7871148459383753 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:11:43.749 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8468946540880503 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:11:44.770 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8132183908045977 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:11:45.778 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8176470588235294 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:11:46.781 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8291316526610644 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:11:48.184 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	get_full_article() requires the `newspaper3k` library.
	You can install it by running `pip3 install newspaper3k` in your shell.
	Failed to download article: 'NoneType' object has no attribute 'authors'
	
	get_full_article() requires the `newspaper3k` library.
	You can install it by running `pip3 install newspaper3k` in your shell.
	Failed to download article: 'NoneType' object has no attribute 'authors'
	
	get_full_article() requires the `newspaper3k` library.
	You can install it by running `pip3 install newspaper3k` in your shell.
	Failed to download article: 'NoneType' object has no attribute 'authors'
	
	get_full_article() requires the `newspaper3k` library.
	You can install it by running `pip3 install newspaper3k` in your shell.
	Failed to download article: 'NoneType' object has no attribute 'authors'
	
	get_full_article() requires the `newspaper3k` library.
	You can install it by running `pip3 install newspaper3k` in your shell.
	Failed to download article: 'NoneType' object has no attribute 'authors'
	#setValue(news_articles_string=)
	
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:48.188 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_4/954/3414, processId:692 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:48.200 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:48.200 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:48.200 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:48.201 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:48.226 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:48.233 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:48.235 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_4/954/3414
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:48.237 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_4/954/3414
[WI-954][TI-3414] - [INFO] 2024-05-05 03:11:48.237 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3414] - [INFO] 2024-05-05 03:11:48.706 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3414, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 03:11:48.806 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7993630573248407 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:11:49.902 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7232876712328767 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:11:51.915 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7386018237082067 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:11:52.978 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.795903330386089 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:12:41.560 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7401129943502825 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:12:48.611 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7072463768115943 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:20:49.122 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7287234042553192 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:26:09.943 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7625698324022346 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:26:15.974 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7386363636363635 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 03:26:19.022 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3415, taskName=extract_gnews_article, firstSubmitTime=1714850779009, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13471700679104, processDefineVersion=5, appIds=null, processInstanceId=955, scheduleTime=0, globalParams=[{"prop":"topic","direct":"IN","type":"VARCHAR","value":"'Singapore'"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"from gnews import GNews\nfrom newspaper import Article, ArticleException\nimport json\n\n# define the topic to search by in Google News\ntopic = ${topic}\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = Article(url)\n        article.download()\n        article.parse()\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n\n    except ArticleException as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_article'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3415'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13471571877568'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505032619'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='955'}, topic=Property{prop='topic', direct=IN, type=VARCHAR, value=''Singapore''}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_articles_main'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13471700679104'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.023 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract_gnews_article to wait queue success
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.023 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.026 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.027 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.027 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.027 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714850779027
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.027 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 955_3415
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.027 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3415,
  "taskName" : "extract_gnews_article",
  "firstSubmitTime" : 1714850779009,
  "startTime" : 1714850779027,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13471700679104/5/955/3415.log",
  "processId" : 0,
  "processDefineCode" : 13471700679104,
  "processDefineVersion" : 5,
  "processInstanceId" : 955,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"'Singapore'\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"from gnews import GNews\\nfrom newspaper import Article, ArticleException\\nimport json\\n\\n# define the topic to search by in Google News\\ntopic = ${topic}\\n\\n# Initialize GNews client\\ngoogle_news = GNews()\\n\\n# Fetch Google News articles for a specific topic, which is singapore related\\nnews_results = google_news.get_news(topic)[:5]\\n\\n# List to store the relevant information about the news articles\\nnews_articles = []\\n\\n# Process each news article\\nfor news in news_results:\\n    # Extract basic information from Google News\\n    title = news.get('title')\\n    datetime = news.get('published date')\\n    url = news.get('url')\\n    description = news.get('description')\\n    \\n    # Extract publisher information\\n    publisher_info = news.get('publisher', {})\\n    publisher_href = publisher_info.get('href')\\n    \\n    # Initialize and handle exceptions in article parsing\\n    try:\\n        article = Article(url)\\n        article.download()\\n        article.parse()\\n\\n        # If parsing is successful, extract authors and text\\n        authors = article.authors if article.authors else \\\"Unknown Author\\\"\\n        context = article.text\\n\\n    except ArticleException as e:\\n        print(f\\\"Failed to download article: {e}\\\")  # Log the error message\\n        continue\\n    \\n    # store the article data in a dictionary\\n    article_data = {\\n        'title': title,\\n        'datetime': datetime,\\n        'description': description,\\n        'url': url,\\n        'context': context,\\n        'authors': authors,\\n        'publisher_href': publisher_href,\\n    }\\n\\n\\n    # store the article data into the article_data list for JSON serialization\\n    news_articles.append(article_data)\\n\\n\\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\\nnews_articles_string = '|'.join(news_articles)\\n\\n# save the news_articles_string dictionary into a parameter to pass it downstream\\n\\nprint(\\\"#setValue(news_articles_string=%s)\\\" % news_articles_string)\\n\\nprint(news_articles_string) \",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_article"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3415"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471571877568"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505032619"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "955"
    },
    "topic" : {
      "prop" : "topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "'Singapore'"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_articles_main"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471700679104"
    }
  },
  "taskAppId" : "955_3415",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.028 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.029 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.030 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.048 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.052 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.061 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3415 check successfully
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.061 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.062 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.064 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.064 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.064 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "news_articles_string",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "from gnews import GNews\nfrom newspaper import Article, ArticleException\nimport json\n\n# define the topic to search by in Google News\ntopic = ${topic}\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = Article(url)\n        article.download()\n        article.parse()\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n\n    except ArticleException as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ",
  "resourceList" : [ ]
}
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.064 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.065 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.065 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.066 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.067 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.067 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from gnews import GNews
from newspaper import Article, ArticleException
import json

# define the topic to search by in Google News
topic = ${topic}

# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = Article(url)
        article.download()
        article.parse()

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text

    except ArticleException as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.068 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3415
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.069 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3415/py_955_3415.py
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.069 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from gnews import GNews
from newspaper import Article, ArticleException
import json

# define the topic to search by in Google News
topic = 'Singapore'

# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = Article(url)
        article.download()
        article.parse()

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text

    except ArticleException as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.070 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.070 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.070 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3415/py_955_3415.py
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.070 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.071 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3415/955_3415.sh
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:19.080 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 898
[WI-0][TI-0] - [INFO] 2024-05-05 03:26:19.086 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-3415] - [INFO] 2024-05-05 03:26:19.709 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3415, success=true)
[WI-0][TI-3415] - [INFO] 2024-05-05 03:26:19.718 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3415)
[WI-0][TI-0] - [INFO] 2024-05-05 03:26:20.092 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3415/py_955_3415.py", line 4, in <module>
	    from newspaper import Article, ArticleException
	  File "/usr/local/lib/python3.11/dist-packages/newspaper/__init__.py", line 10, in <module>
	    from .api import (build, build_article, fulltext, hot, languages,
	  File "/usr/local/lib/python3.11/dist-packages/newspaper/api.py", line 14, in <module>
	    from .article import Article
	  File "/usr/local/lib/python3.11/dist-packages/newspaper/article.py", line 15, in <module>
	    from . import network
	  File "/usr/local/lib/python3.11/dist-packages/newspaper/network.py", line 14, in <module>
	    from .configuration import Configuration
	  File "/usr/local/lib/python3.11/dist-packages/newspaper/configuration.py", line 15, in <module>
	    from .parsers import Parser
	  File "/usr/local/lib/python3.11/dist-packages/newspaper/parsers.py", line 12, in <module>
	    import lxml.html.clean
	  File "/usr/local/lib/python3.11/dist-packages/lxml/html/clean.py", line 18, in <module>
	    raise ImportError(
	ImportError: lxml.html.clean module is now a separate project lxml_html_clean.
	Install lxml[html_clean] or lxml_html_clean directly.
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:20.093 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3415, processId:898 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:20.093 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:20.093 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:20.094 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:20.094 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:20.097 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:20.097 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:20.097 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3415
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:20.098 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3415
[WI-955][TI-3415] - [INFO] 2024-05-05 03:26:20.098 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3415] - [INFO] 2024-05-05 03:26:20.697 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3415, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 03:26:20.784 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3416, taskName=extract_gnews_article, firstSubmitTime=1714850780765, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13471700679104, processDefineVersion=5, appIds=null, processInstanceId=955, scheduleTime=0, globalParams=[{"prop":"topic","direct":"IN","type":"VARCHAR","value":"'Singapore'"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"from gnews import GNews\nfrom newspaper import Article, ArticleException\nimport json\n\n# define the topic to search by in Google News\ntopic = ${topic}\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = Article(url)\n        article.download()\n        article.parse()\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n\n    except ArticleException as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_article'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3416'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13471571877568'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505032620'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='955'}, topic=Property{prop='topic', direct=IN, type=VARCHAR, value=''Singapore''}, news_articles_string=Property{prop='news_articles_string', direct=OUT, type=VARCHAR, value=''}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_articles_main'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13471700679104'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.787 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract_gnews_article to wait queue success
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.787 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.788 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.788 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.788 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.788 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714850780788
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.788 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 955_3416
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.788 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3416,
  "taskName" : "extract_gnews_article",
  "firstSubmitTime" : 1714850780765,
  "startTime" : 1714850780788,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13471700679104/5/955/3416.log",
  "processId" : 0,
  "processDefineCode" : 13471700679104,
  "processDefineVersion" : 5,
  "processInstanceId" : 955,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"'Singapore'\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"from gnews import GNews\\nfrom newspaper import Article, ArticleException\\nimport json\\n\\n# define the topic to search by in Google News\\ntopic = ${topic}\\n\\n# Initialize GNews client\\ngoogle_news = GNews()\\n\\n# Fetch Google News articles for a specific topic, which is singapore related\\nnews_results = google_news.get_news(topic)[:5]\\n\\n# List to store the relevant information about the news articles\\nnews_articles = []\\n\\n# Process each news article\\nfor news in news_results:\\n    # Extract basic information from Google News\\n    title = news.get('title')\\n    datetime = news.get('published date')\\n    url = news.get('url')\\n    description = news.get('description')\\n    \\n    # Extract publisher information\\n    publisher_info = news.get('publisher', {})\\n    publisher_href = publisher_info.get('href')\\n    \\n    # Initialize and handle exceptions in article parsing\\n    try:\\n        article = Article(url)\\n        article.download()\\n        article.parse()\\n\\n        # If parsing is successful, extract authors and text\\n        authors = article.authors if article.authors else \\\"Unknown Author\\\"\\n        context = article.text\\n\\n    except ArticleException as e:\\n        print(f\\\"Failed to download article: {e}\\\")  # Log the error message\\n        continue\\n    \\n    # store the article data in a dictionary\\n    article_data = {\\n        'title': title,\\n        'datetime': datetime,\\n        'description': description,\\n        'url': url,\\n        'context': context,\\n        'authors': authors,\\n        'publisher_href': publisher_href,\\n    }\\n\\n\\n    # store the article data into the article_data list for JSON serialization\\n    news_articles.append(article_data)\\n\\n\\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\\nnews_articles_string = '|'.join(news_articles)\\n\\n# save the news_articles_string dictionary into a parameter to pass it downstream\\n\\nprint(\\\"#setValue(news_articles_string=%s)\\\" % news_articles_string)\\n\\nprint(news_articles_string) \",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_article"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3416"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471571877568"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505032620"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "955"
    },
    "topic" : {
      "prop" : "topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "'Singapore'"
    },
    "news_articles_string" : {
      "prop" : "news_articles_string",
      "direct" : "OUT",
      "type" : "VARCHAR",
      "value" : ""
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_articles_main"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471700679104"
    }
  },
  "taskAppId" : "955_3416",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.789 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.789 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.789 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.795 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.795 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.796 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3416 check successfully
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.796 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.796 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.796 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.797 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.797 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "news_articles_string",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "from gnews import GNews\nfrom newspaper import Article, ArticleException\nimport json\n\n# define the topic to search by in Google News\ntopic = ${topic}\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = Article(url)\n        article.download()\n        article.parse()\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n\n    except ArticleException as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ",
  "resourceList" : [ ]
}
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.797 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.797 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}] successfully
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.797 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.797 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.797 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.797 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from gnews import GNews
from newspaper import Article, ArticleException
import json

# define the topic to search by in Google News
topic = ${topic}

# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = Article(url)
        article.download()
        article.parse()

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text

    except ArticleException as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.798 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3416
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.799 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3416/py_955_3416.py
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.799 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from gnews import GNews
from newspaper import Article, ArticleException
import json

# define the topic to search by in Google News
topic = 'Singapore'

# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = Article(url)
        article.download()
        article.parse()

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text

    except ArticleException as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.800 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.800 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.800 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3416/py_955_3416.py
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.800 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.800 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3416/955_3416.sh
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:20.805 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 910
[WI-0][TI-3416] - [INFO] 2024-05-05 03:26:21.706 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3416, success=true)
[WI-0][TI-3416] - [INFO] 2024-05-05 03:26:21.714 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3416)
[WI-0][TI-0] - [INFO] 2024-05-05 03:26:21.806 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3416/py_955_3416.py", line 4, in <module>
	    from newspaper import Article, ArticleException
	  File "/usr/local/lib/python3.11/dist-packages/newspaper/__init__.py", line 10, in <module>
	    from .api import (build, build_article, fulltext, hot, languages,
	  File "/usr/local/lib/python3.11/dist-packages/newspaper/api.py", line 14, in <module>
	    from .article import Article
	  File "/usr/local/lib/python3.11/dist-packages/newspaper/article.py", line 15, in <module>
	    from . import network
	  File "/usr/local/lib/python3.11/dist-packages/newspaper/network.py", line 14, in <module>
	    from .configuration import Configuration
	  File "/usr/local/lib/python3.11/dist-packages/newspaper/configuration.py", line 15, in <module>
	    from .parsers import Parser
	  File "/usr/local/lib/python3.11/dist-packages/newspaper/parsers.py", line 12, in <module>
	    import lxml.html.clean
	  File "/usr/local/lib/python3.11/dist-packages/lxml/html/clean.py", line 18, in <module>
	    raise ImportError(
	ImportError: lxml.html.clean module is now a separate project lxml_html_clean.
	Install lxml[html_clean] or lxml_html_clean directly.
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:21.808 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3416, processId:910 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:21.809 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:21.809 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:21.809 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:21.810 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:21.814 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:21.814 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:21.814 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3416
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:21.815 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3416
[WI-955][TI-3416] - [INFO] 2024-05-05 03:26:21.815 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3416] - [INFO] 2024-05-05 03:26:22.701 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3416, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 03:26:22.809 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3417, taskName=extract_gnews_article, firstSubmitTime=1714850782800, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13471700679104, processDefineVersion=5, appIds=null, processInstanceId=955, scheduleTime=0, globalParams=[{"prop":"topic","direct":"IN","type":"VARCHAR","value":"'Singapore'"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"from gnews import GNews\nfrom newspaper import Article, ArticleException\nimport json\n\n# define the topic to search by in Google News\ntopic = ${topic}\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = Article(url)\n        article.download()\n        article.parse()\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n\n    except ArticleException as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_article'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3417'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13471571877568'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505032622'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='955'}, topic=Property{prop='topic', direct=IN, type=VARCHAR, value=''Singapore''}, news_articles_string=Property{prop='news_articles_string', direct=OUT, type=VARCHAR, value=''}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_articles_main'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13471700679104'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.810 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract_gnews_article to wait queue success
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.811 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.812 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.813 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.813 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.813 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714850782813
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.813 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 955_3417
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.813 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3417,
  "taskName" : "extract_gnews_article",
  "firstSubmitTime" : 1714850782800,
  "startTime" : 1714850782813,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13471700679104/5/955/3417.log",
  "processId" : 0,
  "processDefineCode" : 13471700679104,
  "processDefineVersion" : 5,
  "processInstanceId" : 955,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"'Singapore'\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"from gnews import GNews\\nfrom newspaper import Article, ArticleException\\nimport json\\n\\n# define the topic to search by in Google News\\ntopic = ${topic}\\n\\n# Initialize GNews client\\ngoogle_news = GNews()\\n\\n# Fetch Google News articles for a specific topic, which is singapore related\\nnews_results = google_news.get_news(topic)[:5]\\n\\n# List to store the relevant information about the news articles\\nnews_articles = []\\n\\n# Process each news article\\nfor news in news_results:\\n    # Extract basic information from Google News\\n    title = news.get('title')\\n    datetime = news.get('published date')\\n    url = news.get('url')\\n    description = news.get('description')\\n    \\n    # Extract publisher information\\n    publisher_info = news.get('publisher', {})\\n    publisher_href = publisher_info.get('href')\\n    \\n    # Initialize and handle exceptions in article parsing\\n    try:\\n        article = Article(url)\\n        article.download()\\n        article.parse()\\n\\n        # If parsing is successful, extract authors and text\\n        authors = article.authors if article.authors else \\\"Unknown Author\\\"\\n        context = article.text\\n\\n    except ArticleException as e:\\n        print(f\\\"Failed to download article: {e}\\\")  # Log the error message\\n        continue\\n    \\n    # store the article data in a dictionary\\n    article_data = {\\n        'title': title,\\n        'datetime': datetime,\\n        'description': description,\\n        'url': url,\\n        'context': context,\\n        'authors': authors,\\n        'publisher_href': publisher_href,\\n    }\\n\\n\\n    # store the article data into the article_data list for JSON serialization\\n    news_articles.append(article_data)\\n\\n\\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\\nnews_articles_string = '|'.join(news_articles)\\n\\n# save the news_articles_string dictionary into a parameter to pass it downstream\\n\\nprint(\\\"#setValue(news_articles_string=%s)\\\" % news_articles_string)\\n\\nprint(news_articles_string) \",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_article"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3417"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471571877568"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505032622"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "955"
    },
    "topic" : {
      "prop" : "topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "'Singapore'"
    },
    "news_articles_string" : {
      "prop" : "news_articles_string",
      "direct" : "OUT",
      "type" : "VARCHAR",
      "value" : ""
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_articles_main"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471700679104"
    }
  },
  "taskAppId" : "955_3417",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.814 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.814 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.815 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.830 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.831 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.832 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3417 check successfully
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.832 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.832 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.832 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.832 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.832 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "news_articles_string",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "from gnews import GNews\nfrom newspaper import Article, ArticleException\nimport json\n\n# define the topic to search by in Google News\ntopic = ${topic}\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = Article(url)\n        article.download()\n        article.parse()\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n\n    except ArticleException as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ",
  "resourceList" : [ ]
}
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.832 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.833 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}] successfully
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.833 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.833 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.833 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.833 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from gnews import GNews
from newspaper import Article, ArticleException
import json

# define the topic to search by in Google News
topic = ${topic}

# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = Article(url)
        article.download()
        article.parse()

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text

    except ArticleException as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.833 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3417
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.833 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3417/py_955_3417.py
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.834 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from gnews import GNews
from newspaper import Article, ArticleException
import json

# define the topic to search by in Google News
topic = 'Singapore'

# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = Article(url)
        article.download()
        article.parse()

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text

    except ArticleException as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.834 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.835 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.835 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3417/py_955_3417.py
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.835 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.835 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3417/955_3417.sh
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:22.840 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 921
[WI-0][TI-3417] - [INFO] 2024-05-05 03:26:23.746 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3417, success=true)
[WI-0][TI-3417] - [INFO] 2024-05-05 03:26:23.753 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3417)
[WI-0][TI-0] - [INFO] 2024-05-05 03:26:23.840 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3417/py_955_3417.py", line 4, in <module>
	    from newspaper import Article, ArticleException
	  File "/usr/local/lib/python3.11/dist-packages/newspaper/__init__.py", line 10, in <module>
	    from .api import (build, build_article, fulltext, hot, languages,
	  File "/usr/local/lib/python3.11/dist-packages/newspaper/api.py", line 14, in <module>
	    from .article import Article
	  File "/usr/local/lib/python3.11/dist-packages/newspaper/article.py", line 15, in <module>
	    from . import network
	  File "/usr/local/lib/python3.11/dist-packages/newspaper/network.py", line 14, in <module>
	    from .configuration import Configuration
	  File "/usr/local/lib/python3.11/dist-packages/newspaper/configuration.py", line 15, in <module>
	    from .parsers import Parser
	  File "/usr/local/lib/python3.11/dist-packages/newspaper/parsers.py", line 12, in <module>
	    import lxml.html.clean
	  File "/usr/local/lib/python3.11/dist-packages/lxml/html/clean.py", line 18, in <module>
	    raise ImportError(
	ImportError: lxml.html.clean module is now a separate project lxml_html_clean.
	Install lxml[html_clean] or lxml_html_clean directly.
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:23.845 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3417, processId:921 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:23.846 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:23.846 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:23.846 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:23.848 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:23.861 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:23.861 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:23.861 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3417
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:23.863 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3417
[WI-955][TI-3417] - [INFO] 2024-05-05 03:26:23.864 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3417] - [INFO] 2024-05-05 03:26:24.720 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3417, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 03:26:24.797 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3418, taskName=extract_gnews_article, firstSubmitTime=1714850784788, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13471700679104, processDefineVersion=5, appIds=null, processInstanceId=955, scheduleTime=0, globalParams=[{"prop":"topic","direct":"IN","type":"VARCHAR","value":"'Singapore'"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"from gnews import GNews\nfrom newspaper import Article, ArticleException\nimport json\n\n# define the topic to search by in Google News\ntopic = ${topic}\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = Article(url)\n        article.download()\n        article.parse()\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n\n    except ArticleException as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_article'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3418'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13471571877568'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505032624'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='955'}, topic=Property{prop='topic', direct=IN, type=VARCHAR, value=''Singapore''}, news_articles_string=Property{prop='news_articles_string', direct=OUT, type=VARCHAR, value=''}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_gnews_articles_main'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13471700679104'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.799 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract_gnews_article to wait queue success
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.799 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.801 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.801 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.802 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.802 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714850784802
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.802 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 955_3418
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.803 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3418,
  "taskName" : "extract_gnews_article",
  "firstSubmitTime" : 1714850784788,
  "startTime" : 1714850784802,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13471700679104/5/955/3418.log",
  "processId" : 0,
  "processDefineCode" : 13471700679104,
  "processDefineVersion" : 5,
  "processInstanceId" : 955,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"'Singapore'\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"from gnews import GNews\\nfrom newspaper import Article, ArticleException\\nimport json\\n\\n# define the topic to search by in Google News\\ntopic = ${topic}\\n\\n# Initialize GNews client\\ngoogle_news = GNews()\\n\\n# Fetch Google News articles for a specific topic, which is singapore related\\nnews_results = google_news.get_news(topic)[:5]\\n\\n# List to store the relevant information about the news articles\\nnews_articles = []\\n\\n# Process each news article\\nfor news in news_results:\\n    # Extract basic information from Google News\\n    title = news.get('title')\\n    datetime = news.get('published date')\\n    url = news.get('url')\\n    description = news.get('description')\\n    \\n    # Extract publisher information\\n    publisher_info = news.get('publisher', {})\\n    publisher_href = publisher_info.get('href')\\n    \\n    # Initialize and handle exceptions in article parsing\\n    try:\\n        article = Article(url)\\n        article.download()\\n        article.parse()\\n\\n        # If parsing is successful, extract authors and text\\n        authors = article.authors if article.authors else \\\"Unknown Author\\\"\\n        context = article.text\\n\\n    except ArticleException as e:\\n        print(f\\\"Failed to download article: {e}\\\")  # Log the error message\\n        continue\\n    \\n    # store the article data in a dictionary\\n    article_data = {\\n        'title': title,\\n        'datetime': datetime,\\n        'description': description,\\n        'url': url,\\n        'context': context,\\n        'authors': authors,\\n        'publisher_href': publisher_href,\\n    }\\n\\n\\n    # store the article data into the article_data list for JSON serialization\\n    news_articles.append(article_data)\\n\\n\\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\\nnews_articles_string = '|'.join(news_articles)\\n\\n# save the news_articles_string dictionary into a parameter to pass it downstream\\n\\nprint(\\\"#setValue(news_articles_string=%s)\\\" % news_articles_string)\\n\\nprint(news_articles_string) \",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_article"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3418"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471571877568"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505032624"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "955"
    },
    "topic" : {
      "prop" : "topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "'Singapore'"
    },
    "news_articles_string" : {
      "prop" : "news_articles_string",
      "direct" : "OUT",
      "type" : "VARCHAR",
      "value" : ""
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_gnews_articles_main"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13471700679104"
    }
  },
  "taskAppId" : "955_3418",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"news_articles_string\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.804 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.804 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.804 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.807 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.808 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.809 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3418 check successfully
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.809 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.809 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.810 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.810 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.810 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "news_articles_string",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "from gnews import GNews\nfrom newspaper import Article, ArticleException\nimport json\n\n# define the topic to search by in Google News\ntopic = ${topic}\n\n# Initialize GNews client\ngoogle_news = GNews()\n\n# Fetch Google News articles for a specific topic, which is singapore related\nnews_results = google_news.get_news(topic)[:5]\n\n# List to store the relevant information about the news articles\nnews_articles = []\n\n# Process each news article\nfor news in news_results:\n    # Extract basic information from Google News\n    title = news.get('title')\n    datetime = news.get('published date')\n    url = news.get('url')\n    description = news.get('description')\n    \n    # Extract publisher information\n    publisher_info = news.get('publisher', {})\n    publisher_href = publisher_info.get('href')\n    \n    # Initialize and handle exceptions in article parsing\n    try:\n        article = Article(url)\n        article.download()\n        article.parse()\n\n        # If parsing is successful, extract authors and text\n        authors = article.authors if article.authors else \"Unknown Author\"\n        context = article.text\n\n    except ArticleException as e:\n        print(f\"Failed to download article: {e}\")  # Log the error message\n        continue\n    \n    # store the article data in a dictionary\n    article_data = {\n        'title': title,\n        'datetime': datetime,\n        'description': description,\n        'url': url,\n        'context': context,\n        'authors': authors,\n        'publisher_href': publisher_href,\n    }\n\n\n    # store the article data into the article_data list for JSON serialization\n    news_articles.append(article_data)\n\n\n# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node\nnews_articles_string = '|'.join(news_articles)\n\n# save the news_articles_string dictionary into a parameter to pass it downstream\n\nprint(\"#setValue(news_articles_string=%s)\" % news_articles_string)\n\nprint(news_articles_string) ",
  "resourceList" : [ ]
}
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.811 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.811 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"news_articles_string","direct":"OUT","type":"VARCHAR","value":""}] successfully
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.811 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.811 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.811 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.811 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from gnews import GNews
from newspaper import Article, ArticleException
import json

# define the topic to search by in Google News
topic = ${topic}

# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = Article(url)
        article.download()
        article.parse()

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text

    except ArticleException as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.812 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3418
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.812 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3418/py_955_3418.py
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.813 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from gnews import GNews
from newspaper import Article, ArticleException
import json

# define the topic to search by in Google News
topic = 'Singapore'

# Initialize GNews client
google_news = GNews()

# Fetch Google News articles for a specific topic, which is singapore related
news_results = google_news.get_news(topic)[:5]

# List to store the relevant information about the news articles
news_articles = []

# Process each news article
for news in news_results:
    # Extract basic information from Google News
    title = news.get('title')
    datetime = news.get('published date')
    url = news.get('url')
    description = news.get('description')
    
    # Extract publisher information
    publisher_info = news.get('publisher', {})
    publisher_href = publisher_info.get('href')
    
    # Initialize and handle exceptions in article parsing
    try:
        article = Article(url)
        article.download()
        article.parse()

        # If parsing is successful, extract authors and text
        authors = article.authors if article.authors else "Unknown Author"
        context = article.text

    except ArticleException as e:
        print(f"Failed to download article: {e}")  # Log the error message
        continue
    
    # store the article data in a dictionary
    article_data = {
        'title': title,
        'datetime': datetime,
        'description': description,
        'url': url,
        'context': context,
        'authors': authors,
        'publisher_href': publisher_href,
    }


    # store the article data into the article_data list for JSON serialization
    news_articles.append(article_data)


# create a string containing all the article data separated by a vertical line (|) for it to be passed into the dynamic node
news_articles_string = '|'.join(news_articles)

# save the news_articles_string dictionary into a parameter to pass it downstream

print("#setValue(news_articles_string=%s)" % news_articles_string)

print(news_articles_string) 
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.813 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.813 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.814 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3418/py_955_3418.py
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.814 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.814 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3418/955_3418.sh
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:24.818 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 938
[WI-0][TI-3418] - [INFO] 2024-05-05 03:26:25.730 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3418, success=true)
[WI-0][TI-3418] - [INFO] 2024-05-05 03:26:25.751 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3418)
[WI-0][TI-0] - [INFO] 2024-05-05 03:26:25.832 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3418/py_955_3418.py", line 4, in <module>
	    from newspaper import Article, ArticleException
	  File "/usr/local/lib/python3.11/dist-packages/newspaper/__init__.py", line 10, in <module>
	    from .api import (build, build_article, fulltext, hot, languages,
	  File "/usr/local/lib/python3.11/dist-packages/newspaper/api.py", line 14, in <module>
	    from .article import Article
	  File "/usr/local/lib/python3.11/dist-packages/newspaper/article.py", line 15, in <module>
	    from . import network
	  File "/usr/local/lib/python3.11/dist-packages/newspaper/network.py", line 14, in <module>
	    from .configuration import Configuration
	  File "/usr/local/lib/python3.11/dist-packages/newspaper/configuration.py", line 15, in <module>
	    from .parsers import Parser
	  File "/usr/local/lib/python3.11/dist-packages/newspaper/parsers.py", line 12, in <module>
	    import lxml.html.clean
	  File "/usr/local/lib/python3.11/dist-packages/lxml/html/clean.py", line 18, in <module>
	    raise ImportError(
	ImportError: lxml.html.clean module is now a separate project lxml_html_clean.
	Install lxml[html_clean] or lxml_html_clean directly.
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:25.838 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3418, processId:938 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:25.839 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:25.839 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:25.839 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:25.840 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:25.847 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:25.847 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:25.848 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3418
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:25.848 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13471700679104_5/955/3418
[WI-955][TI-3418] - [INFO] 2024-05-05 03:26:25.849 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3418] - [INFO] 2024-05-05 03:26:26.728 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3418, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 03:26:29.031 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7740112994350282 is over then the MaxCpuUsagePercentageThresholds 0.7
