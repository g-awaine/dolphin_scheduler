[WI-0][TI-0] - [INFO] 2024-04-25 12:06:06.200 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7002881844380404 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 12:06:12.180 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1727, taskName=read kafka, firstSubmitTime=1714017972137, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.10:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=4, appIds=null, processInstanceId=649, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# Split the string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n#     --bootstrap-server kafka:9092 \\\n#     --topic ${input_topic} \\\n#     --group ${input_topic}_consumers \\\n#     --max-messages 1 \\\n#     --timeout-ms 10000)\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\n# echo \"#{setValue(message=${message})}\"\n# echo \"#{setValue(message=${message})}\"\n","resourceList":[]}, environmentConfig=export JAVA_HOME=/opt/java/openjdk, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='read kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='649'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1727'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386664399040'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425120612'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.183 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: read kafka to wait queue success
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.184 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.194 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.194 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.194 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.194 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714017972194
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.194 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 649_1727
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.195 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1727,
  "taskName" : "read kafka",
  "firstSubmitTime" : 1714017972137,
  "startTime" : 1714017972194,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.10:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/4/649/1727.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 4,
  "processInstanceId" : 649,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\ntopics=\\\"reddit_post_processed_topic|reddit_post_sa_topic\\\"\\n\\n# Split the string into input and output topics\\nIFS='|' read -r input_topic output_topic <<< \\\"$topics\\\"\\n\\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\\\\n#     --bootstrap-server kafka:9092 \\\\\\n#     --topic ${input_topic} \\\\\\n#     --group ${input_topic}_consumers \\\\\\n#     --max-messages 1 \\\\\\n#     --timeout-ms 10000)\\n\\n# Print the input and output topics\\necho \\\"Input Topic: $input_topic\\\"\\necho \\\"Output Topic: $output_topic\\\"\\n\\n# echo \\\"#{setValue(message=${message})}\\\"\\n# echo \\\"#{setValue(message=${message})}\\\"\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export JAVA_HOME=/opt/java/openjdk",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "read kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "649"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1727"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386664399040"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425120612"
    }
  },
  "taskAppId" : "649_1727",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.196 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.196 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.196 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.202 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.204 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.205 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1727 check successfully
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.205 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.205 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.206 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.206 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.206 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# Split the string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n#     --bootstrap-server kafka:9092 \\\n#     --topic ${input_topic} \\\n#     --group ${input_topic}_consumers \\\n#     --max-messages 1 \\\n#     --timeout-ms 10000)\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\n# echo \"#{setValue(message=${message})}\"\n# echo \"#{setValue(message=${message})}\"\n",
  "resourceList" : [ ]
}
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.207 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.208 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.208 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.210 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.210 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.210 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.211 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.211 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export JAVA_HOME=/opt/java/openjdk
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

topics="reddit_post_processed_topic|reddit_post_sa_topic"

# Split the string into input and output topics
IFS='|' read -r input_topic output_topic <<< "$topics"

# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \
#     --bootstrap-server kafka:9092 \
#     --topic ${input_topic} \
#     --group ${input_topic}_consumers \
#     --max-messages 1 \
#     --timeout-ms 10000)

# Print the input and output topics
echo "Input Topic: $input_topic"
echo "Output Topic: $output_topic"

# echo "#{setValue(message=${message})}"
# echo "#{setValue(message=${message})}"

[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.211 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.211 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1727/649_1727.sh
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:12.221 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2338
[WI-0][TI-1727] - [INFO] 2024-04-25 12:06:13.198 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1727, success=true)
[WI-0][TI-1727] - [INFO] 2024-04-25 12:06:13.206 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1727)
[WI-0][TI-0] - [INFO] 2024-04-25 12:06:13.239 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Input Topic: reddit_post_processed_topic
	Output Topic: reddit_post_sa_topic
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:13.241 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1727, processId:2338 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:13.242 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:13.242 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:13.243 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:13.243 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:13.245 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:13.246 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:13.246 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1727
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:13.246 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1727
[WI-649][TI-1727] - [INFO] 2024-04-25 12:06:13.247 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1727] - [INFO] 2024-04-25 12:06:14.192 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1727, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 12:06:14.277 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1728, taskName=sentiment anaysis, firstSubmitTime=1714017974269, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.10:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=4, appIds=null, processInstanceId=649, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# Example usage with PySpark DataFrame\n# Assuming `df` is your DataFrame containing messages\n# and `message` is the column containing the messages\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='649'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1728'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425120614'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.279 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.279 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.281 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.281 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.281 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.281 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714017974281
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.282 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 649_1728
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.289 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1728,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714017974269,
  "startTime" : 1714017974281,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.10:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/4/649/1728.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 4,
  "processInstanceId" : 649,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# Example usage with PySpark DataFrame\\n# Assuming `df` is your DataFrame containing messages\\n# and `message` is the column containing the messages\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "649"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1728"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425120614"
    }
  },
  "taskAppId" : "649_1728",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.290 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.290 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.290 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.294 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.294 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.295 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1728 check successfully
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.295 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.298 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.299 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.300 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.301 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# Example usage with PySpark DataFrame\n# Assuming `df` is your DataFrame containing messages\n# and `message` is the column containing the messages\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.301 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.302 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.302 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.302 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.302 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.302 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# Example usage with PySpark DataFrame
# Assuming `df` is your DataFrame containing messages
# and `message` is the column containing the messages
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.303 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1728
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.304 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1728/py_649_1728.py
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.304 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# Example usage with PySpark DataFrame
# Assuming `df` is your DataFrame containing messages
# and `message` is the column containing the messages
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.316 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.317 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.317 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1728/py_649_1728.py
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.317 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.317 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1728/649_1728.sh
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:14.320 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2349
[WI-0][TI-1728] - [INFO] 2024-04-25 12:06:15.200 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1728, success=true)
[WI-0][TI-1728] - [INFO] 2024-04-25 12:06:15.207 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1728)
[WI-0][TI-0] - [INFO] 2024-04-25 12:06:15.329 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1728/py_649_1728.py", line 29
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:15.336 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1728, processId:2349 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:15.337 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:15.337 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:15.337 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:15.338 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:15.340 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:15.340 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:15.340 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1728
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:15.341 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1728
[WI-649][TI-1728] - [INFO] 2024-04-25 12:06:15.341 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1728] - [INFO] 2024-04-25 12:06:16.195 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1728, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 12:06:16.313 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1729, taskName=sentiment anaysis, firstSubmitTime=1714017976298, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.10:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=4, appIds=null, processInstanceId=649, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# Example usage with PySpark DataFrame\n# Assuming `df` is your DataFrame containing messages\n# and `message` is the column containing the messages\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='649'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1729'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425120616'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.314 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.314 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.316 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.316 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.316 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.316 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714017976316
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.316 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 649_1729
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.317 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1729,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714017976298,
  "startTime" : 1714017976316,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.10:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/4/649/1729.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 4,
  "processInstanceId" : 649,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# Example usage with PySpark DataFrame\\n# Assuming `df` is your DataFrame containing messages\\n# and `message` is the column containing the messages\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "649"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1729"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425120616"
    }
  },
  "taskAppId" : "649_1729",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.317 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.317 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.318 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.319 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.320 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.320 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1729 check successfully
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.321 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.321 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.321 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.321 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.321 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# Example usage with PySpark DataFrame\n# Assuming `df` is your DataFrame containing messages\n# and `message` is the column containing the messages\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.322 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.322 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.322 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.322 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.322 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.322 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# Example usage with PySpark DataFrame
# Assuming `df` is your DataFrame containing messages
# and `message` is the column containing the messages
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.323 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1729
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.323 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1729/py_649_1729.py
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.323 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# Example usage with PySpark DataFrame
# Assuming `df` is your DataFrame containing messages
# and `message` is the column containing the messages
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.324 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.324 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.324 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1729/py_649_1729.py
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.324 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.325 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1729/649_1729.sh
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:16.327 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2360
[WI-0][TI-1729] - [INFO] 2024-04-25 12:06:17.210 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1729, success=true)
[WI-0][TI-1729] - [INFO] 2024-04-25 12:06:17.217 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1729)
[WI-0][TI-0] - [INFO] 2024-04-25 12:06:17.330 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1729/py_649_1729.py", line 29
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:17.331 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1729, processId:2360 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:17.332 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:17.334 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:17.334 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:17.335 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:17.337 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:17.337 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:17.338 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1729
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:17.338 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1729
[WI-649][TI-1729] - [INFO] 2024-04-25 12:06:17.338 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1729] - [INFO] 2024-04-25 12:06:18.200 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1729, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 12:06:18.268 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1730, taskName=sentiment anaysis, firstSubmitTime=1714017978256, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.10:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=4, appIds=null, processInstanceId=649, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# Example usage with PySpark DataFrame\n# Assuming `df` is your DataFrame containing messages\n# and `message` is the column containing the messages\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='649'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1730'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425120618'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.269 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.270 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.272 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.272 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.272 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.272 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714017978272
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.272 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 649_1730
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.272 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1730,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714017978256,
  "startTime" : 1714017978272,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.10:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/4/649/1730.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 4,
  "processInstanceId" : 649,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# Example usage with PySpark DataFrame\\n# Assuming `df` is your DataFrame containing messages\\n# and `message` is the column containing the messages\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "649"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1730"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425120618"
    }
  },
  "taskAppId" : "649_1730",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.273 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.273 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.273 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.281 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.282 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.283 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1730 check successfully
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.284 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.284 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.285 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.285 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.285 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# Example usage with PySpark DataFrame\n# Assuming `df` is your DataFrame containing messages\n# and `message` is the column containing the messages\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.286 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.286 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.286 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.286 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.286 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.286 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# Example usage with PySpark DataFrame
# Assuming `df` is your DataFrame containing messages
# and `message` is the column containing the messages
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.287 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1730
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.287 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1730/py_649_1730.py
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.287 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# Example usage with PySpark DataFrame
# Assuming `df` is your DataFrame containing messages
# and `message` is the column containing the messages
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.288 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.288 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.288 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1730/py_649_1730.py
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.288 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.288 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1730/649_1730.sh
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:18.294 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2377
[WI-0][TI-0] - [INFO] 2024-04-25 12:06:19.295 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1730/py_649_1730.py", line 29
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-0][TI-1730] - [INFO] 2024-04-25 12:06:19.296 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1730, success=true)
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:19.299 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1730, processId:2377 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:19.303 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:19.304 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:19.305 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:19.305 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:19.308 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:19.308 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:19.308 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1730
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:19.312 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1730
[WI-649][TI-1730] - [INFO] 2024-04-25 12:06:19.317 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1730] - [INFO] 2024-04-25 12:06:19.320 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1730)
[WI-0][TI-1730] - [INFO] 2024-04-25 12:06:19.338 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1730, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 12:06:20.419 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1731, taskName=sentiment anaysis, firstSubmitTime=1714017980398, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.10:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=4, appIds=null, processInstanceId=649, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# Example usage with PySpark DataFrame\n# Assuming `df` is your DataFrame containing messages\n# and `message` is the column containing the messages\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='649'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1731'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425120620'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.421 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.421 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.422 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.423 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.423 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.423 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714017980423
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.423 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 649_1731
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.424 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1731,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714017980398,
  "startTime" : 1714017980423,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.10:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/4/649/1731.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 4,
  "processInstanceId" : 649,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# Example usage with PySpark DataFrame\\n# Assuming `df` is your DataFrame containing messages\\n# and `message` is the column containing the messages\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "649"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1731"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425120620"
    }
  },
  "taskAppId" : "649_1731",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.425 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.425 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.425 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.435 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.436 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.437 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1731 check successfully
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.437 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.437 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.438 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.438 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.438 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# Example usage with PySpark DataFrame\n# Assuming `df` is your DataFrame containing messages\n# and `message` is the column containing the messages\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.438 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.438 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.438 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.438 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.439 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.439 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# Example usage with PySpark DataFrame
# Assuming `df` is your DataFrame containing messages
# and `message` is the column containing the messages
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.439 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1731
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.440 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1731/py_649_1731.py
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.440 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# Example usage with PySpark DataFrame
# Assuming `df` is your DataFrame containing messages
# and `message` is the column containing the messages
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.440 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.441 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.441 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1731/py_649_1731.py
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.441 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.441 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1731/649_1731.sh
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:20.445 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2388
[WI-0][TI-1731] - [INFO] 2024-04-25 12:06:21.305 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1731, success=true)
[WI-0][TI-1731] - [INFO] 2024-04-25 12:06:21.311 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1731)
[WI-0][TI-0] - [INFO] 2024-04-25 12:06:21.447 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1731/py_649_1731.py", line 29
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:21.451 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1731, processId:2388 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:21.453 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:21.453 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:21.454 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:21.454 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:21.460 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:21.460 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:21.460 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1731
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:21.461 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_4/649/1731
[WI-649][TI-1731] - [INFO] 2024-04-25 12:06:21.461 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1731] - [INFO] 2024-04-25 12:06:22.311 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1731, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 12:06:23.296 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7977839335180055 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 12:06:26.321 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7020057306590257 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 12:06:27.344 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7817109144542774 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 12:06:28.345 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7661290322580645 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 12:08:49.806 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7312138728323699 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 12:09:20.957 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1732, taskName=read kafka, firstSubmitTime=1714018160932, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.10:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=5, appIds=null, processInstanceId=650, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"input_topic","direct":"OUT","type":"VARCHAR","value":""},{"prop":"output_topic","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n#     --bootstrap-server kafka:9092 \\\n#     --topic ${input_topic} \\\n#     --group ${input_topic}_consumers \\\n#     --max-messages 1 \\\n#     --timeout-ms 10000)\n\n","resourceList":[]}, environmentConfig=export JAVA_HOME=/opt/java/openjdk, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='read kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='650'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1732'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386664399040'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425120920'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:20.994 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: read kafka to wait queue success
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:20.997 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.011 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.012 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.012 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.012 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714018161012
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.012 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 650_1732
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.013 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1732,
  "taskName" : "read kafka",
  "firstSubmitTime" : 1714018160932,
  "startTime" : 1714018161012,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.10:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/5/650/1732.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 5,
  "processInstanceId" : 650,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"input_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"},{\"prop\":\"output_topic\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\ntopics=\\\"reddit_post_processed_topic|reddit_post_sa_topic\\\"\\n\\n# parse the topics string into input and output topics\\nIFS='|' read -r input_topic output_topic <<< \\\"$topics\\\"\\n\\n# Print the input and output topics\\necho \\\"Input Topic: $input_topic\\\"\\necho \\\"Output Topic: $output_topic\\\"\\n\\necho \\\"#{setValue(input_topic=${input_topic})}\\\"\\necho \\\"#{setValue(output_topic=${output_topic})}\\\"\\n\\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\\\\n#     --bootstrap-server kafka:9092 \\\\\\n#     --topic ${input_topic} \\\\\\n#     --group ${input_topic}_consumers \\\\\\n#     --max-messages 1 \\\\\\n#     --timeout-ms 10000)\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export JAVA_HOME=/opt/java/openjdk",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "read kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "650"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1732"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386664399040"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425120920"
    }
  },
  "taskAppId" : "650_1732",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.014 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.014 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.014 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.031 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.032 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.032 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1732 check successfully
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.033 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.033 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.033 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.034 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.034 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "input_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  }, {
    "prop" : "output_topic",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\ntopics=\"reddit_post_processed_topic|reddit_post_sa_topic\"\n\n# parse the topics string into input and output topics\nIFS='|' read -r input_topic output_topic <<< \"$topics\"\n\n# Print the input and output topics\necho \"Input Topic: $input_topic\"\necho \"Output Topic: $output_topic\"\n\necho \"#{setValue(input_topic=${input_topic})}\"\necho \"#{setValue(output_topic=${output_topic})}\"\n\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n#     --bootstrap-server kafka:9092 \\\n#     --topic ${input_topic} \\\n#     --group ${input_topic}_consumers \\\n#     --max-messages 1 \\\n#     --timeout-ms 10000)\n\n",
  "resourceList" : [ ]
}
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.035 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.035 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.035 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.035 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.035 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.036 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.036 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.036 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export JAVA_HOME=/opt/java/openjdk
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

topics="reddit_post_processed_topic|reddit_post_sa_topic"

# parse the topics string into input and output topics
IFS='|' read -r input_topic output_topic <<< "$topics"

# Print the input and output topics
echo "Input Topic: $input_topic"
echo "Output Topic: $output_topic"

echo "#{setValue(input_topic=${input_topic})}"
echo "#{setValue(output_topic=${output_topic})}"

# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \
#     --bootstrap-server kafka:9092 \
#     --topic ${input_topic} \
#     --group ${input_topic}_consumers \
#     --max-messages 1 \
#     --timeout-ms 10000)


[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.036 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.037 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1732/650_1732.sh
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:21.044 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2437
[WI-0][TI-1732] - [INFO] 2024-04-25 12:09:21.941 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1732, success=true)
[WI-0][TI-1732] - [INFO] 2024-04-25 12:09:21.946 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1732)
[WI-0][TI-0] - [INFO] 2024-04-25 12:09:22.045 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Input Topic: reddit_post_processed_topic
	Output Topic: reddit_post_sa_topic
	#{setValue(input_topic=reddit_post_processed_topic)}
	#{setValue(output_topic=reddit_post_sa_topic)}
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:22.047 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1732, processId:2437 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:22.048 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:22.048 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:22.048 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:22.048 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:22.054 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:22.055 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:22.055 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1732
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:22.056 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1732
[WI-650][TI-1732] - [INFO] 2024-04-25 12:09:22.056 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1732] - [INFO] 2024-04-25 12:09:22.977 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1732, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 12:09:23.075 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1733, taskName=sentiment anaysis, firstSubmitTime=1714018163068, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.10:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=5, appIds=null, processInstanceId=650, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# Example usage with PySpark DataFrame\n# Assuming `df` is your DataFrame containing messages\n# and `message` is the column containing the messages\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1733'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425120923'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='650'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.076 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.078 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.080 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.080 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.080 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.080 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714018163080
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.080 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 650_1733
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.080 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1733,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714018163068,
  "startTime" : 1714018163080,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.10:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/5/650/1733.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 5,
  "processInstanceId" : 650,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# Example usage with PySpark DataFrame\\n# Assuming `df` is your DataFrame containing messages\\n# and `message` is the column containing the messages\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1733"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425120923"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "650"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "650_1733",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.084 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.084 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.084 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.087 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.087 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.098 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1733 check successfully
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.098 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.098 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.100 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.100 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.100 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# Example usage with PySpark DataFrame\n# Assuming `df` is your DataFrame containing messages\n# and `message` is the column containing the messages\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.101 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.101 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.101 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.101 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.101 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.101 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# Example usage with PySpark DataFrame
# Assuming `df` is your DataFrame containing messages
# and `message` is the column containing the messages
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.102 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1733
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.102 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1733/py_650_1733.py
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.102 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# Example usage with PySpark DataFrame
# Assuming `df` is your DataFrame containing messages
# and `message` is the column containing the messages
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.104 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.104 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.104 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1733/py_650_1733.py
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.104 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.104 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1733/650_1733.sh
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:23.111 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2448
[WI-0][TI-1733] - [INFO] 2024-04-25 12:09:23.981 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1733, success=true)
[WI-0][TI-1733] - [INFO] 2024-04-25 12:09:23.988 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1733)
[WI-0][TI-0] - [INFO] 2024-04-25 12:09:24.114 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1733/py_650_1733.py", line 29
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:24.116 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1733, processId:2448 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:24.117 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:24.117 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:24.117 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:24.118 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:24.122 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:24.123 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:24.123 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1733
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:24.123 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1733
[WI-650][TI-1733] - [INFO] 2024-04-25 12:09:24.124 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1733] - [INFO] 2024-04-25 12:09:24.983 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1733, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 12:09:25.008 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1734, taskName=sentiment anaysis, firstSubmitTime=1714018164994, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.10:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=5, appIds=null, processInstanceId=650, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# Example usage with PySpark DataFrame\n# Assuming `df` is your DataFrame containing messages\n# and `message` is the column containing the messages\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1734'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425120924'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='650'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.009 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.009 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.012 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.012 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.013 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.013 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714018165013
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.013 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 650_1734
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.014 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1734,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714018164994,
  "startTime" : 1714018165013,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.10:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/5/650/1734.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 5,
  "processInstanceId" : 650,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# Example usage with PySpark DataFrame\\n# Assuming `df` is your DataFrame containing messages\\n# and `message` is the column containing the messages\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1734"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425120924"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "650"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "650_1734",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.017 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.017 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.017 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.020 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.021 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.021 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1734 check successfully
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.022 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.022 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.022 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.023 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.025 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# Example usage with PySpark DataFrame\n# Assuming `df` is your DataFrame containing messages\n# and `message` is the column containing the messages\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.026 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.026 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.026 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.026 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.026 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.026 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# Example usage with PySpark DataFrame
# Assuming `df` is your DataFrame containing messages
# and `message` is the column containing the messages
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.027 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1734
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.028 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1734/py_650_1734.py
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.028 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# Example usage with PySpark DataFrame
# Assuming `df` is your DataFrame containing messages
# and `message` is the column containing the messages
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.029 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.029 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.029 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1734/py_650_1734.py
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.029 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.029 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1734/650_1734.sh
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:25.033 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2459
[WI-0][TI-1734] - [INFO] 2024-04-25 12:09:26.011 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1734, success=true)
[WI-0][TI-1734] - [INFO] 2024-04-25 12:09:26.024 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1734)
[WI-0][TI-0] - [INFO] 2024-04-25 12:09:26.034 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1734/py_650_1734.py", line 29
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:26.050 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1734, processId:2459 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:26.062 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:26.062 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:26.065 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:26.065 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:26.082 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:26.083 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:26.084 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1734
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:26.086 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1734
[WI-650][TI-1734] - [INFO] 2024-04-25 12:09:26.088 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1734] - [INFO] 2024-04-25 12:09:26.988 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1734, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 12:09:27.094 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1735, taskName=sentiment anaysis, firstSubmitTime=1714018167033, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.10:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=5, appIds=null, processInstanceId=650, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# Example usage with PySpark DataFrame\n# Assuming `df` is your DataFrame containing messages\n# and `message` is the column containing the messages\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1735'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425120927'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='650'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.096 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.098 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.100 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.100 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.100 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.100 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714018167100
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.100 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 650_1735
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.101 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1735,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714018167033,
  "startTime" : 1714018167100,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.10:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/5/650/1735.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 5,
  "processInstanceId" : 650,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# Example usage with PySpark DataFrame\\n# Assuming `df` is your DataFrame containing messages\\n# and `message` is the column containing the messages\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1735"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425120927"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "650"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "650_1735",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.102 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.102 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.102 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.109 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.110 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.110 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1735 check successfully
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.111 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.111 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.111 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.111 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.111 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# Example usage with PySpark DataFrame\n# Assuming `df` is your DataFrame containing messages\n# and `message` is the column containing the messages\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.119 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.120 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.120 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.120 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.120 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.120 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# Example usage with PySpark DataFrame
# Assuming `df` is your DataFrame containing messages
# and `message` is the column containing the messages
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.121 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1735
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.121 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1735/py_650_1735.py
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.121 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# Example usage with PySpark DataFrame
# Assuming `df` is your DataFrame containing messages
# and `message` is the column containing the messages
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.123 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.124 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.124 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1735/py_650_1735.py
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.127 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.128 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1735/650_1735.sh
[WI-0][TI-0] - [INFO] 2024-04-25 12:09:27.152 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:27.152 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2470
[WI-0][TI-1735] - [INFO] 2024-04-25 12:09:27.999 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1735, success=true)
[WI-0][TI-1735] - [INFO] 2024-04-25 12:09:28.007 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1735)
[WI-0][TI-0] - [INFO] 2024-04-25 12:09:28.016 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8394366197183099 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 12:09:28.164 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1735/py_650_1735.py", line 29
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:28.168 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1735, processId:2470 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:28.168 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:28.170 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:28.171 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:28.174 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:28.178 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:28.179 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:28.179 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1735
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:28.180 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1735
[WI-650][TI-1735] - [INFO] 2024-04-25 12:09:28.180 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1735] - [INFO] 2024-04-25 12:09:28.993 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1735, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 12:09:29.090 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1736, taskName=sentiment anaysis, firstSubmitTime=1714018169072, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.10:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=5, appIds=null, processInstanceId=650, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# Example usage with PySpark DataFrame\n# Assuming `df` is your DataFrame containing messages\n# and `message` is the column containing the messages\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={output_topic=Property{prop='output_topic', direct=IN, type=VARCHAR, value='reddit_post_sa_topic'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment anaysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, input_topic=Property{prop='input_topic', direct=IN, type=VARCHAR, value='reddit_post_processed_topic'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240425'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1736'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240425120929'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='650'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240424'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.092 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment anaysis to wait queue success
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.093 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.097 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.097 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.097 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.097 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714018169097
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.097 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 650_1736
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.099 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1736,
  "taskName" : "sentiment anaysis",
  "firstSubmitTime" : 1714018169072,
  "startTime" : 1714018169097,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.10:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240425/13386218435520/5/650/1736.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 5,
  "processInstanceId" : 650,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql.functions import udf, split\\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\n\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .getOrCreate()\\n\\n# download the model\\nnltk.download('vader_lexicon')\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# Example usage with PySpark DataFrame\\n# Assuming `df` is your DataFrame containing messages\\n# and `message` is the column containing the messages\\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\\n\\n# split the sentiment_score column into separate columns\\n            \\nsa_reddit_df = df.withColumn(\\\"negative\\\", df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# Show the DataFrame with the sentiment scores\\nsa_reddit_df.show()\\n\\n# send to repective kafka topic\\nsa_reddit_json = sa_reddit_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"${output_topic}\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nspark \\\\\\n    .createDataFrame(sa_reddit_json, StringType()) \\\\\\n    .write \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .options(**kafka_params) \\\\\\n    .save()\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "output_topic" : {
      "prop" : "output_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_sa_topic"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment anaysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "input_topic" : {
      "prop" : "input_topic",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "reddit_post_processed_topic"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1736"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240425120929"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "650"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "650_1736",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"output_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_sa_topic\"},{\"prop\":\"input_topic\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"reddit_post_processed_topic\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.101 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.101 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.101 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.111 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.111 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.112 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1736 check successfully
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.112 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.112 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.113 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.113 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.114 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import FloatType , ArrayType, StringType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\n\n\n# init Spark session\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis\") \\\n    .master(\"local\") \\\n    .getOrCreate()\n\n# download the model\nnltk.download('vader_lexicon')\n\n# init the sentiment analysis object\nsia = SIA()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# Example usage with PySpark DataFrame\n# Assuming `df` is your DataFrame containing messages\n# and `message` is the column containing the messages\ndf = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))\n\n# split the sentiment_score column into separate columns\n            \nsa_reddit_df = df.withColumn(\"negative\", df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# Show the DataFrame with the sentiment scores\nsa_reddit_df.show()\n\n# send to repective kafka topic\nsa_reddit_json = sa_reddit_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"${output_topic}\"  # Kafka topic name\n}\n\n# write data to Kafka\nspark \\\n    .createDataFrame(sa_reddit_json, StringType()) \\\n    .write \\\n    .format(\"kafka\") \\\n    .options(**kafka_params) \\\n    .save()",
  "resourceList" : [ ]
}
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.114 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.114 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"output_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_sa_topic"},{"prop":"input_topic","direct":"IN","type":"VARCHAR","value":"reddit_post_processed_topic"}] successfully
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.114 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.114 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.114 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.114 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# Example usage with PySpark DataFrame
# Assuming `df` is your DataFrame containing messages
# and `message` is the column containing the messages
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "${output_topic}"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.115 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1736
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.115 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1736/py_650_1736.py
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.115 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

from pyspark.sql.functions import udf, split
from pyspark.sql.types import FloatType , ArrayType, StringType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession


# init Spark session
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .master("local") \
    .getOrCreate()

# download the model
nltk.download('vader_lexicon')

# init the sentiment analysis object
sia = SIA()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# Example usage with PySpark DataFrame
# Assuming `df` is your DataFrame containing messages
# and `message` is the column containing the messages
df = df.withColumn('sentiment_score', analyze_sentiment_udf(df['content']))

# split the sentiment_score column into separate columns
            
sa_reddit_df = df.withColumn("negative", df["sentiment_score"][0]) \
            .withColumn("neutral", df["sentiment_score"][1]) \
            .withColumn("positive", df["sentiment_score"][2]) \
            .withColumn("compound", df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# Show the DataFrame with the sentiment scores
sa_reddit_df.show()

# send to repective kafka topic
sa_reddit_json = sa_reddit_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_post_sa_topic"  # Kafka topic name
}

# write data to Kafka
spark \
    .createDataFrame(sa_reddit_json, StringType()) \
    .write \
    .format("kafka") \
    .options(**kafka_params) \
    .save()
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.116 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.117 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.117 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1736/py_650_1736.py
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.117 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.117 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1736/650_1736.sh
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:29.122 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2481
[WI-0][TI-1736] - [INFO] 2024-04-25 12:09:29.998 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1736, success=true)
[WI-0][TI-1736] - [INFO] 2024-04-25 12:09:30.006 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1736)
[WI-0][TI-0] - [INFO] 2024-04-25 12:09:30.041 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.75 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 12:09:30.123 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1736/py_650_1736.py", line 29
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:30.124 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1736, processId:2481 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:30.125 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:30.126 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:30.126 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:30.127 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:30.129 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:30.130 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:30.130 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1736
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:30.131 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_5/650/1736
[WI-650][TI-1736] - [INFO] 2024-04-25 12:09:30.132 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1736] - [INFO] 2024-04-25 12:09:31.005 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1736, success=true)
[WI-0][TI-0] - [INFO] 2024-04-25 12:19:52.987 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.752808988764045 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 12:20:53.204 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8787878787878788 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 12:20:55.276 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8222222222222222 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 12:21:06.362 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7114285714285715 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 12:28:02.471 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7289972899728997 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 12:29:38.830 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.732394366197183 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 12:29:39.838 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.784741144414169 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 12:29:41.869 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.713527851458886 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [WARN] 2024-04-25 12:32:01.633 +0800 o.s.c.k.c.p.AbstractKubernetesProfileEnvironmentPostProcessor:[258] - Not running inside kubernetes. Skipping 'kubernetes' profile activation.
[WI-0][TI-0] - [WARN] 2024-04-25 12:32:05.074 +0800 o.s.c.k.c.p.AbstractKubernetesProfileEnvironmentPostProcessor:[258] - Not running inside kubernetes. Skipping 'kubernetes' profile activation.
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:05.122 +0800 o.a.d.s.w.WorkerServer:[634] - No active profile set, falling back to 1 default profile: "default"
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:13.100 +0800 o.s.c.c.s.GenericScope:[283] - BeanFactory id=7e7bf7c6-2cb3-34d2-a0d5-a56161c67d0e
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:15.091 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'org.springframework.cloud.commons.config.CommonsConfigAutoConfiguration' of type [org.springframework.cloud.commons.config.CommonsConfigAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:15.197 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'org.springframework.cloud.client.loadbalancer.LoadBalancerDefaultMappingsProviderAutoConfiguration' of type [org.springframework.cloud.client.loadbalancer.LoadBalancerDefaultMappingsProviderAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:15.201 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'loadBalancerClientsDefaultsMappingsProvider' of type [org.springframework.cloud.client.loadbalancer.LoadBalancerDefaultMappingsProviderAutoConfiguration$$Lambda$392/302905744] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:15.219 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'defaultsBindHandlerAdvisor' of type [org.springframework.cloud.commons.config.DefaultsBindHandlerAdvisor] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:15.253 +0800 o.a.d.c.u.NetUtils:[312] - Get all NetworkInterfaces: [name:eth0 (eth0), name:lo (lo)]
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:15.401 +0800 o.a.d.s.w.c.WorkerConfig:[98] - 
****************************Worker Configuration**************************************
  listen-port -> 1234
  exec-threads -> 100
  max-heartbeat-interval -> PT10S
  host-weight -> 100
  tenantConfig -> TenantConfig(autoCreateTenantEnabled=true, distributedTenantEnabled=false, defaultTenantEnabled=false)
  server-load-protection -> WorkerServerLoadProtection(enabled=true, maxCpuUsagePercentageThresholds=0.7, maxJVMMemoryUsagePercentageThresholds=0.7, maxSystemMemoryUsagePercentageThresholds=0.7, maxDiskUsagePercentageThresholds=0.7)
  registry-disconnect-strategy -> ConnectStrategyProperties(strategy=WAITING, maxWaitingTime=PT1M40S)
  task-execute-threads-full-policy: REJECT
  address -> 172.18.1.1:1234
  registry-path: /nodes/worker/172.18.1.1:1234
****************************Worker Configuration**************************************
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:15.402 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'workerConfig' of type [org.apache.dolphinscheduler.server.worker.config.WorkerConfig$$EnhancerBySpringCGLIB$$da954724] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:15.855 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'io.kubernetes.client.spring.extended.manifests.config.KubernetesManifestsAutoConfiguration' of type [io.kubernetes.client.spring.extended.manifests.config.KubernetesManifestsAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:15.967 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'kubernetes.manifests-io.kubernetes.client.spring.extended.manifests.config.KubernetesManifestsProperties' of type [io.kubernetes.client.spring.extended.manifests.config.KubernetesManifestsProperties] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:15.979 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'io.kubernetes.client.spring.extended.controller.config.KubernetesInformerAutoConfiguration' of type [io.kubernetes.client.spring.extended.controller.config.KubernetesInformerAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:16.075 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'defaultApiClient' of type [io.kubernetes.client.openapi.ApiClient] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:16.420 +0800 o.e.j.u.log:[170] - Logging initialized @23926ms to org.eclipse.jetty.util.log.Slf4jLog
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:17.591 +0800 o.s.b.w.e.j.JettyServletWebServerFactory:[166] - Server initialized with port: 1235
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:17.626 +0800 o.e.j.s.Server:[375] - jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 1.8.0_402-b06
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:18.429 +0800 o.e.j.s.h.C.application:[2368] - Initializing Spring embedded WebApplicationContext
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:18.510 +0800 o.s.b.w.s.c.ServletWebServerApplicationContext:[292] - Root WebApplicationContext: initialization completed in 13369 ms
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.090 +0800 o.e.j.s.session:[334] - DefaultSessionIdManager workerName=node0
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.125 +0800 o.e.j.s.session:[339] - No SessionScavenger set, using defaults
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.140 +0800 o.e.j.s.session:[132] - node0 Scavenging every 660000ms
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.190 +0800 o.e.j.s.h.ContextHandler:[921] - Started o.s.b.w.e.j.JettyEmbeddedWebAppContext@52433946{application,/,[file:///tmp/jetty-docbase.1235.2100819646468438913/],AVAILABLE}
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.286 +0800 o.e.j.s.Server:[415] - Started @28792ms
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.693 +0800 o.a.c.f.i.CuratorFrameworkImpl:[338] - Starting
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.725 +0800 o.a.z.ZooKeeper:[98] - Client environment:zookeeper.version=3.8.0-5a02a05eddb59aee6ac762f7ea82e92a68eb9c0f, built on 2022-02-25 08:49 UTC
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.726 +0800 o.a.z.ZooKeeper:[98] - Client environment:host.name=15dedf891bed
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.727 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.version=1.8.0_402
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.728 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.vendor=Temurin
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.729 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.home=/opt/java/openjdk
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.730 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.class.path=/opt/dolphinscheduler/conf:/opt/dolphinscheduler/libs/kerb-client-1.0.1.jar:/opt/dolphinscheduler/libs/spring-cloud-starter-kubernetes-client-config-2.1.3.jar:/opt/dolphinscheduler/libs/oauth2-oidc-sdk-9.35.jar:/opt/dolphinscheduler/libs/jsqlparser-4.4.jar:/opt/dolphinscheduler/libs/reactive-streams-1.0.4.jar:/opt/dolphinscheduler/libs/kubernetes-model-extensions-5.10.2.jar:/opt/dolphinscheduler/libs/zookeeper-3.8.0.jar:/opt/dolphinscheduler/libs/kubernetes-model-apps-5.10.2.jar:/opt/dolphinscheduler/libs/grpc-api-1.41.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-pigeon-3.2.1.jar:/opt/dolphinscheduler/libs/client-java-api-13.0.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-obs-3.2.1.jar:/opt/dolphinscheduler/libs/spring-web-5.3.22.jar:/opt/dolphinscheduler/libs/aws-java-sdk-emr-1.12.300.jar:/opt/dolphinscheduler/libs/spring-context-5.3.22.jar:/opt/dolphinscheduler/libs/gapic-google-cloud-storage-v2-2.18.0-alpha.jar:/opt/dolphinscheduler/libs/spring-cloud-kubernetes-client-config-2.1.3.jar:/opt/dolphinscheduler/libs/jetty-io-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/annotations-13.0.jar:/opt/dolphinscheduler/libs/jpam-1.1.jar:/opt/dolphinscheduler/libs/joda-time-2.10.13.jar:/opt/dolphinscheduler/libs/spring-cloud-kubernetes-client-autoconfig-2.1.3.jar:/opt/dolphinscheduler/libs/kerb-identity-1.0.1.jar:/opt/dolphinscheduler/libs/vertica-jdbc-12.0.4-0.jar:/opt/dolphinscheduler/libs/grpc-googleapis-1.52.1.jar:/opt/dolphinscheduler/libs/aliyun-java-sdk-kms-2.11.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-dvc-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-hana-3.2.1.jar:/opt/dolphinscheduler/libs/kubernetes-httpclient-okhttp-6.0.0.jar:/opt/dolphinscheduler/libs/jline-2.12.jar:/opt/dolphinscheduler/libs/sshd-core-2.8.0.jar:/opt/dolphinscheduler/libs/jna-platform-5.10.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-sqlserver-3.2.1.jar:/opt/dolphinscheduler/libs/commons-beanutils-1.9.4.jar:/opt/dolphinscheduler/libs/grpc-services-1.41.0.jar:/opt/dolphinscheduler/libs/jmespath-java-1.12.300.jar:/opt/dolphinscheduler/libs/curator-client-5.3.0.jar:/opt/dolphinscheduler/libs/proto-google-common-protos-2.0.1.jar:/opt/dolphinscheduler/libs/mybatis-3.5.10.jar:/opt/dolphinscheduler/libs/jackson-annotations-2.13.4.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-all-3.2.1.jar:/opt/dolphinscheduler/libs/jakarta.xml.bind-api-2.3.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-jupyter-3.2.1.jar:/opt/dolphinscheduler/libs/mybatis-plus-extension-3.5.2.jar:/opt/dolphinscheduler/libs/j2objc-annotations-1.3.jar:/opt/dolphinscheduler/libs/Java-WebSocket-1.5.1.jar:/opt/dolphinscheduler/libs/azure-storage-common-12.20.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-sagemaker-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-databend-3.2.1.jar:/opt/dolphinscheduler/libs/google-auth-library-oauth2-http-1.15.0.jar:/opt/dolphinscheduler/libs/jetty-security-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-python-3.2.1.jar:/opt/dolphinscheduler/libs/snappy-java-1.1.10.1.jar:/opt/dolphinscheduler/libs/kubernetes-model-batch-5.10.2.jar:/opt/dolphinscheduler/libs/netty-transport-native-epoll-4.1.53.Final-linux-x86_64.jar:/opt/dolphinscheduler/libs/jackson-core-asl-1.9.13.jar:/opt/dolphinscheduler/libs/gson-fire-1.8.5.jar:/opt/dolphinscheduler/libs/clickhouse-jdbc-0.4.6.jar:/opt/dolphinscheduler/libs/grpc-netty-shaded-1.41.0.jar:/opt/dolphinscheduler/libs/caffeine-2.9.3.jar:/opt/dolphinscheduler/libs/aliyun-java-sdk-core-4.5.10.jar:/opt/dolphinscheduler/libs/jackson-module-jaxb-annotations-2.13.3.jar:/opt/dolphinscheduler/libs/jetty-http-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/jersey-servlet-1.19.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-dinky-3.2.1.jar:/opt/dolphinscheduler/libs/simpleclient_tracer_common-0.15.0.jar:/opt/dolphinscheduler/libs/netty-handler-4.1.53.Final.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-datafactory-3.2.1.jar:/opt/dolphinscheduler/libs/json-path-2.7.0.jar:/opt/dolphinscheduler/libs/mybatis-plus-annotation-3.5.2.jar:/opt/dolphinscheduler/libs/azure-resourcemanager-datafactory-1.0.0-beta.19.jar:/opt/dolphinscheduler/libs/commons-codec-1.11.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-master-3.2.1.jar:/opt/dolphinscheduler/libs/spring-aop-5.3.22.jar:/opt/dolphinscheduler/libs/kubernetes-model-core-5.10.2.jar:/opt/dolphinscheduler/libs/kubernetes-model-networking-5.10.2.jar:/opt/dolphinscheduler/libs/kotlin-stdlib-jdk7-1.6.21.jar:/opt/dolphinscheduler/libs/kerby-xdr-1.0.1.jar:/opt/dolphinscheduler/libs/aliyun-java-sdk-ram-3.1.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-k8s-3.2.1.jar:/opt/dolphinscheduler/libs/guava-31.1-jre.jar:/opt/dolphinscheduler/libs/netty-codec-dns-4.1.53.Final.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-kubeflow-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-azure-sql-3.2.1.jar:/opt/dolphinscheduler/libs/HikariCP-4.0.3.jar:/opt/dolphinscheduler/libs/hive-metastore-2.3.9.jar:/opt/dolphinscheduler/libs/msal4j-1.13.3.jar:/opt/dolphinscheduler/libs/jackson-core-2.13.4.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-hive-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-mr-3.2.1.jar:/opt/dolphinscheduler/libs/kubernetes-model-node-5.10.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-common-3.2.1.jar:/opt/dolphinscheduler/libs/jose4j-0.7.8.jar:/opt/dolphinscheduler/libs/jul-to-slf4j-1.7.36.jar:/opt/dolphinscheduler/libs/azure-identity-1.7.1.jar:/opt/dolphinscheduler/libs/spring-boot-autoconfigure-2.7.3.jar:/opt/dolphinscheduler/libs/commons-math3-3.1.1.jar:/opt/dolphinscheduler/libs/jackson-jaxrs-json-provider-2.13.3.jar:/opt/dolphinscheduler/libs/perfmark-api-0.23.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-doris-3.2.1.jar:/opt/dolphinscheduler/libs/httpclient-4.5.13.jar:/opt/dolphinscheduler/libs/hive-service-2.3.9.jar:/opt/dolphinscheduler/libs/kerby-util-1.0.1.jar:/opt/dolphinscheduler/libs/commons-collections-3.2.2.jar:/opt/dolphinscheduler/libs/hadoop-yarn-client-3.2.4.jar:/opt/dolphinscheduler/libs/commons-text-1.8.jar:/opt/dolphinscheduler/libs/dolphinscheduler-worker-3.2.1.jar:/opt/dolphinscheduler/libs/hadoop-yarn-common-3.2.4.jar:/opt/dolphinscheduler/libs/zeppelin-client-0.10.1.jar:/opt/dolphinscheduler/libs/azure-core-management-1.10.1.jar:/opt/dolphinscheduler/libs/hadoop-mapreduce-client-jobclient-3.2.4.jar:/opt/dolphinscheduler/libs/jta-1.1.jar:/opt/dolphinscheduler/libs/annotations-4.1.1.4.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-alert-3.2.1.jar:/opt/dolphinscheduler/libs/curator-framework-5.3.0.jar:/opt/dolphinscheduler/libs/hive-jdbc-2.3.9.jar:/opt/dolphinscheduler/libs/kyuubi-hive-jdbc-shaded-1.7.0.jar:/opt/dolphinscheduler/libs/client-java-proto-13.0.2.jar:/opt/dolphinscheduler/libs/checker-qual-3.19.0.jar:/opt/dolphinscheduler/libs/jetty-servlet-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/google-cloud-core-grpc-2.10.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-shell-3.2.1.jar:/opt/dolphinscheduler/libs/spring-security-rsa-1.0.10.RELEASE.jar:/opt/dolphinscheduler/libs/avro-1.7.7.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-postgresql-3.2.1.jar:/opt/dolphinscheduler/libs/netty-nio-client-2.17.282.jar:/opt/dolphinscheduler/libs/spring-webmvc-5.3.22.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-mysql-3.2.1.jar:/opt/dolphinscheduler/libs/opentracing-util-0.33.0.jar:/opt/dolphinscheduler/libs/auto-value-1.10.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-all-3.2.1.jar:/opt/dolphinscheduler/libs/jetcd-core-0.5.11.jar:/opt/dolphinscheduler/libs/grpc-context-1.41.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-datasync-3.2.1.jar:/opt/dolphinscheduler/libs/jackson-dataformat-cbor-2.13.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-gcs-3.2.1.jar:/opt/dolphinscheduler/libs/client-java-extended-13.0.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-redshift-3.2.1.jar:/opt/dolphinscheduler/libs/opencsv-2.3.jar:/opt/dolphinscheduler/libs/jcip-annotations-1.0-1.jar:/opt/dolphinscheduler/libs/accessors-smart-2.4.8.jar:/opt/dolphinscheduler/libs/hadoop-client-3.2.4.jar:/opt/dolphinscheduler/libs/commons-collections4-4.3.jar:/opt/dolphinscheduler/libs/metrics-core-4.2.11.jar:/opt/dolphinscheduler/libs/netty-resolver-4.1.53.Final.jar:/opt/dolphinscheduler/libs/parquet-hadoop-bundle-1.8.1.jar:/opt/dolphinscheduler/libs/bucket4j-core-6.2.0.jar:/opt/dolphinscheduler/libs/spring-cloud-starter-3.1.3.jar:/opt/dolphinscheduler/libs/mssql-jdbc-11.2.1.jre8.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/dolphinscheduler/libs/kubernetes-model-coordination-5.10.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-linkis-3.2.1.jar:/opt/dolphinscheduler/libs/commons-net-3.6.jar:/opt/dolphinscheduler/libs/netty-transport-native-kqueue-4.1.53.Final-osx-x86_64.jar:/opt/dolphinscheduler/libs/dolphinscheduler-meter-3.2.1.jar:/opt/dolphinscheduler/libs/kubernetes-client-api-6.0.0.jar:/opt/dolphinscheduler/libs/kubernetes-model-certificates-5.10.2.jar:/opt/dolphinscheduler/libs/opencensus-api-0.31.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-mlflow-3.2.1.jar:/opt/dolphinscheduler/libs/kotlin-stdlib-jdk8-1.6.21.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-jdbc-3.2.1.jar:/opt/dolphinscheduler/libs/audience-annotations-0.12.0.jar:/opt/dolphinscheduler/libs/simpleclient_httpserver-0.15.0.jar:/opt/dolphinscheduler/libs/jetty-xml-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/kerby-asn1-1.0.1.jar:/opt/dolphinscheduler/libs/lang-tag-1.6.jar:/opt/dolphinscheduler/libs/api-common-2.6.0.jar:/opt/dolphinscheduler/libs/HdrHistogram-2.1.12.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-abs-3.2.1.jar:/opt/dolphinscheduler/libs/failsafe-2.4.4.jar:/opt/dolphinscheduler/libs/hbase-noop-htrace-4.1.1.jar:/opt/dolphinscheduler/libs/jamon-runtime-2.3.1.jar:/opt/dolphinscheduler/libs/jetty-util-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/threetenbp-1.6.5.jar:/opt/dolphinscheduler/libs/jakarta.activation-api-1.2.2.jar:/opt/dolphinscheduler/libs/kubernetes-model-common-5.10.2.jar:/opt/dolphinscheduler/libs/okhttp-4.9.3.jar:/opt/dolphinscheduler/libs/profiles-2.17.282.jar:/opt/dolphinscheduler/libs/hadoop-auth-3.2.4.jar:/opt/dolphinscheduler/libs/grpc-netty-1.41.0.jar:/opt/dolphinscheduler/libs/httpcore-nio-4.4.15.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-seatunnel-3.2.1.jar:/opt/dolphinscheduler/libs/aws-java-sdk-sagemaker-1.12.300.jar:/opt/dolphinscheduler/libs/oshi-core-6.1.1.jar:/opt/dolphinscheduler/libs/bonecp-0.8.0.RELEASE.jar:/opt/dolphinscheduler/libs/jackson-module-parameter-names-2.13.3.jar:/opt/dolphinscheduler/libs/bcutil-jdk15on-1.69.jar:/opt/dolphinscheduler/libs/aws-json-protocol-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-base-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-all-3.2.1.jar:/opt/dolphinscheduler/libs/derby-10.14.2.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-snowflake-3.2.1.jar:/opt/dolphinscheduler/libs/netty-codec-http2-4.1.53.Final.jar:/opt/dolphinscheduler/libs/jetty-continuation-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/jackson-mapper-asl-1.9.13.jar:/opt/dolphinscheduler/libs/datanucleus-core-4.1.17.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/dolphinscheduler/libs/failureaccess-1.0.1.jar:/opt/dolphinscheduler/libs/error_prone_annotations-2.5.1.jar:/opt/dolphinscheduler/libs/kerb-admin-1.0.1.jar:/opt/dolphinscheduler/libs/token-provider-1.0.1.jar:/opt/dolphinscheduler/libs/reactor-netty-core-1.0.22.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-all-3.2.1.jar:/opt/dolphinscheduler/libs/jakarta.servlet-api-4.0.4.jar:/opt/dolphinscheduler/libs/http-client-spi-2.17.282.jar:/opt/dolphinscheduler/libs/regions-2.17.282.jar:/opt/dolphinscheduler/libs/logback-core-1.2.11.jar:/opt/dolphinscheduler/libs/json-1.8.jar:/opt/dolphinscheduler/libs/gax-grpc-2.23.0.jar:/opt/dolphinscheduler/libs/google-http-client-1.42.3.jar:/opt/dolphinscheduler/libs/hive-serde-2.3.9.jar:/opt/dolphinscheduler/libs/jettison-1.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-http-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-zookeeper-3.2.1.jar:/opt/dolphinscheduler/libs/spring-security-crypto-5.7.3.jar:/opt/dolphinscheduler/libs/auth-2.17.282.jar:/opt/dolphinscheduler/libs/proto-google-cloud-storage-v2-2.18.0-alpha.jar:/opt/dolphinscheduler/libs/jetty-server-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/javax.annotation-api-1.3.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-starrocks-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-dataquality-3.2.1.jar:/opt/dolphinscheduler/libs/jcl-over-slf4j-1.7.36.jar:/opt/dolphinscheduler/libs/commons-lang3-3.12.0.jar:/opt/dolphinscheduler/libs/tomcat-embed-el-9.0.65.jar:/opt/dolphinscheduler/libs/opentracing-noop-0.33.0.jar:/opt/dolphinscheduler/libs/client-java-13.0.2.jar:/opt/dolphinscheduler/libs/spring-beans-5.3.22.jar:/opt/dolphinscheduler/libs/snakeyaml-1.33.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-ssh-3.2.1.jar:/opt/dolphinscheduler/libs/commons-pool-1.6.jar:/opt/dolphinscheduler/libs/javax.servlet-api-3.1.0.jar:/opt/dolphinscheduler/libs/hadoop-common-3.2.4.jar:/opt/dolphinscheduler/libs/kubernetes-model-apiextensions-5.10.2.jar:/opt/dolphinscheduler/libs/spring-boot-2.7.3.jar:/opt/dolphinscheduler/libs/bcprov-ext-jdk15on-1.69.jar:/opt/dolphinscheduler/libs/spring-boot-starter-2.7.3.jar:/opt/dolphinscheduler/libs/paranamer-2.3.jar:/opt/dolphinscheduler/libs/httpmime-4.5.13.jar:/opt/dolphinscheduler/libs/reactor-core-3.4.22.jar:/opt/dolphinscheduler/libs/azure-core-1.36.0.jar:/opt/dolphinscheduler/libs/bcpkix-jdk15on-1.69.jar:/opt/dolphinscheduler/libs/kubernetes-model-scheduling-5.10.2.jar:/opt/dolphinscheduler/libs/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/dolphinscheduler/libs/websocket-client-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/unirest-java-3.7.04-standalone.jar:/opt/dolphinscheduler/libs/hadoop-mapreduce-client-core-3.2.4.jar:/opt/dolphinscheduler/libs/google-auth-library-credentials-1.15.0.jar:/opt/dolphinscheduler/libs/azure-storage-internal-avro-12.6.0.jar:/opt/dolphinscheduler/libs/jackson-datatype-jdk8-2.13.3.jar:/opt/dolphinscheduler/libs/spring-expression-5.3.22.jar:/opt/dolphinscheduler/libs/aspectjweaver-1.9.7.jar:/opt/dolphinscheduler/libs/websocket-common-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/client-java-api-fluent-13.0.2.jar:/opt/dolphinscheduler/libs/mybatis-plus-3.5.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-athena-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-oss-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-openmldb-3.2.1.jar:/opt/dolphinscheduler/libs/curator-recipes-5.3.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-api-3.2.1.jar:/opt/dolphinscheduler/libs/netty-all-4.1.53.Final.jar:/opt/dolphinscheduler/libs/netty-resolver-dns-4.1.53.Final.jar:/opt/dolphinscheduler/libs/kubernetes-model-autoscaling-5.10.2.jar:/opt/dolphinscheduler/libs/kerb-util-1.0.1.jar:/opt/dolphinscheduler/libs/grpc-alts-1.41.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-presto-3.2.1.jar:/opt/dolphinscheduler/libs/kerb-simplekdc-1.0.1.jar:/opt/dolphinscheduler/libs/protobuf-java-util-3.17.2.jar:/opt/dolphinscheduler/libs/azure-core-http-netty-1.13.0.jar:/opt/dolphinscheduler/libs/transaction-api-1.1.jar:/opt/dolphinscheduler/libs/datanucleus-api-jdo-4.2.4.jar:/opt/dolphinscheduler/libs/zeppelin-common-0.10.1.jar:/opt/dolphinscheduler/libs/zt-zip-1.15.jar:/opt/dolphinscheduler/libs/animal-sniffer-annotations-1.19.jar:/opt/dolphinscheduler/libs/google-http-client-jackson2-1.42.3.jar:/opt/dolphinscheduler/libs/netty-buffer-4.1.53.Final.jar:/opt/dolphinscheduler/libs/jsr305-3.0.0.jar:/opt/dolphinscheduler/libs/netty-transport-classes-epoll-4.1.79.Final.jar:/opt/dolphinscheduler/libs/spring-boot-actuator-autoconfigure-2.7.3.jar:/opt/dolphinscheduler/libs/simpleclient-0.15.0.jar:/opt/dolphinscheduler/libs/aliyun-sdk-oss-3.15.1.jar:/opt/dolphinscheduler/libs/json-utils-2.17.282.jar:/opt/dolphinscheduler/libs/tephra-api-0.6.0.jar:/opt/dolphinscheduler/libs/spring-jdbc-5.3.22.jar:/opt/dolphinscheduler/libs/grpc-xds-1.41.0.jar:/opt/dolphinscheduler/libs/logback-classic-1.2.11.jar:/opt/dolphinscheduler/libs/google-cloud-storage-2.18.0.jar:/opt/dolphinscheduler/libs/micrometer-registry-prometheus-1.9.3.jar:/opt/dolphinscheduler/libs/grpc-core-1.41.0.jar:/opt/dolphinscheduler/libs/aws-java-sdk-kms-1.12.300.jar:/opt/dolphinscheduler/libs/kubernetes-model-rbac-5.10.2.jar:/opt/dolphinscheduler/libs/ini4j-0.5.4.jar:/opt/dolphinscheduler/libs/spring-boot-starter-web-2.7.3.jar:/opt/dolphinscheduler/libs/spring-cloud-commons-3.1.3.jar:/opt/dolphinscheduler/libs/netty-codec-http-4.1.53.Final.jar:/opt/dolphinscheduler/libs/jetty-client-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/jetcd-common-0.5.11.jar:/opt/dolphinscheduler/libs/metrics-spi-2.17.282.jar:/opt/dolphinscheduler/libs/utils-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-emr-3.2.1.jar:/opt/dolphinscheduler/libs/protocol-core-2.17.282.jar:/opt/dolphinscheduler/libs/micrometer-core-1.9.3.jar:/opt/dolphinscheduler/libs/netty-transport-native-unix-common-4.1.53.Final.jar:/opt/dolphinscheduler/libs/zjsonpatch-0.4.11.jar:/opt/dolphinscheduler/libs/annotations-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-sql-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-common-3.2.1.jar:/opt/dolphinscheduler/libs/apache-client-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-oceanbase-3.2.1.jar:/opt/dolphinscheduler/libs/jdo-api-3.0.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-datax-3.2.1.jar:/opt/dolphinscheduler/libs/postgresql-42.4.1.jar:/opt/dolphinscheduler/libs/jetty-util-ajax-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/gax-2.23.0.jar:/opt/dolphinscheduler/libs/grpc-stub-1.41.0.jar:/opt/dolphinscheduler/libs/libfb303-0.9.3.jar:/opt/dolphinscheduler/libs/spring-core-5.3.22.jar:/opt/dolphinscheduler/libs/jaxb-api-2.3.1.jar:/opt/dolphinscheduler/libs/hive-service-rpc-2.3.9.jar:/opt/dolphinscheduler/libs/kubernetes-model-flowcontrol-5.10.2.jar:/opt/dolphinscheduler/libs/commons-logging-1.1.1.jar:/opt/dolphinscheduler/libs/mybatis-spring-2.0.7.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-oracle-3.2.1.jar:/opt/dolphinscheduler/libs/opencensus-proto-0.2.0.jar:/opt/dolphinscheduler/libs/grpc-protobuf-1.41.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-clickhouse-3.2.1.jar:/opt/dolphinscheduler/libs/spring-cloud-starter-bootstrap-3.1.3.jar:/opt/dolphinscheduler/libs/kubernetes-model-admissionregistration-5.10.2.jar:/opt/dolphinscheduler/libs/grpc-google-cloud-storage-v2-2.18.0-alpha.jar:/opt/dolphinscheduler/libs/esdk-obs-java-bundle-3.23.3.jar:/opt/dolphinscheduler/libs/dnsjava-2.1.7.jar:/opt/dolphinscheduler/libs/asm-9.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-spark-3.2.1.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/dolphinscheduler/libs/re2j-1.6.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-vertica-3.2.1.jar:/opt/dolphinscheduler/libs/json-smart-2.4.8.jar:/opt/dolphinscheduler/libs/reactor-netty-http-1.0.22.jar:/opt/dolphinscheduler/libs/google-api-services-storage-v1-rev20220705-2.0.0.jar:/opt/dolphinscheduler/libs/kubernetes-client-6.0.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-kyuubi-3.2.1.jar:/opt/dolphinscheduler/libs/auto-value-annotations-1.10.1.jar:/opt/dolphinscheduler/libs/commons-cli-1.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-data-quality-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-chunjun-3.2.1.jar:/opt/dolphinscheduler/libs/kerb-server-1.0.1.jar:/opt/dolphinscheduler/libs/content-type-2.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-remoteshell-3.2.1.jar:/opt/dolphinscheduler/libs/opencensus-contrib-http-util-0.31.1.jar:/opt/dolphinscheduler/libs/druid-1.2.20.jar:/opt/dolphinscheduler/libs/javolution-5.5.1.jar:/opt/dolphinscheduler/libs/protobuf-java-3.17.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-db2-3.2.1.jar:/opt/dolphinscheduler/libs/aws-java-sdk-core-1.12.300.jar:/opt/dolphinscheduler/libs/spring-boot-starter-logging-2.7.3.jar:/opt/dolphinscheduler/libs/kerby-pkix-1.0.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-procedure-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-etcd-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-sqoop-3.2.1.jar:/opt/dolphinscheduler/libs/zookeeper-jute-3.8.0.jar:/opt/dolphinscheduler/libs/netty-resolver-dns-native-macos-4.1.53.Final-osx-x86_64.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-sagemaker-3.2.1.jar:/opt/dolphinscheduler/libs/spring-boot-configuration-processor-2.6.1.jar:/opt/dolphinscheduler/libs/hive-common-2.3.9.jar:/opt/dolphinscheduler/libs/kerb-common-1.0.1.jar:/opt/dolphinscheduler/libs/stax-api-1.0.1.jar:/opt/dolphinscheduler/libs/kubernetes-model-storageclass-5.10.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-s3-3.2.1.jar:/opt/dolphinscheduler/libs/spring-boot-starter-jetty-2.7.3.jar:/opt/dolphinscheduler/libs/okio-2.8.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-dms-3.2.1.jar:/opt/dolphinscheduler/libs/simpleclient_common-0.15.0.jar:/opt/dolphinscheduler/libs/sdk-core-2.17.282.jar:/opt/dolphinscheduler/libs/javax.activation-api-1.2.0.jar:/opt/dolphinscheduler/libs/netty-handler-proxy-4.1.53.Final.jar:/opt/dolphinscheduler/libs/kerby-config-1.0.1.jar:/opt/dolphinscheduler/libs/netty-tcnative-classes-2.0.53.Final.jar:/opt/dolphinscheduler/libs/jackson-jaxrs-base-2.13.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-trino-3.2.1.jar:/opt/dolphinscheduler/libs/presto-jdbc-0.238.1.jar:/opt/dolphinscheduler/libs/websocket-api-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-flink-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-api-3.2.1.jar:/opt/dolphinscheduler/libs/kubernetes-model-metrics-5.10.2.jar:/opt/dolphinscheduler/libs/datasync-2.17.282.jar:/opt/dolphinscheduler/libs/hadoop-yarn-api-3.2.4.jar:/opt/dolphinscheduler/libs/google-http-client-apache-v2-1.42.3.jar:/opt/dolphinscheduler/libs/hadoop-annotations-3.2.4.jar:/opt/dolphinscheduler/libs/google-oauth-client-1.34.1.jar:/opt/dolphinscheduler/libs/sshd-common-2.8.0.jar:/opt/dolphinscheduler/libs/LatencyUtils-2.0.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-spark-3.2.1.jar:/opt/dolphinscheduler/libs/slf4j-api-1.7.36.jar:/opt/dolphinscheduler/libs/snowflake-jdbc-3.13.29.jar:/opt/dolphinscheduler/libs/jackson-datatype-jsr310-2.13.3.jar:/opt/dolphinscheduler/libs/trino-jdbc-402.jar:/opt/dolphinscheduler/libs/logging-interceptor-4.9.3.jar:/opt/dolphinscheduler/libs/simpleclient_tracer_otel_agent-0.15.0.jar:/opt/dolphinscheduler/libs/janino-3.0.16.jar:/opt/dolphinscheduler/libs/jackson-dataformat-yaml-2.13.3.jar:/opt/dolphinscheduler/libs/ion-java-1.0.2.jar:/opt/dolphinscheduler/libs/commons-dbcp-1.4.jar:/opt/dolphinscheduler/libs/jsp-api-2.1.jar:/opt/dolphinscheduler/libs/google-http-client-gson-1.42.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-hivecli-3.2.1.jar:/opt/dolphinscheduler/libs/spring-boot-actuator-2.7.3.jar:/opt/dolphinscheduler/libs/google-cloud-core-http-2.10.0.jar:/opt/dolphinscheduler/libs/DmJdbcDriver18-8.1.2.79.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-java-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-dameng-3.2.1.jar:/opt/dolphinscheduler/libs/sshd-sftp-2.8.0.jar:/opt/dolphinscheduler/libs/kerb-crypto-1.0.1.jar:/opt/dolphinscheduler/libs/conscrypt-openjdk-uber-2.5.2.jar:/opt/dolphinscheduler/libs/httpcore-4.4.15.jar:/opt/dolphinscheduler/libs/lz4-java-1.4.0.jar:/opt/dolphinscheduler/libs/guava-retrying-2.0.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-flink-stream-3.2.1.jar:/opt/dolphinscheduler/libs/spring-tx-5.3.22.jar:/opt/dolphinscheduler/libs/grpc-auth-1.41.0.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/dolphinscheduler/libs/databend-jdbc-0.0.7.jar:/opt/dolphinscheduler/libs/bcprov-jdk15on-1.69.jar:/opt/dolphinscheduler/libs/commons-lang-2.6.jar:/opt/dolphinscheduler/libs/kubernetes-model-policy-5.10.2.jar:/opt/dolphinscheduler/libs/commons-io-2.11.0.jar:/opt/dolphinscheduler/libs/grpc-grpclb-1.41.0.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/dolphinscheduler/libs/opentracing-api-0.33.0.jar:/opt/dolphinscheduler/libs/sshd-scp-2.8.0.jar:/opt/dolphinscheduler/libs/reload4j-1.2.18.3.jar:/opt/dolphinscheduler/libs/netty-tcnative-2.0.48.Final.jar:/opt/dolphinscheduler/libs/jdom2-2.0.6.1.jar:/opt/dolphinscheduler/libs/spring-boot-starter-json-2.7.3.jar:/opt/dolphinscheduler/libs/netty-transport-native-epoll-4.1.53.Final.jar:/opt/dolphinscheduler/libs/google-cloud-core-2.10.0.jar:/opt/dolphinscheduler/libs/javax.jdo-3.2.0-m3.jar:/opt/dolphinscheduler/libs/gax-httpjson-0.108.0.jar:/opt/dolphinscheduler/libs/mybatis-plus-core-3.5.2.jar:/opt/dolphinscheduler/libs/auto-service-annotations-1.0.1.jar:/opt/dolphinscheduler/libs/nimbus-jose-jwt-9.22.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-zeppelin-3.2.1.jar:/opt/dolphinscheduler/libs/commons-compress-1.21.jar:/opt/dolphinscheduler/libs/hadoop-hdfs-client-3.2.4.jar:/opt/dolphinscheduler/libs/msal4j-persistence-extension-1.1.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-pytorch-3.2.1.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/dolphinscheduler/libs/jetty-servlets-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/woodstox-core-6.4.0.jar:/opt/dolphinscheduler/libs/spring-cloud-kubernetes-commons-2.1.3.jar:/opt/dolphinscheduler/libs/grpc-protobuf-lite-1.41.0.jar:/opt/dolphinscheduler/libs/jetty-webapp-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/eventstream-1.0.1.jar:/opt/dolphinscheduler/libs/commons-compiler-3.1.7.jar:/opt/dolphinscheduler/libs/netty-transport-4.1.53.Final.jar:/opt/dolphinscheduler/libs/spring-cloud-context-3.1.3.jar:/opt/dolphinscheduler/libs/aws-java-sdk-dms-1.12.300.jar:/opt/dolphinscheduler/libs/hive-storage-api-2.4.0.jar:/opt/dolphinscheduler/libs/client-java-spring-integration-13.0.2.jar:/opt/dolphinscheduler/libs/spring-boot-starter-actuator-2.7.3.jar:/opt/dolphinscheduler/libs/third-party-jackson-core-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-hdfs-3.2.1.jar:/opt/dolphinscheduler/libs/netty-codec-4.1.53.Final.jar:/opt/dolphinscheduler/libs/google-http-client-appengine-1.42.3.jar:/opt/dolphinscheduler/libs/commons-configuration2-2.1.1.jar:/opt/dolphinscheduler/libs/datanucleus-rdbms-4.1.19.jar:/opt/dolphinscheduler/libs/swagger-annotations-1.6.2.jar:/opt/dolphinscheduler/libs/simpleclient_tracer_otel-0.15.0.jar:/opt/dolphinscheduler/libs/kotlin-stdlib-common-1.6.21.jar:/opt/dolphinscheduler/libs/aws-core-2.17.282.jar:/opt/dolphinscheduler/libs/kerb-core-1.0.1.jar:/opt/dolphinscheduler/libs/httpasyncclient-4.1.5.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-worker-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-spi-3.2.1.jar:/opt/dolphinscheduler/libs/okhttp-2.7.5.jar:/opt/dolphinscheduler/libs/google-api-client-2.2.0.jar:/opt/dolphinscheduler/libs/kotlin-stdlib-1.6.21.jar:/opt/dolphinscheduler/libs/jakarta.websocket-api-1.1.2.jar:/opt/dolphinscheduler/libs/libthrift-0.9.3.jar:/opt/dolphinscheduler/libs/spring-boot-starter-aop-2.7.3.jar:/opt/dolphinscheduler/libs/netty-common-4.1.53.Final.jar:/opt/dolphinscheduler/libs/log4j-1.2-api-2.17.2.jar:/opt/dolphinscheduler/libs/jackson-databind-2.13.4.jar:/opt/dolphinscheduler/libs/jackson-dataformat-xml-2.13.3.jar:/opt/dolphinscheduler/libs/kubernetes-model-events-5.10.2.jar:/opt/dolphinscheduler/libs/gson-2.9.1.jar:/opt/dolphinscheduler/libs/proto-google-iam-v1-1.9.0.jar:/opt/dolphinscheduler/libs/netty-codec-socks-4.1.53.Final.jar:/opt/dolphinscheduler/libs/zjsonpatch-0.3.0.jar:/opt/dolphinscheduler/libs/hadoop-mapreduce-client-common-3.2.4.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-api-3.2.1.jar:/opt/dolphinscheduler/libs/azure-storage-blob-12.21.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-zeppelin-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-api-3.2.1.jar:/opt/dolphinscheduler/libs/aws-java-sdk-s3-1.12.300.jar:/opt/dolphinscheduler/libs/stax2-api-4.2.1.jar:/opt/dolphinscheduler/libs/jakarta.annotation-api-1.3.5.jar:/opt/dolphinscheduler/libs/kubernetes-model-discovery-5.10.2.jar:/opt/dolphinscheduler/libs/jna-5.10.0.jar:/opt/dolphinscheduler/libs/spring-jcl-5.3.22.jar
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.731 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.732 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.io.tmpdir=/tmp
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.732 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.compiler=<NA>
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.733 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.name=Linux
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.734 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.arch=amd64
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.735 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.version=6.5.0-28-generic
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.735 +0800 o.a.z.ZooKeeper:[98] - Client environment:user.name=root
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.736 +0800 o.a.z.ZooKeeper:[98] - Client environment:user.home=/root
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.736 +0800 o.a.z.ZooKeeper:[98] - Client environment:user.dir=/opt/dolphinscheduler
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.737 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.memory.free=3202MB
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.738 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.memory.max=3840MB
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.739 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.memory.total=3840MB
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.742 +0800 o.a.z.ZooKeeper:[637] - Initiating client connection, connectString=dolphinscheduler-zookeeper:2181 sessionTimeout=30000 watcher=org.apache.curator.ConnectionState@6996bbc4
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.746 +0800 o.a.z.c.X509Util:[77] - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.750 +0800 o.a.z.ClientCnxnSocket:[239] - jute.maxbuffer value is 1048575 Bytes
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.764 +0800 o.a.z.ClientCnxn:[1732] - zookeeper.request.timeout value is 0. feature enabled=false
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.849 +0800 o.a.c.f.i.CuratorFrameworkImpl:[386] - Default schema
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.847 +0800 o.a.z.ClientCnxn:[1171] - Opening socket connection to server dolphinscheduler-zookeeper/172.18.0.4:2181.
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.916 +0800 o.a.z.ClientCnxn:[1173] - SASL config status: Will not attempt to authenticate using SASL (unknown error)
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.928 +0800 o.a.z.ClientCnxn:[1005] - Socket connection established, initiating session, client: /172.18.1.1:42608, server: dolphinscheduler-zookeeper/172.18.0.4:2181
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:21.971 +0800 o.a.z.ClientCnxn:[1444] - Session establishment complete on server dolphinscheduler-zookeeper/172.18.0.4:2181, session id = 0x10000e3bab20001, negotiated timeout = 30000
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:22.009 +0800 o.a.c.f.s.ConnectionStateManager:[252] - State change: CONNECTED
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:22.192 +0800 o.a.c.f.i.EnsembleTracker:[201] - New config event received: {}
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:22.193 +0800 o.a.c.f.i.EnsembleTracker:[201] - New config event received: {}
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:22.599 +0800 o.a.d.s.w.r.WorkerRpcServer:[41] - WorkerRpcServer starting...
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.205 +0800 o.a.d.e.b.NettyRemotingServer:[112] - WorkerRpcServer bind success at port: 1234
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.206 +0800 o.a.d.s.w.r.WorkerRpcServer:[43] - WorkerRpcServer started...
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.298 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: JAVA - JavaTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.339 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: JAVA - JavaTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.446 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: JUPYTER - JupyterTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.460 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: JUPYTER - JupyterTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.461 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SPARK - SparkTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.464 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SPARK - SparkTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.465 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: FLINK_STREAM - FlinkStreamTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.472 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: FLINK_STREAM - FlinkStreamTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.473 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: PYTHON - PythonTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.476 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: PYTHON - PythonTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.476 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DATASYNC - DatasyncTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.481 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DATASYNC - DatasyncTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.482 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DATA_FACTORY - DatafactoryTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.560 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DATA_FACTORY - DatafactoryTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.561 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: CHUNJUN - ChunJunTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.695 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: CHUNJUN - ChunJunTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.696 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: REMOTESHELL - RemoteShellTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.700 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: REMOTESHELL - RemoteShellTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.700 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: PIGEON - PigeonTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.708 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: PIGEON - PigeonTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.709 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SHELL - ShellTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.711 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SHELL - ShellTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.711 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: PROCEDURE - ProcedureTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.714 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: PROCEDURE - ProcedureTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.720 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: MR - MapReduceTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.722 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: MR - MapReduceTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.723 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SQOOP - SqoopTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.726 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SQOOP - SqoopTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.727 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: PYTORCH - PytorchTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.730 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: PYTORCH - PytorchTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.738 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: K8S - K8sTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.744 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: K8S - K8sTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.757 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SEATUNNEL - SeatunnelTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.825 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SEATUNNEL - SeatunnelTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.831 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SAGEMAKER - SagemakerTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.834 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SAGEMAKER - SagemakerTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.839 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: HTTP - HttpTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.844 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: HTTP - HttpTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.850 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: EMR - EmrTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:23.855 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: EMR - EmrTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.097 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DMS - DmsTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.105 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DMS - DmsTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.106 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DATA_QUALITY - DataQualityTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.110 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DATA_QUALITY - DataQualityTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.112 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: KUBEFLOW - KubeflowTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.114 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: KUBEFLOW - KubeflowTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.156 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SQL - SqlTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.165 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SQL - SqlTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.166 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DVC - DvcTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.168 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DVC - DvcTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.169 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DATAX - DataxTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.172 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DATAX - DataxTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.172 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: ZEPPELIN - ZeppelinTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.175 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: ZEPPELIN - ZeppelinTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.176 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DINKY - DinkyTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.179 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DINKY - DinkyTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.179 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: MLFLOW - MlflowTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.181 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: MLFLOW - MlflowTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.181 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: OPENMLDB - OpenmldbTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.184 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: OPENMLDB - OpenmldbTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.185 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: LINKIS - LinkisTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.186 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: LINKIS - LinkisTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.188 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: FLINK - FlinkTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.190 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: FLINK - FlinkTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.192 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: HIVECLI - HiveCliTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.286 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: HIVECLI - HiveCliTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.419 +0800 o.a.d.c.u.JSONUtils:[72] - init timezone: sun.util.calendar.ZoneInfo[id="Asia/Shanghai",offset=28800000,dstSavings=0,useDaylight=false,transitions=31,lastRule=null]
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:24.605 +0800 o.a.d.s.w.r.WorkerRegistryClient:[104] - Worker node: 172.18.1.1:1234 registry to ZK /nodes/worker/172.18.1.1:1234 successfully
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:25.671 +0800 o.a.d.c.m.BaseHeartBeatTask:[48] - Starting WorkerHeartBeatTask...
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:25.673 +0800 o.a.d.c.m.BaseHeartBeatTask:[50] - Started WorkerHeartBeatTask, heartBeatInterval: 10000...
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:25.673 +0800 o.a.d.s.w.r.WorkerRegistryClient:[114] - Worker node: 172.18.1.1:1234 registry finished
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:25.675 +0800 o.a.d.s.w.m.MessageRetryRunner:[69] - Message retry runner staring
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:25.678 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.3884297520661157 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:25.699 +0800 o.a.d.s.w.m.MessageRetryRunner:[72] - Injected message sender: TaskInstanceExecutionFinishEventSender
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:25.744 +0800 o.a.d.s.w.m.MessageRetryRunner:[72] - Injected message sender: TaskInstanceExecutionInfoUpdateEventSender
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:25.745 +0800 o.a.d.s.w.m.MessageRetryRunner:[72] - Injected message sender: TaskInstanceExecutionRunningEventSender
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:25.746 +0800 o.a.d.s.w.m.MessageRetryRunner:[75] - Message retry runner started
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:26.691 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.5051456829724674 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:28.171 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.9230769230769231 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:29.292 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.8319999999999999 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:30.414 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.3711790393013101 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:31.880 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.9508196721311475 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:32.674 +0800 o.s.b.a.e.w.EndpointLinksResolver:[58] - Exposing 3 endpoint(s) beneath base path '/actuator'
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:32.899 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 2.0 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:32.944 +0800 o.e.j.s.h.C.application:[2368] - Initializing Spring DispatcherServlet 'dispatcherServlet'
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:32.945 +0800 o.s.w.s.DispatcherServlet:[525] - Initializing Servlet 'dispatcherServlet'
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:32.947 +0800 o.s.w.s.DispatcherServlet:[547] - Completed initialization in 2 ms
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:33.009 +0800 o.e.j.s.AbstractConnector:[333] - Started ServerConnector@acb5508{HTTP/1.1, (http/1.1)}{0.0.0.0:1235}
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:33.061 +0800 o.s.b.w.e.j.JettyWebServer:[172] - Jetty started on port(s) 1235 (http/1.1) with context path '/'
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:33.087 +0800 o.a.d.s.w.WorkerServer:[61] - Started WorkerServer in 37.624 seconds (JVM running for 40.593)
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:33.927 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.225130890052356 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:34.956 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9473684210526315 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:35.979 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8981132075471698 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:36.991 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8370786516853933 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-25 12:32:44.300 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7109004739336493 is over then the MaxCpuUsagePercentageThresholds 0.7
