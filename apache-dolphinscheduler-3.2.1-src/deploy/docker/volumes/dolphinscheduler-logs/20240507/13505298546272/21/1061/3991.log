[INFO] 2024-05-07 10:51:44.417 +0800 - ***********************************************************************************************
[INFO] 2024-05-07 10:51:44.429 +0800 - *********************************  Initialize task context  ***********************************
[INFO] 2024-05-07 10:51:44.435 +0800 - ***********************************************************************************************
[INFO] 2024-05-07 10:51:44.436 +0800 - Begin to initialize task
[INFO] 2024-05-07 10:51:44.436 +0800 - Set task startTime: 1715050304436
[INFO] 2024-05-07 10:51:44.437 +0800 - Set task appId: 1061_3991
[INFO] 2024-05-07 10:51:44.458 +0800 - End initialize task {
  "taskInstanceId" : 3991,
  "taskName" : "[null check] filtered data persistence",
  "firstSubmitTime" : 1715050304042,
  "startTime" : 1715050304436,
  "taskType" : "DATA_QUALITY",
  "workflowInstanceHost" : "172.18.0.14:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240507/13505298546272/21/1061/3991.log",
  "processId" : 0,
  "processDefineCode" : 13505298546272,
  "processDefineVersion" : 21,
  "processInstanceId" : 1061,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 2,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13001194483488,
  "taskParams" : "{\"localParams\":[],\"resourceList\":[],\"ruleId\":6,\"ruleInputParameter\":{\"check_type\":\"0\",\"comparison_type\":1,\"comparison_name\":\"0\",\"failure_strategy\":\"0\",\"operator\":\"0\",\"src_connector_type\":1,\"src_datasource_id\":6,\"src_database\":\"persistence\",\"src_field\":\"content\",\"src_table\":\"filtered_data_persistence\",\"threshold\":\"0\"},\"sparkParameters\":{\"deployMode\":\"local\",\"driverCores\":1,\"driverMemory\":\"512M\",\"executorCores\":1,\"executorMemory\":\"1G\",\"numExecutors\":1,\"others\":\"--conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n--packages org.postgresql:postgresql:42.7.3\",\"yarnQueue\":\"\"}}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "[null check] filtered data persistence"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13001194483488"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1061"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240507"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240506"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3991"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "qualti_test"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13505264408800"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13505298546272"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240507105144"
    }
  },
  "taskAppId" : "1061_3991",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "dataQualityTaskExecutionContext" : {
    "ruleId" : 6,
    "ruleName" : "$t(uniqueness_check)",
    "ruleType" : 0,
    "ruleInputEntryList" : "[{\"id\":1,\"field\":\"src_connector_type\",\"type\":\"select\",\"title\":\"$t(src_connector_type)\",\"data\":\"\",\"options\":\"[{\\\"label\\\":\\\"HIVE\\\",\\\"value\\\":\\\"HIVE\\\"},{\\\"label\\\":\\\"JDBC\\\",\\\"value\\\":\\\"JDBC\\\"}]\",\"placeholder\":\"please select source connector type\",\"optionSourceType\":2,\"dataType\":2,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":1,\"createTime\":null,\"updateTime\":null},{\"id\":2,\"field\":\"src_datasource_id\",\"type\":\"select\",\"title\":\"$t(src_datasource_id)\",\"data\":\"\",\"options\":null,\"placeholder\":\"please select source datasource id\",\"optionSourceType\":1,\"dataType\":2,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":2,\"createTime\":null,\"updateTime\":null},{\"id\":30,\"field\":\"src_database\",\"type\":\"select\",\"title\":\"$t(src_database)\",\"data\":null,\"options\":null,\"placeholder\":\"Please select source database\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":true,\"valuesMap\":null,\"index\":2,\"createTime\":null,\"updateTime\":null},{\"id\":3,\"field\":\"src_table\",\"type\":\"select\",\"title\":\"$t(src_table)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter source table name\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":true,\"valuesMap\":null,\"index\":3,\"createTime\":null,\"updateTime\":null},{\"id\":4,\"field\":\"src_filter\",\"type\":\"input\",\"title\":\"$t(src_filter)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter filter expression\",\"optionSourceType\":0,\"dataType\":3,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":4,\"createTime\":null,\"updateTime\":null},{\"id\":5,\"field\":\"src_field\",\"type\":\"select\",\"title\":\"$t(src_field)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter column, only single column is supported\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":null,\"index\":5,\"createTime\":null,\"updateTime\":null},{\"id\":6,\"field\":\"statistics_name\",\"type\":\"input\",\"title\":\"$t(statistics_name)\",\"data\":\"duplicate_count.duplicates\",\"options\":null,\"placeholder\":\"Please enter statistics name, the alias in statistics execute sql\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":1,\"isShow\":false,\"canEdit\":false,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":\"{\\\"statistics_name\\\":\\\"duplicate_count.duplicates\\\"}\",\"index\":6,\"createTime\":null,\"updateTime\":null},{\"id\":7,\"field\":\"check_type\",\"type\":\"select\",\"title\":\"$t(check_type)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"Expected - Actual\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"Actual - Expected\\\",\\\"value\\\":\\\"1\\\"},{\\\"label\\\":\\\"Actual / Expected\\\",\\\"value\\\":\\\"2\\\"},{\\\"label\\\":\\\"(Expected - Actual) / Expected\\\",\\\"value\\\":\\\"3\\\"}]\",\"placeholder\":\"please select check type\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":7,\"createTime\":null,\"updateTime\":null},{\"id\":8,\"field\":\"operator\",\"type\":\"select\",\"title\":\"$t(operator)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"=\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"<\\\",\\\"value\\\":\\\"1\\\"},{\\\"label\\\":\\\"<=\\\",\\\"value\\\":\\\"2\\\"},{\\\"label\\\":\\\">\\\",\\\"value\\\":\\\"3\\\"},{\\\"label\\\":\\\">=\\\",\\\"value\\\":\\\"4\\\"},{\\\"label\\\":\\\"!=\\\",\\\"value\\\":\\\"5\\\"}]\",\"placeholder\":\"please select operator\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":8,\"createTime\":null,\"updateTime\":null},{\"id\":9,\"field\":\"threshold\",\"type\":\"input\",\"title\":\"$t(threshold)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter threshold, number is needed\",\"optionSourceType\":0,\"dataType\":2,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":null,\"index\":9,\"createTime\":null,\"updateTime\":null},{\"id\":10,\"field\":\"failure_strategy\",\"type\":\"select\",\"title\":\"$t(failure_strategy)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"Alert\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"Block\\\",\\\"value\\\":\\\"1\\\"}]\",\"placeholder\":\"please select failure strategy\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":10,\"createTime\":null,\"updateTime\":null},{\"id\":17,\"field\":\"comparison_name\",\"type\":\"input\",\"title\":\"$t(comparison_name)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter comparison name, the alias in comparison execute sql\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":false,\"canEdit\":false,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":\"\",\"index\":11,\"createTime\":null,\"updateTime\":null},{\"id\":19,\"field\":\"comparison_type\",\"type\":\"select\",\"title\":\"$t(comparison_type)\",\"data\":\"\",\"options\":null,\"placeholder\":\"Please enter comparison title\",\"optionSourceType\":3,\"dataType\":0,\"inputType\":2,\"isShow\":true,\"canEdit\":false,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":12,\"createTime\":null,\"updateTime\":null}]",
    "executeSqlList" : "[{\"id\":6,\"index\":1,\"sql\":\"SELECT ${src_field} FROM ${src_table} group by ${src_field} having count(*) > 1\",\"tableAlias\":\"duplicate_items\",\"type\":0,\"createTime\":\"2021-03-03 11:31:24\",\"updateTime\":\"2021-03-03 11:31:24\",\"errorOutputSql\":true},{\"id\":7,\"index\":1,\"sql\":\"SELECT COUNT(*) AS duplicates FROM duplicate_items\",\"tableAlias\":\"duplicate_count\",\"type\":1,\"createTime\":\"2021-03-03 11:31:24\",\"updateTime\":\"2021-03-03 11:31:24\",\"errorOutputSql\":false}]",
    "comparisonNeedStatisticsValueTable" : false,
    "compareWithFixedValue" : true,
    "hdfsPath" : "hdfs://mycluster:8020/user/default/data_quality_error_data",
    "sourceConnectorType" : "JDBC",
    "sourceType" : 1,
    "sourceConnectionParams" : "{\"user\":\"root\",\"password\":\"****\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"persistence\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence\",\"driverClassName\":\"org.postgresql.Driver\",\"validationQuery\":\"select version()\",\"other\":{\"password\":\"****\"}}",
    "targetConnectorType" : null,
    "targetType" : 0,
    "targetConnectionParams" : null,
    "writerConnectorType" : "JDBC",
    "writerType" : 1,
    "writerTable" : "t_ds_dq_execute_result",
    "writerConnectionParams" : "{\"user\":\"root\",\"password\":\"****\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"dolphinscheduler\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\"}",
    "statisticsValueConnectorType" : "JDBC",
    "statisticsValueType" : 1,
    "statisticsValueTable" : "t_ds_dq_task_statistics_value",
    "statisticsValueWriterConnectionParams" : "{\"user\":\"root\",\"password\":\"****\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"dolphinscheduler\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\"}"
  },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[INFO] 2024-05-07 10:51:44.461 +0800 - ***********************************************************************************************
[INFO] 2024-05-07 10:51:44.461 +0800 - *********************************  Load task instance plugin  *********************************
[INFO] 2024-05-07 10:51:44.462 +0800 - ***********************************************************************************************
[INFO] 2024-05-07 10:51:44.603 +0800 - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[INFO] 2024-05-07 10:51:44.646 +0800 - create linux os user: default
[INFO] 2024-05-07 10:51:44.648 +0800 - execute cmd: sudo useradd -g root
 default
[INFO] 2024-05-07 10:51:44.851 +0800 - create user default success
[INFO] 2024-05-07 10:51:44.852 +0800 - TenantCode: default check successfully
[INFO] 2024-05-07 10:51:44.866 +0800 - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1061/3991 check successfully
[INFO] 2024-05-07 10:51:44.867 +0800 - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.dq.DataQualityTaskChannel successfully
[INFO] 2024-05-07 10:51:44.907 +0800 - Download resources successfully: 
ResourceContext(resourceItemMap={})
[INFO] 2024-05-07 10:51:44.917 +0800 - Download upstream files: [] successfully
[INFO] 2024-05-07 10:51:44.923 +0800 - Task plugin instance: DATA_QUALITY create successfully
[INFO] 2024-05-07 10:51:44.931 +0800 - Initialize data quality task params {
  "localParams" : [ ],
  "varPool" : null,
  "ruleId" : 6,
  "ruleInputParameter" : {
    "check_type" : "0",
    "comparison_type" : "1",
    "comparison_name" : "0",
    "failure_strategy" : "0",
    "operator" : "0",
    "src_connector_type" : "1",
    "src_datasource_id" : "6",
    "src_database" : "persistence",
    "src_field" : "content",
    "src_table" : "filtered_data_persistence",
    "threshold" : "0"
  },
  "sparkParameters" : {
    "localParams" : null,
    "varPool" : null,
    "mainJar" : null,
    "mainClass" : null,
    "deployMode" : "local",
    "mainArgs" : null,
    "driverCores" : 1,
    "driverMemory" : "512M",
    "numExecutors" : 1,
    "executorCores" : 1,
    "executorMemory" : "1G",
    "appName" : null,
    "yarnQueue" : "",
    "others" : "--conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n--packages org.postgresql:postgresql:42.7.3",
    "programType" : null,
    "resourceList" : [ ]
  }
}
[INFO] 2024-05-07 10:51:45.012 +0800 - start register processor: HANA
[INFO] 2024-05-07 10:51:45.013 +0800 - done register processor: HANA
[INFO] 2024-05-07 10:51:45.017 +0800 - start register processor: SQLSERVER
[INFO] 2024-05-07 10:51:45.019 +0800 - done register processor: SQLSERVER
[INFO] 2024-05-07 10:51:45.024 +0800 - start register processor: SAGEMAKER
[INFO] 2024-05-07 10:51:45.026 +0800 - done register processor: SAGEMAKER
[INFO] 2024-05-07 10:51:45.031 +0800 - start register processor: DATABEND
[INFO] 2024-05-07 10:51:45.032 +0800 - done register processor: DATABEND
[INFO] 2024-05-07 10:51:45.112 +0800 - start register processor: AZURESQL
[INFO] 2024-05-07 10:51:45.124 +0800 - done register processor: AZURESQL
[INFO] 2024-05-07 10:51:45.132 +0800 - start register processor: HIVE
[INFO] 2024-05-07 10:51:45.132 +0800 - done register processor: HIVE
[INFO] 2024-05-07 10:51:45.133 +0800 - start register processor: DORIS
[INFO] 2024-05-07 10:51:45.134 +0800 - done register processor: DORIS
[INFO] 2024-05-07 10:51:45.148 +0800 - start register processor: POSTGRESQL
[INFO] 2024-05-07 10:51:45.150 +0800 - done register processor: POSTGRESQL
[INFO] 2024-05-07 10:51:45.155 +0800 - start register processor: MYSQL
[INFO] 2024-05-07 10:51:45.158 +0800 - done register processor: MYSQL
[INFO] 2024-05-07 10:51:45.161 +0800 - start register processor: REDSHIFT
[INFO] 2024-05-07 10:51:45.162 +0800 - done register processor: REDSHIFT
[INFO] 2024-05-07 10:51:45.167 +0800 - start register processor: SNOWFLAKE
[INFO] 2024-05-07 10:51:45.168 +0800 - done register processor: SNOWFLAKE
[INFO] 2024-05-07 10:51:45.171 +0800 - start register processor: STARROCKS
[INFO] 2024-05-07 10:51:45.173 +0800 - done register processor: STARROCKS
[INFO] 2024-05-07 10:51:45.177 +0800 - start register processor: SSH
[INFO] 2024-05-07 10:51:45.182 +0800 - done register processor: SSH
[INFO] 2024-05-07 10:51:45.191 +0800 - start register processor: ATHENA
[INFO] 2024-05-07 10:51:45.192 +0800 - done register processor: ATHENA
[INFO] 2024-05-07 10:51:45.196 +0800 - start register processor: PRESTO
[INFO] 2024-05-07 10:51:45.199 +0800 - done register processor: PRESTO
[INFO] 2024-05-07 10:51:45.209 +0800 - start register processor: OCEANBASE
[INFO] 2024-05-07 10:51:45.212 +0800 - done register processor: OCEANBASE
[INFO] 2024-05-07 10:51:45.225 +0800 - start register processor: ORACLE
[INFO] 2024-05-07 10:51:45.226 +0800 - done register processor: ORACLE
[INFO] 2024-05-07 10:51:45.228 +0800 - start register processor: CLICKHOUSE
[INFO] 2024-05-07 10:51:45.231 +0800 - done register processor: CLICKHOUSE
[INFO] 2024-05-07 10:51:45.246 +0800 - start register processor: VERTICA
[INFO] 2024-05-07 10:51:45.247 +0800 - done register processor: VERTICA
[INFO] 2024-05-07 10:51:45.251 +0800 - start register processor: KYUUBI
[INFO] 2024-05-07 10:51:45.257 +0800 - done register processor: KYUUBI
[INFO] 2024-05-07 10:51:45.260 +0800 - start register processor: DB2
[INFO] 2024-05-07 10:51:45.262 +0800 - done register processor: DB2
[INFO] 2024-05-07 10:51:45.265 +0800 - start register processor: TRINO
[INFO] 2024-05-07 10:51:45.265 +0800 - done register processor: TRINO
[INFO] 2024-05-07 10:51:45.270 +0800 - start register processor: SPARK
[INFO] 2024-05-07 10:51:45.270 +0800 - done register processor: SPARK
[INFO] 2024-05-07 10:51:45.275 +0800 - start register processor: DAMENG
[INFO] 2024-05-07 10:51:45.276 +0800 - done register processor: DAMENG
[INFO] 2024-05-07 10:51:45.283 +0800 - start register processor: ZEPPELIN
[INFO] 2024-05-07 10:51:45.283 +0800 - done register processor: ZEPPELIN
[INFO] 2024-05-07 10:51:45.303 +0800 - data quality configuration: {
  "name" : "$t(uniqueness_check)",
  "env" : {
    "type" : "batch",
    "config" : null
  },
  "readers" : [ {
    "type" : "JDBC",
    "config" : {
      "database" : "persistence",
      "password" : "****",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "output_table" : "persistence_filtered_data_persistence",
      "table" : "filtered_data_persistence",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence?password=root"
    }
  } ],
  "transformers" : [ {
    "type" : "sql",
    "config" : {
      "index" : 1,
      "output_table" : "duplicate_items",
      "sql" : "SELECT content FROM persistence_filtered_data_persistence group by content having count(*) > 1"
    }
  }, {
    "type" : "sql",
    "config" : {
      "index" : 2,
      "output_table" : "duplicate_count",
      "sql" : "SELECT COUNT(*) AS duplicates FROM duplicate_items"
    }
  } ],
  "writers" : [ {
    "type" : "JDBC",
    "config" : {
      "database" : "dolphinscheduler",
      "password" : "****",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "table" : "t_ds_dq_execute_result",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler",
      "sql" : "select 0 as rule_type,'$t(uniqueness_check)' as rule_name,0 as process_definition_id,1061 as process_instance_id,3991 as task_instance_id,duplicate_count.duplicates AS statistics_value,0 AS comparison_value,1 AS comparison_type,0 as check_type,0 as threshold,0 as operator,0 as failure_strategy,'hdfs://mycluster:8020/user/default/data_quality_error_data/0_1061_[null check] filtered data persistence' as error_output_path,'2024-05-07 10:51:44' as create_time,'2024-05-07 10:51:44' as update_time from duplicate_count "
    }
  }, {
    "type" : "JDBC",
    "config" : {
      "database" : "dolphinscheduler",
      "password" : "****",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "table" : "t_ds_dq_task_statistics_value",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler",
      "sql" : "select 0 as process_definition_id,3991 as task_instance_id,6 as rule_id,'KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG=' as unique_code,'duplicate_count.duplicates'AS statistics_name,duplicate_count.duplicates AS statistics_value,'2024-05-07 10:51:44' as data_time,'2024-05-07 10:51:44' as create_time,'2024-05-07 10:51:44' as update_time from duplicate_count"
    }
  }, {
    "type" : "hdfs_file",
    "config" : {
      "path" : "hdfs://mycluster:8020/user/default/data_quality_error_data/0_1061_[null check] filtered data persistence",
      "input_table" : "duplicate_items"
    }
  } ]
}
[INFO] 2024-05-07 10:51:45.329 +0800 - Trying to get data quality jar in path
[INFO] 2024-05-07 10:51:45.330 +0800 - data quality jar path is empty, will try to auto discover it from build-in rules.
[INFO] 2024-05-07 10:51:45.333 +0800 - Try to get data quality jar from path /opt/dolphinscheduler/conf/../libs
[INFO] 2024-05-07 10:51:45.337 +0800 - get default data quality jar name: /opt/dolphinscheduler/conf/../libs/dolphinscheduler-data-quality-3.2.1.jar
[INFO] 2024-05-07 10:51:45.339 +0800 - Success initialized task plugin instance successfully
[INFO] 2024-05-07 10:51:45.340 +0800 - Set taskVarPool: null successfully
[INFO] 2024-05-07 10:51:45.373 +0800 - ***********************************************************************************************
[INFO] 2024-05-07 10:51:45.374 +0800 - *********************************  Execute task instance  *************************************
[INFO] 2024-05-07 10:51:45.374 +0800 - ***********************************************************************************************
[INFO] 2024-05-07 10:51:45.405 +0800 - Final Shell file is: 
[INFO] 2024-05-07 10:51:45.406 +0800 - ****************************** Script Content *****************************************************************
[INFO] 2024-05-07 10:51:45.406 +0800 - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${SPARK_HOME}/bin/spark-submit --master local --driver-cores 1 --driver-memory 512M --num-executors 1 --executor-cores 1 --executor-memory 1G --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
--packages org.postgresql:postgresql:42.7.3 /opt/dolphinscheduler/conf/../libs/dolphinscheduler-data-quality-3.2.1.jar "{\"name\":\"$t(uniqueness_check)\",\"env\":{\"type\":\"batch\",\"config\":null},\"readers\":[{\"type\":\"JDBC\",\"config\":{\"database\":\"persistence\",\"password\":\"****\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"output_table\":\"persistence_filtered_data_persistence\",\"table\":\"filtered_data_persistence\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence?password=root\"} }],\"transformers\":[{\"type\":\"sql\",\"config\":{\"index\":1,\"output_table\":\"duplicate_items\",\"sql\":\"SELECT content FROM persistence_filtered_data_persistence group by content having count(*) > 1\"} },{\"type\":\"sql\",\"config\":{\"index\":2,\"output_table\":\"duplicate_count\",\"sql\":\"SELECT COUNT(*) AS duplicates FROM duplicate_items\"} }],\"writers\":[{\"type\":\"JDBC\",\"config\":{\"database\":\"dolphinscheduler\",\"password\":\"****\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"table\":\"t_ds_dq_execute_result\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\",\"sql\":\"select 0 as rule_type,'$t(uniqueness_check)' as rule_name,0 as process_definition_id,1061 as process_instance_id,3991 as task_instance_id,duplicate_count.duplicates AS statistics_value,0 AS comparison_value,1 AS comparison_type,0 as check_type,0 as threshold,0 as operator,0 as failure_strategy,'hdfs://mycluster:8020/user/default/data_quality_error_data/0_1061_[null check] filtered data persistence' as error_output_path,'2024-05-07 10:51:44' as create_time,'2024-05-07 10:51:44' as update_time from duplicate_count \"} },{\"type\":\"JDBC\",\"config\":{\"database\":\"dolphinscheduler\",\"password\":\"****\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"table\":\"t_ds_dq_task_statistics_value\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\",\"sql\":\"select 0 as process_definition_id,3991 as task_instance_id,6 as rule_id,'KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG=' as unique_code,'duplicate_count.duplicates'AS statistics_name,duplicate_count.duplicates AS statistics_value,'2024-05-07 10:51:44' as data_time,'2024-05-07 10:51:44' as create_time,'2024-05-07 10:51:44' as update_time from duplicate_count\"} },{\"type\":\"hdfs_file\",\"config\":{\"path\":\"hdfs://mycluster:8020/user/default/data_quality_error_data/0_1061_[null check] filtered data persistence\",\"input_table\":\"duplicate_items\"} }]}"
[INFO] 2024-05-07 10:51:45.406 +0800 - ****************************** Script Content *****************************************************************
[INFO] 2024-05-07 10:51:45.419 +0800 - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1061/3991/1061_3991.sh
[INFO] 2024-05-07 10:51:45.434 +0800 - process start, process id is: 92
[INFO] 2024-05-07 10:51:46.463 +0800 -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[INFO] 2024-05-07 10:51:48.466 +0800 -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[INFO] 2024-05-07 10:51:49.469 +0800 -  -> 
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.postgresql#postgresql added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-0b920a18-38dc-4fb1-9fa9-d11731028ae1;1.0
		confs: [default]
[INFO] 2024-05-07 10:51:50.472 +0800 -  -> 
		found org.postgresql#postgresql;42.7.3 in central
		found org.checkerframework#checker-qual;3.42.0 in central
[INFO] 2024-05-07 10:51:51.478 +0800 -  -> 
	downloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.7.3/postgresql-42.7.3.jar ...
[INFO] 2024-05-07 10:51:52.480 +0800 -  -> 
		[SUCCESSFUL ] org.postgresql#postgresql;42.7.3!postgresql.jar (1331ms)
	downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.42.0/checker-qual-3.42.0.jar ...
		[SUCCESSFUL ] org.checkerframework#checker-qual;3.42.0!checker-qual.jar (424ms)
	:: resolution report :: resolve 1976ms :: artifacts dl 1782ms
		:: modules in use:
		org.checkerframework#checker-qual;3.42.0 from central in [default]
		org.postgresql#postgresql;42.7.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   2   |   2   |   2   |   0   ||   2   |   2   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-0b920a18-38dc-4fb1-9fa9-d11731028ae1
		confs: [default]
		2 artifacts copied, 0 already retrieved (1289kB/28ms)
[INFO] 2024-05-07 10:51:53.489 +0800 -  -> 
	24/05/07 10:51:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2024-05-07 10:51:55.543 +0800 -  -> 
	24/05/07 10:51:54 INFO SparkContext: Running Spark version 3.5.1
	24/05/07 10:51:54 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/05/07 10:51:54 INFO SparkContext: Java version 1.8.0_402
	24/05/07 10:51:54 INFO ResourceUtils: ==============================================================
	24/05/07 10:51:54 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/05/07 10:51:54 INFO ResourceUtils: ==============================================================
	24/05/07 10:51:54 INFO SparkContext: Submitted application: (uniqueness_check)
	24/05/07 10:51:54 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/05/07 10:51:54 INFO ResourceProfile: Limiting resource is cpu
	24/05/07 10:51:54 INFO ResourceProfileManager: Added ResourceProfile id: 0
	24/05/07 10:51:54 INFO SecurityManager: Changing view acls to: default
	24/05/07 10:51:54 INFO SecurityManager: Changing modify acls to: default
	24/05/07 10:51:54 INFO SecurityManager: Changing view acls groups to: 
	24/05/07 10:51:54 INFO SecurityManager: Changing modify acls groups to: 
	24/05/07 10:51:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
[INFO] 2024-05-07 10:51:56.545 +0800 -  -> 
	24/05/07 10:51:55 INFO Utils: Successfully started service 'sparkDriver' on port 35027.
	24/05/07 10:51:56 INFO SparkEnv: Registering MapOutputTracker
	24/05/07 10:51:56 INFO SparkEnv: Registering BlockManagerMaster
	24/05/07 10:51:56 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/05/07 10:51:56 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/05/07 10:51:56 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/05/07 10:51:56 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7e02fb52-f038-462b-905f-f1be352a1d4e
	24/05/07 10:51:56 INFO MemoryStore: MemoryStore started with capacity 93.3 MiB
[INFO] 2024-05-07 10:51:57.558 +0800 -  -> 
	24/05/07 10:51:56 INFO SparkEnv: Registering OutputCommitCoordinator
	24/05/07 10:51:56 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/05/07 10:51:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/05/07 10:51:57 INFO SparkContext: Added JAR file:///tmp/jars/org.postgresql_postgresql-42.7.3.jar at spark://56042a532902:35027/jars/org.postgresql_postgresql-42.7.3.jar with timestamp 1715050314480
	24/05/07 10:51:57 INFO SparkContext: Added JAR file:///tmp/jars/org.checkerframework_checker-qual-3.42.0.jar at spark://56042a532902:35027/jars/org.checkerframework_checker-qual-3.42.0.jar with timestamp 1715050314480
	24/05/07 10:51:57 INFO SparkContext: Added JAR file:/opt/dolphinscheduler/libs/dolphinscheduler-data-quality-3.2.1.jar at spark://56042a532902:35027/jars/dolphinscheduler-data-quality-3.2.1.jar with timestamp 1715050314480
[INFO] 2024-05-07 10:51:58.559 +0800 -  -> 
	24/05/07 10:51:57 INFO Executor: Starting executor ID driver on host 56042a532902
	24/05/07 10:51:57 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/05/07 10:51:57 INFO Executor: Java version 1.8.0_402
	24/05/07 10:51:57 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/05/07 10:51:57 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@33956d1a for default.
	24/05/07 10:51:57 INFO Executor: Fetching spark://56042a532902:35027/jars/org.postgresql_postgresql-42.7.3.jar with timestamp 1715050314480
	24/05/07 10:51:58 INFO TransportClientFactory: Successfully created connection to 56042a532902/172.18.1.1:35027 after 236 ms (0 ms spent in bootstraps)
	24/05/07 10:51:58 INFO Utils: Fetching spark://56042a532902:35027/jars/org.postgresql_postgresql-42.7.3.jar to /tmp/spark-7cb29114-5e3c-4d08-b803-0e8455771639/userFiles-78de8830-93b6-4142-a4a1-ef10ddd0ceea/fetchFileTemp6492543576534540086.tmp
	24/05/07 10:51:58 INFO Executor: Adding file:/tmp/spark-7cb29114-5e3c-4d08-b803-0e8455771639/userFiles-78de8830-93b6-4142-a4a1-ef10ddd0ceea/org.postgresql_postgresql-42.7.3.jar to class loader default
	24/05/07 10:51:58 INFO Executor: Fetching spark://56042a532902:35027/jars/org.checkerframework_checker-qual-3.42.0.jar with timestamp 1715050314480
	24/05/07 10:51:58 INFO Utils: Fetching spark://56042a532902:35027/jars/org.checkerframework_checker-qual-3.42.0.jar to /tmp/spark-7cb29114-5e3c-4d08-b803-0e8455771639/userFiles-78de8830-93b6-4142-a4a1-ef10ddd0ceea/fetchFileTemp5884759776946420626.tmp
[INFO] 2024-05-07 10:51:59.569 +0800 -  -> 
	24/05/07 10:51:58 INFO Utils: Fetching spark://56042a532902:35027/jars/dolphinscheduler-data-quality-3.2.1.jar to /tmp/spark-7cb29114-5e3c-4d08-b803-0e8455771639/userFiles-78de8830-93b6-4142-a4a1-ef10ddd0ceea/fetchFileTemp1478902421825940652.tmp
	24/05/07 10:51:58 INFO Executor: Adding file:/tmp/spark-7cb29114-5e3c-4d08-b803-0e8455771639/userFiles-78de8830-93b6-4142-a4a1-ef10ddd0ceea/dolphinscheduler-data-quality-3.2.1.jar to class loader default
	24/05/07 10:51:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44473.
	24/05/07 10:51:58 INFO NettyBlockTransferService: Server created on 56042a532902:44473
	24/05/07 10:51:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/05/07 10:51:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 56042a532902, 44473, None)
	24/05/07 10:51:58 INFO BlockManagerMasterEndpoint: Registering block manager 56042a532902:44473 with 93.3 MiB RAM, BlockManagerId(driver, 56042a532902, 44473, None)
	24/05/07 10:51:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 56042a532902, 44473, None)
	24/05/07 10:51:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 56042a532902, 44473, None)
[INFO] 2024-05-07 10:52:00.571 +0800 -  -> 
	24/05/07 10:51:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/05/07 10:51:59 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1061/3991/spark-warehouse'.
[INFO] 2024-05-07 10:52:10.640 +0800 -  -> 
	24/05/07 10:52:10 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[INFO] 2024-05-07 10:52:13.645 +0800 -  -> 
	24/05/07 10:52:13 INFO CodeGenerator: Code generated in 906.578445 ms
[INFO] 2024-05-07 10:52:14.677 +0800 -  -> 
	24/05/07 10:52:14 INFO DAGScheduler: Registering RDD 2 (save at JdbcWriter.java:87) as input to shuffle 0
	24/05/07 10:52:14 INFO DAGScheduler: Got map stage job 0 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 10:52:14 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (save at JdbcWriter.java:87)
	24/05/07 10:52:14 INFO DAGScheduler: Parents of final stage: List()
	24/05/07 10:52:14 INFO DAGScheduler: Missing parents: List()
	24/05/07 10:52:14 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at save at JdbcWriter.java:87), which has no missing parents
[INFO] 2024-05-07 10:52:15.685 +0800 -  -> 
	24/05/07 10:52:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 34.8 KiB, free 93.3 MiB)
	24/05/07 10:52:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 93.2 MiB)
	24/05/07 10:52:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 56042a532902:44473 (size: 16.4 KiB, free: 93.3 MiB)
	24/05/07 10:52:15 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
	24/05/07 10:52:15 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 10:52:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
	24/05/07 10:52:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (56042a532902, executor driver, partition 0, PROCESS_LOCAL, 7878 bytes) 
	24/05/07 10:52:15 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2024-05-07 10:52:16.688 +0800 -  -> 
	24/05/07 10:52:16 INFO CodeGenerator: Code generated in 85.788943 ms
	24/05/07 10:52:16 INFO CodeGenerator: Code generated in 12.536012 ms
	24/05/07 10:52:16 INFO CodeGenerator: Code generated in 5.633002 ms
[INFO] 2024-05-07 10:52:17.692 +0800 -  -> 
	24/05/07 10:52:16 INFO CodeGenerator: Code generated in 17.104237 ms
	24/05/07 10:52:16 INFO CodeGenerator: Code generated in 19.397807 ms
	24/05/07 10:52:17 INFO JDBCRDD: closed connection
	24/05/07 10:52:17 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2491 bytes result sent to driver
	24/05/07 10:52:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2398 ms on 56042a532902 (executor driver) (1/1)
	24/05/07 10:52:17 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2024-05-07 10:52:18.696 +0800 -  -> 
	24/05/07 10:52:17 INFO DAGScheduler: ShuffleMapStage 0 (save at JdbcWriter.java:87) finished in 3.109 s
	24/05/07 10:52:17 INFO DAGScheduler: looking for newly runnable stages
	24/05/07 10:52:17 INFO DAGScheduler: running: Set()
	24/05/07 10:52:17 INFO DAGScheduler: waiting: Set()
	24/05/07 10:52:17 INFO DAGScheduler: failed: Set()
	24/05/07 10:52:18 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
	24/05/07 10:52:18 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
	24/05/07 10:52:18 INFO CodeGenerator: Code generated in 24.532835 ms
[INFO] 2024-05-07 10:52:19.724 +0800 -  -> 
	24/05/07 10:52:18 INFO DAGScheduler: Registering RDD 5 (save at JdbcWriter.java:87) as input to shuffle 1
	24/05/07 10:52:18 INFO DAGScheduler: Got map stage job 1 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 10:52:18 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (save at JdbcWriter.java:87)
	24/05/07 10:52:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
	24/05/07 10:52:18 INFO DAGScheduler: Missing parents: List()
	24/05/07 10:52:18 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[5] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 10:52:18 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 40.5 KiB, free 93.2 MiB)
	24/05/07 10:52:18 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 93.2 MiB)
	24/05/07 10:52:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 56042a532902:44473 (size: 19.1 KiB, free: 93.3 MiB)
	24/05/07 10:52:18 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 56042a532902:44473 in memory (size: 16.4 KiB, free: 93.3 MiB)
	24/05/07 10:52:18 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
	24/05/07 10:52:18 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[5] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 10:52:18 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
	24/05/07 10:52:18 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (56042a532902, executor driver, partition 0, NODE_LOCAL, 8032 bytes) 
	24/05/07 10:52:18 INFO Executor: Running task 0.0 in stage 2.0 (TID 1)
	24/05/07 10:52:19 INFO ShuffleBlockFetcherIterator: Getting 1 (2.9 KiB) non-empty blocks including 1 (2.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
	24/05/07 10:52:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 66 ms
	24/05/07 10:52:19 INFO CodeGenerator: Code generated in 221.407799 ms
	24/05/07 10:52:19 INFO Executor: Finished task 0.0 in stage 2.0 (TID 1). 5420 bytes result sent to driver
	24/05/07 10:52:19 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 648 ms on 56042a532902 (executor driver) (1/1)
	24/05/07 10:52:19 INFO DAGScheduler: ShuffleMapStage 2 (save at JdbcWriter.java:87) finished in 0.693 s
	24/05/07 10:52:19 INFO DAGScheduler: looking for newly runnable stages
	24/05/07 10:52:19 INFO DAGScheduler: running: Set()
	24/05/07 10:52:19 INFO DAGScheduler: waiting: Set()
	24/05/07 10:52:19 INFO DAGScheduler: failed: Set()
	24/05/07 10:52:19 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
	24/05/07 10:52:19 INFO CodeGenerator: Code generated in 24.418577 ms
[INFO] 2024-05-07 10:52:20.734 +0800 -  -> 
	24/05/07 10:52:19 INFO SparkContext: Starting job: save at JdbcWriter.java:87
	24/05/07 10:52:19 INFO DAGScheduler: Got job 2 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 10:52:19 INFO DAGScheduler: Final stage: ResultStage 5 (save at JdbcWriter.java:87)
	24/05/07 10:52:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
	24/05/07 10:52:20 INFO DAGScheduler: Missing parents: List()
	24/05/07 10:52:20 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[10] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 10:52:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 51.4 KiB, free 93.2 MiB)
	24/05/07 10:52:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.1 KiB, free 93.2 MiB)
	24/05/07 10:52:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 56042a532902:44473 (size: 23.1 KiB, free: 93.3 MiB)
	24/05/07 10:52:20 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
	24/05/07 10:52:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[10] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 10:52:20 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
	24/05/07 10:52:20 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 2) (56042a532902, executor driver, partition 0, NODE_LOCAL, 8043 bytes) 
	24/05/07 10:52:20 INFO Executor: Running task 0.0 in stage 5.0 (TID 2)
[INFO] 2024-05-07 10:52:21.735 +0800 -  -> 
	24/05/07 10:52:21 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
	24/05/07 10:52:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
	24/05/07 10:52:21 INFO CodeGenerator: Code generated in 16.127584 ms
[INFO] 2024-05-07 10:52:22.754 +0800 -  -> 
	24/05/07 10:52:22 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 56042a532902:44473 in memory (size: 19.1 KiB, free: 93.3 MiB)
[INFO] 2024-05-07 10:52:23.352 +0800 - process id:92, cmd:sudo -u default kill -9 92 96 99 102 120 121 122 123 124 125 126 127 128 129 130 131 132 133 184 186 187 188 189 190 191 192 193 194 195 196 197 198 202 203 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 230 231 232 233 234 235 252 290 298 300 301 302 303 304 305 306 307 308 310 311 312 313 314
[ERROR] 2024-05-07 10:52:23.494 +0800 - kill task error
org.apache.dolphinscheduler.common.shell.AbstractShell$ExitCodeException: kill: (92): Operation not permitted
kill: (132): No such process
kill: (186): No such process
kill: (190): No such process
kill: (193): No such process
kill: (198): No such process
kill: (202): No such process
kill: (212): No such process
kill: (213): No such process
kill: (214): No such process
kill: (219): No such process
kill: (223): No such process
kill: (232): No such process
kill: (233): No such process
kill: (234): No such process
kill: (252): No such process
kill: (290): No such process
kill: (303): No such process
kill: (311): No such process
kill: (312): No such process
kill: (314): No such process

	at org.apache.dolphinscheduler.common.shell.AbstractShell.runCommand(AbstractShell.java:205)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.run(AbstractShell.java:118)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execute(ShellExecutor.java:125)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:103)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:86)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeShell(OSUtils.java:345)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeCmd(OSUtils.java:334)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.killProcess(TaskInstanceKillOperationFunction.java:135)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.doKill(TaskInstanceKillOperationFunction.java:96)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.operate(TaskInstanceKillOperationFunction.java:69)
	at org.apache.dolphinscheduler.server.worker.rpc.TaskInstanceOperatorImpl.killTask(TaskInstanceOperatorImpl.java:49)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.dolphinscheduler.extract.base.server.ServerMethodInvokerImpl.invoke(ServerMethodInvokerImpl.java:41)
	at org.apache.dolphinscheduler.extract.base.server.JdkDynamicServerHandler.lambda$processReceived$0(JdkDynamicServerHandler.java:108)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[INFO] 2024-05-07 10:52:23.494 +0800 - Get appIds from worker 172.18.1.1:1234, taskLogPath: /opt/dolphinscheduler/logs/20240507/13505298546272/21/1061/3991.log
[INFO] 2024-05-07 10:52:23.496 +0800 - Start finding appId in /opt/dolphinscheduler/logs/20240507/13505298546272/21/1061/3991.log, fetch way: log 
[INFO] 2024-05-07 10:52:23.512 +0800 - The appId is empty
[INFO] 2024-05-07 10:52:23.513 +0800 - Begin to kill process process, pid is : 92
[INFO] 2024-05-07 10:52:23.599 +0800 - Success kill task: 1061_3991, pid: 92
[INFO] 2024-05-07 10:52:23.600 +0800 - kill task by cancelApplication, taskInstanceId: 3991
[INFO] 2024-05-07 10:52:23.758 +0800 - process has killed. execute path:/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1061/3991, processId:92 ,exitStatusCode:137 ,processWaitForStatus:true ,processExitValue:137
[INFO] 2024-05-07 10:52:23.762 +0800 - Start finding appId in /opt/dolphinscheduler/logs/20240507/13505298546272/21/1061/3991.log, fetch way: log 
[INFO] 2024-05-07 10:52:23.786 +0800 - ***********************************************************************************************
[INFO] 2024-05-07 10:52:23.791 +0800 - *********************************  Finalize task instance  ************************************
[INFO] 2024-05-07 10:52:23.791 +0800 - ***********************************************************************************************
[INFO] 2024-05-07 10:52:23.794 +0800 - Upload output files: [] successfully
[INFO] 2024-05-07 10:52:23.880 +0800 - Send task execute status: KILL to master : 172.18.1.1:1234
[INFO] 2024-05-07 10:52:23.882 +0800 - Remove the current task execute context from worker cache
[INFO] 2024-05-07 10:52:23.882 +0800 - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1061/3991
[INFO] 2024-05-07 10:52:24.113 +0800 - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1061/3991
[INFO] 2024-05-07 10:52:24.118 +0800 - FINALIZE_SESSION
