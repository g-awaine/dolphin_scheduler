[INFO] 2024-05-05 00:13:34.290 +0800 - ***********************************************************************************************
[INFO] 2024-05-05 00:13:34.425 +0800 - *********************************  Initialize task context  ***********************************
[INFO] 2024-05-05 00:13:34.480 +0800 - ***********************************************************************************************
[INFO] 2024-05-05 00:13:34.480 +0800 - Begin to initialize task
[INFO] 2024-05-05 00:13:34.480 +0800 - Set task startTime: 1714839214480
[INFO] 2024-05-05 00:13:34.481 +0800 - Set task appId: 937_3114
[INFO] 2024-05-05 00:13:34.481 +0800 - End initialize task {
  "taskInstanceId" : 3114,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1714839213867,
  "startTime" : 1714839214480,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13377949373536/25/937/3114.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 25,
  "processInstanceId" : 937,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_preprocessed|reddit_post_filtered\\\",\\\"gnews_preprocessed|gnews_filtered\\\"\"},{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"[\\\"singapore\\\", \\\"sg\\\"]\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import col, from_json\\nfrom pyspark.sql.types import StringType, StructType, StructField, IntegerType\\n\\nspark = SparkSession.builder \\\\\\n    .master(\\\"local\\\") \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the topics parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# filter rows containing specific keywords\\nkeywords = ${keywords}\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = parsed_df.filter(filter_condition)\\n\\n# stream the data to kafka\\nkafka_write = filtered_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export HADOOP_HOME=/opt/hadoop-3.4.0\nexport SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "[\"singapore\", \"sg\"]"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_preprocessed|reddit_post_filtered\",\"gnews_preprocessed|gnews_filtered\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3114"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505001333"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "937"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_data_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "937_3114",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[INFO] 2024-05-05 00:13:34.523 +0800 - ***********************************************************************************************
[INFO] 2024-05-05 00:13:34.615 +0800 - *********************************  Load task instance plugin  *********************************
[INFO] 2024-05-05 00:13:34.629 +0800 - ***********************************************************************************************
[INFO] 2024-05-05 00:13:34.675 +0800 - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[INFO] 2024-05-05 00:13:34.718 +0800 - TenantCode: default check successfully
[INFO] 2024-05-05 00:13:34.720 +0800 - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/937/3114 check successfully
[INFO] 2024-05-05 00:13:34.720 +0800 - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[INFO] 2024-05-05 00:13:34.721 +0800 - Download resources successfully: 
ResourceContext(resourceItemMap={})
[INFO] 2024-05-05 00:13:34.722 +0800 - Download upstream files: [] successfully
[INFO] 2024-05-05 00:13:34.722 +0800 - Task plugin instance: PYTHON create successfully
[INFO] 2024-05-05 00:13:34.723 +0800 - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, from_json\nfrom pyspark.sql.types import StringType, StructType, StructField, IntegerType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the topics parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# filter rows containing specific keywords\nkeywords = ${keywords}\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = parsed_df.filter(filter_condition)\n\n# stream the data to kafka\nkafka_write = filtered_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()\n",
  "resourceList" : [ ]
}
[INFO] 2024-05-05 00:13:34.724 +0800 - Success initialized task plugin instance successfully
[INFO] 2024-05-05 00:13:34.729 +0800 - Set taskVarPool: null successfully
[INFO] 2024-05-05 00:13:34.730 +0800 - ***********************************************************************************************
[INFO] 2024-05-05 00:13:34.753 +0800 - *********************************  Execute task instance  *************************************
[INFO] 2024-05-05 00:13:34.757 +0800 - ***********************************************************************************************
[INFO] 2024-05-05 00:13:34.772 +0800 - raw python script : from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json
from pyspark.sql.types import StringType, StructType, StructField, IntegerType

spark = SparkSession.builder \
    .master("local") \
    .appName("Reddit Keyword Filtering") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the topics parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# filter rows containing specific keywords
keywords = ${keywords}

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = parsed_df.filter(filter_condition)

# stream the data to kafka
kafka_write = filtered_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()

[INFO] 2024-05-05 00:13:35.163 +0800 - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/937/3114
[INFO] 2024-05-05 00:13:35.241 +0800 - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/937/3114/py_937_3114.py
[INFO] 2024-05-05 00:13:35.252 +0800 - #-*- encoding=utf8 -*-

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json
from pyspark.sql.types import StringType, StructType, StructField, IntegerType

spark = SparkSession.builder \
    .master("local") \
    .appName("Reddit Keyword Filtering") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the topics parameter
topics = "reddit_post_preprocessed|reddit_post_filtered","gnews_preprocessed|gnews_filtered"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# filter rows containing specific keywords
keywords = ["singapore", "sg"]

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = parsed_df.filter(filter_condition)

# stream the data to kafka
kafka_write = filtered_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()

[INFO] 2024-05-05 00:13:35.257 +0800 - Final Shell file is: 
[INFO] 2024-05-05 00:13:35.258 +0800 - ****************************** Script Content *****************************************************************
[INFO] 2024-05-05 00:13:35.339 +0800 - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export HADOOP_HOME=/opt/hadoop-3.4.0
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/937/3114/py_937_3114.py
[INFO] 2024-05-05 00:13:35.444 +0800 - ****************************** Script Content *****************************************************************
[INFO] 2024-05-05 00:13:35.508 +0800 - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/937/3114/937_3114.sh
[INFO] 2024-05-05 00:13:35.975 +0800 -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[INFO] 2024-05-05 00:13:35.954 +0800 - process start, process id is: 7869
[INFO] 2024-05-05 00:14:35.371 +0800 -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[INFO] 2024-05-05 00:14:36.375 +0800 -  -> 
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-e3b104e4-2317-4cb0-bb29-b19fa3910062;1.0
		confs: [default]
[INFO] 2024-05-05 00:14:39.386 +0800 -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
[INFO] 2024-05-05 00:14:41.397 +0800 -  -> 
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
[INFO] 2024-05-05 00:14:42.398 +0800 -  -> 
		found org.apache.kafka#kafka-clients;3.4.1 in central
[INFO] 2024-05-05 00:14:43.409 +0800 -  -> 
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
[INFO] 2024-05-05 00:14:45.420 +0800 -  -> 
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[INFO] 2024-05-05 00:14:47.446 +0800 -  -> 
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[INFO] 2024-05-05 00:14:48.454 +0800 -  -> 
		found commons-logging#commons-logging;1.1.3 in central
[INFO] 2024-05-05 00:14:49.458 +0800 -  -> 
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
[INFO] 2024-05-05 00:14:51.473 +0800 -  -> 
	:: resolution report :: resolve 13000ms :: artifacts dl 822ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
[INFO] 2024-05-05 00:14:52.478 +0800 -  -> 
	:: retrieving :: org.apache.spark#spark-submit-parent-e3b104e4-2317-4cb0-bb29-b19fa3910062
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/345ms)
[INFO] 2024-05-05 00:15:01.537 +0800 -  -> 
	24/05/05 00:15:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2024-05-05 00:15:07.582 +0800 -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[INFO] 2024-05-05 00:15:34.721 +0800 -  -> 
	24/05/05 00:15:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 00:15:34 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 00:15:34 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 00:15:34 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 00:15:34 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
	24/05/05 00:15:34 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
	24/05/05 00:15:34 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
	24/05/05 00:15:34 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
	24/05/05 00:15:34 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.
	24/05/05 00:15:34 WARN Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.
	24/05/05 00:15:34 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.
	24/05/05 00:15:34 WARN Utils: Service 'SparkUI' could not bind on port 4051. Attempting port 4052.
	24/05/05 00:15:34 WARN Utils: Service 'SparkUI' could not bind on port 4052. Attempting port 4053.
	24/05/05 00:15:34 WARN Utils: Service 'SparkUI' could not bind on port 4053. Attempting port 4054.
[INFO] 2024-05-05 00:15:35.723 +0800 -  -> 
	24/05/05 00:15:34 WARN Utils: Service 'SparkUI' could not bind on port 4054. Attempting port 4055.
	24/05/05 00:15:34 WARN Utils: Service 'SparkUI' could not bind on port 4055. Attempting port 4056.
	24/05/05 00:15:34 ERROR SparkUI: Failed to bind SparkUI
	java.net.BindException: Failed to bind to /0.0.0.0:4056: Service 'SparkUI' failed after 16 retries (starting from 4040)! Consider explicitly setting the appropriate port for the service 'SparkUI' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.
		at org.sparkproject.jetty.server.ServerConnector.openAcceptChannel(ServerConnector.java:349)
		at org.sparkproject.jetty.server.ServerConnector.open(ServerConnector.java:310)
		at org.sparkproject.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
		at org.sparkproject.jetty.server.ServerConnector.doStart(ServerConnector.java:234)
		at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
		at org.apache.spark.ui.JettyUtils$.newConnector$1(JettyUtils.scala:304)
		at org.apache.spark.ui.JettyUtils$.httpConnect$1(JettyUtils.scala:341)
		at org.apache.spark.ui.JettyUtils$.$anonfun$startJettyServer$8(JettyUtils.scala:344)
		at org.apache.spark.ui.JettyUtils$.$anonfun$startJettyServer$8$adapted(JettyUtils.scala:344)
		at org.apache.spark.util.Utils$.$anonfun$startServiceOnPort$2(Utils.scala:2256)
		at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)
		at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2248)
		at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:345)
		at org.apache.spark.ui.WebUI.initServer(WebUI.scala:145)
		at org.apache.spark.ui.SparkUI.bind(SparkUI.scala:145)
		at org.apache.spark.SparkContext.$anonfun$new$14(SparkContext.scala:511)
		at org.apache.spark.SparkContext.$anonfun$new$14$adapted(SparkContext.scala:511)
		at scala.Option.foreach(Option.scala:407)
		at org.apache.spark.SparkContext.<init>(SparkContext.scala:511)
		at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
		at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
		at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
		at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
		at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:238)
		at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
		at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
[INFO] 2024-05-05 00:15:36.726 +0800 -  -> 
	ERROR:root:Exception while sending command.
	Traceback (most recent call last):
	  File "/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py", line 516, in send_command
	    raise Py4JNetworkError("Answer from Java side is empty")
	py4j.protocol.Py4JNetworkError: Answer from Java side is empty
	
	During handling of the above exception, another exception occurred:
	
	Traceback (most recent call last):
	  File "/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py", line 1038, in send_command
	    response = connection.send_command(command)
	               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py", line 539, in send_command
	    raise Py4JNetworkError(
	py4j.protocol.Py4JNetworkError: Error while sending or receiving
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/937/3114/py_937_3114.py", line 13, in <module>
	    .getOrCreate()
	     ^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py", line 497, in getOrCreate
	    sc = SparkContext.getOrCreate(sparkConf)
	         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 515, in getOrCreate
	    SparkContext(conf=conf or SparkConf())
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 203, in __init__
	    self._do_init(
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 296, in _do_init
[INFO] 2024-05-05 00:15:37.728 +0800 -  -> 
	           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py", line 1587, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/py4j/protocol.py", line 334, in get_return_value
	    raise Py4JError(
	py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext
[INFO] 2024-05-05 00:15:37.729 +0800 - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/937/3114, processId:7869 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[INFO] 2024-05-05 00:15:37.732 +0800 - ***********************************************************************************************
[INFO] 2024-05-05 00:15:37.733 +0800 - *********************************  Finalize task instance  ************************************
[INFO] 2024-05-05 00:15:37.734 +0800 - ***********************************************************************************************
[INFO] 2024-05-05 00:15:37.742 +0800 - Upload output files: [] successfully
[INFO] 2024-05-05 00:15:37.748 +0800 - Send task execute status: FAILURE to master : 172.18.1.1:1234
[INFO] 2024-05-05 00:15:37.749 +0800 - Remove the current task execute context from worker cache
[INFO] 2024-05-05 00:15:37.750 +0800 - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/937/3114
[INFO] 2024-05-05 00:15:37.769 +0800 - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/937/3114
[INFO] 2024-05-05 00:15:37.771 +0800 - FINALIZE_SESSION
