[INFO] 2024-05-14 10:45:16.823 +0800 - ***********************************************************************************************
[INFO] 2024-05-14 10:45:16.844 +0800 - *********************************  Initialize task context  ***********************************
[INFO] 2024-05-14 10:45:16.844 +0800 - ***********************************************************************************************
[INFO] 2024-05-14 10:45:16.844 +0800 - Begin to initialize task
[INFO] 2024-05-14 10:45:16.844 +0800 - Set task startTime: 1715654716844
[INFO] 2024-05-14 10:45:16.852 +0800 - Set task appId: 1112_4013
[INFO] 2024-05-14 10:45:16.865 +0800 - End initialize task {
  "taskInstanceId" : 4013,
  "taskName" : "Extract Reddit Data",
  "firstSubmitTime" : 1715654716219,
  "startTime" : 1715654716844,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.16:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240514/13045665829504/20/1112/4013.log",
  "processId" : 0,
  "processDefineCode" : 13045665829504,
  "processDefineVersion" : 20,
  "processInstanceId" : 1112,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"subreddit\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"singapore\"},{\"prop\":\"limit\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"3\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import praw\\nimport json\\n\\nclass RedditPostDataExtractor:\\n    def __init__(self):\\n        self.data = []\\n\\n    def extract_data(self, hot_posts):\\n        if hot_posts:\\n            for post in hot_posts:\\n                post.comments.replace_more(limit=None)\\n                # initialise a dictionary to contain the data of the post\\n                post_data = {}\\n\\n                # get the relevant post's data and store it inside post_data\\n\\n                # get the poster's name\\n                if post.author is not None:\\n                    post_data['author'] = post.author.name\\n                else:\\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\\n\\n                # get the unix time in which the post was created\\n                post_data['unix_time'] = post.created_utc\\n                post_data['permalink'] = post.permalink\\n                post_data['score'] = post.score\\n                post_data['post_title'] = post.title\\n                post_data['body'] = post.selftext\\n\\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\\n                # t3 represents that it is a post\\n                post_data['name'] = post.name\\n\\n                # store the posts data in the dictionary where the name is used as the key\\n                self.data.append(post_data)\\n\\n                # extract the comments from the post\\n                comments = post.comments.list()\\n\\n                # extract the data in the comments and store them inside the dictionary\\n                self.extract_comments_data(comments)\\n        \\n        return self.data\\n\\n    def extract_comments_data(self, comments):\\n        if comments:\\n            for comment in comments:\\n                # initialise a dictionary to contain the data of the comment\\n                comment_data = {}\\n\\n\\n                # get the relevant post's data and store it inside post_data\\n\\n                # get the commenter name\\n                if comment.author is not None:\\n                    comment_data['author'] = comment.author.name\\n                else:\\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\\n\\n                # get the unix time in which the comment was created\\n                comment_data['unix_time'] = comment.created_utc\\n                comment_data['permalink'] = comment.permalink\\n                comment_data['score'] = comment.score\\n                comment_data['post_title'] = comment.submission.title\\n                comment_data['body'] = comment.body\\n\\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\\n                # t1 represents that it is a post\\n                comment_data['name'] = comment.name\\n\\n                # store the comment's data in the dictionary where the name is used as the key\\n                self.data.append(comment_data)\\n\\n                # extract the comment's replies from the post\\n                # replies = comment.replies\\n\\n                # extract the data in the comments and store them inside the dictionary\\n                # self.extract_comments_data(replies)\\n        \\n        return\\n    \\ndef get_hot_posts(reddit_instance, subreddit_name, limit):\\n    # Define the subreddit\\n    subreddit = reddit_instance.subreddit(subreddit_name)\\n\\n    # Get hot posts from the subreddit\\n    hot_posts = subreddit.hot(limit=limit)\\n\\n    # Filter out posts that are not pinned\\n    filtered_posts = [post for post in hot_posts if not post.stickied]\\n    return filtered_posts\\n\\n\\n# initialize Reddit instance\\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\\n                     user_agent='Prawlingv1')\\n\\n\\n# choose the subreddit to extract data from\\nsubreddit = \\\"${subreddit}\\\"\\nlimit = ${limit}\\n\\n# get hot posts from the subreddit\\nhot_posts = get_hot_posts(reddit, subreddit, limit)\\n\\n# initialise a reddit data extractor object\\ndata_extractor = RedditPostDataExtractor()\\n\\n# extract data from the hot posts from r/singapore\\ndata = data_extractor.extract_data(hot_posts)\\n\\nwith open(\\\"/local_storage/reddit/in/${system.workflow.instance.id}-${system.datetime}.json\\\", \\\"w\\\") as fp:\\n    json.dump(data, fp, indent=4)\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "Extract Reddit Data"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240514"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "4013"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13045662268160"
    },
    "subreddit" : {
      "prop" : "subreddit",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "singapore"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240514104516"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1112"
    },
    "limit" : {
      "prop" : "limit",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240513"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13045665829504"
    }
  },
  "taskAppId" : "1112_4013",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[INFO] 2024-05-14 10:45:16.867 +0800 - ***********************************************************************************************
[INFO] 2024-05-14 10:45:16.868 +0800 - *********************************  Load task instance plugin  *********************************
[INFO] 2024-05-14 10:45:16.868 +0800 - ***********************************************************************************************
[INFO] 2024-05-14 10:45:17.039 +0800 - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[INFO] 2024-05-14 10:45:17.090 +0800 - create linux os user: default
[INFO] 2024-05-14 10:45:17.091 +0800 - execute cmd: sudo useradd -g root
 default
[INFO] 2024-05-14 10:45:17.343 +0800 - create user default success
[INFO] 2024-05-14 10:45:17.345 +0800 - TenantCode: default check successfully
[INFO] 2024-05-14 10:45:17.352 +0800 - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_20/1112/4013 check successfully
[INFO] 2024-05-14 10:45:17.354 +0800 - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[INFO] 2024-05-14 10:45:17.368 +0800 - Download resources successfully: 
ResourceContext(resourceItemMap={})
[INFO] 2024-05-14 10:45:17.382 +0800 - Download upstream files: [] successfully
[INFO] 2024-05-14 10:45:17.385 +0800 - Task plugin instance: PYTHON create successfully
[INFO] 2024-05-14 10:45:17.403 +0800 - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import praw\nimport json\n\nclass RedditPostDataExtractor:\n    def __init__(self):\n        self.data = []\n\n    def extract_data(self, hot_posts):\n        if hot_posts:\n            for post in hot_posts:\n                post.comments.replace_more(limit=None)\n                # initialise a dictionary to contain the data of the post\n                post_data = {}\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the poster's name\n                if post.author is not None:\n                    post_data['author'] = post.author.name\n                else:\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the post was created\n                post_data['unix_time'] = post.created_utc\n                post_data['permalink'] = post.permalink\n                post_data['score'] = post.score\n                post_data['post_title'] = post.title\n                post_data['body'] = post.selftext\n\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\n                # t3 represents that it is a post\n                post_data['name'] = post.name\n\n                # store the posts data in the dictionary where the name is used as the key\n                self.data.append(post_data)\n\n                # extract the comments from the post\n                comments = post.comments.list()\n\n                # extract the data in the comments and store them inside the dictionary\n                self.extract_comments_data(comments)\n        \n        return self.data\n\n    def extract_comments_data(self, comments):\n        if comments:\n            for comment in comments:\n                # initialise a dictionary to contain the data of the comment\n                comment_data = {}\n\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the commenter name\n                if comment.author is not None:\n                    comment_data['author'] = comment.author.name\n                else:\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the comment was created\n                comment_data['unix_time'] = comment.created_utc\n                comment_data['permalink'] = comment.permalink\n                comment_data['score'] = comment.score\n                comment_data['post_title'] = comment.submission.title\n                comment_data['body'] = comment.body\n\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\n                # t1 represents that it is a post\n                comment_data['name'] = comment.name\n\n                # store the comment's data in the dictionary where the name is used as the key\n                self.data.append(comment_data)\n\n                # extract the comment's replies from the post\n                # replies = comment.replies\n\n                # extract the data in the comments and store them inside the dictionary\n                # self.extract_comments_data(replies)\n        \n        return\n    \ndef get_hot_posts(reddit_instance, subreddit_name, limit):\n    # Define the subreddit\n    subreddit = reddit_instance.subreddit(subreddit_name)\n\n    # Get hot posts from the subreddit\n    hot_posts = subreddit.hot(limit=limit)\n\n    # Filter out posts that are not pinned\n    filtered_posts = [post for post in hot_posts if not post.stickied]\n    return filtered_posts\n\n\n# initialize Reddit instance\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\n                     user_agent='Prawlingv1')\n\n\n# choose the subreddit to extract data from\nsubreddit = \"${subreddit}\"\nlimit = ${limit}\n\n# get hot posts from the subreddit\nhot_posts = get_hot_posts(reddit, subreddit, limit)\n\n# initialise a reddit data extractor object\ndata_extractor = RedditPostDataExtractor()\n\n# extract data from the hot posts from r/singapore\ndata = data_extractor.extract_data(hot_posts)\n\nwith open(\"/local_storage/reddit/in/${system.workflow.instance.id}-${system.datetime}.json\", \"w\") as fp:\n    json.dump(data, fp, indent=4)",
  "resourceList" : [ ]
}
[INFO] 2024-05-14 10:45:17.404 +0800 - Success initialized task plugin instance successfully
[INFO] 2024-05-14 10:45:17.404 +0800 - Set taskVarPool: null successfully
[INFO] 2024-05-14 10:45:17.410 +0800 - ***********************************************************************************************
[INFO] 2024-05-14 10:45:17.411 +0800 - *********************************  Execute task instance  *************************************
[INFO] 2024-05-14 10:45:17.411 +0800 - ***********************************************************************************************
[INFO] 2024-05-14 10:45:17.412 +0800 - raw python script : import praw
import json

class RedditPostDataExtractor:
    def __init__(self):
        self.data = []

    def extract_data(self, hot_posts):
        if hot_posts:
            for post in hot_posts:
                post.comments.replace_more(limit=None)
                # initialise a dictionary to contain the data of the post
                post_data = {}

                # get the relevant post's data and store it inside post_data

                # get the poster's name
                if post.author is not None:
                    post_data['author'] = post.author.name
                else:
                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the post was created
                post_data['unix_time'] = post.created_utc
                post_data['permalink'] = post.permalink
                post_data['score'] = post.score
                post_data['post_title'] = post.title
                post_data['body'] = post.selftext

                # get the unique name of the post (name refers to the id of the post prefixed with t3_)
                # t3 represents that it is a post
                post_data['name'] = post.name

                # store the posts data in the dictionary where the name is used as the key
                self.data.append(post_data)

                # extract the comments from the post
                comments = post.comments.list()

                # extract the data in the comments and store them inside the dictionary
                self.extract_comments_data(comments)
        
        return self.data

    def extract_comments_data(self, comments):
        if comments:
            for comment in comments:
                # initialise a dictionary to contain the data of the comment
                comment_data = {}


                # get the relevant post's data and store it inside post_data

                # get the commenter name
                if comment.author is not None:
                    comment_data['author'] = comment.author.name
                else:
                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the comment was created
                comment_data['unix_time'] = comment.created_utc
                comment_data['permalink'] = comment.permalink
                comment_data['score'] = comment.score
                comment_data['post_title'] = comment.submission.title
                comment_data['body'] = comment.body

                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)
                # t1 represents that it is a post
                comment_data['name'] = comment.name

                # store the comment's data in the dictionary where the name is used as the key
                self.data.append(comment_data)

                # extract the comment's replies from the post
                # replies = comment.replies

                # extract the data in the comments and store them inside the dictionary
                # self.extract_comments_data(replies)
        
        return
    
def get_hot_posts(reddit_instance, subreddit_name, limit):
    # Define the subreddit
    subreddit = reddit_instance.subreddit(subreddit_name)

    # Get hot posts from the subreddit
    hot_posts = subreddit.hot(limit=limit)

    # Filter out posts that are not pinned
    filtered_posts = [post for post in hot_posts if not post.stickied]
    return filtered_posts


# initialize Reddit instance
reddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',
                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',
                     user_agent='Prawlingv1')


# choose the subreddit to extract data from
subreddit = "${subreddit}"
limit = ${limit}

# get hot posts from the subreddit
hot_posts = get_hot_posts(reddit, subreddit, limit)

# initialise a reddit data extractor object
data_extractor = RedditPostDataExtractor()

# extract data from the hot posts from r/singapore
data = data_extractor.extract_data(hot_posts)

with open("/local_storage/reddit/in/${system.workflow.instance.id}-${system.datetime}.json", "w") as fp:
    json.dump(data, fp, indent=4)
[INFO] 2024-05-14 10:45:17.419 +0800 - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_20/1112/4013
[INFO] 2024-05-14 10:45:17.421 +0800 - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_20/1112/4013/py_1112_4013.py
[INFO] 2024-05-14 10:45:17.421 +0800 - #-*- encoding=utf8 -*-

import praw
import json

class RedditPostDataExtractor:
    def __init__(self):
        self.data = []

    def extract_data(self, hot_posts):
        if hot_posts:
            for post in hot_posts:
                post.comments.replace_more(limit=None)
                # initialise a dictionary to contain the data of the post
                post_data = {}

                # get the relevant post's data and store it inside post_data

                # get the poster's name
                if post.author is not None:
                    post_data['author'] = post.author.name
                else:
                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the post was created
                post_data['unix_time'] = post.created_utc
                post_data['permalink'] = post.permalink
                post_data['score'] = post.score
                post_data['post_title'] = post.title
                post_data['body'] = post.selftext

                # get the unique name of the post (name refers to the id of the post prefixed with t3_)
                # t3 represents that it is a post
                post_data['name'] = post.name

                # store the posts data in the dictionary where the name is used as the key
                self.data.append(post_data)

                # extract the comments from the post
                comments = post.comments.list()

                # extract the data in the comments and store them inside the dictionary
                self.extract_comments_data(comments)
        
        return self.data

    def extract_comments_data(self, comments):
        if comments:
            for comment in comments:
                # initialise a dictionary to contain the data of the comment
                comment_data = {}


                # get the relevant post's data and store it inside post_data

                # get the commenter name
                if comment.author is not None:
                    comment_data['author'] = comment.author.name
                else:
                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the comment was created
                comment_data['unix_time'] = comment.created_utc
                comment_data['permalink'] = comment.permalink
                comment_data['score'] = comment.score
                comment_data['post_title'] = comment.submission.title
                comment_data['body'] = comment.body

                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)
                # t1 represents that it is a post
                comment_data['name'] = comment.name

                # store the comment's data in the dictionary where the name is used as the key
                self.data.append(comment_data)

                # extract the comment's replies from the post
                # replies = comment.replies

                # extract the data in the comments and store them inside the dictionary
                # self.extract_comments_data(replies)
        
        return
    
def get_hot_posts(reddit_instance, subreddit_name, limit):
    # Define the subreddit
    subreddit = reddit_instance.subreddit(subreddit_name)

    # Get hot posts from the subreddit
    hot_posts = subreddit.hot(limit=limit)

    # Filter out posts that are not pinned
    filtered_posts = [post for post in hot_posts if not post.stickied]
    return filtered_posts


# initialize Reddit instance
reddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',
                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',
                     user_agent='Prawlingv1')


# choose the subreddit to extract data from
subreddit = "singapore"
limit = 3

# get hot posts from the subreddit
hot_posts = get_hot_posts(reddit, subreddit, limit)

# initialise a reddit data extractor object
data_extractor = RedditPostDataExtractor()

# extract data from the hot posts from r/singapore
data = data_extractor.extract_data(hot_posts)

with open("/local_storage/reddit/in/1112-20240514104516.json", "w") as fp:
    json.dump(data, fp, indent=4)
[INFO] 2024-05-14 10:45:17.448 +0800 - Final Shell file is: 
[INFO] 2024-05-14 10:45:17.448 +0800 - ****************************** Script Content *****************************************************************
[INFO] 2024-05-14 10:45:17.449 +0800 - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_20/1112/4013/py_1112_4013.py
[INFO] 2024-05-14 10:45:17.449 +0800 - ****************************** Script Content *****************************************************************
[INFO] 2024-05-14 10:45:17.450 +0800 - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_20/1112/4013/1112_4013.sh
[INFO] 2024-05-14 10:45:17.470 +0800 - process start, process id is: 447
[INFO] 2024-05-14 10:45:18.477 +0800 -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[INFO] 2024-05-14 10:45:21.481 +0800 - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_20/1112/4013, processId:447 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[INFO] 2024-05-14 10:45:21.483 +0800 - ***********************************************************************************************
[INFO] 2024-05-14 10:45:21.483 +0800 - *********************************  Finalize task instance  ************************************
[INFO] 2024-05-14 10:45:21.484 +0800 - ***********************************************************************************************
[INFO] 2024-05-14 10:45:21.485 +0800 - Upload output files: [] successfully
[INFO] 2024-05-14 10:45:21.503 +0800 - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[INFO] 2024-05-14 10:45:21.504 +0800 - Remove the current task execute context from worker cache
[INFO] 2024-05-14 10:45:21.505 +0800 - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_20/1112/4013
[INFO] 2024-05-14 10:45:21.528 +0800 - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_20/1112/4013
[INFO] 2024-05-14 10:45:21.532 +0800 - FINALIZE_SESSION
