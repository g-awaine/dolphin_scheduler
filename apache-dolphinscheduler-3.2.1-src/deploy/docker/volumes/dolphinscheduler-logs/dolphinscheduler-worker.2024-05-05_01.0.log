[WI-0][TI-0] - [INFO] 2024-05-05 01:00:00.846 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8367346938775511 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:01.864 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8922957151839985 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:02.868 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9316574839302112 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:03.880 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9658119658119658 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:04.891 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9911764705882353 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:05.972 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9540229885057472 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:07.000 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9945311942959002 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:08.006 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8677946238921849 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:09.010 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.931835277924489 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:10.026 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7488088415882113 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:11.031 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8184438040345822 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:12.035 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7449275362318841 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:13.041 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8895348837209303 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:14.044 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7014771261089779 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:15.050 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7012729224564964 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:16.053 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7012729224564964 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:17.074 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7013478832909517 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:18.078 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8011363636363636 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:19.082 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7018903054564528 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:20.147 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8732782369146005 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:21.151 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8955223880597014 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:22.156 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7027629927307875 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:23.186 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7288629737609329 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:24.202 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7031654349826109 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:25.209 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7023516028992545 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:26.240 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7026021351311208 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:27.257 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7033243042312309 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:28.314 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8230088495575222 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:29.341 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7032853325507183 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:30.359 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7033201286940332 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:31.362 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8032786885245902 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:32.366 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7029974193191767 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:33.370 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7032219041523331 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:34.374 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.702844713958801 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:35.381 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7032570979658572 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:36.389 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7037172023980309 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:37.423 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7037259511426358 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:38.427 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7029888694096764 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:39.431 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7030006006808511 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:40.442 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7030688011217481 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:41.446 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9344262295081966 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:42.455 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9587912087912088 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:43.078 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 1:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:43.481 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9682080924855492 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:44.023 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 1:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:44.058 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 1:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:44.093 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 1:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:44.552 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0032362459546924 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:45.580 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9893617021276596 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:46.756 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9212444032731202 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:47.849 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.919732441471572 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:48.138 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:00:47 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:48.870 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.972972972972973 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:49.130 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:00:48 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:49.952 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0265486725663717 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:50.161 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:00:50 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:51.028 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9885965749936338 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:51.127 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:00:50 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:52.041 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9825504166163507 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:53.052 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9739130434782608 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:54.070 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9883474760095095 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:54.173 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:54.180 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:55.073 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9862258953168044 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:55.178 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:55.184 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:56.078 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.969187675070028 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:57.083 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9940298507462687 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:58.091 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.993920972644377 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:00:59.101 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.940625 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:00.106 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9783783783783784 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:01.185 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.992248062015504 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:02.195 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9769736842105263 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:03.261 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9485714285714286 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:04.270 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9077809798270894 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:05.273 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7383720930232558 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:06.276 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8515406162464986 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:07.282 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7352184491935546 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:08.284 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7347754445803814 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:09.286 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7086834733893557 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:10.288 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7340918494905745 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:11.308 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7265415549597856 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:12.316 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.781437125748503 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:13.319 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.861197849600824 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:14.329 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9011299435028248 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:15.336 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9831932773109244 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:16.340 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9329446064139941 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:17.349 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8401084010840107 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:18.354 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8693181818181819 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:19.365 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.845303867403315 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:20.384 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9518413597733711 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:21.513 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.748135984917594 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:22.517 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7707736389684814 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:23.522 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8169014084507042 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:24.524 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7417571416101627 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:25.534 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7707736389684814 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:26.538 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8888888888888888 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:27.544 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9226361031518625 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:28.555 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8493975903614458 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:29.563 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7869822485207101 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:30.567 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8595505617977528 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:31.622 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9177578487923317 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:32.629 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9090909090909091 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:33.631 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7929155313351499 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:34.634 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8753462603878116 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:35.636 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7959770114942528 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:36.638 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7606837606837606 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:37.645 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.736677700026624 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:38.646 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7357644503909396 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:39.659 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8651685393258427 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:40.660 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8208092485549132 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:41.682 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7988980716253443 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:42.698 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8493150684931507 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:43.701 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8472622478386168 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:44.703 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8409090909090909 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:45.705 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7311262239046323 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:46.714 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7288428015627644 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:47.716 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7289537515511624 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:48.719 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7299660210689654 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:49.722 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8448753462603877 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:50.728 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7301207147803876 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:51.757 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7303744283739285 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:52.769 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7953890489913545 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:53.771 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8022598870056497 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:54.776 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7292806364632166 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:55.783 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.729050279329609 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:56.790 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.723365689769914 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:57.793 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7931034482758621 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:58.800 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7793296089385475 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:01:59.803 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8816901408450704 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:00.806 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8388888888888889 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:01.820 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7982954545454546 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:02.821 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8863636363636364 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:03.838 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8314285714285714 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:04.840 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7982543310477327 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:05.842 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7111102892593452 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:06.844 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7111711328013699 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:07.845 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7117340349826506 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:08.850 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.817109144542773 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:09.857 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7115087548090756 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:10.861 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7449856733524355 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:11.910 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7796143250688706 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:12.916 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7115560775639836 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:13.943 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8855585831062671 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:14.951 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8153409090909092 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:15.961 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7111524423015323 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:16.963 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7114823097401564 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:17.969 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7420289855072464 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:18.986 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7982708933717579 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:19.999 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7842565597667638 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:21.010 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7404371584699453 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:22.069 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7792767845934591 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:23.070 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7922437673130194 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:24.071 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8200000000000001 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:25.073 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7114465194213183 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:26.074 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7114968247027962 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:27.075 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.711634418595218 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:28.076 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7109699116754582 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:29.077 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7112578249069998 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:30.085 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7119463908744231 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:31.089 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.711095376626496 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:32.096 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7116336232547994 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:33.097 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7109430689363296 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:34.098 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7111430970516134 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:35.099 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7107146074010801 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:36.100 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.711052030573681 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:37.102 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.709985479072307 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:38.184 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7100345913431569 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:39.186 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7102024081714865 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:40.187 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7103398032288037 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:41.189 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7105652822374835 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:42.201 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7105652822374835 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:43.203 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7109899940210284 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:44.204 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7105998795456936 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:45.205 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7108726813092815 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:46.210 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7110333400738433 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:47.213 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7104060153186541 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:48.214 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7105801948703326 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:49.216 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.710863137224258 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:50.218 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7098148785525122 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:51.219 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7101725829057881 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:52.228 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7101966419534516 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:53.234 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7107362804274875 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:54.236 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8212290502793296 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:55.239 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7103745993721186 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:56.240 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7869318181818181 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:57.257 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8608695652173912 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:58.260 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7114542739903998 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:02:59.263 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7120553525117745 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:00.264 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8480243161094225 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:01.266 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7103161418513497 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:02.318 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7103398032288037 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:03.327 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7111096927540312 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:04.328 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7113702623906706 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:05.331 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8668555240793201 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:06.346 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8280802292263612 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:07.353 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.883008356545961 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:08.357 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9180790960451977 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:09.383 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8817204301075269 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:10.331 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[55] - Receive TaskInstanceKillRequest: TaskInstanceKillRequest(taskInstanceId=3258)
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:10.421 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9331089445991952 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3258] - [INFO] 2024-05-05 01:03:10.589 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[134] - process id:18118, cmd:sudo -u default kill -9 18118 18122 18125 18128 18129 18147 18148 18149 18150 18151 18152 18153 18154 18155 18156 18157 18158 18159 18160 18169 18170 18172 18173 18174 18175 18176 18177 18178 18179 18180 18181 18182 18183 18184 18185 18186 18187 18188 18189 18190 18191 18192 18193 18194 18195 18196 18219 18220 18221 18222 18223 18224 18225 18226 18227 18250 18251 18274 18275 18276 18277 18278 18279 18281 18308 18313 18358 18171 18280
[WI-0][TI-3258] - [ERROR] 2024-05-05 01:03:10.736 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[139] - kill task error
org.apache.dolphinscheduler.common.shell.AbstractShell$ExitCodeException: kill: (18118): Operation not permitted
kill: (18147): No such process
kill: (18148): No such process
kill: (18149): No such process
kill: (18150): No such process
kill: (18151): No such process
kill: (18152): No such process
kill: (18154): No such process
kill: (18155): No such process
kill: (18157): No such process
kill: (18158): No such process
kill: (18160): No such process
kill: (18169): No such process
kill: (18170): No such process
kill: (18172): No such process
kill: (18173): No such process
kill: (18174): No such process
kill: (18175): No such process
kill: (18176): No such process
kill: (18177): No such process
kill: (18178): No such process
kill: (18179): No such process
kill: (18180): No such process
kill: (18181): No such process
kill: (18183): No such process
kill: (18184): No such process
kill: (18186): No such process
kill: (18187): No such process
kill: (18188): No such process
kill: (18189): No such process
kill: (18190): No such process
kill: (18191): No such process
kill: (18192): No such process
kill: (18193): No such process
kill: (18194): No such process
kill: (18195): No such process
kill: (18196): No such process
kill: (18219): No such process
kill: (18220): No such process
kill: (18221): No such process
kill: (18222): No such process
kill: (18223): No such process
kill: (18224): No such process
kill: (18225): No such process
kill: (18226): No such process
kill: (18227): No such process
kill: (18250): No such process
kill: (18251): No such process
kill: (18274): No such process
kill: (18275): No such process
kill: (18276): No such process
kill: (18277): No such process
kill: (18278): No such process
kill: (18279): No such process
kill: (18308): No such process
kill: (18313): No such process
kill: (18358): No such process
kill: (18171): No such process

	at org.apache.dolphinscheduler.common.shell.AbstractShell.runCommand(AbstractShell.java:205)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.run(AbstractShell.java:118)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execute(ShellExecutor.java:125)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:103)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:86)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeShell(OSUtils.java:345)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeCmd(OSUtils.java:334)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.killProcess(TaskInstanceKillOperationFunction.java:135)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.doKill(TaskInstanceKillOperationFunction.java:96)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.operate(TaskInstanceKillOperationFunction.java:69)
	at org.apache.dolphinscheduler.server.worker.rpc.TaskInstanceOperatorImpl.killTask(TaskInstanceOperatorImpl.java:49)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.dolphinscheduler.extract.base.server.ServerMethodInvokerImpl.invoke(ServerMethodInvokerImpl.java:41)
	at org.apache.dolphinscheduler.extract.base.server.JdkDynamicServerHandler.lambda$processReceived$0(JdkDynamicServerHandler.java:108)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[WI-0][TI-3258] - [INFO] 2024-05-05 01:03:10.736 +0800 o.a.d.p.t.a.u.ProcessUtils:[182] - Get appIds from worker 172.18.1.1:1234, taskLogPath: /opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log
[WI-0][TI-3258] - [INFO] 2024-05-05 01:03:10.744 +0800 o.a.d.p.t.a.u.LogUtils:[79] - Start finding appId in /opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, fetch way: log 
[WI-0][TI-3258] - [INFO] 2024-05-05 01:03:10.752 +0800 o.a.d.p.t.a.u.ProcessUtils:[188] - The appId is empty
[WI-0][TI-3258] - [INFO] 2024-05-05 01:03:10.755 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[220] - Begin to kill process process, pid is : 18118
[WI-0][TI-3258] - [INFO] 2024-05-05 01:03:10.762 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[225] - Success kill task: 938_3258, pid: 18118
[WI-0][TI-3258] - [INFO] 2024-05-05 01:03:10.762 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[119] - kill task by cancelApplication, taskInstanceId: 3258
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:11.423 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8879985262963986 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-938][TI-3258] - [INFO] 2024-05-05 01:03:11.427 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has killed. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, processId:18118 ,exitStatusCode:137 ,processWaitForStatus:true ,processExitValue:137
[WI-938][TI-3258] - [INFO] 2024-05-05 01:03:11.454 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-938][TI-3258] - [INFO] 2024-05-05 01:03:11.456 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-938][TI-3258] - [INFO] 2024-05-05 01:03:11.456 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-938][TI-3258] - [INFO] 2024-05-05 01:03:11.466 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-938][TI-3258] - [INFO] 2024-05-05 01:03:11.494 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: KILL to master : 172.18.1.1:1234
[WI-938][TI-3258] - [INFO] 2024-05-05 01:03:11.500 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-938][TI-3258] - [INFO] 2024-05-05 01:03:11.501 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258
[WI-938][TI-3258] - [INFO] 2024-05-05 01:03:11.502 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258
[WI-938][TI-3258] - [INFO] 2024-05-05 01:03:11.502 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3258] - [INFO] 2024-05-05 01:03:11.649 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3258, processInstanceId=938, status=9, startTime=1714841447434, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, endTime=1714842191466, processId=18118, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3258] - [INFO] 2024-05-05 01:03:11.684 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3258, processInstanceId=938, status=9, startTime=1714841447434, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, endTime=1714842191466, processId=18118, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842191629)
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:12.469 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7703081232492996 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:13.470 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7357954545454546 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:17.490 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7166666666666667 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:21.544 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7369942196531791 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:26.591 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7725947521865889 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:31.622 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7469879518072289 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:31.929 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3259, taskName=sentiment analysis, firstSubmitTime=1714842211827, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=939, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3259'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505010331'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='939'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:31.945 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:31.948 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:31.965 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:31.979 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:31.991 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:31.991 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714842211991
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:31.992 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 939_3259
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:31.993 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3259,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714842211827,
  "startTime" : 1714842211991,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 939,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3259"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505010331"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "939"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "939_3259",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:32.005 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:32.006 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:32.006 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:32.077 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:32.080 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:32.123 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259 check successfully
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:32.123 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:32.123 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:32.124 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:32.124 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:32.135 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:32.179 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:32.181 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:32.183 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:32.184 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:32.184 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:32.184 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:32.185 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:32.206 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259/py_939_3259.py
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:32.189 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3260, taskName=sentiment analysis, firstSubmitTime=1714842212095, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=940, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3260'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505010332'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='940'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:32.216 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:32.217 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:32.218 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:32.219 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259/py_939_3259.py
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:32.219 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.219 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.221 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.221 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.221 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.222 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714842212222
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.222 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 940_3260
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.222 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3260,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714842212095,
  "startTime" : 1714842212222,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 940,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3260"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505010332"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "940"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "940_3260",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.224 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.225 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.225 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.219 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:32.219 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259/939_3259.sh
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.302 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.303 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:32.304 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 18762
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.306 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260 check successfully
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.306 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.306 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.306 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.308 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:32.283 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3261, taskName=sentiment analysis, firstSubmitTime=1714842212079, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=941, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3261'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505010332'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='941'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.309 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.309 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.309 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.309 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.309 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.310 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.310 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.310 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.355 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.315 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260/py_940_3260.py
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.355 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.365 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.366 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.366 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.366 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714842212366
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.366 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 941_3261
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.370 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3261,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714842212079,
  "startTime" : 1714842212366,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 941,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3261"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505010332"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "941"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "941_3261",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.386 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.388 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.388 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.359 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.407 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.408 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.410 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261 check successfully
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.410 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.411 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.411 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.415 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.416 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.431 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.431 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.431 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.431 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.431 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.431 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.436 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.437 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261/py_941_3261.py
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:32.373 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3262, taskName=sentiment analysis, firstSubmitTime=1714842212199, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=942, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3262'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505010332'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='942'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.439 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.441 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.445 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.405 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.465 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.466 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.466 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.466 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714842212466
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.466 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 942_3262
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.466 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3262,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714842212199,
  "startTime" : 1714842212466,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 942,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3262"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505010332"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "942"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "942_3262",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.465 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.469 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.475 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.476 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.487 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.487 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.488 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262 check successfully
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.488 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.488 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.488 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.488 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.489 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.489 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.489 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.489 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.489 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.489 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.489 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.490 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.492 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.495 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.495 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261/py_941_3261.py
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.499 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.498 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262/py_942_3262.py
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.502 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.469 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260/py_940_3260.py
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.504 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.504 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260/940_3260.sh
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.505 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261/941_3261.sh
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.503 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.507 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.517 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262/py_942_3262.py
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.517 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.518 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262/942_3262.sh
[WI-942][TI-3262] - [INFO] 2024-05-05 01:03:32.565 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 18774
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:32.623 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 18773
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:32.751 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 18772
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:32.797 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:32.801 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0142857142857142 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3259] - [INFO] 2024-05-05 01:03:32.809 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3259, processInstanceId=939, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:32.752 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-3259] - [INFO] 2024-05-05 01:03:32.888 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3259, success=true)
[WI-0][TI-3261] - [INFO] 2024-05-05 01:03:32.944 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3261, success=true)
[WI-0][TI-3259] - [INFO] 2024-05-05 01:03:32.948 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3259, processInstanceId=939, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714842212750)
[WI-0][TI-3260] - [INFO] 2024-05-05 01:03:32.952 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3260, processInstanceId=940, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3260] - [INFO] 2024-05-05 01:03:32.944 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3260, success=true)
[WI-0][TI-3262] - [INFO] 2024-05-05 01:03:32.943 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3262, success=true)
[WI-0][TI-3260] - [INFO] 2024-05-05 01:03:32.966 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3260, processInstanceId=940, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714842212750)
[WI-0][TI-3261] - [INFO] 2024-05-05 01:03:33.015 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3261, processInstanceId=941, startTime=1714842212366, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3262] - [INFO] 2024-05-05 01:03:33.022 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3262)
[WI-0][TI-3260] - [INFO] 2024-05-05 01:03:33.023 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3260)
[WI-0][TI-3259] - [INFO] 2024-05-05 01:03:33.022 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3259)
[WI-0][TI-3261] - [INFO] 2024-05-05 01:03:33.023 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3261)
[WI-0][TI-3261] - [INFO] 2024-05-05 01:03:33.033 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3261, processInstanceId=941, startTime=1714842212366, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714842212750)
[WI-0][TI-3261] - [WARN] 2024-05-05 01:03:33.035 +0800 o.a.d.s.w.m.MessageRetryRunner:[139] - Retry send message to master error
java.util.ConcurrentModificationException: null
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:911)
	at java.util.ArrayList$Itr.next(ArrayList.java:861)
	at org.apache.dolphinscheduler.server.worker.message.MessageRetryRunner.run(MessageRetryRunner.java:127)
[WI-0][TI-3261] - [INFO] 2024-05-05 01:03:33.040 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3261)
[WI-0][TI-3260] - [INFO] 2024-05-05 01:03:33.059 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3260, success=true)
[WI-0][TI-3259] - [INFO] 2024-05-05 01:03:33.143 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3259, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:33.356 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:33.801 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:33.859 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0362694300518136 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:34.365 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:34.821 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:34.859 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:34.861 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9393939393939393 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:34.964 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:35.871 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9801136363636364 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:36.872 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9859154929577465 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:37.877 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9811320754716981 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:38.888 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9889807162534434 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:39.895 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9714285714285714 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:40.907 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9601226993865031 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:41.912 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9852941176470589 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:42.823 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[55] - Receive TaskInstanceKillRequest: TaskInstanceKillRequest(taskInstanceId=3261)
[WI-0][TI-3261] - [INFO] 2024-05-05 01:03:43.029 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[134] - process id:18773, cmd:sudo -u default kill -9 18773 18791 18799 18802 18817 18890 18892 18893 18894 18895 18896 18897 18898 18916 18917 18920 18921 18922 18924
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:43.085 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9976303317535545 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3261] - [ERROR] 2024-05-05 01:03:43.257 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[139] - kill task error
org.apache.dolphinscheduler.common.shell.AbstractShell$ExitCodeException: kill: (18773): Operation not permitted
kill: (18890): No such process
kill: (18892): No such process
kill: (18893): No such process
kill: (18894): No such process
kill: (18895): No such process
kill: (18896): No such process
kill: (18897): No such process
kill: (18898): No such process
kill: (18916): No such process
kill: (18917): No such process
kill: (18920): No such process
kill: (18922): No such process
kill: (18924): No such process

	at org.apache.dolphinscheduler.common.shell.AbstractShell.runCommand(AbstractShell.java:205)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.run(AbstractShell.java:118)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execute(ShellExecutor.java:125)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:103)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:86)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeShell(OSUtils.java:345)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeCmd(OSUtils.java:334)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.killProcess(TaskInstanceKillOperationFunction.java:135)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.doKill(TaskInstanceKillOperationFunction.java:96)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.operate(TaskInstanceKillOperationFunction.java:69)
	at org.apache.dolphinscheduler.server.worker.rpc.TaskInstanceOperatorImpl.killTask(TaskInstanceOperatorImpl.java:49)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.dolphinscheduler.extract.base.server.ServerMethodInvokerImpl.invoke(ServerMethodInvokerImpl.java:41)
	at org.apache.dolphinscheduler.extract.base.server.JdkDynamicServerHandler.lambda$processReceived$0(JdkDynamicServerHandler.java:108)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[WI-0][TI-3261] - [INFO] 2024-05-05 01:03:43.268 +0800 o.a.d.p.t.a.u.ProcessUtils:[182] - Get appIds from worker 172.18.1.1:1234, taskLogPath: /opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log
[WI-0][TI-3261] - [INFO] 2024-05-05 01:03:43.268 +0800 o.a.d.p.t.a.u.LogUtils:[79] - Start finding appId in /opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, fetch way: log 
[WI-0][TI-3261] - [INFO] 2024-05-05 01:03:43.269 +0800 o.a.d.p.t.a.u.ProcessUtils:[188] - The appId is empty
[WI-0][TI-3261] - [INFO] 2024-05-05 01:03:43.281 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[220] - Begin to kill process process, pid is : 18773
[WI-0][TI-3261] - [INFO] 2024-05-05 01:03:43.290 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[225] - Success kill task: 941_3261, pid: 18773
[WI-0][TI-3261] - [INFO] 2024-05-05 01:03:43.290 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[119] - kill task by cancelApplication, taskInstanceId: 3261
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:43.862 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has killed. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, processId:18773 ,exitStatusCode:137 ,processWaitForStatus:true ,processExitValue:137
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:43.863 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:43.863 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:43.865 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:43.866 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:43.876 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: KILL to master : 172.18.1.1:1234
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:43.877 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:43.878 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:43.879 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261
[WI-941][TI-3261] - [INFO] 2024-05-05 01:03:43.879 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3261] - [INFO] 2024-05-05 01:03:44.066 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3261, processInstanceId=941, status=9, startTime=1714842212366, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, endTime=1714842223865, processId=18773, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:44.091 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9972144846796658 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3261] - [INFO] 2024-05-05 01:03:44.094 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3261, processInstanceId=941, status=9, startTime=1714842212366, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, endTime=1714842223865, processId=18773, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842224064)
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:45.097 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9826086956521739 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:45.596 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[55] - Receive TaskInstanceKillRequest: TaskInstanceKillRequest(taskInstanceId=3260)
[WI-0][TI-3260] - [INFO] 2024-05-05 01:03:45.945 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[134] - process id:18772, cmd:sudo -u default kill -9 18772 18777 18788 18794 18813 18904 18908 18912 18915 18918 18923 18925 18926 18933 18934 18935 18936 18937 18938
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:46.107 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9854227405247813 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3260] - [ERROR] 2024-05-05 01:03:46.112 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[139] - kill task error
org.apache.dolphinscheduler.common.shell.AbstractShell$ExitCodeException: kill: (18772): Operation not permitted
kill: (18918): No such process
kill: (18933): No such process
kill: (18934): No such process
kill: (18937): No such process
kill: (18938): No such process

	at org.apache.dolphinscheduler.common.shell.AbstractShell.runCommand(AbstractShell.java:205)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.run(AbstractShell.java:118)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execute(ShellExecutor.java:125)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:103)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:86)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeShell(OSUtils.java:345)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeCmd(OSUtils.java:334)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.killProcess(TaskInstanceKillOperationFunction.java:135)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.doKill(TaskInstanceKillOperationFunction.java:96)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.operate(TaskInstanceKillOperationFunction.java:69)
	at org.apache.dolphinscheduler.server.worker.rpc.TaskInstanceOperatorImpl.killTask(TaskInstanceOperatorImpl.java:49)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.dolphinscheduler.extract.base.server.ServerMethodInvokerImpl.invoke(ServerMethodInvokerImpl.java:41)
	at org.apache.dolphinscheduler.extract.base.server.JdkDynamicServerHandler.lambda$processReceived$0(JdkDynamicServerHandler.java:108)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[WI-0][TI-3260] - [INFO] 2024-05-05 01:03:46.120 +0800 o.a.d.p.t.a.u.ProcessUtils:[182] - Get appIds from worker 172.18.1.1:1234, taskLogPath: /opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log
[WI-0][TI-3260] - [INFO] 2024-05-05 01:03:46.121 +0800 o.a.d.p.t.a.u.LogUtils:[79] - Start finding appId in /opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, fetch way: log 
[WI-0][TI-3260] - [INFO] 2024-05-05 01:03:46.130 +0800 o.a.d.p.t.a.u.ProcessUtils:[188] - The appId is empty
[WI-0][TI-3260] - [INFO] 2024-05-05 01:03:46.131 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[220] - Begin to kill process process, pid is : 18772
[WI-0][TI-3260] - [INFO] 2024-05-05 01:03:46.139 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[225] - Success kill task: 940_3260, pid: 18772
[WI-0][TI-3260] - [INFO] 2024-05-05 01:03:46.155 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[119] - kill task by cancelApplication, taskInstanceId: 3260
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:47.023 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has killed. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, processId:18772 ,exitStatusCode:137 ,processWaitForStatus:true ,processExitValue:137
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:47.023 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:47.023 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:47.023 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:47.024 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:47.037 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: KILL to master : 172.18.1.1:1234
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:47.038 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:47.038 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:47.040 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260
[WI-940][TI-3260] - [INFO] 2024-05-05 01:03:47.041 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:47.108 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9632768361581922 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3260] - [INFO] 2024-05-05 01:03:47.108 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3260, processInstanceId=940, status=9, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, endTime=1714842227024, processId=18772, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3260] - [INFO] 2024-05-05 01:03:47.155 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3260, processInstanceId=940, status=9, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, endTime=1714842227024, processId=18772, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842227107)
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:48.118 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9827089337175793 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:48.437 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:48.927 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[55] - Receive TaskInstanceKillRequest: TaskInstanceKillRequest(taskInstanceId=3259)
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:48.944 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-bc848931-d4a5-4085-865e-d3697718ae70;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:49.130 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9662921348314606 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3259] - [INFO] 2024-05-05 01:03:49.143 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[134] - process id:18762, cmd:sudo -u default kill -9 18762 18765 18768 18771 18804 18883 18884 18885 18886 18887 18888 18889 18891 18905 18906 18907 18909 18910 18911
[WI-0][TI-3259] - [ERROR] 2024-05-05 01:03:49.270 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[139] - kill task error
org.apache.dolphinscheduler.common.shell.AbstractShell$ExitCodeException: kill: (18762): Operation not permitted
kill: (18891): No such process
kill: (18906): No such process

	at org.apache.dolphinscheduler.common.shell.AbstractShell.runCommand(AbstractShell.java:205)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.run(AbstractShell.java:118)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execute(ShellExecutor.java:125)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:103)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:86)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeShell(OSUtils.java:345)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeCmd(OSUtils.java:334)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.killProcess(TaskInstanceKillOperationFunction.java:135)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.doKill(TaskInstanceKillOperationFunction.java:96)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.operate(TaskInstanceKillOperationFunction.java:69)
	at org.apache.dolphinscheduler.server.worker.rpc.TaskInstanceOperatorImpl.killTask(TaskInstanceOperatorImpl.java:49)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.dolphinscheduler.extract.base.server.ServerMethodInvokerImpl.invoke(ServerMethodInvokerImpl.java:41)
	at org.apache.dolphinscheduler.extract.base.server.JdkDynamicServerHandler.lambda$processReceived$0(JdkDynamicServerHandler.java:108)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[WI-0][TI-3259] - [INFO] 2024-05-05 01:03:49.271 +0800 o.a.d.p.t.a.u.ProcessUtils:[182] - Get appIds from worker 172.18.1.1:1234, taskLogPath: /opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log
[WI-0][TI-3259] - [INFO] 2024-05-05 01:03:49.271 +0800 o.a.d.p.t.a.u.LogUtils:[79] - Start finding appId in /opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, fetch way: log 
[WI-0][TI-3259] - [INFO] 2024-05-05 01:03:49.271 +0800 o.a.d.p.t.a.u.ProcessUtils:[188] - The appId is empty
[WI-0][TI-3259] - [INFO] 2024-05-05 01:03:49.271 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[220] - Begin to kill process process, pid is : 18762
[WI-0][TI-3259] - [INFO] 2024-05-05 01:03:49.271 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[225] - Success kill task: 939_3259, pid: 18762
[WI-0][TI-3259] - [INFO] 2024-05-05 01:03:49.272 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[119] - kill task by cancelApplication, taskInstanceId: 3259
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:49.441 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-3af72606-dc8b-4da8-a13d-fba561355563;1.0
		confs: [default]
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:49.445 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has killed. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, processId:18762 ,exitStatusCode:137 ,processWaitForStatus:true ,processExitValue:137
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:49.446 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:49.446 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:49.451 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:49.451 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:49.465 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: KILL to master : 172.18.1.1:1234
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:49.465 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:49.465 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:49.465 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259
[WI-939][TI-3259] - [INFO] 2024-05-05 01:03:49.466 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:50.137 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9584775086505191 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3259] - [INFO] 2024-05-05 01:03:50.241 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3259, processInstanceId=939, status=9, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, endTime=1714842229451, processId=18762, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3259] - [INFO] 2024-05-05 01:03:50.256 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3259, processInstanceId=939, status=9, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, endTime=1714842229451, processId=18762, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842230237)
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:50.947 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:51.157 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9467680608365019 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:51.954 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:52.162 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9794635660510024 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:53.014 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 3515ms :: artifacts dl 30ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-bc848931-d4a5-4085-865e-d3697718ae70
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/13ms)
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:53.188 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.92090395480226 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:54.017 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:03:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:54.194 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8997050147492625 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3191] - [INFO] 2024-05-05 01:03:55.292 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3191, processInstanceId=936, status=6, startTime=1714839942174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13377949373536/25/936/3191.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/936/3191, endTime=1714840005766, processId=14716, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714841935002)
[WI-0][TI-3191] - [INFO] 2024-05-05 01:03:55.303 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3191, processInstanceId=936, status=6, startTime=1714839942174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13377949373536/25/936/3191.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/936/3191, endTime=1714840005766, processId=14716, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842235292)
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:56.221 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7476038338658147 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:57.021 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:03:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:03:56 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:03:56 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:03:56 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:03:56 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:57.232 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7890410958904109 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:58.235 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8918918918918919 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:59.238 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9719101123595506 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:59.910 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 2:>                                                          (0 + 1) / 1]
	24/05/05 01:03:59 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:59.910 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 2:>                                                          (0 + 1) / 1]
	24/05/05 01:03:59 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
[WI-0][TI-0] - [INFO] 2024-05-05 01:03:59.912 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:03:59 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
	
	[Stage 2:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:00.234 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:03:59 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
	
	[Stage 2:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:00.284 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9910714285714285 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:00.913 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:00.914 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:01.246 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:01.288 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.961038961038961 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:01.911 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:02.289 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.836734693877551 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:03.293 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7086481141583917 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:04.300 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7090831653673787 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:05.303 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7124937839175407 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:06.309 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.742765273311897 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:07.356 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7907608695652174 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:08.289 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:04:07 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:08.361 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8603351955307263 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:09.369 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8209366391184573 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:10.310 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:04:09 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:10.371 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7199021810819135 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:11.377 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8297872340425532 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:12.728 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.821852731591449 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:13.734 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9446153846153846 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:14.737 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8953168044077134 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:15.742 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8210227272727273 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:16.744 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8735632183908046 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:17.325 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:17.759 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7273580998362991 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:18.331 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:18.761 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7269087324997768 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:19.762 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7272091723429118 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:20.764 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7275096121860468 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:21.765 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7268898431648345 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:22.769 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.725984745768441 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:23.770 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7256316146225722 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:24.771 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7243568827666235 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:25.774 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8310249307479224 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:26.782 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7258901002586248 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:27.803 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7251556033820261 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:28.804 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.78125 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:29.805 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7254343701987536 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:30.829 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8515406162464987 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:31.831 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9903536977491961 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:32.866 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9753424657534246 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:32.987 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 3:>                                                          (0 + 1) / 1]
	24/05/05 01:04:32 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:32.982 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:04:32 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
	
	[Stage 3:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:33.053 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:04:32 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
	
	[Stage 3:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:33.420 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:04:32 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
	
	[Stage 3:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:33.422 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 1:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:33.907 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9923519009725907 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:34.055 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:34.444 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:34.911 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9796511627906976 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:35.927 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8022598870056497 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:36.004 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:36.005 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:36.485 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:04:36 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:36.933 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8236714754038146 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:37.946 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7338870493327789 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:38.488 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:38.951 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7346446110815178 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:39.963 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7339425243269779 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:40.976 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7343079832493354 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:41.977 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7344205239185707 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:42.978 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7351422953484714 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:44.027 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7571801566579635 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:45.036 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8132530120481927 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:46.037 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7765742022900195 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:47.040 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7348321125852083 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:48.048 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7264957264957266 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:49.051 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7746478873239436 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:50.054 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9090909090909092 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:51.076 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.924791086350975 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:52.106 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9628179381391323 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:53.108 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7845659163987138 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:54.122 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.76536312849162 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:55.160 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.795053003533569 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:55.824 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[55] - Receive TaskInstanceKillRequest: TaskInstanceKillRequest(taskInstanceId=3262)
[WI-0][TI-3262] - [INFO] 2024-05-05 01:04:56.022 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[134] - process id:18774, cmd:sudo -u default kill -9 18774 18782 18792 18803 18822 19522 19603 18899 18900 18901 18902 18903 18913 18914 18919 18927 18928 18929 18930 18931 18932 18962 18964 18965 18967 18968 18969 18970 18971 18972 18973 18974 18975 18976 18977 18978 18979 18980 18981 18982 18983 18984 18985 18986 18987 18988 18989 18990 18991 19014 19015 19016 19017 19018 19019 19020 19021 19022 19045 19075 19252 19258 19259 19260 19261 19262 19270 19297 19310 19361 19362 19521 19601 19602 19608 19613 19614 19615 19616 19618 19638 19639 19640 19641 19642 19643 19644 19645 19646 19647 18966 19263
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:56.166 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9065155807365439 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3262] - [ERROR] 2024-05-05 01:04:56.153 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[139] - kill task error
org.apache.dolphinscheduler.common.shell.AbstractShell$ExitCodeException: kill: (18774): Operation not permitted
kill: (18899): No such process
kill: (18900): No such process
kill: (18901): No such process
kill: (18902): No such process
kill: (18903): No such process
kill: (18914): No such process
kill: (18927): No such process
kill: (18928): No such process
kill: (18929): No such process
kill: (18930): No such process
kill: (18931): No such process
kill: (18932): No such process
kill: (18962): No such process
kill: (18964): No such process
kill: (18965): No such process
kill: (18967): No such process
kill: (18968): No such process
kill: (18969): No such process
kill: (18970): No such process
kill: (18971): No such process
kill: (18972): No such process
kill: (18973): No such process
kill: (18974): No such process
kill: (18975): No such process
kill: (18976): No such process
kill: (18977): No such process
kill: (18978): No such process
kill: (18979): No such process
kill: (18980): No such process
kill: (18981): No such process
kill: (18982): No such process
kill: (18983): No such process
kill: (18984): No such process
kill: (18985): No such process
kill: (18986): No such process
kill: (18987): No such process
kill: (18988): No such process
kill: (18989): No such process
kill: (18990): No such process
kill: (18991): No such process
kill: (19014): No such process
kill: (19015): No such process
kill: (19016): No such process
kill: (19017): No such process
kill: (19018): No such process
kill: (19019): No such process
kill: (19020): No such process
kill: (19021): No such process
kill: (19022): No such process
kill: (19045): No such process
kill: (19075): No such process
kill: (19252): No such process
kill: (19258): No such process
kill: (19259): No such process
kill: (19260): No such process
kill: (19261): No such process
kill: (19262): No such process
kill: (19270): No such process
kill: (19297): No such process
kill: (19310): No such process
kill: (19361): No such process
kill: (19362): No such process
kill: (19521): No such process
kill: (19602): No such process
kill: (19608): No such process
kill: (19613): No such process
kill: (19614): No such process
kill: (19615): No such process
kill: (19616): No such process
kill: (19618): No such process
kill: (19638): No such process
kill: (19639): No such process
kill: (19640): No such process
kill: (19641): No such process
kill: (19642): No such process
kill: (19643): No such process
kill: (19644): No such process
kill: (19645): No such process
kill: (19646): No such process
kill: (19647): No such process
kill: (18966): No such process

	at org.apache.dolphinscheduler.common.shell.AbstractShell.runCommand(AbstractShell.java:205)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.run(AbstractShell.java:118)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execute(ShellExecutor.java:125)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:103)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:86)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeShell(OSUtils.java:345)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeCmd(OSUtils.java:334)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.killProcess(TaskInstanceKillOperationFunction.java:135)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.doKill(TaskInstanceKillOperationFunction.java:96)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.operate(TaskInstanceKillOperationFunction.java:69)
	at org.apache.dolphinscheduler.server.worker.rpc.TaskInstanceOperatorImpl.killTask(TaskInstanceOperatorImpl.java:49)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.dolphinscheduler.extract.base.server.ServerMethodInvokerImpl.invoke(ServerMethodInvokerImpl.java:41)
	at org.apache.dolphinscheduler.extract.base.server.JdkDynamicServerHandler.lambda$processReceived$0(JdkDynamicServerHandler.java:108)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[WI-0][TI-3262] - [INFO] 2024-05-05 01:04:56.199 +0800 o.a.d.p.t.a.u.ProcessUtils:[182] - Get appIds from worker 172.18.1.1:1234, taskLogPath: /opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log
[WI-0][TI-3262] - [INFO] 2024-05-05 01:04:56.203 +0800 o.a.d.p.t.a.u.LogUtils:[79] - Start finding appId in /opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log, fetch way: log 
[WI-0][TI-3262] - [INFO] 2024-05-05 01:04:56.217 +0800 o.a.d.p.t.a.u.ProcessUtils:[188] - The appId is empty
[WI-0][TI-3262] - [INFO] 2024-05-05 01:04:56.235 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[220] - Begin to kill process process, pid is : 18774
[WI-0][TI-3262] - [INFO] 2024-05-05 01:04:56.238 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[225] - Success kill task: 942_3262, pid: 18774
[WI-0][TI-3262] - [INFO] 2024-05-05 01:04:56.238 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[119] - kill task by cancelApplication, taskInstanceId: 3262
[WI-942][TI-3262] - [INFO] 2024-05-05 01:04:56.575 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has killed. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262, processId:18774 ,exitStatusCode:137 ,processWaitForStatus:true ,processExitValue:137
[WI-942][TI-3262] - [INFO] 2024-05-05 01:04:56.578 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-942][TI-3262] - [INFO] 2024-05-05 01:04:56.578 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-942][TI-3262] - [INFO] 2024-05-05 01:04:56.578 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-942][TI-3262] - [INFO] 2024-05-05 01:04:56.579 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-942][TI-3262] - [INFO] 2024-05-05 01:04:56.583 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: KILL to master : 172.18.1.1:1234
[WI-942][TI-3262] - [INFO] 2024-05-05 01:04:56.587 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-942][TI-3262] - [INFO] 2024-05-05 01:04:56.588 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262
[WI-942][TI-3262] - [INFO] 2024-05-05 01:04:56.590 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262
[WI-942][TI-3262] - [INFO] 2024-05-05 01:04:56.594 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3262] - [INFO] 2024-05-05 01:04:56.612 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3262, processInstanceId=942, status=9, startTime=1714842212466, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262, endTime=1714842296579, processId=18774, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3262] - [INFO] 2024-05-05 01:04:56.622 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3262, processInstanceId=942, status=9, startTime=1714842212466, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262, endTime=1714842296579, processId=18774, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842296600)
[WI-0][TI-0] - [INFO] 2024-05-05 01:04:57.198 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.918918918918919 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:05:02.238 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8063583815028902 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:05:09.320 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8701657458563536 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:05:09.640 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:05:09 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
[WI-0][TI-0] - [INFO] 2024-05-05 01:05:10.133 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:05:10 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
[WI-0][TI-0] - [INFO] 2024-05-05 01:05:10.133 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:05:10 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
[WI-0][TI-0] - [INFO] 2024-05-05 01:05:10.196 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:05:09 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
	
	[Stage 4:>                                                          (0 + 1) / 1]
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:05:10.342 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9937888198757764 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:05:10.668 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 4:>                                                          (0 + 1) / 1]
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:05:11.136 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 4:>                                                          (0 + 1) / 1]
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:05:11.139 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 4:>                                                          (0 + 1) / 1]
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:05:11.347 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7348066298342543 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:05:12.374 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.850415512465374 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:05:17.388 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8236914600550965 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:05:22.413 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9421487603305785 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:05:23.501 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7708333333333334 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:05:49.606 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8083333333333333 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:05:54.673 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9266862170087976 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:05:54.747 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:05:54 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
[WI-0][TI-0] - [INFO] 2024-05-05 01:05:55.486 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:05:54 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
[WI-0][TI-0] - [INFO] 2024-05-05 01:05:55.493 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:05:55 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
	
	[Stage 5:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-05-05 01:05:55.707 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8985507246376812 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:05:55.772 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 5:>                                                          (0 + 1) / 1]
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:05:56.473 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 5:>                                                          (0 + 1) / 1]
	24/05/05 01:05:55 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:05:56.489 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:05:56.494 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:06:02.734 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7507082152974505 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:07:11.282 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7982456140350878 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:07:14.414 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7114285714285715 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:07:23.452 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7102272727272727 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:07:24.512 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8465909090909091 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:07:25.527 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7555555555555555 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:07:26.529 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.925414364640884 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:07:27.553 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8322784810126582 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:07:28.554 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8018575851393188 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:07:29.556 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8659217877094972 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:07:30.558 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8158640226628896 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:07:35.579 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.736231884057971 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:07:36.638 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7853403141361256 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:07:37.643 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9439775910364145 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:07:38.647 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8983050847457628 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:07:39.653 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8658536585365854 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:07:40.655 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.853185595567867 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:07:41.662 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7777777777777778 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:07:42.674 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9085714285714286 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:07:43.701 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8226744186046512 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:07:44.721 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7209302325581396 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:03.831 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.725212464589235 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3258] - [INFO] 2024-05-05 01:08:12.341 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3258, processInstanceId=938, status=9, startTime=1714841447434, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, endTime=1714842191466, processId=18118, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842191629)
[WI-0][TI-3258] - [INFO] 2024-05-05 01:08:12.354 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3258, processInstanceId=938, status=9, startTime=1714841447434, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, endTime=1714842191466, processId=18118, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842492341)
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:23.912 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9171270718232045 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:25.535 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3263, taskName=sentiment analysis, firstSubmitTime=1714842505513, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=943, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3263'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505010825'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='943'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.542 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.544 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.551 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.551 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.551 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.551 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714842505551
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.552 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 943_3263
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.552 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3263,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714842505513,
  "startTime" : 1714842505551,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/943/3263.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 943,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3263"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505010825"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "943"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "943_3263",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.556 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.557 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.557 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.563 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.565 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.565 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263 check successfully
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.565 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.566 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.566 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.566 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.566 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.567 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.567 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.567 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.569 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.570 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.578 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.579 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.581 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263/py_943_3263.py
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.581 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.582 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.582 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.582 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263/py_943_3263.py
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.583 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.583 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263/943_3263.sh
[WI-943][TI-3263] - [INFO] 2024-05-05 01:08:25.592 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 20088
[WI-0][TI-3263] - [INFO] 2024-05-05 01:08:25.734 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3263, success=true)
[WI-0][TI-3263] - [INFO] 2024-05-05 01:08:25.745 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3263)
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:26.598 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:27.955 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8620689655172413 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:28.988 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8602739726027397 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:30.607 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-6c9dd678-643f-4819-b490-aaf9d34c5bca;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:31.003 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7142857142857143 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:31.608 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 621ms :: artifacts dl 34ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-6c9dd678-643f-4819-b490-aaf9d34c5bca
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/16ms)
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:32.019 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8575851393188856 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:32.615 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:08:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:33.040 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9550561797752809 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:33.619 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:34.046 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8673881367251532 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:35.050 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8954802259887006 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:36.057 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9099099099099099 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:37.063 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:38.101 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8136200716845879 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:38.637 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:08:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:08:38 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:08:38 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:08:38 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:08:38 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:39.111 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7875354107648725 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:40.116 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7654986522911051 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:41.117 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7918918918918919 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:42.153 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:43.156 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8328611898016998 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:44.158 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8211143695014662 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3261] - [INFO] 2024-05-05 01:08:44.440 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3261, processInstanceId=941, status=9, startTime=1714842212366, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, endTime=1714842223865, processId=18773, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842224064)
[WI-0][TI-3261] - [INFO] 2024-05-05 01:08:44.451 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3261, processInstanceId=941, status=9, startTime=1714842212366, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, endTime=1714842223865, processId=18773, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842524440)
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:45.172 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8387978142076502 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:46.178 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8526912181303117 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:47.184 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7296416938110749 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3260] - [INFO] 2024-05-05 01:08:47.645 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3260, processInstanceId=940, status=9, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, endTime=1714842227024, processId=18772, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842227107)
[WI-0][TI-3260] - [INFO] 2024-05-05 01:08:47.662 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3260, processInstanceId=940, status=9, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, endTime=1714842227024, processId=18772, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842527645)
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:49.261 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9138381201044387 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:50.279 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7254335260115607 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3259] - [INFO] 2024-05-05 01:08:50.677 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3259, processInstanceId=939, status=9, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, endTime=1714842229451, processId=18762, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842230237)
[WI-0][TI-3259] - [INFO] 2024-05-05 01:08:50.696 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3259, processInstanceId=939, status=9, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, endTime=1714842229451, processId=18762, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842530677)
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:51.286 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8977353664853664 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:52.296 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8631284916201118 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:53.318 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8310249307479225 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:54.321 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9341317365269461 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:54.675 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:08:53 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:55.328 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9635854341736694 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3191] - [INFO] 2024-05-05 01:08:55.764 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3191, processInstanceId=936, status=6, startTime=1714839942174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13377949373536/25/936/3191.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/936/3191, endTime=1714840005766, processId=14716, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842235292)
[WI-0][TI-3191] - [INFO] 2024-05-05 01:08:55.770 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3191, processInstanceId=936, status=6, startTime=1714839942174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13377949373536/25/936/3191.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/936/3191, endTime=1714840005766, processId=14716, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842535763)
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:56.330 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.808695652173913 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:57.333 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7922848664688427 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:57.700 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:08:56 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[WI-0][TI-0] - [INFO] 2024-05-05 01:08:58.337 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9171270718232045 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:00.375 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7574931880108992 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:01.383 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8783783783783784 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:02.706 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:03.408 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7506848686123567 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:03.714 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:13.464 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9317507418397626 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:15.522 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8512396694214875 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:16.593 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7768817204301075 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:21.647 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8631284916201116 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:22.668 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.799410029498525 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:22.760 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 1:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:23.676 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7035652923780739 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:23.772 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:09:23 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:24.681 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7051559732153208 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:25.694 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9285714285714286 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:26.713 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8457300275482094 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:26.798 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:27.714 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8490028490028491 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:28.715 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7077191565494394 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:29.717 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7033898305084746 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:30.719 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7075296666947024 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:31.721 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7075662523539591 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:32.729 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7080772585729247 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:33.732 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7080768609027154 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:34.733 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7069761097633405 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:35.737 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7071011770441691 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:36.738 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7066174112345414 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:37.740 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7068727155089194 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:39.190 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7054884255103053 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:40.205 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7059632437402236 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:41.207 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7063145853701505 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:42.208 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7063145853701505 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:43.225 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7066997289678688 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:44.228 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7060087769791897 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:45.232 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7053951718462217 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:46.233 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7054198273991991 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:47.246 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7059338161447345 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:48.250 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7059946596867591 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:49.250 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7046771782833094 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:50.253 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7046950734427284 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:51.256 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7053333341286737 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:52.259 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7058948444642219 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:53.268 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7060264733035041 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:54.269 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.705678710705461 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:55.269 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7052961519641031 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:56.272 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.706035222048109 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3262] - [INFO] 2024-05-05 01:09:57.210 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3262, processInstanceId=942, status=9, startTime=1714842212466, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262, endTime=1714842296579, processId=18774, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842296600)
[WI-0][TI-3262] - [INFO] 2024-05-05 01:09:57.219 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3262, processInstanceId=942, status=9, startTime=1714842212466, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262, endTime=1714842296579, processId=18774, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842597210)
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:57.273 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7063984937843152 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:58.275 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.705972787825247 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:09:59.276 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.705972787825247 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:00.278 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7054102833141755 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:01.284 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.705685669934124 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:02.285 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7065740651817264 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:03.293 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7065842057720638 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:04.295 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7054351377022575 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:05.299 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7054345411969436 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:06.355 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7055240169940388 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:07.359 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7060244849524575 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:08.360 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7060966620954476 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:09.363 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7062539406632304 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:10.365 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7058495100603603 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:11.366 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7051410605824715 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:12.367 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7056284054239831 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:13.375 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7059767645273402 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:14.375 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7059767645273402 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:15.376 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7060145431972248 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:16.378 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7072421511333701 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:17.383 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.708542731552924 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:18.385 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7085928379992973 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:19.386 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7084538522611429 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:20.387 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7088920848318044 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:21.388 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.707409172621281 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:22.391 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7073558848132332 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:23.414 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7069621913060146 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:24.426 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7072232617984278 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:25.427 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7082597891989988 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:26.428 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7084095120328046 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:27.430 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7082325487896609 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:28.432 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7082321511194516 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:29.434 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7080195963925745 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:30.436 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7080195963925745 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:31.438 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7081068850035184 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:32.444 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7046924885863679 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:33.461 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7051758567257863 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:34.463 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7052128400552523 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:35.466 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7054253947821294 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:36.468 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7049587287915021 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:37.471 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.705371908138977 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:38.474 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7038178129609868 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:39.506 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7044678049181068 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:40.508 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7040186364166892 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:41.509 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7042689698134509 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:42.509 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7053452642349531 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:43.520 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7053448665647438 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:44.522 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7053939788355937 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:45.524 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7053939788355937 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:46.526 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7055442981747136 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:47.533 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7048612995902207 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:48.534 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7051859973161237 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:49.536 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7052969473045217 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:50.537 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7055106950420268 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:51.539 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7051494116568671 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:52.547 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8125 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:53.605 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8894736842105263 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:54.607 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7764705882352941 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:55.609 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8773841961852861 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:56.613 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7057870758374984 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:57.634 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8526011560693642 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:58.659 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7874659400544959 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:10:59.665 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7052961519641031 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:00.843 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7046435751506226 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:01.845 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7052671220388234 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:02.849 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8725212464589235 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:03.864 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9138888888888888 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:04.037 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[55] - Receive TaskInstanceKillRequest: TaskInstanceKillRequest(taskInstanceId=3263)
[WI-0][TI-3263] - [INFO] 2024-05-05 01:11:04.176 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[134] - process id:20088, cmd:sudo -u default kill -9 20088 20092 20095 20098 20099 20378 20117 20118 20119 20120 20121 20122 20123 20124 20125 20126 20127 20128 20129 20130 20132 20134 20135 20137 20138 20139 20140 20141 20142 20143 20144 20145 20146 20147 20148 20149 20150 20151 20152 20153 20160 20161 20162 20163 20164 20165 20166 20167 20190 20191 20192 20193 20194 20195 20196 20197 20198 20221 20222 20245 20246 20247 20248 20249 20250 20252 20282 20287 20329 20377 20379 20380 20385 20386 20392 20393 20394 20428 20429 20430 20431 20432 20136 20251
[WI-0][TI-3263] - [ERROR] 2024-05-05 01:11:04.364 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[139] - kill task error
org.apache.dolphinscheduler.common.shell.AbstractShell$ExitCodeException: kill: (20088): Operation not permitted
kill: (20117): No such process
kill: (20118): No such process
kill: (20119): No such process
kill: (20120): No such process
kill: (20121): No such process
kill: (20123): No such process
kill: (20124): No such process
kill: (20125): No such process
kill: (20126): No such process
kill: (20127): No such process
kill: (20128): No such process
kill: (20129): No such process
kill: (20132): No such process
kill: (20134): No such process
kill: (20135): No such process
kill: (20137): No such process
kill: (20138): No such process
kill: (20139): No such process
kill: (20140): No such process
kill: (20141): No such process
kill: (20142): No such process
kill: (20143): No such process
kill: (20144): No such process
kill: (20145): No such process
kill: (20146): No such process
kill: (20147): No such process
kill: (20148): No such process
kill: (20149): No such process
kill: (20150): No such process
kill: (20151): No such process
kill: (20152): No such process
kill: (20153): No such process
kill: (20160): No such process
kill: (20161): No such process
kill: (20162): No such process
kill: (20163): No such process
kill: (20164): No such process
kill: (20165): No such process
kill: (20166): No such process
kill: (20167): No such process
kill: (20190): No such process
kill: (20191): No such process
kill: (20192): No such process
kill: (20193): No such process
kill: (20194): No such process
kill: (20195): No such process
kill: (20196): No such process
kill: (20197): No such process
kill: (20198): No such process
kill: (20221): No such process
kill: (20222): No such process
kill: (20245): No such process
kill: (20246): No such process
kill: (20247): No such process
kill: (20248): No such process
kill: (20249): No such process
kill: (20250): No such process
kill: (20252): No such process
kill: (20282): No such process
kill: (20287): No such process
kill: (20329): No such process
kill: (20377): No such process
kill: (20379): No such process
kill: (20380): No such process
kill: (20385): No such process
kill: (20386): No such process
kill: (20392): No such process
kill: (20393): No such process
kill: (20394): No such process
kill: (20428): No such process
kill: (20429): No such process
kill: (20430): No such process
kill: (20431): No such process
kill: (20432): No such process
kill: (20251): No such process

	at org.apache.dolphinscheduler.common.shell.AbstractShell.runCommand(AbstractShell.java:205)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.run(AbstractShell.java:118)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execute(ShellExecutor.java:125)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:103)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:86)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeShell(OSUtils.java:345)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeCmd(OSUtils.java:334)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.killProcess(TaskInstanceKillOperationFunction.java:135)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.doKill(TaskInstanceKillOperationFunction.java:96)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.operate(TaskInstanceKillOperationFunction.java:69)
	at org.apache.dolphinscheduler.server.worker.rpc.TaskInstanceOperatorImpl.killTask(TaskInstanceOperatorImpl.java:49)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.dolphinscheduler.extract.base.server.ServerMethodInvokerImpl.invoke(ServerMethodInvokerImpl.java:41)
	at org.apache.dolphinscheduler.extract.base.server.JdkDynamicServerHandler.lambda$processReceived$0(JdkDynamicServerHandler.java:108)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[WI-0][TI-3263] - [INFO] 2024-05-05 01:11:04.370 +0800 o.a.d.p.t.a.u.ProcessUtils:[182] - Get appIds from worker 172.18.1.1:1234, taskLogPath: /opt/dolphinscheduler/logs/20240505/13386218435520/20/943/3263.log
[WI-0][TI-3263] - [INFO] 2024-05-05 01:11:04.371 +0800 o.a.d.p.t.a.u.LogUtils:[79] - Start finding appId in /opt/dolphinscheduler/logs/20240505/13386218435520/20/943/3263.log, fetch way: log 
[WI-0][TI-3263] - [INFO] 2024-05-05 01:11:04.374 +0800 o.a.d.p.t.a.u.ProcessUtils:[188] - The appId is empty
[WI-0][TI-3263] - [INFO] 2024-05-05 01:11:04.374 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[220] - Begin to kill process process, pid is : 20088
[WI-0][TI-3263] - [INFO] 2024-05-05 01:11:04.378 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[225] - Success kill task: 943_3263, pid: 20088
[WI-0][TI-3263] - [INFO] 2024-05-05 01:11:04.404 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[119] - kill task by cancelApplication, taskInstanceId: 3263
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:04.870 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9810666313184871 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-943][TI-3263] - [INFO] 2024-05-05 01:11:05.298 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has killed. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263, processId:20088 ,exitStatusCode:137 ,processWaitForStatus:true ,processExitValue:137
[WI-943][TI-3263] - [INFO] 2024-05-05 01:11:05.300 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-943][TI-3263] - [INFO] 2024-05-05 01:11:05.300 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-943][TI-3263] - [INFO] 2024-05-05 01:11:05.300 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-943][TI-3263] - [INFO] 2024-05-05 01:11:05.300 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-943][TI-3263] - [INFO] 2024-05-05 01:11:05.303 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: KILL to master : 172.18.1.1:1234
[WI-943][TI-3263] - [INFO] 2024-05-05 01:11:05.303 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-943][TI-3263] - [INFO] 2024-05-05 01:11:05.304 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263
[WI-943][TI-3263] - [INFO] 2024-05-05 01:11:05.312 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263
[WI-0][TI-3263] - [INFO] 2024-05-05 01:11:05.314 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3263, processInstanceId=943, status=9, startTime=1714842505551, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/943/3263.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263, endTime=1714842665300, processId=20088, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-943][TI-3263] - [INFO] 2024-05-05 01:11:05.314 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3263] - [INFO] 2024-05-05 01:11:05.353 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3263, processInstanceId=943, status=9, startTime=1714842505551, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/943/3263.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263, endTime=1714842665300, processId=20088, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842665311)
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:05.871 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8253521126760563 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:07.932 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8436803380199607 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:08.987 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8795811518324607 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:09.993 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7894736842105263 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:10.994 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.807799442896936 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:14.009 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.775 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:16.047 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7802816901408451 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:16.450 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3266, taskName=sentiment analysis, firstSubmitTime=1714842676441, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=945, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=13, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3266'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505011116'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='945'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.452 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.454 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.454 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.454 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.454 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714842676454
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.454 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 945_3266
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.455 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3266,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714842676441,
  "startTime" : 1714842676454,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3266.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 945,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\",\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 13,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3266"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505011116"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "945"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "945_3266",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.452 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.456 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.456 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.456 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:16.468 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3265, taskName=sentiment analysis, firstSubmitTime=1714842676441, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=946, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=13, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3265'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505011116'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='946'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.471 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.471 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.473 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.474 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.474 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.474 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714842676474
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.474 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 946_3265
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.475 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3265,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714842676441,
  "startTime" : 1714842676474,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3265.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 946,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\",\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 13,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3265"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505011116"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "946"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "946_3265",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.476 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.477 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.477 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.477 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.477 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.481 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3266 check successfully
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.481 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.481 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.481 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.482 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.482 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3265 check successfully
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.482 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.482 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.482 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.482 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.483 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.485 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.485 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.485 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.485 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.485 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.482 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.485 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.486 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.487 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.487 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3266
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.488 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3266/py_945_3266.py
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.488 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.488 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.489 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.489 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.489 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.489 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.489 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.490 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.491 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3265
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.491 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.492 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3265/py_946_3265.py
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.492 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.492 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3266/py_945_3266.py
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.493 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.493 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3266/945_3266.sh
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.495 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.496 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.497 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3265/py_946_3265.py
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.497 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.498 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3265/946_3265.sh
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:16.500 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 20453
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:16.510 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 20455
[WI-0][TI-3266] - [INFO] 2024-05-05 01:11:17.308 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3266, success=true)
[WI-0][TI-3265] - [INFO] 2024-05-05 01:11:17.310 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3265, success=true)
[WI-0][TI-3265] - [INFO] 2024-05-05 01:11:17.326 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3265)
[WI-0][TI-3266] - [INFO] 2024-05-05 01:11:17.334 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3266)
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:17.500 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:17.510 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:18.089 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8953168044077134 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:19.142 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.981081081081081 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:20.169 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9573333333333334 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:21.171 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9702702702702704 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:22.185 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8746081504702194 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:23.187 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9482758620689656 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:24.198 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9401993355481727 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:25.236 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9590643274853801 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:26.237 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9290433403805496 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:26.549 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-944b2186-9eaf-431b-a88a-ddbc8611702b;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:26.567 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-6b072a03-3397-4f8c-a513-4e8beed29167;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:27.239 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8932038834951457 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:27.553 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:27.569 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:28.253 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9611650485436893 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:28.553 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 2081ms :: artifacts dl 12ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-944b2186-9eaf-431b-a88a-ddbc8611702b
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/21ms)
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:28.571 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:29.314 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9191374663072776 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:29.556 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:11:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:29.593 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: resolution report :: resolve 2414ms :: artifacts dl 11ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	
	:: problems summary ::
	:::: ERRORS
		unknown resolver null
	
		unknown resolver null
	
	
	:: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS
	:: retrieving :: org.apache.spark#spark-submit-parent-6b072a03-3397-4f8c-a513-4e8beed29167
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/38ms)
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:30.332 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.927536231884058 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:30.557 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:30.599 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:11:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:31.343 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8992805755395683 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:31.604 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:32.350 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9226361031518624 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:33.368 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9640883977900553 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:34.397 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9510869565217391 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:35.403 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9830985915492958 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:36.406 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.956043956043956 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:37.412 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9306666666666668 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:37.576 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:11:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:11:37 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:11:37 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:11:37 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:11:37 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:38.420 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.946127946127946 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:38.633 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:11:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:11:37 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:11:37 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:11:37 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:11:37 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
	24/05/05 01:11:37 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:39.475 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9347258485639687 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:40.478 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.886685552407932 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:41.480 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8689458689458689 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:42.481 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9166666666666666 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:43.484 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8785310734463277 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:44.498 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.850415512465374 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:45.502 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7695167286245352 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:46.509 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8272727272727273 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:46.621 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3265/py_946_3265.py", line 51, in <module>
	    parsed_topics = topics.split("|")
	                    ^^^^^^^^^^^^
	AttributeError: 'tuple' object has no attribute 'split'
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:46.668 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3266/py_945_3266.py", line 51, in <module>
	    parsed_topics = topics.split("|")
	                    ^^^^^^^^^^^^
	AttributeError: 'tuple' object has no attribute 'split'
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:47.514 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9277777777777778 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:48.515 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7478991596638656 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:48.626 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3265, processId:20455 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:48.631 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:48.631 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:48.632 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:48.632 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:48.637 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:48.638 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:48.638 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3265
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:48.639 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3265
[WI-946][TI-3265] - [INFO] 2024-05-05 01:11:48.639 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:48.700 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3266, processId:20453 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:48.700 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:48.700 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:48.700 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:48.702 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:48.708 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:48.709 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:48.709 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3266
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:48.709 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3266
[WI-945][TI-3266] - [INFO] 2024-05-05 01:11:48.710 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3266] - [INFO] 2024-05-05 01:11:49.410 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3266, success=true)
[WI-0][TI-3265] - [INFO] 2024-05-05 01:11:49.414 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3265, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:49.526 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7019498607242339 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:11:55.564 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8016528925619835 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:12:06.623 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8732782369146005 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:12:08.656 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8448275862068966 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:12:09.721 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7454068241469816 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:12:11.763 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7131147540983607 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:12:20.846 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7527777777777778 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:12:22.957 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8461670631773723 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:12:23.986 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8205128205128205 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:12:28.012 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7366863905325444 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:12:33.054 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8516483516483517 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:12:34.076 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8689839572192513 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:12:35.078 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7811634349030472 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:12:39.102 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8666666666666667 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:12:40.116 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8431372549019608 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:12:41.119 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8181818181818182 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:12:42.121 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7412790697674418 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:12:43.136 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7434402332361516 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:12:50.194 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7679083094555874 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:12:53.286 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7282913165266106 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:12:59.389 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7556818181818181 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:02.506 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.711484593837535 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:08.549 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.876453388372893 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:09.641 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7111111111111111 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3258] - [INFO] 2024-05-05 01:13:12.756 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3258, processInstanceId=938, status=9, startTime=1714841447434, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, endTime=1714842191466, processId=18118, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842492341)
[WI-0][TI-3258] - [INFO] 2024-05-05 01:13:12.783 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3258, processInstanceId=938, status=9, startTime=1714841447434, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, endTime=1714842191466, processId=18118, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842792756)
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:14.496 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3295, taskName=sentiment analysis, firstSubmitTime=1714842794481, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=946, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3295'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505011314'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='946'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.497 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.501 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.514 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.514 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.514 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.514 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714842794514
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.516 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 946_3295
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.517 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3295,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714842794481,
  "startTime" : 1714842794514,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3295.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 946,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\",\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3295"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505011314"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "946"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "946_3295",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.527 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.529 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.529 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.535 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.537 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.538 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3295 check successfully
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.540 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.541 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.542 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.542 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.543 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.544 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.544 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.545 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:14.528 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3296, taskName=sentiment analysis, firstSubmitTime=1714842794513, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=945, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3296'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505011314'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='945'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.545 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.546 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.546 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.546 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.549 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3295
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.547 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.551 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.551 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.551 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714842794551
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.551 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 945_3296
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.551 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3296,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714842794513,
  "startTime" : 1714842794551,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3296.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 945,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\",\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3296"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505011314"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "945"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "945_3296",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.550 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3295/py_946_3295.py
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.553 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.552 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.556 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.556 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.556 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3295/py_946_3295.py
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.556 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.552 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.556 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3295/946_3295.sh
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.558 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.559 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.568 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.571 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:14.573 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 20814
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.583 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3296 check successfully
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.584 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.584 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-0][TI-3295] - [INFO] 2024-05-05 01:13:14.584 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3295, success=true)
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.585 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.588 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.590 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.592 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.592 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.593 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.593 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.596 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.596 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.597 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3296
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.598 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3296/py_945_3296.py
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.598 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-0][TI-3295] - [INFO] 2024-05-05 01:13:14.596 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3295)
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.601 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.603 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.603 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3296/py_945_3296.py
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.603 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.604 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3296/945_3296.sh
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:14.616 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 20821
[WI-0][TI-3296] - [INFO] 2024-05-05 01:13:14.903 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3296, processInstanceId=945, startTime=1714842794551, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3296.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3296] - [INFO] 2024-05-05 01:13:14.910 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3296, processInstanceId=945, startTime=1714842794551, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3296.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714842794892)
[WI-0][TI-3296] - [INFO] 2024-05-05 01:13:14.910 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3296, processInstanceId=945, startTime=1714842794551, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3296] - [INFO] 2024-05-05 01:13:14.913 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3296, processInstanceId=945, startTime=1714842794551, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714842794892)
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:15.578 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-3296] - [INFO] 2024-05-05 01:13:15.579 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3296, success=true)
[WI-0][TI-3296] - [INFO] 2024-05-05 01:13:15.602 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3296)
[WI-0][TI-3296] - [INFO] 2024-05-05 01:13:15.615 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3296, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:15.617 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-3296] - [INFO] 2024-05-05 01:13:15.643 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3296)
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:15.906 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8784530386740332 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:16.924 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8850267379679144 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:17.928 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9749303621169916 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:18.929 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9460227272727273 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:19.931 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9490084985835695 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:20.934 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9577836411609499 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:21.937 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8839779005524863 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:22.937 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8997289972899729 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:23.622 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-1058632d-6646-468d-a6fd-4c465fa116dd;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:23.950 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9490616621983914 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:24.634 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:24.659 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-70b834e6-0fe5-4e61-b52a-4ee147399c43;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:24.964 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9522471910112359 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:25.636 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 1785ms :: artifacts dl 12ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-1058632d-6646-468d-a6fd-4c465fa116dd
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/8ms)
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:25.663 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:25.987 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9767441860465116 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:26.644 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:13:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:26.668 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 1845ms :: artifacts dl 67ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-70b834e6-0fe5-4e61-b52a-4ee147399c43
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/22ms)
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:27.043 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8832807570977917 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:27.650 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:27.670 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:13:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:28.047 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9020172910662825 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:28.677 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:29.049 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8563136053749629 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:30.053 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9579670031477261 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:31.066 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9670658682634731 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:31.671 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:13:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:13:31 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:13:31 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:13:31 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:13:31 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:32.074 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9888888888888889 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:33.092 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8933717579250721 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:33.691 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:13:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:13:32 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:13:32 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:13:32 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:13:32 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
	24/05/05 01:13:32 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:34.099 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8511235955056179 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:35.105 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8934426229508197 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:36.113 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.943089430894309 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:37.202 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9148936170212766 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:38.208 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8766666666666667 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:39.214 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9090909090909092 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:40.221 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9512195121951219 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:41.228 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8895264614988411 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:42.232 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9750692520775623 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:43.237 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7410071942446044 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:43.768 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3295/py_946_3295.py", line 51, in <module>
	    parsed_topics = topics.split("|")
	                    ^^^^^^^^^^^^
	AttributeError: 'tuple' object has no attribute 'split'
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:44.240 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8803680981595092 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:44.776 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3296/py_945_3296.py", line 51, in <module>
	    parsed_topics = topics.split("|")
	                    ^^^^^^^^^^^^
	AttributeError: 'tuple' object has no attribute 'split'
[WI-0][TI-3261] - [INFO] 2024-05-05 01:13:45.043 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3261, processInstanceId=941, status=9, startTime=1714842212366, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, endTime=1714842223865, processId=18773, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842524440)
[WI-0][TI-3261] - [INFO] 2024-05-05 01:13:45.059 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3261, processInstanceId=941, status=9, startTime=1714842212366, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, endTime=1714842223865, processId=18773, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842825043)
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:45.773 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3295, processId:20814 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:45.773 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:45.773 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:45.774 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:45.774 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:45.780 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3296, processId:20821 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:45.781 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:45.780 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:45.782 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:45.783 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3295
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:45.782 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:45.783 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:45.783 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3295
[WI-946][TI-3295] - [INFO] 2024-05-05 01:13:45.783 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:45.784 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:45.793 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:45.794 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:45.796 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3296
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:45.796 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3296
[WI-945][TI-3296] - [INFO] 2024-05-05 01:13:45.797 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3295] - [INFO] 2024-05-05 01:13:46.061 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3295, processInstanceId=946, status=6, startTime=1714842794514, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3295.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3295, endTime=1714842825774, processId=20814, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3295] - [INFO] 2024-05-05 01:13:46.069 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3295, processInstanceId=946, status=6, startTime=1714842794514, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3295.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3295, endTime=1714842825774, processId=20814, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842826061)
[WI-0][TI-3296] - [INFO] 2024-05-05 01:13:46.072 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3296, processInstanceId=945, status=6, startTime=1714842794551, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3296.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3296, endTime=1714842825783, processId=20821, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3296] - [INFO] 2024-05-05 01:13:46.076 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3296, processInstanceId=945, status=6, startTime=1714842794551, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3296.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3296, endTime=1714842825783, processId=20821, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842826061)
[WI-0][TI-3296] - [INFO] 2024-05-05 01:13:46.670 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3296, success=true)
[WI-0][TI-3295] - [INFO] 2024-05-05 01:13:46.672 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3295, success=true)
[WI-0][TI-3296] - [INFO] 2024-05-05 01:13:46.674 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3296, success=true)
[WI-0][TI-3295] - [INFO] 2024-05-05 01:13:46.678 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3295, success=true)
[WI-0][TI-3260] - [INFO] 2024-05-05 01:13:48.081 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3260, processInstanceId=940, status=9, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, endTime=1714842227024, processId=18772, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842527645)
[WI-0][TI-3260] - [INFO] 2024-05-05 01:13:48.093 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3260, processInstanceId=940, status=9, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, endTime=1714842227024, processId=18772, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842828081)
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:49.319 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8596491228070174 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:50.357 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8698224852071006 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3259] - [INFO] 2024-05-05 01:13:51.098 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3259, processInstanceId=939, status=9, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, endTime=1714842229451, processId=18762, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842530677)
[WI-0][TI-3259] - [INFO] 2024-05-05 01:13:51.138 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3259, processInstanceId=939, status=9, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, endTime=1714842229451, processId=18762, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842831098)
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:51.363 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9098591549295775 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:52.372 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8441926345609065 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:53.373 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.922437673130194 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:54.375 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7402234636871509 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:55.381 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8676056338028169 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3191] - [INFO] 2024-05-05 01:13:56.153 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3191, processInstanceId=936, status=6, startTime=1714839942174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13377949373536/25/936/3191.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/936/3191, endTime=1714840005766, processId=14716, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842535763)
[WI-0][TI-3191] - [INFO] 2024-05-05 01:13:56.158 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3191, processInstanceId=936, status=6, startTime=1714839942174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13377949373536/25/936/3191.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/936/3191, endTime=1714840005766, processId=14716, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842836153)
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:56.383 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7635327635327636 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:13:58.420 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7115384615384615 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:04.718 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3304, taskName=sentiment analysis, firstSubmitTime=1714842844709, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=946, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3304'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505011404'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='946'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.737 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.737 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.740 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.741 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.741 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.741 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714842844741
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.741 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 946_3304
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.741 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3304,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714842844709,
  "startTime" : 1714842844741,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3304.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 946,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\",\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3304"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505011404"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "946"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "946_3304",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.742 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.742 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.742 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:04.748 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3305, taskName=sentiment analysis, firstSubmitTime=1714842844713, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=945, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3305'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505011404'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='945'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.749 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.750 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.752 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.752 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.755 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.755 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714842844755
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.756 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 945_3305
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.753 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.756 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3305,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714842844713,
  "startTime" : 1714842844755,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3305.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 945,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\",\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3305"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505011404"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "945"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "945_3305",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.756 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.758 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.758 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3304 check successfully
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.760 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.760 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.760 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.761 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.761 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.761 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.762 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.763 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.764 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.764 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.765 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.766 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.766 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.766 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.766 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.766 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3305 check successfully
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.772 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.772 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.773 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.774 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.769 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3304
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.774 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3304/py_946_3304.py
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.775 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.774 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.776 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.776 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.776 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.776 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.777 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.777 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.777 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.777 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.778 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3304/py_946_3304.py
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.778 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.778 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3304/946_3304.sh
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.779 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3305
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.779 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3305/py_945_3305.py
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.779 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.780 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.780 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.780 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3305/py_945_3305.py
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.780 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.780 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3305/945_3305.sh
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:04.783 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 21161
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:04.809 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:04.822 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:04.823 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 21160
[WI-0][TI-3304] - [INFO] 2024-05-05 01:14:05.174 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3304, processInstanceId=946, startTime=1714842844741, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3304.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3304] - [INFO] 2024-05-05 01:14:05.179 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3304, processInstanceId=946, startTime=1714842844741, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3304.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714842845172)
[WI-0][TI-3304] - [INFO] 2024-05-05 01:14:05.180 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3304, processInstanceId=946, startTime=1714842844741, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3304] - [INFO] 2024-05-05 01:14:05.194 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3304, processInstanceId=946, startTime=1714842844741, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714842845172)
[WI-0][TI-3305] - [INFO] 2024-05-05 01:14:05.194 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3305, processInstanceId=945, startTime=1714842844755, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3305.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3305] - [INFO] 2024-05-05 01:14:05.202 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3305, processInstanceId=945, startTime=1714842844755, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3305.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714842845172)
[WI-0][TI-3305] - [INFO] 2024-05-05 01:14:05.227 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3305, processInstanceId=945, startTime=1714842844755, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3305] - [INFO] 2024-05-05 01:14:05.248 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3305, processInstanceId=945, startTime=1714842844755, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714842845172)
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:05.491 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7185792349726776 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3305] - [INFO] 2024-05-05 01:14:05.712 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3305, success=true)
[WI-0][TI-3304] - [INFO] 2024-05-05 01:14:05.714 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3304, success=true)
[WI-0][TI-3305] - [INFO] 2024-05-05 01:14:05.720 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3305)
[WI-0][TI-3304] - [INFO] 2024-05-05 01:14:05.730 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3304)
[WI-0][TI-3305] - [INFO] 2024-05-05 01:14:05.730 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3305, success=true)
[WI-0][TI-3305] - [INFO] 2024-05-05 01:14:05.743 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3305)
[WI-0][TI-3304] - [INFO] 2024-05-05 01:14:05.742 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3304, success=true)
[WI-0][TI-3304] - [INFO] 2024-05-05 01:14:05.756 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3304)
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:05.828 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:05.829 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:06.508 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8513513513513513 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:07.510 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8769230769230769 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:08.514 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8987730061349692 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:09.532 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8500000000000001 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:10.542 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8356860485587619 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:10.836 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-a3516c55-9b9e-419d-acc6-1de0128d8ef4;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:10.836 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-4def5290-df85-4901-96ea-6dfa03e683ff;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:11.548 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.957247698454056 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:11.846 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 1623ms :: artifacts dl 51ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:11.847 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:12.549 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9849849849849849 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:12.898 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: retrieving :: org.apache.spark#spark-submit-parent-a3516c55-9b9e-419d-acc6-1de0128d8ef4
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/65ms)
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:12.901 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:13.565 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9727272727272727 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:13.903 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:14:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:13.907 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 2496ms :: artifacts dl 43ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-4def5290-df85-4901-96ea-6dfa03e683ff
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/120ms)
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:14.576 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9458689458689459 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:14.922 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:14.922 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:14:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:15.588 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7834394904458599 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:15.924 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:16.600 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7833333333333333 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:17.601 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8687943262411347 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:17.926 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:14:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:14:17 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:14:17 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:14:17 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:14:17 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:18.602 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9674556213017751 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:19.608 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9678362573099415 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:19.958 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:14:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:14:19 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:14:19 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:14:19 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:14:19 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
	24/05/05 01:14:19 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:20.608 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.919889502762431 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:21.611 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8830409356725146 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:22.623 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.943502824858757 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:23.627 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9376770538243626 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:24.629 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9375 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:25.636 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8419452887537995 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:26.668 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8548895899053628 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:27.673 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9193548387096775 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:28.682 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8651026392961876 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:29.683 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.91005291005291 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:29.970 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3305/py_945_3305.py", line 51, in <module>
	    parsed_topics = topics.split("|")
	                    ^^^^^^^^^^^^
	AttributeError: 'tuple' object has no attribute 'split'
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:30.696 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8575492072410584 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:30.988 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3305, processId:21161 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:31.009 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:31.010 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:31.010 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:31.010 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:31.049 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:31.051 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:31.051 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3305
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:31.052 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3305
[WI-945][TI-3305] - [INFO] 2024-05-05 01:14:31.054 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3305] - [INFO] 2024-05-05 01:14:31.404 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3305, processInstanceId=945, status=6, startTime=1714842844755, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3305.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3305, endTime=1714842871010, processId=21161, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3305] - [INFO] 2024-05-05 01:14:31.412 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3305, processInstanceId=945, status=6, startTime=1714842844755, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3305.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3305, endTime=1714842871010, processId=21161, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842871388)
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:31.711 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8245006999073362 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3305] - [INFO] 2024-05-05 01:14:31.791 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3305, success=true)
[WI-0][TI-3305] - [INFO] 2024-05-05 01:14:31.795 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3305, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:31.988 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3304/py_946_3304.py", line 51, in <module>
	    parsed_topics = topics.split("|")
	                    ^^^^^^^^^^^^
	AttributeError: 'tuple' object has no attribute 'split'
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:33.008 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3304, processId:21160 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:33.008 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:33.009 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:33.009 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:33.009 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:33.046 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:33.046 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:33.046 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3304
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:33.050 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3304
[WI-946][TI-3304] - [INFO] 2024-05-05 01:14:33.051 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3304] - [INFO] 2024-05-05 01:14:33.416 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3304, processInstanceId=946, status=6, startTime=1714842844741, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3304.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3304, endTime=1714842873009, processId=21160, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3304] - [INFO] 2024-05-05 01:14:33.437 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3304, processInstanceId=946, status=6, startTime=1714842844741, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3304.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3304, endTime=1714842873009, processId=21160, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842873416)
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:33.728 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7891737891737891 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3304] - [INFO] 2024-05-05 01:14:33.802 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3304, success=true)
[WI-0][TI-3304] - [INFO] 2024-05-05 01:14:33.806 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3304, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:40.775 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7988668555240794 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:42.825 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8392370572207085 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:43.893 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9036458333333334 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:44.905 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8739255014326648 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:45.906 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8644986449864498 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:47.964 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9237057220708447 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:49.044 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7591623036649214 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:50.055 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9267526507358759 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:51.064 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7212643678160919 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:53.078 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8074712643678161 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:54.162 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8534704370179949 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:56.202 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7122905027932961 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:57.284 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8258575197889182 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3262] - [INFO] 2024-05-05 01:14:57.478 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3262, processInstanceId=942, status=9, startTime=1714842212466, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262, endTime=1714842296579, processId=18774, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842597210)
[WI-0][TI-3262] - [INFO] 2024-05-05 01:14:57.484 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3262, processInstanceId=942, status=9, startTime=1714842212466, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262, endTime=1714842296579, processId=18774, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842897478)
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:58.285 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8921832884097035 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:14:59.293 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8945868945868947 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:00.295 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8867924528301887 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:01.296 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7344632768361582 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:02.310 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7643312101910829 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:03.337 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7310924369747899 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:04.342 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7485207100591716 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:07.384 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8474576271186441 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:08.403 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7879656160458453 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:10.420 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8614457831325302 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:11.431 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8425655976676384 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:16.185 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3315, taskName=sentiment analysis, firstSubmitTime=1714842916160, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=946, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3315'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505011516'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='946'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.190 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.192 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.203 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.203 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.203 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.204 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714842916204
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.205 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 946_3315
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:16.199 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3314, taskName=sentiment analysis, firstSubmitTime=1714842916159, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=945, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3314'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505011516'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='945'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.213 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.213 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.205 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3315,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714842916160,
  "startTime" : 1714842916204,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3315.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 946,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\",\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3315"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505011516"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "946"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "946_3315",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.215 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.216 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.216 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.216 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714842916216
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.216 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.216 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 945_3314
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.217 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.218 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.217 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3314,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714842916159,
  "startTime" : 1714842916216,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3314.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 945,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\",\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3314"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505011516"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "945"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "945_3314",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.226 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.226 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.226 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.225 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:16.226 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3316, taskName=sentiment analysis, firstSubmitTime=1714842916195, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=945, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3316'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505011516'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='945'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.234 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.236 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.236 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3314 check successfully
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.237 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.237 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.237 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.237 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.237 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.238 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.239 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.240 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.239 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.240 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3315 check successfully
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.240 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.240 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.241 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.241 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.241 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.239 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.252 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.252 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:16.248 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3317, taskName=sentiment analysis, firstSubmitTime=1714842916226, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=946, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3317'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505011516'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='946'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.241 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.253 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.254 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714842916254
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.254 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 945_3316
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.254 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.254 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.259 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.241 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.259 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.260 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.260 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714842916260
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.260 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 946_3317
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.257 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3314
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.260 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.260 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3317,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714842916226,
  "startTime" : 1714842916260,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3317.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 946,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\",\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3317"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505011516"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "946"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "946_3317",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.267 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3314/py_945_3314.py
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.269 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.255 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3316,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714842916195,
  "startTime" : 1714842916254,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3316.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 945,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\",\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3316"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505011516"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "945"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "945_3316",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.268 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.268 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.272 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.269 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.271 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.271 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.272 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.272 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.272 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.279 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.279 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3314/py_945_3314.py
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.279 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.280 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3314/945_3314.sh
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.277 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.282 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.283 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.284 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.284 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.284 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.285 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3317 check successfully
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.285 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.285 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.285 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.285 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.285 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.286 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.286 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.286 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.286 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.286 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.284 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.286 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.284 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:16.288 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 21515
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.287 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3317
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.288 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3315
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.289 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3317/py_946_3317.py
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.298 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.299 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.299 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.299 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3317/py_946_3317.py
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.299 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.299 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3317/946_3317.sh
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.300 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3315/py_946_3315.py
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.301 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.300 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.303 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3316 check successfully
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.307 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.310 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.312 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3315/py_946_3315.py
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.312 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.312 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3315/946_3315.sh
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.309 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.316 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.319 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:16.319 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 21518
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.319 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.325 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.331 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.331 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:16.330 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 21521
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.333 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.333 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.334 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.337 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.339 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3316
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.340 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3316/py_945_3316.py
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.340 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.342 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.344 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.345 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3316/py_945_3316.py
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.345 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.346 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3316/945_3316.sh
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:16.352 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 21531
[WI-0][TI-3314] - [INFO] 2024-05-05 01:15:16.572 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3314, processInstanceId=945, startTime=1714842916216, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3314.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3314] - [INFO] 2024-05-05 01:15:16.629 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3314, processInstanceId=945, startTime=1714842916216, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3314.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714842916562)
[WI-0][TI-3314] - [INFO] 2024-05-05 01:15:16.630 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3314, processInstanceId=945, startTime=1714842916216, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3314] - [INFO] 2024-05-05 01:15:16.643 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3314, processInstanceId=945, startTime=1714842916216, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714842916562)
[WI-0][TI-3315] - [INFO] 2024-05-05 01:15:16.696 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3315, processInstanceId=946, startTime=1714842916204, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3315.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3315] - [INFO] 2024-05-05 01:15:16.835 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3315, processInstanceId=946, startTime=1714842916204, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3315.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714842916562)
[WI-0][TI-3315] - [INFO] 2024-05-05 01:15:16.836 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3315, processInstanceId=946, startTime=1714842916204, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3315] - [INFO] 2024-05-05 01:15:16.899 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3315, processInstanceId=946, startTime=1714842916204, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714842916562)
[WI-0][TI-3316] - [INFO] 2024-05-05 01:15:16.938 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3316, processInstanceId=945, startTime=1714842916254, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3316.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3316] - [INFO] 2024-05-05 01:15:16.941 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3316, processInstanceId=945, startTime=1714842916254, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3316.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714842916562)
[WI-0][TI-3316] - [INFO] 2024-05-05 01:15:16.941 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3316, processInstanceId=945, startTime=1714842916254, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3316] - [INFO] 2024-05-05 01:15:16.949 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3316, processInstanceId=945, startTime=1714842916254, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714842916562)
[WI-0][TI-3317] - [INFO] 2024-05-05 01:15:16.955 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3317, processInstanceId=946, startTime=1714842916260, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3317.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3315] - [INFO] 2024-05-05 01:15:16.956 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3315, success=true)
[WI-0][TI-3314] - [INFO] 2024-05-05 01:15:16.959 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3314, success=true)
[WI-0][TI-3317] - [INFO] 2024-05-05 01:15:16.966 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3317, processInstanceId=946, startTime=1714842916260, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3317.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714842916562)
[WI-0][TI-3317] - [INFO] 2024-05-05 01:15:16.967 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3317, processInstanceId=946, startTime=1714842916260, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3317] - [INFO] 2024-05-05 01:15:16.971 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3317, success=true)
[WI-0][TI-3316] - [INFO] 2024-05-05 01:15:16.975 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3316, success=true)
[WI-0][TI-3317] - [INFO] 2024-05-05 01:15:16.975 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3317, processInstanceId=946, startTime=1714842916260, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714842916562)
[WI-0][TI-3317] - [WARN] 2024-05-05 01:15:16.977 +0800 o.a.d.s.w.m.MessageRetryRunner:[139] - Retry send message to master error
java.util.ConcurrentModificationException: null
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:911)
	at java.util.ArrayList$Itr.next(ArrayList.java:861)
	at org.apache.dolphinscheduler.server.worker.message.MessageRetryRunner.run(MessageRetryRunner.java:127)
[WI-0][TI-3317] - [INFO] 2024-05-05 01:15:16.978 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3317)
[WI-0][TI-3314] - [INFO] 2024-05-05 01:15:16.981 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3314)
[WI-0][TI-3315] - [INFO] 2024-05-05 01:15:16.982 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3315)
[WI-0][TI-3315] - [INFO] 2024-05-05 01:15:16.986 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3315, success=true)
[WI-0][TI-3316] - [INFO] 2024-05-05 01:15:16.990 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3316)
[WI-0][TI-3315] - [INFO] 2024-05-05 01:15:16.995 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3315)
[WI-0][TI-3314] - [INFO] 2024-05-05 01:15:16.996 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3314, success=true)
[WI-0][TI-3314] - [INFO] 2024-05-05 01:15:17.000 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3314)
[WI-0][TI-3317] - [INFO] 2024-05-05 01:15:17.005 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3317, success=true)
[WI-0][TI-3317] - [INFO] 2024-05-05 01:15:17.014 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3317)
[WI-0][TI-3316] - [INFO] 2024-05-05 01:15:17.017 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3316, success=true)
[WI-0][TI-3316] - [INFO] 2024-05-05 01:15:17.026 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3316)
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:17.355 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:17.358 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:17.364 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:17.366 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:17.463 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9593220338983051 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:18.358 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:18.475 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.94 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:19.480 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9685714285714286 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:20.481 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9916201117318435 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:21.484 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9825072886297376 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:22.499 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9739130434782609 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:23.501 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9888888888888889 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:24.531 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9737609329446064 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:25.561 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9807692307692307 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:26.566 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9855491329479769 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:27.572 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9835526315789473 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:28.600 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9923469387755103 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:29.610 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.967479674796748 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:30.400 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-adec50f3-5c90-45c0-96c5-96bb8557fd90;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:30.402 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-fbe2710c-098a-4a0a-8276-9cc4160cadde;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:30.474 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:30.611 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9661177452491011 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:31.389 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-b37620cb-f60c-4955-814e-c5d9ed47919a;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:31.403 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:31.403 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:31.482 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-9d002cd6-e50b-404d-919d-a5981a4b7b9d;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:31.614 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9735973597359736 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:32.405 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:32.410 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:32.490 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:32.634 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9833887043189369 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:33.392 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:33.408 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 2803ms :: artifacts dl 211ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-fbe2710c-098a-4a0a-8276-9cc4160cadde
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:33.414 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 2500ms :: artifacts dl 117ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-adec50f3-5c90-45c0-96c5-96bb8557fd90
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/10ms)
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:33.491 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 2540ms :: artifacts dl 89ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-9d002cd6-e50b-404d-919d-a5981a4b7b9d
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/45ms)
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:33.639 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9607142857142857 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:34.399 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:34.416 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:15:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:34.640 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9886363636363636 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:35.402 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: resolution report :: resolve 3095ms :: artifacts dl 18ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-b37620cb-f60c-4955-814e-c5d9ed47919a
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/63ms)
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:35.416 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:15:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:35.416 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:35.498 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:15:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:35.674 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9779874213836478 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:36.412 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:15:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:36.417 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:36.499 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:36.687 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9603174603174603 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:37.693 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9594202898550724 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:38.705 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.949640287769784 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:39.546 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:15:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:15:39 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:15:39 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:15:39 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:15:39 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:39.707 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9965635738831615 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:40.548 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:15:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:15:40 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:15:40 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:15:40 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:15:40 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
	24/05/05 01:15:40 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:40.568 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:15:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:15:40 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:15:40 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:15:40 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:15:40 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
	24/05/05 01:15:40 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
	24/05/05 01:15:40 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:40.715 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.953560371517028 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:41.546 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:15:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:15:40 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:15:40 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:15:40 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:15:40 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
	24/05/05 01:15:40 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
	24/05/05 01:15:40 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
	24/05/05 01:15:40 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:41.719 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9490616621983914 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:42.721 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9483204134366924 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:43.729 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9547801794683674 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:44.739 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9480949172842278 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:45.740 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9523809523809523 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:46.741 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9469026548672566 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:47.757 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9223300970873786 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:48.564 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3315/py_946_3315.py", line 51, in <module>
	    parsed_topics = topics.split("|")
	                    ^^^^^^^^^^^^
	AttributeError: 'tuple' object has no attribute 'split'
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:48.789 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8968609865470853 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:49.794 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9520958083832336 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:50.572 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3317/py_946_3317.py", line 51, in <module>
	    parsed_topics = topics.split("|")
	                    ^^^^^^^^^^^^
	AttributeError: 'tuple' object has no attribute 'split'
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:50.575 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3315, processId:21521 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:50.580 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:50.580 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:50.581 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:50.581 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:50.588 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:50.589 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:50.589 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3315
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:50.592 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3315
[WI-946][TI-3315] - [INFO] 2024-05-05 01:15:50.593 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:50.604 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3314/py_945_3314.py", line 51, in <module>
	    parsed_topics = topics.split("|")
	                    ^^^^^^^^^^^^
	AttributeError: 'tuple' object has no attribute 'split'
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:50.605 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3316/py_945_3316.py", line 51, in <module>
	    parsed_topics = topics.split("|")
	                    ^^^^^^^^^^^^
	AttributeError: 'tuple' object has no attribute 'split'
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:50.801 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.896774193548387 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3315] - [INFO] 2024-05-05 01:15:51.104 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3315, processInstanceId=946, status=6, startTime=1714842916204, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3315.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3315, endTime=1714842950581, processId=21521, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3315] - [INFO] 2024-05-05 01:15:51.104 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3315, success=true)
[WI-0][TI-3315] - [INFO] 2024-05-05 01:15:51.114 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3315, processInstanceId=946, status=6, startTime=1714842916204, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3315.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3315, endTime=1714842950581, processId=21521, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842951100)
[WI-0][TI-3315] - [WARN] 2024-05-05 01:15:51.115 +0800 o.a.d.s.w.m.MessageRetryRunner:[139] - Retry send message to master error
java.util.ConcurrentModificationException: null
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:911)
	at java.util.ArrayList$Itr.next(ArrayList.java:861)
	at org.apache.dolphinscheduler.server.worker.message.MessageRetryRunner.run(MessageRetryRunner.java:127)
[WI-0][TI-3315] - [INFO] 2024-05-05 01:15:51.130 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3315, success=true)
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:51.613 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3316, processId:21531 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:51.613 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:51.613 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:51.613 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:51.613 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:51.700 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:51.701 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:51.701 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3316
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:51.702 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3316
[WI-945][TI-3316] - [INFO] 2024-05-05 01:15:51.702 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:51.816 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9582948599569098 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3316] - [INFO] 2024-05-05 01:15:52.120 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3316, processInstanceId=945, status=6, startTime=1714842916254, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3316.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3316, endTime=1714842951613, processId=21531, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3316] - [INFO] 2024-05-05 01:15:52.121 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3316, success=true)
[WI-0][TI-3316] - [INFO] 2024-05-05 01:15:52.127 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3316, processInstanceId=945, status=6, startTime=1714842916254, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3316.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3316, endTime=1714842951613, processId=21531, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842952116)
[WI-0][TI-3316] - [WARN] 2024-05-05 01:15:52.128 +0800 o.a.d.s.w.m.MessageRetryRunner:[139] - Retry send message to master error
java.util.ConcurrentModificationException: null
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:911)
	at java.util.ArrayList$Itr.next(ArrayList.java:861)
	at org.apache.dolphinscheduler.server.worker.message.MessageRetryRunner.run(MessageRetryRunner.java:127)
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:52.588 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3317, processId:21518 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:52.588 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:52.590 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:52.590 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:52.591 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:52.595 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:52.596 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:52.596 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3317
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:52.600 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3317
[WI-946][TI-3317] - [INFO] 2024-05-05 01:15:52.600 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:52.638 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3314, processId:21515 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:52.647 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:52.648 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:52.648 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:52.648 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:52.689 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:52.689 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:52.689 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3314
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:52.690 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3314
[WI-945][TI-3314] - [INFO] 2024-05-05 01:15:52.690 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:52.819 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8702413991726449 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3316] - [INFO] 2024-05-05 01:15:53.095 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3316, success=true)
[WI-0][TI-3317] - [INFO] 2024-05-05 01:15:53.103 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3317, success=true)
[WI-0][TI-3314] - [INFO] 2024-05-05 01:15:53.105 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3314, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:54.359 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3321, taskName=sentiment analysis, firstSubmitTime=1714842954174, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=946, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3321'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505011554'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='946'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.371 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.377 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.374 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.397 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.397 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.397 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714842954397
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.397 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 946_3321
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.398 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3321,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714842954174,
  "startTime" : 1714842954397,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3321.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 946,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\",\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3321"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505011554"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "946"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "946_3321",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.399 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.399 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.399 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.423 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.424 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.430 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3321 check successfully
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:54.435 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3322, taskName=sentiment analysis, firstSubmitTime=1714842954212, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=945, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3322'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505011554'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='945'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.438 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.439 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.448 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.448 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.448 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.448 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714842954448
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.431 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.448 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 945_3322
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.449 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3322,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714842954212,
  "startTime" : 1714842954448,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3322.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 945,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\",\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3322"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505011554"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "945"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "945_3322",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.451 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.451 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.451 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.534 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.535 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.535 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.535 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.536 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.536 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.536 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.544 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.538 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.555 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.555 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3322 check successfully
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.553 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.563 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.564 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3321
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.591 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3321/py_946_3321.py
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.564 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.640 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.599 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.644 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.641 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.644 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.647 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3321/py_946_3321.py
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.647 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.648 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3321/946_3321.sh
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.649 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.665 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.684 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-946][TI-3321] - [INFO] 2024-05-05 01:15:54.676 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 22198
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.685 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.737 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.744 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.745 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.745 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.745 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3322
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.746 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3322/py_945_3322.py
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.746 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.750 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.751 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.751 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3322/py_945_3322.py
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.751 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.751 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3322/945_3322.sh
[WI-945][TI-3322] - [INFO] 2024-05-05 01:15:54.818 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 22205
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:54.904 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0132542902610715 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3321] - [INFO] 2024-05-05 01:15:55.167 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3321, processInstanceId=946, startTime=1714842954397, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3321.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3321] - [INFO] 2024-05-05 01:15:55.201 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3321, processInstanceId=946, startTime=1714842954397, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3321.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714842955144)
[WI-0][TI-3321] - [INFO] 2024-05-05 01:15:55.214 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3321, processInstanceId=946, startTime=1714842954397, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3321] - [INFO] 2024-05-05 01:15:55.233 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3321, success=true)
[WI-0][TI-3322] - [INFO] 2024-05-05 01:15:55.233 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3322, success=true)
[WI-0][TI-3321] - [INFO] 2024-05-05 01:15:55.259 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3321)
[WI-0][TI-3321] - [INFO] 2024-05-05 01:15:55.260 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3321, processInstanceId=946, startTime=1714842954397, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714842955144)
[WI-0][TI-3321] - [WARN] 2024-05-05 01:15:55.261 +0800 o.a.d.s.w.m.MessageRetryRunner:[139] - Retry send message to master error
java.util.ConcurrentModificationException: null
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:911)
	at java.util.ArrayList$Itr.next(ArrayList.java:861)
	at org.apache.dolphinscheduler.server.worker.message.MessageRetryRunner.run(MessageRetryRunner.java:127)
[WI-0][TI-3322] - [INFO] 2024-05-05 01:15:55.261 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3322, processInstanceId=945, startTime=1714842954448, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3322] - [INFO] 2024-05-05 01:15:55.273 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3322)
[WI-0][TI-3322] - [INFO] 2024-05-05 01:15:55.277 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3322, processInstanceId=945, startTime=1714842954448, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714842955144)
[WI-0][TI-3322] - [WARN] 2024-05-05 01:15:55.277 +0800 o.a.d.s.w.m.MessageRetryRunner:[139] - Retry send message to master error
java.util.ConcurrentModificationException: null
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:911)
	at java.util.ArrayList$Itr.next(ArrayList.java:861)
	at org.apache.dolphinscheduler.server.worker.message.MessageRetryRunner.run(MessageRetryRunner.java:127)
[WI-0][TI-3321] - [INFO] 2024-05-05 01:15:55.281 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3321, success=true)
[WI-0][TI-3322] - [INFO] 2024-05-05 01:15:55.320 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3322)
[WI-0][TI-3321] - [INFO] 2024-05-05 01:15:55.329 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3321)
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:55.812 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:55.845 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:55.926 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.918444069357066 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:56.833 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:56.903 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:56.929 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.935483870967742 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:57.933 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9861495844875346 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:58.936 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9555555555555556 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:15:59.938 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9854227405247813 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:00.945 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9719887955182073 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:01.961 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9606060606060606 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:02.966 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9597855227882037 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:03.968 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.93717277486911 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:04.970 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9571428571428572 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:05.867 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-6ae071a9-11c6-4c91-ab2d-6195ff8e7611;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:05.975 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-427cb735-19b0-4bbf-9275-7c67e7ee5c43;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:06.008 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.928759894459103 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3263] - [INFO] 2024-05-05 01:16:06.304 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3263, processInstanceId=943, status=9, startTime=1714842505551, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/943/3263.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263, endTime=1714842665300, processId=20088, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842665311)
[WI-0][TI-3263] - [INFO] 2024-05-05 01:16:06.317 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3263, processInstanceId=943, status=9, startTime=1714842505551, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/943/3263.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263, endTime=1714842665300, processId=20088, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842966304)
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:07.310 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:07.354 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8692579505300354 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:07.356 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:08.313 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 1861ms :: artifacts dl 117ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-6ae071a9-11c6-4c91-ab2d-6195ff8e7611
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/153ms)
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:08.366 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9693593314763231 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:08.368 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 1780ms :: artifacts dl 72ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-427cb735-19b0-4bbf-9275-7c67e7ee5c43
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/28ms)
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:09.385 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9611940298507463 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:10.328 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:16:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:10.396 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:16:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:10.404 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9861111111111112 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:11.330 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:11.421 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:11.422 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.828169014084507 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:12.448 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9131736526946107 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:13.491 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8539823008849557 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:14.345 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:16:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:16:13 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:16:13 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:16:13 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:16:13 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:14.426 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:16:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:16:13 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:16:13 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:16:13 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:16:13 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
	24/05/05 01:16:13 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:14.496 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9281250000000001 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:15.503 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7994350282485876 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:16.518 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8262108262108262 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:17.523 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.841225626740947 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:18.528 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8393939393939394 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:19.540 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9320652173913044 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:20.554 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.896797153024911 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:21.382 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3321/py_946_3321.py", line 51, in <module>
	    parsed_topics = topics.split("|")
	                    ^^^^^^^^^^^^
	AttributeError: 'tuple' object has no attribute 'split'
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:21.561 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9569230769230769 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:22.442 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3322/py_945_3322.py", line 51, in <module>
	    parsed_topics = topics.split("|")
	                    ^^^^^^^^^^^^
	AttributeError: 'tuple' object has no attribute 'split'
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:22.563 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7147058823529411 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-946][TI-3321] - [INFO] 2024-05-05 01:16:23.410 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3321, processId:22198 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-946][TI-3321] - [INFO] 2024-05-05 01:16:23.424 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3321] - [INFO] 2024-05-05 01:16:23.424 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-946][TI-3321] - [INFO] 2024-05-05 01:16:23.425 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3321] - [INFO] 2024-05-05 01:16:23.425 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-946][TI-3321] - [INFO] 2024-05-05 01:16:23.433 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-946][TI-3321] - [INFO] 2024-05-05 01:16:23.433 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-946][TI-3321] - [INFO] 2024-05-05 01:16:23.435 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3321
[WI-946][TI-3321] - [INFO] 2024-05-05 01:16:23.438 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3321
[WI-946][TI-3321] - [INFO] 2024-05-05 01:16:23.438 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-945][TI-3322] - [INFO] 2024-05-05 01:16:23.445 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3322, processId:22205 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-945][TI-3322] - [INFO] 2024-05-05 01:16:23.458 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3322] - [INFO] 2024-05-05 01:16:23.463 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-945][TI-3322] - [INFO] 2024-05-05 01:16:23.463 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3322] - [INFO] 2024-05-05 01:16:23.467 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-945][TI-3322] - [INFO] 2024-05-05 01:16:23.499 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-945][TI-3322] - [INFO] 2024-05-05 01:16:23.500 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-945][TI-3322] - [INFO] 2024-05-05 01:16:23.500 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3322
[WI-945][TI-3322] - [INFO] 2024-05-05 01:16:23.502 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3322
[WI-945][TI-3322] - [INFO] 2024-05-05 01:16:23.503 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3321] - [INFO] 2024-05-05 01:16:23.534 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3321, processInstanceId=946, status=6, startTime=1714842954397, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3321.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3321, endTime=1714842983425, processId=22198, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3321] - [INFO] 2024-05-05 01:16:23.546 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3321, processInstanceId=946, status=6, startTime=1714842954397, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3321.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3321, endTime=1714842983425, processId=22198, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842983533)
[WI-0][TI-3322] - [INFO] 2024-05-05 01:16:23.550 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3322, processInstanceId=945, status=6, startTime=1714842954448, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3322.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3322, endTime=1714842983467, processId=22205, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:23.564 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7142857142857142 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3322] - [INFO] 2024-05-05 01:16:23.575 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3322, processInstanceId=945, status=6, startTime=1714842954448, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3322.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3322, endTime=1714842983467, processId=22205, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842983533)
[WI-0][TI-3321] - [INFO] 2024-05-05 01:16:24.391 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3321, success=true)
[WI-0][TI-3322] - [INFO] 2024-05-05 01:16:24.392 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3322, success=true)
[WI-0][TI-3322] - [INFO] 2024-05-05 01:16:24.397 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3322, success=true)
[WI-0][TI-3321] - [INFO] 2024-05-05 01:16:24.397 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3321, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:28.606 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8865435356200527 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:29.636 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7158469945355191 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:35.713 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3325, taskName=sentiment analysis, firstSubmitTime=1714842995692, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=946, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3325'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505011635'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='946'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.715 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.717 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.737 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.740 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.741 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.743 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714842995743
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.754 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 946_3325
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:35.771 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3326, taskName=sentiment analysis, firstSubmitTime=1714842995731, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=945, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3326'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505011635'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='945'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.774 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.774 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.788 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.795 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.797 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.799 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714842995799
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.799 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 945_3326
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.768 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3325,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714842995692,
  "startTime" : 1714842995743,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3325.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 946,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\",\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3325"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505011635"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "946"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "946_3325",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.804 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.804 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.805 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.811 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.803 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3326,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714842995731,
  "startTime" : 1714842995799,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3326.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 945,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\",\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3326"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505011635"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "945"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "945_3326",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.823 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.822 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.825 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.825 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.829 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3325 check successfully
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.832 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.839 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.839 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.840 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.841 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.844 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.846 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3326 check successfully
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.844 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.853 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.854 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.852 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.854 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.854 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.856 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.857 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.857 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.858 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.859 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.861 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.864 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.859 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.864 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.865 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.865 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.865 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.866 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3325
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.868 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3326
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.868 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3325/py_946_3325.py
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.871 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3326/py_945_3326.py
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.872 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.872 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.875 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.875 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3326/py_945_3326.py
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.875 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.875 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3326/945_3326.sh
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.872 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.877 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.877 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.877 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3325/py_946_3325.py
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.879 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:35.879 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 22541
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.880 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3325/946_3325.sh
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:35.890 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:35.896 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 22544
[WI-0][TI-3326] - [INFO] 2024-05-05 01:16:36.416 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3326, success=true)
[WI-0][TI-3325] - [INFO] 2024-05-05 01:16:36.427 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3325, success=true)
[WI-0][TI-3325] - [INFO] 2024-05-05 01:16:36.442 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3325)
[WI-0][TI-3326] - [INFO] 2024-05-05 01:16:36.439 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3326)
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:36.676 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8044692737430167 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:36.895 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:36.908 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:37.706 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8708791208791209 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:38.724 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9497206703910615 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:39.726 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9323943661971832 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:40.729 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9126984126984127 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:41.732 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9138461538461539 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:42.762 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8404255319148936 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:43.248 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-9bc7cff9-9580-4b1e-a4dc-2c29f6ec76cb;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:43.766 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9027355623100304 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:44.250 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:44.251 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-2c55cbb6-1c19-4275-90f4-314b47adbab2;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:44.784 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9250720461095101 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:45.253 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 1811ms :: artifacts dl 31ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-9bc7cff9-9580-4b1e-a4dc-2c29f6ec76cb
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/43ms)
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:45.253 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:45.804 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9370629370629371 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:46.257 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 2083ms :: artifacts dl 76ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-2c55cbb6-1c19-4275-90f4-314b47adbab2
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/8ms)
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:46.259 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:16:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:46.820 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9730458221024259 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:47.268 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:47.280 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:16:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:47.831 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9053254437869822 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:48.840 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7796934385833021 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:49.843 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9322033898305084 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:50.272 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:16:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:16:50 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:16:50 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:16:50 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:16:50 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:50.845 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.873015873015873 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:51.289 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:16:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:16:50 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:16:50 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:16:50 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:16:50 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
	24/05/05 01:16:50 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:51.846 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8368983957219251 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:52.866 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8407821229050279 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:53.869 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8579464975534382 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:54.881 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.754325259515571 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:55.886 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7715231788079471 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:56.284 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3326/py_945_3326.py", line 51, in <module>
	    parsed_topics = topics.split("|")
	                    ^^^^^^^^^^^^
	AttributeError: 'tuple' object has no attribute 'split'
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:57.284 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3326, processId:22541 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:57.285 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:57.290 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:57.292 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:57.293 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:57.295 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:57.296 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:57.296 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3326
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:57.297 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3326
[WI-945][TI-3326] - [INFO] 2024-05-05 01:16:57.297 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-05-05 01:16:57.308 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3325/py_946_3325.py", line 51, in <module>
	    parsed_topics = topics.split("|")
	                    ^^^^^^^^^^^^
	AttributeError: 'tuple' object has no attribute 'split'
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:57.310 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3325, processId:22544 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:57.312 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:57.313 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:57.313 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:57.314 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:57.318 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:57.318 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:57.320 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3325
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:57.322 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3325
[WI-946][TI-3325] - [INFO] 2024-05-05 01:16:57.324 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3325] - [INFO] 2024-05-05 01:16:57.522 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3325, success=true)
[WI-0][TI-3326] - [INFO] 2024-05-05 01:16:57.522 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3326, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:05.824 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3334, taskName=sentiment analysis, firstSubmitTime=1714843025817, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=945, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3334'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505011705'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='945'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.825 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.826 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.829 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:05.830 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3335, taskName=sentiment analysis, firstSubmitTime=1714843025818, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=946, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3335'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505011705'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='946'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.831 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.831 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.832 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.831 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.834 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.833 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714843025832
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.835 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 945_3334
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.835 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.836 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.837 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714843025837
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.837 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 946_3335
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.837 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3335,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714843025818,
  "startTime" : 1714843025837,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3335.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 946,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\",\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3335"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505011705"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "946"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "946_3335",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.838 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.838 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.838 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.842 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.842 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.843 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3335 check successfully
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.843 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.843 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.843 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.843 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.843 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.843 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.844 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.844 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.844 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.844 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.844 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.845 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3335
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.847 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3335/py_946_3335.py
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.848 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.849 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.853 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.854 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3335/py_946_3335.py
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.854 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.854 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3335/946_3335.sh
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.839 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3334,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714843025817,
  "startTime" : 1714843025832,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3334.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 945,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\",\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3334"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505011705"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "945"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "945_3334",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.855 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.856 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.857 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.862 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.867 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:05.869 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 22884
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.868 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3334 check successfully
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.869 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.871 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.872 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.873 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.874 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.874 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.875 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.875 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.875 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.875 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.876 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.877 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3334
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.877 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3334/py_945_3334.py
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.877 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.878 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.878 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.878 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3334/py_945_3334.py
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.878 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.878 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3334/945_3334.sh
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:05.882 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 22888
[WI-0][TI-3335] - [INFO] 2024-05-05 01:17:06.547 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3335, success=true)
[WI-0][TI-3334] - [INFO] 2024-05-05 01:17:06.547 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3334, success=true)
[WI-0][TI-3334] - [INFO] 2024-05-05 01:17:06.562 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3334)
[WI-0][TI-3335] - [INFO] 2024-05-05 01:17:06.563 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3335)
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:06.869 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:06.882 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:06.909 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7596685082872928 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:07.920 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.819620253164557 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:08.924 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.888888888888889 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:09.900 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:09.941 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8204496411135427 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:10.910 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-7a0de051-30ca-4f23-ae57-0590e2dd9735;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:10.909 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-c20dd4db-1a1c-4c07-bee6-e43e30f3df80;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:10.950 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8964285714285714 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:11.924 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 1268ms :: artifacts dl 48ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-7a0de051-30ca-4f23-ae57-0590e2dd9735
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/29ms)
	24/05/05 01:17:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:11.924 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 1320ms :: artifacts dl 57ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-c20dd4db-1a1c-4c07-bee6-e43e30f3df80
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/19ms)
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:11.951 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9288188139421586 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:12.934 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:12.934 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:17:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:12.953 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8994082840236687 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:13.958 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8947197926789763 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:14.964 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8620689655172413 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:15.940 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:17:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:17:15 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:17:15 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:17:15 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:17:15 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:15.940 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:17:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:17:15 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:17:15 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:17:15 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:17:15 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
	24/05/05 01:17:15 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:15.965 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7988505747126436 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:16.968 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8016759776536313 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:17.984 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8601190476190476 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:18.997 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.784375 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:20.001 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7133550488599348 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:20.957 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3334/py_945_3334.py", line 51, in <module>
	    parsed_topics = topics.split("|")
	                    ^^^^^^^^^^^^
	AttributeError: 'tuple' object has no attribute 'split'
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:21.005 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8112835654151539 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:21.958 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3334, processId:22888 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:21.960 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:21.960 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:21.960 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:21.960 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:21.963 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:21.968 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3335/py_946_3335.py", line 51, in <module>
	    parsed_topics = topics.split("|")
	                    ^^^^^^^^^^^^
	AttributeError: 'tuple' object has no attribute 'split'
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:21.968 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:21.969 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3335, processId:22884 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:21.971 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3334
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:21.972 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:21.972 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:21.972 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:21.972 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:21.974 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3334
[WI-945][TI-3334] - [INFO] 2024-05-05 01:17:21.974 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:21.975 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:21.975 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:21.975 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3335
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:21.976 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3335
[WI-946][TI-3335] - [INFO] 2024-05-05 01:17:21.977 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3334] - [INFO] 2024-05-05 01:17:22.588 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3334, success=true)
[WI-0][TI-3335] - [INFO] 2024-05-05 01:17:22.590 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3335, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:24.674 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3340, taskName=sentiment analysis, firstSubmitTime=1714843044659, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=945, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3340'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505011724'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='945'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.676 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:24.679 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3341, taskName=sentiment analysis, firstSubmitTime=1714843044660, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=946, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3341'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505011724'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='946'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.676 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.681 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.682 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.682 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.682 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.687 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.687 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714843044687
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.687 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 945_3340
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.686 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.687 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3340,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714843044659,
  "startTime" : 1714843044687,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3340.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 945,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\",\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3340"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505011724"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "945"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "945_3340",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.688 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.688 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.688 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.688 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.689 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.689 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714843044689
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.691 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 946_3341
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.691 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.691 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3341,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714843044660,
  "startTime" : 1714843044689,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3341.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 946,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\",\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3341"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505011724"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "946"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "946_3341",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.691 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.692 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.693 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.693 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.693 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3340 check successfully
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.693 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.693 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.694 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.694 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.694 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.697 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.697 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.697 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.697 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.697 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.698 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3341 check successfully
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.698 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.698 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.699 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.699 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.699 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.700 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.700 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.701 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3340
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.701 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.701 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3340/py_945_3340.py
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.702 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.702 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.702 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.703 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.703 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.704 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.704 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3340/py_945_3340.py
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.704 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.703 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.704 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3340/945_3340.sh
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.704 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.705 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.706 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3341
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.707 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3341/py_946_3341.py
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.707 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.709 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.709 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:24.709 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 23227
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.709 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3341/py_946_3341.py
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.715 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.715 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3341/946_3341.sh
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:24.725 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 23230
[WI-0][TI-3340] - [INFO] 2024-05-05 01:17:24.738 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3340, processInstanceId=945, startTime=1714843044687, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3340.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3340] - [INFO] 2024-05-05 01:17:24.745 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3340, processInstanceId=945, startTime=1714843044687, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3340.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714843044737)
[WI-0][TI-3340] - [INFO] 2024-05-05 01:17:24.746 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3340, processInstanceId=945, startTime=1714843044687, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3340] - [INFO] 2024-05-05 01:17:24.750 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3340, processInstanceId=945, startTime=1714843044687, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714843044737)
[WI-0][TI-3341] - [INFO] 2024-05-05 01:17:24.752 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3341, processInstanceId=946, startTime=1714843044689, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3341.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3341] - [INFO] 2024-05-05 01:17:24.755 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3341, processInstanceId=946, startTime=1714843044689, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3341.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714843044737)
[WI-0][TI-3341] - [INFO] 2024-05-05 01:17:24.756 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3341, processInstanceId=946, startTime=1714843044689, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3341] - [INFO] 2024-05-05 01:17:24.765 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3341, processInstanceId=946, startTime=1714843044689, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714843044737)
[WI-0][TI-3341] - [INFO] 2024-05-05 01:17:25.615 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3341, success=true)
[WI-0][TI-3340] - [INFO] 2024-05-05 01:17:25.622 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3340, success=true)
[WI-0][TI-3341] - [INFO] 2024-05-05 01:17:25.627 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3341)
[WI-0][TI-3340] - [INFO] 2024-05-05 01:17:25.629 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3340)
[WI-0][TI-3341] - [INFO] 2024-05-05 01:17:25.635 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3341, success=true)
[WI-0][TI-3340] - [INFO] 2024-05-05 01:17:25.640 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3340, success=true)
[WI-0][TI-3341] - [INFO] 2024-05-05 01:17:25.643 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3341)
[WI-0][TI-3340] - [INFO] 2024-05-05 01:17:25.649 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3340)
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:25.710 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:25.725 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:26.025 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8155371161681406 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:27.037 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8914956011730205 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:28.045 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.72 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:28.767 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:28.768 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:29.069 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8348909657320872 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:29.768 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-e38ad73a-77f2-4591-8292-ddd9936dae5e;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:29.771 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-deb97593-6789-408b-8828-195bb9c062de;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 708ms :: artifacts dl 12ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-deb97593-6789-408b-8828-195bb9c062de
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/9ms)
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:30.082 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9243243243243244 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:30.769 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 1509ms :: artifacts dl 20ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-e38ad73a-77f2-4591-8292-ddd9936dae5e
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/17ms)
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:30.780 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:17:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:31.083 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9102990033222592 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:31.770 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:17:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:32.084 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9060150375939849 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:32.781 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:17:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:17:32 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:17:32 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:17:32 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:17:32 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:33.093 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9065227665094566 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:34.102 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8068181818181819 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:34.774 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:17:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:17:33 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:17:33 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:17:33 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:17:33 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
	24/05/05 01:17:33 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:35.116 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8128654970760234 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:36.118 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7790368271954674 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:37.133 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8314606741573033 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:38.133 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8034188034188035 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:38.791 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3340/py_945_3340.py", line 51, in <module>
	    parsed_topics = topics.split("|")
	                    ^^^^^^^^^^^^
	AttributeError: 'tuple' object has no attribute 'split'
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:39.135 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7743732590529248 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:39.780 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3341/py_946_3341.py", line 51, in <module>
	    parsed_topics = topics.split("|")
	                    ^^^^^^^^^^^^
	AttributeError: 'tuple' object has no attribute 'split'
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:39.795 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3340, processId:23227 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:39.797 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:39.797 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:39.797 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:39.797 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:39.801 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:39.801 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:39.801 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3340
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:39.801 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3340
[WI-945][TI-3340] - [INFO] 2024-05-05 01:17:39.802 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3340] - [INFO] 2024-05-05 01:17:40.724 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3340, success=true)
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:40.783 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3341, processId:23230 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:40.786 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:40.786 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:40.786 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:40.787 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:40.789 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:40.789 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:40.790 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3341
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:40.790 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3341
[WI-946][TI-3341] - [INFO] 2024-05-05 01:17:40.791 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3341] - [INFO] 2024-05-05 01:17:40.791 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3341, processInstanceId=946, status=6, startTime=1714843044689, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3341.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3341, endTime=1714843060787, processId=23230, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3341] - [INFO] 2024-05-05 01:17:40.794 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3341, processInstanceId=946, status=6, startTime=1714843044689, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3341.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3341, endTime=1714843060787, processId=23230, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843060791)
[WI-0][TI-3341] - [INFO] 2024-05-05 01:17:41.724 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3341, success=true)
[WI-0][TI-3341] - [INFO] 2024-05-05 01:17:41.726 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3341, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:46.163 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3343, taskName=sentiment analysis, firstSubmitTime=1714843066155, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=946, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3343'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505011746'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='946'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.167 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:46.173 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3344, taskName=sentiment analysis, firstSubmitTime=1714843066160, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=945, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3344'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505011746'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='945'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.174 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.167 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.175 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.175 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.175 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.176 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.183 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714843066183
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.183 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 946_3343
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.183 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.184 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.185 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.185 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714843066185
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.185 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 945_3344
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.185 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3343,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714843066155,
  "startTime" : 1714843066183,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3343.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 946,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\",\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3343"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505011746"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "946"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "946_3343",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.186 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3344,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714843066160,
  "startTime" : 1714843066185,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3344.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 945,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\",\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3344"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505011746"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "945"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "945_3344",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.188 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.189 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.189 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.189 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.189 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.189 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.193 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.194 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.194 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.194 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.195 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3343 check successfully
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.195 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.196 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3344 check successfully
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.196 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.196 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.196 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.197 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.197 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.197 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.197 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.197 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.197 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.197 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.197 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.197 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.197 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.197 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.198 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3344
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.198 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3344/py_945_3344.py
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.198 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.199 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.199 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.199 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3344/py_945_3344.py
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.200 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.200 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3344/945_3344.sh
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.197 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.201 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.201 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.202 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.202 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.203 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.203 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.207 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3343
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.207 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3343/py_946_3343.py
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.208 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.210 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.210 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.210 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3343/py_946_3343.py
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.210 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.211 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3343/946_3343.sh
[WI-945][TI-3344] - [INFO] 2024-05-05 01:17:46.212 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 23570
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:46.214 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-946][TI-3343] - [INFO] 2024-05-05 01:17:46.224 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 23574
[WI-0][TI-3343] - [INFO] 2024-05-05 01:17:46.736 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3343, success=true)
[WI-0][TI-3344] - [INFO] 2024-05-05 01:17:46.742 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3344, success=true)
[WI-0][TI-3343] - [INFO] 2024-05-05 01:17:46.750 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3343)
[WI-0][TI-3344] - [INFO] 2024-05-05 01:17:46.757 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3344)
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:47.156 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7300275482093663 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:47.221 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:47.228 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:48.172 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8066666666666666 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:50.305 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-b9287916-0c11-4dad-8d0a-94cd9072a6ad;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:50.318 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-1c044f9a-b514-4bbd-bcdc-9516042042f3;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:50.334 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8869047619047619 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:51.309 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:51.321 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:51.355 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9621212121212122 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:52.324 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 1239ms :: artifacts dl 53ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-b9287916-0c11-4dad-8d0a-94cd9072a6ad
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/36ms)
	24/05/05 01:17:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:52.324 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: resolution report :: resolve 1445ms :: artifacts dl 38ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-1c044f9a-b514-4bbd-bcdc-9516042042f3
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/11ms)
	24/05/05 01:17:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:52.356 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8576049657945927 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:53.329 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:53.332 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:53.357 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7730496453900708 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:54.359 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8914956011730205 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:55.338 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:17:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:17:54 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:17:54 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:17:54 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:17:54 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:55.365 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9325513196480939 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:56.332 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:17:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:17:55 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:17:55 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:17:55 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:17:55 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
	24/05/05 01:17:55 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:56.371 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7891737891737891 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:57.379 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7869318181818182 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:58.384 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8167701863354037 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:17:59.386 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8685714285714285 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:00.392 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.809384164222874 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:01.375 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3343/py_946_3343.py", line 51, in <module>
	    parsed_topics = topics.split("|")
	                    ^^^^^^^^^^^^
	AttributeError: 'tuple' object has no attribute 'split'
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:01.375 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3344/py_945_3344.py", line 51, in <module>
	    parsed_topics = topics.split("|")
	                    ^^^^^^^^^^^^
	AttributeError: 'tuple' object has no attribute 'split'
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:01.404 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7108108108108109 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-946][TI-3343] - [INFO] 2024-05-05 01:18:02.379 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3343, processId:23574 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-946][TI-3343] - [INFO] 2024-05-05 01:18:02.382 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3343] - [INFO] 2024-05-05 01:18:02.383 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-946][TI-3343] - [INFO] 2024-05-05 01:18:02.386 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3344] - [INFO] 2024-05-05 01:18:02.384 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3344, processId:23570 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-946][TI-3343] - [INFO] 2024-05-05 01:18:02.387 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-945][TI-3344] - [INFO] 2024-05-05 01:18:02.387 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3344] - [INFO] 2024-05-05 01:18:02.388 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-945][TI-3344] - [INFO] 2024-05-05 01:18:02.391 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3344] - [INFO] 2024-05-05 01:18:02.392 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-946][TI-3343] - [INFO] 2024-05-05 01:18:02.392 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-946][TI-3343] - [INFO] 2024-05-05 01:18:02.393 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-946][TI-3343] - [INFO] 2024-05-05 01:18:02.396 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3343
[WI-945][TI-3344] - [INFO] 2024-05-05 01:18:02.397 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-945][TI-3344] - [INFO] 2024-05-05 01:18:02.397 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-945][TI-3344] - [INFO] 2024-05-05 01:18:02.397 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3344
[WI-945][TI-3344] - [INFO] 2024-05-05 01:18:02.398 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3344
[WI-946][TI-3343] - [INFO] 2024-05-05 01:18:02.398 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3343
[WI-946][TI-3343] - [INFO] 2024-05-05 01:18:02.399 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-945][TI-3344] - [INFO] 2024-05-05 01:18:02.398 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3343] - [INFO] 2024-05-05 01:18:02.840 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3343, success=true)
[WI-0][TI-3344] - [INFO] 2024-05-05 01:18:02.840 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3344, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:04.930 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3350, taskName=sentiment analysis, firstSubmitTime=1714843084917, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=946, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3350'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505011804'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='946'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:04.931 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:04.932 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:04.935 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3349, taskName=sentiment analysis, firstSubmitTime=1714843084917, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=945, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3349'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505011804'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='945'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:04.937 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:04.938 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:04.938 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:04.938 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:04.944 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:04.945 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:04.947 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:04.947 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714843084947
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:04.941 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:04.948 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 945_3349
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:04.948 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714843084948
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:04.954 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 946_3350
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:04.960 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3350,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714843084917,
  "startTime" : 1714843084948,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3350.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 946,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\",\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3350"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505011804"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "946"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "946_3350",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:04.969 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:04.960 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3349,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714843084917,
  "startTime" : 1714843084947,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3349.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 945,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\",\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3349"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505011804"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "945"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "945_3349",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:04.970 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:04.972 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:04.971 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:04.973 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:04.973 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:04.982 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:04.982 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:04.985 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:04.987 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3350 check successfully
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:04.987 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:04.987 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:04.988 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:04.988 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:04.988 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:04.991 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:04.991 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:04.990 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:04.992 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3349 check successfully
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:04.993 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:04.992 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:04.994 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:04.995 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:04.996 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:04.996 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:04.997 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:04.995 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:04.997 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:04.997 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:04.998 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:04.998 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:04.999 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:04.999 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:04.999 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:05.000 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3350
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:05.000 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3350/py_946_3350.py
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:05.000 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:05.001 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:05.002 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:05.002 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3350/py_946_3350.py
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:05.002 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:05.002 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3350/946_3350.sh
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:05.000 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3349
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:05.008 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3349/py_945_3349.py
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:05.009 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:05.011 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:05.011 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:05.011 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3349/py_945_3349.py
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:05.012 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:05.012 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3349/945_3349.sh
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:05.016 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 23906
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:05.025 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 23909
[WI-0][TI-3350] - [INFO] 2024-05-05 01:18:05.841 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3350, success=true)
[WI-0][TI-3349] - [INFO] 2024-05-05 01:18:05.843 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3349, success=true)
[WI-0][TI-3350] - [INFO] 2024-05-05 01:18:05.848 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3350)
[WI-0][TI-3349] - [INFO] 2024-05-05 01:18:05.852 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3349)
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:06.016 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:06.031 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:06.469 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7107692307692308 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:07.485 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8908554572271387 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:08.488 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8323699421965318 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:09.489 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8027777777777778 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:10.028 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-b304305a-09ee-44dc-9519-760b30e9c7ae;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:10.042 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-769c4909-3ba6-4b7b-9c5c-bc0ad3509db7;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:10.491 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9365079365079365 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:11.035 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 1335ms :: artifacts dl 12ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-b304305a-09ee-44dc-9519-760b30e9c7ae
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/19ms)
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:11.061 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 1153ms :: artifacts dl 43ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-769c4909-3ba6-4b7b-9c5c-bc0ad3509db7
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/9ms)
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:11.498 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8676470588235293 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:12.036 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:18:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:12.069 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:18:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:12.506 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9166666666666667 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3258] - [INFO] 2024-05-05 01:18:12.869 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3258, processInstanceId=938, status=9, startTime=1714841447434, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, endTime=1714842191466, processId=18118, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842792756)
[WI-0][TI-3258] - [INFO] 2024-05-05 01:18:12.872 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3258, processInstanceId=938, status=9, startTime=1714841447434, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, endTime=1714842191466, processId=18118, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843092869)
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:13.038 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:13.540 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.908761110103392 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:14.546 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8407447763546528 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:15.046 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:18:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:18:14 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:18:14 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:18:14 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:18:14 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
	24/05/05 01:18:14 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:15.077 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:18:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:18:14 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:18:14 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:18:14 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:18:14 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:15.559 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7604456824512535 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:16.602 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8319783197831978 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:17.652 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8186968838526912 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:18.664 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7010164251714903 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:20.056 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3350/py_946_3350.py", line 51, in <module>
	    parsed_topics = topics.split("|")
	                    ^^^^^^^^^^^^
	AttributeError: 'tuple' object has no attribute 'split'
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:20.057 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3350, processId:23906 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:20.059 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:20.060 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:20.060 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:20.061 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:20.063 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:20.064 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:20.064 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3350
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:20.064 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3350
[WI-946][TI-3350] - [INFO] 2024-05-05 01:18:20.065 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-05-05 01:18:20.097 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3349/py_945_3349.py", line 51, in <module>
	    parsed_topics = topics.split("|")
	                    ^^^^^^^^^^^^
	AttributeError: 'tuple' object has no attribute 'split'
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:20.098 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3349, processId:23909 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:20.099 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:20.099 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:20.100 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:20.100 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:20.102 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:20.103 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:20.103 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3349
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:20.104 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3349
[WI-945][TI-3349] - [INFO] 2024-05-05 01:18:20.104 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3349] - [INFO] 2024-05-05 01:18:20.889 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3349, success=true)
[WI-0][TI-3350] - [INFO] 2024-05-05 01:18:20.890 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3350, success=true)
[WI-0][TI-3261] - [INFO] 2024-05-05 01:18:45.352 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3261, processInstanceId=941, status=9, startTime=1714842212366, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, endTime=1714842223865, processId=18773, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842825043)
[WI-0][TI-3261] - [INFO] 2024-05-05 01:18:45.355 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3261, processInstanceId=941, status=9, startTime=1714842212366, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, endTime=1714842223865, processId=18773, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843125352)
[WI-0][TI-3260] - [INFO] 2024-05-05 01:18:48.356 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3260, processInstanceId=940, status=9, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, endTime=1714842227024, processId=18772, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842828081)
[WI-0][TI-3260] - [INFO] 2024-05-05 01:18:48.359 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3260, processInstanceId=940, status=9, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, endTime=1714842227024, processId=18772, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843128356)
[WI-0][TI-3259] - [INFO] 2024-05-05 01:18:51.364 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3259, processInstanceId=939, status=9, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, endTime=1714842229451, processId=18762, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842831098)
[WI-0][TI-3259] - [INFO] 2024-05-05 01:18:51.369 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3259, processInstanceId=939, status=9, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, endTime=1714842229451, processId=18762, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843131364)
[WI-0][TI-3191] - [INFO] 2024-05-05 01:18:56.373 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3191, processInstanceId=936, status=6, startTime=1714839942174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13377949373536/25/936/3191.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/936/3191, endTime=1714840005766, processId=14716, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842836153)
[WI-0][TI-3191] - [INFO] 2024-05-05 01:18:56.377 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3191, processInstanceId=936, status=6, startTime=1714839942174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13377949373536/25/936/3191.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/936/3191, endTime=1714840005766, processId=14716, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843136373)
[WI-0][TI-0] - [INFO] 2024-05-05 01:19:05.831 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8228318680125909 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:19:14.903 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.810344827586207 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:19:15.924 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9188405797101449 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:19:16.926 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9577464788732395 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:19:17.930 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8201634877384196 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:19:23.972 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7028571428571428 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:19:25.022 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7240437158469946 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:19:30.060 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8681318681318682 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:19:33.104 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7915492957746479 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:19:34.184 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7142857142857143 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:19:37.201 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8089887640449438 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3262] - [INFO] 2024-05-05 01:19:57.490 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3262, processInstanceId=942, status=9, startTime=1714842212466, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262, endTime=1714842296579, processId=18774, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842897478)
[WI-0][TI-3262] - [INFO] 2024-05-05 01:19:57.498 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3262, processInstanceId=942, status=9, startTime=1714842212466, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262, endTime=1714842296579, processId=18774, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843197490)
[WI-0][TI-0] - [INFO] 2024-05-05 01:20:07.434 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7619047619047619 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:20:08.497 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8027027027027027 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:20:11.522 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7138810198300283 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:20:12.540 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7118155619596542 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:20:22.583 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.855907780979827 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:20:29.662 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7150837988826816 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:20:32.705 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7852941176470588 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:20:33.733 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7465181058495822 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:20:34.735 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7605713376303016 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:20:35.736 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7385057471264368 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:20:36.737 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.821529745042493 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:20:37.742 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8825301204819278 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:20:38.744 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8924418604651163 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:20:43.770 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7305555555555556 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:20:48.872 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8282548476454293 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:20:49.881 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8951612903225807 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:20:50.882 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8073654390934843 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:20:51.904 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8854748603351955 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:20:52.909 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8392282958199356 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:20:54.922 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7824858757062146 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:20:55.991 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8804347826086957 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:01.007 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8431372549019608 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:02.104 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8036580864762894 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3263] - [INFO] 2024-05-05 01:21:06.632 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3263, processInstanceId=943, status=9, startTime=1714842505551, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/943/3263.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263, endTime=1714842665300, processId=20088, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714842966304)
[WI-0][TI-3263] - [INFO] 2024-05-05 01:21:06.635 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3263, processInstanceId=943, status=9, startTime=1714842505551, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/943/3263.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263, endTime=1714842665300, processId=20088, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843266632)
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:10.136 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8060941828254847 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:11.215 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8780487804878049 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:12.222 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7849162011173184 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:18.293 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8945868945868947 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:19.347 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8346883468834688 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:20.354 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8428571428571427 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:21.356 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8986666666666667 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:22.358 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7026239067055393 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:23.365 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8764367816091955 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:36.399 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7441176470588236 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:37.423 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7694444444444445 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:38.432 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8914956011730205 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:39.433 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8567073170731707 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:41.455 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7755681818181818 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:44.006 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3352, taskName=sentiment analysis, firstSubmitTime=1714843303994, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=945, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3352'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505012143'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='945'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.007 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.007 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.022 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.023 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.023 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.023 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714843304023
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:44.023 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3353, taskName=sentiment analysis, firstSubmitTime=1714843303993, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=946, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3353'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505012144'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='946'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.023 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 945_3352
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.024 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.024 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3352,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714843303994,
  "startTime" : 1714843304023,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3352.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 945,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\",\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3352"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505012143"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "945"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "945_3352",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.024 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.036 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.030 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.037 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.037 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.038 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.038 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.039 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714843304039
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.040 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 946_3353
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.040 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3353,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714843303993,
  "startTime" : 1714843304039,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/946/3353.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 946,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\",\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3353"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505012144"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "946"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "946_3353",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.048 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.049 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.049 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.047 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.055 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.056 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.057 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3353 check successfully
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.057 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.058 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.058 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3352 check successfully
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.059 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.059 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.060 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.060 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.061 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.060 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.063 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.065 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.065 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.067 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.064 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.070 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.070 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.070 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.071 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.071 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.070 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.071 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.072 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.073 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.073 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.075 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3353
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.075 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3352
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.077 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3353/py_946_3353.py
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.078 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.077 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3352/py_945_3352.py
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.081 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.082 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.084 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.084 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.084 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.084 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3353/py_946_3353.py
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.084 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.084 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3353/946_3353.sh
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.084 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3352/py_945_3352.py
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.086 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.087 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3352/945_3352.sh
[WI-946][TI-3353] - [INFO] 2024-05-05 01:21:44.106 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 24283
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:44.126 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:44.110 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-945][TI-3352] - [INFO] 2024-05-05 01:21:44.126 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 24284
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:44.490 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7569230769230769 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3352] - [INFO] 2024-05-05 01:21:44.713 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3352, processInstanceId=945, startTime=1714843304023, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3352.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3352] - [INFO] 2024-05-05 01:21:44.731 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3352, processInstanceId=945, startTime=1714843304023, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3352.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714843304707)
[WI-0][TI-3352] - [INFO] 2024-05-05 01:21:44.732 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3352, processInstanceId=945, startTime=1714843304023, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3352] - [INFO] 2024-05-05 01:21:44.732 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3352, success=true)
[WI-0][TI-3353] - [INFO] 2024-05-05 01:21:44.739 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3353, success=true)
[WI-0][TI-3352] - [INFO] 2024-05-05 01:21:44.787 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3352, processInstanceId=945, startTime=1714843304023, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714843304707)
[WI-0][TI-3352] - [WARN] 2024-05-05 01:21:44.788 +0800 o.a.d.s.w.m.MessageRetryRunner:[139] - Retry send message to master error
java.util.ConcurrentModificationException: null
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:911)
	at java.util.ArrayList$Itr.next(ArrayList.java:861)
	at org.apache.dolphinscheduler.server.worker.message.MessageRetryRunner.run(MessageRetryRunner.java:127)
[WI-0][TI-3353] - [INFO] 2024-05-05 01:21:44.788 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3353, processInstanceId=946, startTime=1714843304039, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3352] - [INFO] 2024-05-05 01:21:44.787 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3352)
[WI-0][TI-3353] - [INFO] 2024-05-05 01:21:44.810 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3353)
[WI-0][TI-3353] - [INFO] 2024-05-05 01:21:44.812 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3353, processInstanceId=946, startTime=1714843304039, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714843304707)
[WI-0][TI-3353] - [WARN] 2024-05-05 01:21:44.812 +0800 o.a.d.s.w.m.MessageRetryRunner:[139] - Retry send message to master error
java.util.ConcurrentModificationException: null
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:911)
	at java.util.ArrayList$Itr.next(ArrayList.java:861)
	at org.apache.dolphinscheduler.server.worker.message.MessageRetryRunner.run(MessageRetryRunner.java:127)
[WI-0][TI-3352] - [INFO] 2024-05-05 01:21:44.821 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3352, success=true)
[WI-0][TI-3353] - [INFO] 2024-05-05 01:21:44.828 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3353)
[WI-0][TI-3352] - [INFO] 2024-05-05 01:21:44.834 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3352)
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:45.149 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:45.152 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:45.527 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8783382789317506 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:46.536 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8746438746438746 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:47.537 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9745762711864407 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:48.538 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9479768786127168 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:49.542 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.935860058309038 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:50.549 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9556786703601108 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:51.551 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.880466472303207 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:52.158 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-87ce480a-cbbd-4ca6-b83c-c061fbfdcaa3;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:52.173 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-64d1944e-bc5a-4148-81e2-116cec2cb985;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:52.560 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9326145552560646 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:53.176 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:53.589 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9918256130790192 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:54.165 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:54.177 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:54.613 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9879518072289157 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:55.170 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:55.187 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:55.664 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.943502824858757 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:56.174 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 3330ms :: artifacts dl 20ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-87ce480a-cbbd-4ca6-b83c-c061fbfdcaa3
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/22ms)
	24/05/05 01:21:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:56.188 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: resolution report :: resolve 2951ms :: artifacts dl 455ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-64d1944e-bc5a-4148-81e2-116cec2cb985
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/53ms)
	24/05/05 01:21:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:56.677 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9276729559748428 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:57.181 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:57.190 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:57.688 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8953017094984715 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:58.691 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9745203909509857 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:21:59.704 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9630769230769232 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:00.706 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9772727272727273 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:01.711 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8978978978978979 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:02.200 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:22:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:22:01 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:22:01 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:22:01 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:22:01 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
	24/05/05 01:22:01 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:02.274 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:22:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:22:01 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:22:01 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:22:01 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:22:01 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:02.713 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8945868945868946 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:03.717 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9220430107526882 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:04.720 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8690807799442897 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:05.735 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8929577464788732 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:06.741 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9068493150684931 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:07.743 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9124738980755122 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:08.764 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9552994818007812 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:09.768 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8067796610169491 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:10.312 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3353/py_946_3353.py", line 51, in <module>
	    parsed_topics = topics.split("|")
	                    ^^^^^^^^^^^^
	AttributeError: 'tuple' object has no attribute 'split'
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:10.772 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8987730061349692 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:11.780 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9196675900277008 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:12.235 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3352/py_945_3352.py", line 51, in <module>
	    parsed_topics = topics.split("|")
	                    ^^^^^^^^^^^^
	AttributeError: 'tuple' object has no attribute 'split'
[WI-946][TI-3353] - [INFO] 2024-05-05 01:22:12.316 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3353, processId:24283 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-946][TI-3353] - [INFO] 2024-05-05 01:22:12.319 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3353] - [INFO] 2024-05-05 01:22:12.319 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-946][TI-3353] - [INFO] 2024-05-05 01:22:12.319 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-946][TI-3353] - [INFO] 2024-05-05 01:22:12.321 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-946][TI-3353] - [INFO] 2024-05-05 01:22:12.366 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-946][TI-3353] - [INFO] 2024-05-05 01:22:12.367 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-946][TI-3353] - [INFO] 2024-05-05 01:22:12.368 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3353
[WI-946][TI-3353] - [INFO] 2024-05-05 01:22:12.369 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/946/3353
[WI-946][TI-3353] - [INFO] 2024-05-05 01:22:12.369 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:12.792 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9019139689301904 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3353] - [INFO] 2024-05-05 01:22:12.814 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3353, success=true)
[WI-945][TI-3352] - [INFO] 2024-05-05 01:22:13.243 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3352, processId:24284 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-945][TI-3352] - [INFO] 2024-05-05 01:22:13.243 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3352] - [INFO] 2024-05-05 01:22:13.245 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-945][TI-3352] - [INFO] 2024-05-05 01:22:13.245 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3352] - [INFO] 2024-05-05 01:22:13.247 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-945][TI-3352] - [INFO] 2024-05-05 01:22:13.264 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-945][TI-3352] - [INFO] 2024-05-05 01:22:13.264 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-945][TI-3352] - [INFO] 2024-05-05 01:22:13.264 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3352
[WI-945][TI-3352] - [INFO] 2024-05-05 01:22:13.270 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3352
[WI-945][TI-3352] - [INFO] 2024-05-05 01:22:13.270 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:13.794 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9011780945043657 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3352] - [INFO] 2024-05-05 01:22:13.800 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3352, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:15.828 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8836565096952909 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:15.876 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3359, taskName=sentiment analysis, firstSubmitTime=1714843335835, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=945, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3359'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505012215'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='945'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:15.884 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:15.886 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:15.903 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:15.904 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:15.908 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:15.908 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714843335908
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:15.910 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 945_3359
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:15.912 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3359,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714843335835,
  "startTime" : 1714843335908,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/945/3359.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 945,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_filtered|reddit_post_sa\\\",\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_filtered|reddit_post_sa\",\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3359"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505012215"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "945"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "945_3359",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:15.990 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:15.991 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:15.993 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:16.012 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:16.018 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:16.019 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3359 check successfully
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:16.023 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:16.026 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:16.032 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:16.032 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:16.034 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:16.037 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:16.037 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:16.038 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:16.038 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:16.038 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:16.038 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:16.041 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3359
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:16.044 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3359/py_945_3359.py
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:16.044 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "reddit_post_filtered|reddit_post_sa","gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:16.045 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:16.045 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:16.045 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3359/py_945_3359.py
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:16.046 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:16.046 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3359/945_3359.sh
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:16.104 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 24635
[WI-0][TI-3359] - [INFO] 2024-05-05 01:22:16.831 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3359, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:16.856 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9291553133514986 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3359] - [INFO] 2024-05-05 01:22:16.864 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3359)
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:17.105 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:17.861 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8803418803418803 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:18.866 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9134328358208955 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:19.869 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9060697380637109 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:20.878 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9070422535211268 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:21.880 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.888268156424581 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:22.889 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9498607242339834 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:23.891 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8058823529411765 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:24.125 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:24.898 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.899736147757256 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:25.126 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-a22ddc1f-0772-4fa0-8f45-3fcae156196b;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:25.924 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9807692307692307 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:26.129 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 1756ms :: artifacts dl 173ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:26.948 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9194444444444445 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:27.136 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: retrieving :: org.apache.spark#spark-submit-parent-a22ddc1f-0772-4fa0-8f45-3fcae156196b
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/185ms)
	24/05/05 01:22:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:27.950 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7354651162790697 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:28.147 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:28.953 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8115942028985508 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:29.956 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7976190476190477 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:31.151 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:22:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:22:30 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:22:30 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:22:30 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:22:30 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:31.974 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7387640449438202 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:33.025 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7174592182209911 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:34.026 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8907103825136612 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:36.162 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3359/py_945_3359.py", line 51, in <module>
	    parsed_topics = topics.split("|")
	                    ^^^^^^^^^^^^
	AttributeError: 'tuple' object has no attribute 'split'
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:37.164 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3359, processId:24635 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:37.167 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:37.169 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:37.170 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:37.172 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:37.174 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:37.175 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:37.175 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3359
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:37.176 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/945/3359
[WI-945][TI-3359] - [INFO] 2024-05-05 01:22:37.177 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3359] - [INFO] 2024-05-05 01:22:37.844 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3359, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:53.072 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8328611898016998 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:55.259 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8775000000000001 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:22:56.321 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7377049180327869 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:00.414 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8812154696132597 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:02.471 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8068181818181819 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:03.491 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8295454545454546 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:05.523 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.824712643678161 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:06.549 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.847887323943662 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:07.550 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7891737891737892 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:08.564 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8701657458563536 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:09.572 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7476038338658147 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:10.574 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7964601769911505 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:11.576 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8466076696165191 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:12.577 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8309037900874635 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3258] - [INFO] 2024-05-05 01:23:13.156 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3258, processInstanceId=938, status=9, startTime=1714841447434, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, endTime=1714842191466, processId=18118, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843092869)
[WI-0][TI-3258] - [INFO] 2024-05-05 01:23:13.217 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3258, processInstanceId=938, status=9, startTime=1714841447434, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, endTime=1714842191466, processId=18118, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843393156)
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:21.620 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7191977077363897 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:22.639 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.825214899713467 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:23.641 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8488372093023256 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:26.659 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8619718309859155 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:27.733 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8457583547557841 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:29.753 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8012232415902141 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:32.812 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7478260869565218 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:36.629 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3361, taskName=sentiment analysis, firstSubmitTime=1714843416618, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=948, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=13, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3361'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505012336'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='948'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.630 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.631 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.658 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.660 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.665 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.666 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714843416666
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.671 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 948_3361
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:36.672 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3362, taskName=sentiment analysis, firstSubmitTime=1714843416645, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13386218435520, processDefineVersion=20, appIds=null, processInstanceId=948, scheduleTime=0, globalParams=[{"prop":"topics","direct":"IN","type":"VARCHAR","value":"\"gnews_filtered|gnews_sa\""}], executorId=1, cmdTypeIfComplement=13, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='sentiment analysis'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, topics=Property{prop='topics', direct=IN, type=VARCHAR, value='"gnews_filtered|gnews_sa"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3362'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13386027735104'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505012336'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='948'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='sentiment_analysis_subprocess'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13386218435520'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.675 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: sentiment analysis to wait queue success
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.676 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.683 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.686 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.686 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.686 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714843416686
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.686 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 948_3362
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.686 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3362,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714843416645,
  "startTime" : 1714843416686,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/948/3362.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 948,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 13,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3362"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505012336"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "948"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "948_3362",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.692 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.695 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.696 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.711 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.717 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.721 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/948/3362 check successfully
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.735 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.740 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.699 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3361,
  "taskName" : "sentiment analysis",
  "firstSubmitTime" : 1714843416618,
  "startTime" : 1714843416666,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13386218435520/20/948/3361.log",
  "processId" : 0,
  "processDefineCode" : 13386218435520,
  "processDefineVersion" : 20,
  "processInstanceId" : 948,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"gnews_filtered|gnews_sa\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 13,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"# use kafka streaming api\\nfrom pyspark.sql.functions import udf, col, from_json\\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\nfrom pyspark.sql import SparkSession\\nimport os\\n\\n# access the nltk data directory variable\\nnltk_data_dir = os.environ.get('NLTK_DATA')\\n\\n# download the model\\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\\n\\n# init the sentiment analysis object\\nsia = SIA()\\n\\n# init Spark session\\nspark = SparkSession.builder \\\\\\n    .master('local') \\\\\\n    .appName(\\\"Sentiment Analysis\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define a udf to perform sentiment analysis\\ndef analyze_sentiment(message):\\n    # Perform sentiment analysis\\n    pol_score = sia.polarity_scores(message)\\n    # Return the compound score\\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\\n\\n# Register the function as a UDF\\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"authors\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the input parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# perform sentiment analysis on the parsed dataframe \\nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\\n\\n# split the sentiment_score column into separate columns\\nsa_reddit_df = sa_reddit_df.withColumn(\\\"negative\\\", sa_reddit_df[\\\"sentiment_score\\\"][0]) \\\\\\n            .withColumn(\\\"neutral\\\", sa_reddit_df[\\\"sentiment_score\\\"][1]) \\\\\\n            .withColumn(\\\"positive\\\", sa_reddit_df[\\\"sentiment_score\\\"][2]) \\\\\\n            .withColumn(\\\"compound\\\", sa_reddit_df[\\\"sentiment_score\\\"][3]) \\\\\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\\n\\n# stream the data to kafka\\nkafka_write = sa_reddit_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport NLTK_DATA=/local_storage/nltk_data",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment analysis"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"gnews_filtered|gnews_sa\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3361"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386027735104"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505012336"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "948"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "sentiment_analysis_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13386218435520"
    }
  },
  "taskAppId" : "948_3361",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.740 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.743 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.748 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.746 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.749 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.753 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.755 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.756 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.757 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.757 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.757 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.757 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.758 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/948/3362
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.758 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/948/3362/py_948_3362.py
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.760 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.770 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.770 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.771 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/948/3362/py_948_3362.py
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.771 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.772 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.774 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/948/3362/948_3362.sh
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.778 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.782 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/948/3361 check successfully
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.783 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.802 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.805 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.807 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.808 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "# use kafka streaming api\nfrom pyspark.sql.functions import udf, col, from_json\nfrom pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom pyspark.sql import SparkSession\nimport os\n\n# access the nltk data directory variable\nnltk_data_dir = os.environ.get('NLTK_DATA')\n\n# download the model\nnltk.download('vader_lexicon', download_dir=nltk_data_dir)\n\n# init the sentiment analysis object\nsia = SIA()\n\n# init Spark session\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"Sentiment Analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define a udf to perform sentiment analysis\ndef analyze_sentiment(message):\n    # Perform sentiment analysis\n    pol_score = sia.polarity_scores(message)\n    # Return the compound score\n    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]\n\n# Register the function as a UDF\nanalyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"authors\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the input parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# perform sentiment analysis on the parsed dataframe \nsa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))\n\n# split the sentiment_score column into separate columns\nsa_reddit_df = sa_reddit_df.withColumn(\"negative\", sa_reddit_df[\"sentiment_score\"][0]) \\\n            .withColumn(\"neutral\", sa_reddit_df[\"sentiment_score\"][1]) \\\n            .withColumn(\"positive\", sa_reddit_df[\"sentiment_score\"][2]) \\\n            .withColumn(\"compound\", sa_reddit_df[\"sentiment_score\"][3]) \\\n            .drop('sentiment_score')  # drop sentiment_score column after splitting\n\n# stream the data to kafka\nkafka_write = sa_reddit_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()",
  "resourceList" : [ ]
}
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.812 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.815 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.821 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.826 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.829 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.831 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : # use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.854 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/948/3361
[WI-948][TI-3362] - [INFO] 2024-05-05 01:23:36.856 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 24814
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.862 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/948/3361/py_948_3361.py
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.871 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

# use kafka streaming api
from pyspark.sql.functions import udf, col, from_json
from pyspark.sql.types import FloatType, ArrayType, StringType, StructType, StructField, IntegerType
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
from pyspark.sql import SparkSession
import os

# access the nltk data directory variable
nltk_data_dir = os.environ.get('NLTK_DATA')

# download the model
nltk.download('vader_lexicon', download_dir=nltk_data_dir)

# init the sentiment analysis object
sia = SIA()

# init Spark session
spark = SparkSession.builder \
    .master('local') \
    .appName("Sentiment Analysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define a udf to perform sentiment analysis
def analyze_sentiment(message):
    # Perform sentiment analysis
    pol_score = sia.polarity_scores(message)
    # Return the compound score
    return [pol_score['neg'], pol_score['neu'], pol_score['pos'], pol_score['compound']]

# Register the function as a UDF
analyze_sentiment_udf = udf(lambda x:analyze_sentiment(x), ArrayType(FloatType()))

# define the schema for the JSON data
schema = StructType([
    StructField("authors", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the input parameter
topics = "gnews_filtered|gnews_sa"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# perform sentiment analysis on the parsed dataframe 
sa_reddit_df = parsed_df.withColumn('sentiment_score', analyze_sentiment_udf(parsed_df['content']))

# split the sentiment_score column into separate columns
sa_reddit_df = sa_reddit_df.withColumn("negative", sa_reddit_df["sentiment_score"][0]) \
            .withColumn("neutral", sa_reddit_df["sentiment_score"][1]) \
            .withColumn("positive", sa_reddit_df["sentiment_score"][2]) \
            .withColumn("compound", sa_reddit_df["sentiment_score"][3]) \
            .drop('sentiment_score')  # drop sentiment_score column after splitting

# stream the data to kafka
kafka_write = sa_reddit_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.882 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.883 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.885 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export NLTK_DATA=/local_storage/nltk_data
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/948/3361/py_948_3361.py
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.886 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.890 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/948/3361/948_3361.sh
[WI-948][TI-3361] - [INFO] 2024-05-05 01:23:36.954 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 24820
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:36.955 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-3362] - [INFO] 2024-05-05 01:23:36.986 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3362, success=true)
[WI-0][TI-3361] - [INFO] 2024-05-05 01:23:36.994 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3361, success=true)
[WI-0][TI-3362] - [INFO] 2024-05-05 01:23:37.014 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3362)
[WI-0][TI-3361] - [INFO] 2024-05-05 01:23:37.018 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3361)
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:37.855 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.969187675070028 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:37.863 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:38.900 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:38.903 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8694362017804154 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:38.974 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[nltk_data] Downloading package vader_lexicon to
	[nltk_data]     /local_storage/nltk_data...
	[nltk_data]   Package vader_lexicon is already up-to-date!
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:39.905 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9166666666666666 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:40.913 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9009287925696595 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:41.917 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9206798866855525 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:42.922 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8881789137380192 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:43.932 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8163934426229509 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:44.970 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8584070796460177 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:45.016 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-ac6f0b2f-d78a-4b60-b511-a2e19471448a;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:45.969 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-614d8496-e7e8-44a5-992b-1527a8cba3eb;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:45.974 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.956656346749226 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:46.024 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[WI-0][TI-3261] - [INFO] 2024-05-05 01:23:46.282 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3261, processInstanceId=941, status=9, startTime=1714842212366, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, endTime=1714842223865, processId=18773, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843125352)
[WI-0][TI-3261] - [INFO] 2024-05-05 01:23:46.287 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3261, processInstanceId=941, status=9, startTime=1714842212366, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, endTime=1714842223865, processId=18773, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843426282)
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:46.985 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.994475138121547 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:47.029 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 1952ms :: artifacts dl 66ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-ac6f0b2f-d78a-4b60-b511-a2e19471448a
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/54ms)
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:47.990 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in spark-list
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:47.992 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9694444444444444 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:49.002 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:49.048 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:23:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:49.107 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9948320413436693 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3260] - [INFO] 2024-05-05 01:23:49.293 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3260, processInstanceId=940, status=9, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, endTime=1714842227024, processId=18772, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843128356)
[WI-0][TI-3260] - [INFO] 2024-05-05 01:23:49.298 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3260, processInstanceId=940, status=9, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, endTime=1714842227024, processId=18772, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843429293)
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:50.004 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 3872ms :: artifacts dl 244ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from spark-list in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:50.126 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9811320754716982 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:51.014 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: retrieving :: org.apache.spark#spark-submit-parent-614d8496-e7e8-44a5-992b-1527a8cba3eb
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/180ms)
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:51.080 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:51.132 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9076923076923077 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:52.020 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:23:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:52.138 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9109195402298851 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3259] - [INFO] 2024-05-05 01:23:52.309 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3259, processInstanceId=939, status=9, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, endTime=1714842229451, processId=18762, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843131364)
[WI-0][TI-3259] - [INFO] 2024-05-05 01:23:52.318 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3259, processInstanceId=939, status=9, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, endTime=1714842229451, processId=18762, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843432309)
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:53.044 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:53.144 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8737201365187713 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:54.155 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9438596491228071 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:55.103 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:23:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:23:54 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:23:54 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:23:54 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:23:54 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:55.163 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8985915492957747 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:56.053 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:23:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/05 01:23:55 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
	24/05/05 01:23:55 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
	24/05/05 01:23:55 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
	24/05/05 01:23:55 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
	24/05/05 01:23:55 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:56.171 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9653179190751445 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:57.174 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8463687150837989 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3191] - [INFO] 2024-05-05 01:23:57.363 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3191, processInstanceId=936, status=6, startTime=1714839942174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13377949373536/25/936/3191.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/936/3191, endTime=1714840005766, processId=14716, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843136373)
[WI-0][TI-3191] - [INFO] 2024-05-05 01:23:57.366 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3191, processInstanceId=936, status=6, startTime=1714839942174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13377949373536/25/936/3191.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/936/3191, endTime=1714840005766, processId=14716, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843437363)
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:58.176 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.884297520661157 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:23:59.218 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9513513513513514 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:00.255 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8731563421828908 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:01.274 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9675675675675677 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:02.276 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.90625 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:03.280 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8975069252077563 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:04.294 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8529411764705882 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:05.299 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8579881656804733 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:06.302 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.907563025210084 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:07.343 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.875 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:08.345 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.973404255319149 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:09.406 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9641873278236915 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:10.416 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9585798816568047 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:11.417 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9110584978509506 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:12.433 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.944 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:13.186 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:24:12 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:13.434 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9401197604790419 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:14.435 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8509316770186335 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:15.197 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:24:15 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:15.198 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:24:14 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:15.437 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8885869565217391 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:16.438 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8595505617977528 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:17.200 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:24:16 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:17.440 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8804780876494024 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:18.442 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8271604938271604 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:19.453 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8151815181518152 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:20.204 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:20.482 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8746438746438746 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:21.206 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:21.218 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:21.486 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7745664739884394 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:22.219 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:22.487 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7435897435897436 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:23.491 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7354150971120593 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:24.492 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7352359466827644 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:25.493 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.734598282502133 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:26.497 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7350862238489585 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:27.498 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7343026147015097 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:28.500 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7333171879228353 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:29.518 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7323156555006837 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:30.520 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7322806605222643 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:31.521 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7314285714285715 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:32.541 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7342197004628682 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:33.542 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7346941210225771 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:34.544 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7334480214216988 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:35.546 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7336560029411688 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:36.548 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7340685857833298 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:37.550 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7344443841311293 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:38.553 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7330555209251082 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:39.566 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7337194313395541 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:40.578 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7337319579511474 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:41.580 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7340823054055511 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:42.582 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7345205379762125 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:43.583 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.733813082673847 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:44.587 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7338208372429286 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:45.588 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7342340165904034 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:46.589 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7346217450444824 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:47.590 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7342789533240557 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:48.591 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7346889513098561 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:49.602 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7349889934827818 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:50.604 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7350009235890611 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:51.608 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7344531328757342 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:52.609 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7344531328757342 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:53.612 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7346732433365882 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:54.617 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7338234220992891 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:55.618 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7340224560390496 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:56.642 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7345481760557597 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3262] - [INFO] 2024-05-05 01:24:57.633 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3262, processInstanceId=942, status=9, startTime=1714842212466, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262, endTime=1714842296579, processId=18774, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843197490)
[WI-0][TI-3262] - [INFO] 2024-05-05 01:24:57.636 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3262, processInstanceId=942, status=9, startTime=1714842212466, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262, endTime=1714842296579, processId=18774, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843497633)
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:57.645 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7345481760557597 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:58.646 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7339107107102331 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:24:59.656 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7338860551572557 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:00.657 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7341031830915399 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:01.659 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7344511445246876 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:02.662 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7340934401714118 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:03.664 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7343791662168022 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:04.665 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7344338458705826 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:05.667 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7334740688204088 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:06.670 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.733605697659691 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:07.672 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7336138498989818 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:08.685 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7337820643975207 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:09.694 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7347326950328804 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:10.696 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7352337594966132 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:11.697 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7347217591021243 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:12.700 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7347211625968103 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:13.717 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7351580033217393 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:14.719 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.734450349184269 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:15.720 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7349124419674893 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:16.814 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7350247838016198 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:17.817 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.734371212812616 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:18.820 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7348072581971263 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:19.836 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7338160652004169 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:21.591 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7338126850036377 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:22.600 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7339558462789899 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:23.601 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7339548521034667 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:24.602 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.734818392962987 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:25.603 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7348529902711972 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:26.606 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7359925142559799 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:27.608 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7348307207394758 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:28.609 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7349237755684547 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:29.611 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7350486440141786 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:30.717 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7344312610142221 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:31.720 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7355986219136567 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:32.721 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7350126548602358 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:33.724 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7350114618496079 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:34.726 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7343968625411166 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:35.726 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7348730726167674 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:36.748 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7350983527903425 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:37.750 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7347907548834399 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:38.751 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7338751192264996 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:39.753 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7340236490496775 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:40.762 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7338144745195796 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:41.763 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7338142756844749 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:42.767 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7344022310889423 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:43.768 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7347893630377073 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:44.769 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7336965653025187 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:45.774 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7341654184792972 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:46.777 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7341777462557858 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:47.781 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7336719097495413 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:48.782 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7336707167389134 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:49.784 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7341489151656108 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:50.794 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7341489151656108 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:51.796 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7344620804554437 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:52.797 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7344716245404672 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:53.798 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7342133377395192 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:54.800 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7345511585823296 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:55.801 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7345052276731541 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:56.802 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7345050288380494 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:57.804 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7349418695629784 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:58.805 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7337635727327877 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:25:59.806 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7346026568744354 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:00.814 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7346022592042262 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:02.589 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7350892063755283 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:03.596 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7348734702869767 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:04.613 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7348718796061394 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:05.622 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7341051714425865 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:06.623 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7341051714425865 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3263] - [INFO] 2024-05-05 01:26:06.647 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3263, processInstanceId=943, status=9, startTime=1714842505551, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/943/3263.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263, endTime=1714842665300, processId=20088, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843266632)
[WI-0][TI-3263] - [INFO] 2024-05-05 01:26:06.652 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3263, processInstanceId=943, status=9, startTime=1714842505551, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/943/3263.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263, endTime=1714842665300, processId=20088, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843566647)
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:07.628 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7341308211710871 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:08.629 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7345159647688054 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:09.630 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7350295558441315 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:10.631 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7350444684769808 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:11.639 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7354578466595603 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:12.640 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7346909396609026 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:13.642 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7350275674930851 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:14.643 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7351273827156223 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:15.650 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7354858824093168 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:16.652 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7344380214077804 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:17.653 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7347613272879508 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:18.658 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.734188085681228 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:19.660 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7341878868461233 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:20.661 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7348014919790913 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:21.672 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.733708495408798 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:22.675 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7347690818570324 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:23.677 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7589041095890411 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:24.679 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7353242294692316 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:25.680 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7347655028251486 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:26.682 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7620396600566572 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:27.684 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7013698630136986 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:28.686 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8950276243093922 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:29.689 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8382749326145553 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:30.693 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.876056338028169 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:31.715 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8674033149171271 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:32.724 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7377438802212727 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:33.725 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8953488372093024 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:34.729 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.826923076923077 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:35.748 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7913279132791328 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:36.750 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7347907548834399 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:37.751 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7348154104364172 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:38.753 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7346790095546233 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:39.754 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.73518067052367 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:40.757 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7356051834721103 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:41.787 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7358732131931864 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:42.789 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9480874316939891 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:43.705 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 1:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:43.706 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 1:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:43.795 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9028213166144201 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:44.853 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7406347962382446 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:45.901 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9304723442654478 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:46.788 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:26:46 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:46.789 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:26:45 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:46.902 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8379549808429119 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:47.905 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8142857142857143 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:48.795 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:48.906 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9033149171270719 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:49.797 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:49.907 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7476667197556396 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:51.059 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7475160027463105 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:52.554 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7466944160938724 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:53.555 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7458714375957018 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:54.556 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7458734259467483 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:55.561 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7469358019109247 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:56.562 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.74693560307582 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:57.563 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.747427918794948 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:58.565 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7460517810356246 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:26:59.568 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7466526607218947 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:00.570 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7468149101672938 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:01.571 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7471781819035002 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:02.581 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7464743056330184 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:03.583 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7471744040365117 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:04.584 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7471746028716163 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:05.585 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7474263281141107 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:06.586 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7469767619424837 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:07.623 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7472390254455249 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:08.624 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7473871575984935 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:09.627 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7472738215888396 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:10.629 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7465486699621597 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:11.631 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7460130081902168 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:12.644 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.746862232922202 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:13.648 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7471751993769303 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:14.649 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7475094411878568 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:15.654 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7690058479532164 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:16.657 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7392550143266476 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:17.662 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8836565096952909 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:18.670 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8526011560693643 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:19.671 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9202279202279202 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:20.682 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9460227272727273 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:21.689 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9333333333333335 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:22.706 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7472222222222222 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:23.711 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8911764705882352 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:24.718 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8368580060422961 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:25.740 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8491620111731844 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:26.748 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9027855153203341 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:27.753 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8792134831460673 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:28.754 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8847262247838616 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:29.755 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7560927549856411 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:30.757 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7566268260767468 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:31.759 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.755874434040729 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:32.784 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.753124743378443 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:33.785 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7521969787801188 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:34.787 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7522091077215028 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:35.788 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7506478544797449 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:36.791 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7513791699946692 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:37.793 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7643835616438356 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:38.797 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7504281416891002 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:39.799 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7500282842936373 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:40.803 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7508667719299711 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:41.805 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7508948076797275 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:42.835 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7501968964623855 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:43.842 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7501955046166529 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:44.847 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7505086699064859 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:45.852 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7494639902666239 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:46.857 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7492345345558511 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:47.859 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7495202606012416 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:48.861 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7622950819672132 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:49.863 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.770718232044199 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:50.866 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7489925522334849 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:51.870 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7490927651262315 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:52.888 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7500664606337312 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:53.903 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7469248659801686 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:54.904 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7437413171195235 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:55.905 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7441531046212658 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:56.907 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7444787965226921 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:57.909 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7438224418422231 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:58.913 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7440216746170883 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:27:59.916 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7437258079813605 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:00.918 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.744051301047682 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:01.920 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7441759706583013 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:02.935 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7398373983739838 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:03.937 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8817204301075269 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:04.944 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.725 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:05.945 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7433160088306646 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:06.947 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7443056111465369 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:07.951 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7444923173098088 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:08.953 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7439681879739359 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:09.958 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7443189330985488 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:10.961 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7438628053684683 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:11.962 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7438876597565502 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:12.981 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7154929577464788 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3258] - [INFO] 2024-05-05 01:28:13.740 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3258, processInstanceId=938, status=9, startTime=1714841447434, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, endTime=1714842191466, processId=18118, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843393156)
[WI-0][TI-3258] - [INFO] 2024-05-05 01:28:13.747 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3258, processInstanceId=938, status=9, startTime=1714841447434, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, endTime=1714842191466, processId=18118, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843693740)
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:13.994 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7450160489754722 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:14.995 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7451285896447075 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:16.000 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7444515561133543 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:17.003 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7447770491796759 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:18.006 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7440990214727995 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:19.008 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7114612511671334 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:20.011 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7450293709274842 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:21.016 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7452912367603161 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:22.017 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7851239669421487 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:23.035 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7449854283693553 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:24.036 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7440642253294847 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:25.038 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7446028696279974 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:26.040 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7447750608286294 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:27.042 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7444503631027264 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:28.044 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7444491700920984 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:29.049 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7443443839919448 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:30.058 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7443439863217355 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:31.063 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7817679558011049 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:32.067 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7442908973487924 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:33.082 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7436904153327316 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:34.085 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7036011080332409 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:35.086 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7432102285549878 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:36.088 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7439860831333549 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:37.091 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7447245567120468 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:38.096 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.744585769808997 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:39.101 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7445424237561821 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:40.105 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9220055710306407 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:41.121 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7002801120448179 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:42.124 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7429463743711094 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:43.144 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7429684450677262 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:44.148 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7438055408583274 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:45.150 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7166666666666667 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:46.155 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7440849041803689 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3261] - [INFO] 2024-05-05 01:28:46.800 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3261, processInstanceId=941, status=9, startTime=1714842212366, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, endTime=1714842223865, processId=18773, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843426282)
[WI-0][TI-3261] - [INFO] 2024-05-05 01:28:46.811 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3261, processInstanceId=941, status=9, startTime=1714842212366, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, endTime=1714842223865, processId=18773, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843726800)
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:47.162 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7443211202847001 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:48.164 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.744365062842829 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:49.166 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7439093327829578 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3260] - [INFO] 2024-05-05 01:28:49.817 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3260, processInstanceId=940, status=9, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, endTime=1714842227024, processId=18772, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843429293)
[WI-0][TI-3260] - [INFO] 2024-05-05 01:28:49.821 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3260, processInstanceId=940, status=9, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, endTime=1714842227024, processId=18772, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843729817)
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:50.169 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7443213191198047 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:51.173 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7442543116895357 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:52.174 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7445925302025553 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3259] - [INFO] 2024-05-05 01:28:52.825 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3259, processInstanceId=939, status=9, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, endTime=1714842229451, processId=18762, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843432309)
[WI-0][TI-3259] - [INFO] 2024-05-05 01:28:52.835 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3259, processInstanceId=939, status=9, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, endTime=1714842229451, processId=18762, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843732825)
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:53.194 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7433523956548167 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:54.197 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.807909604519774 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:55.199 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8764705882352941 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:56.201 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8693181818181819 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:57.204 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8045977011494253 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3191] - [INFO] 2024-05-05 01:28:57.857 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3191, processInstanceId=936, status=6, startTime=1714839942174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13377949373536/25/936/3191.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/936/3191, endTime=1714840005766, processId=14716, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843437363)
[WI-0][TI-3191] - [INFO] 2024-05-05 01:28:57.897 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3191, processInstanceId=936, status=6, startTime=1714839942174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13377949373536/25/936/3191.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/936/3191, endTime=1714840005766, processId=14716, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843737857)
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:58.205 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8034188034188033 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:28:59.210 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.861271676300578 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:00.214 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7285714285714286 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:01.218 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7578347578347578 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:02.220 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.745132367511696 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:03.265 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7441234781906721 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:04.267 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7442145446686045 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:05.275 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7444197424966094 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:06.312 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7445432190966007 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:07.316 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7492625368731564 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:08.318 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8052325581395348 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:09.327 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7507002801120448 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:10.334 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7444444444444445 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:11.339 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7451116886608118 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:12.344 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7450222128637166 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:13.366 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7658402203856749 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:14.370 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7449210057954468 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:15.372 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7060518731988473 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:16.375 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7988668555240793 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:17.378 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.744690357074046 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:18.381 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7486187845303868 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:19.383 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.780281690140845 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:20.399 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7451683566656387 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:21.400 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7447110359249302 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:22.402 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7449991479915765 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:23.433 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7762803234501348 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:24.441 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7448132371687233 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:25.442 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7453135062920375 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:26.450 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7456739943367785 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:27.452 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7409470752089137 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:28.457 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8535911602209945 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:29.462 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7457364285596405 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:30.465 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7446153962395907 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:31.471 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7448784550830504 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:32.477 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8136986301369863 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:33.530 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7451210339107306 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:34.536 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7151898734177216 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:35.540 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7449961654650067 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:36.558 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7450086920766 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:37.561 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7122093023255813 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:38.574 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7437236207952092 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:39.576 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7439319999848886 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:40.580 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7439319999848886 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:41.671 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7443074006624788 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:42.691 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7206703910614525 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:43.710 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.744007954994867 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:44.712 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7445336750115772 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:45.714 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.744452351453773 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:46.715 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7443579047790614 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:47.718 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7447191881642211 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:48.721 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7449325382315168 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:49.722 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7365359069157024 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:50.726 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7442771777265711 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:51.727 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7447028836856393 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:52.729 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7448243719345841 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:53.747 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7451124840012304 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:54.757 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7451242152724051 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:55.759 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7444682582621454 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:56.760 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7441411745149865 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:57.762 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7443972741297833 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3262] - [INFO] 2024-05-05 01:29:58.033 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3262, processInstanceId=942, status=9, startTime=1714842212466, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262, endTime=1714842296579, processId=18774, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843497633)
[WI-0][TI-3262] - [INFO] 2024-05-05 01:29:58.040 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3262, processInstanceId=942, status=9, startTime=1714842212466, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262, endTime=1714842296579, processId=18774, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843798033)
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:58.765 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7443970752946786 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:29:59.769 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7447597505255709 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:00.770 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7043478260869566 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:01.772 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7445259204424955 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:02.774 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7448516123439219 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:03.808 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7442372118705353 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:04.810 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7446881698878948 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:05.811 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7436846491146967 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:06.817 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7437804876351408 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:07.821 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7437804876351408 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:08.822 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7430209375353554 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:09.824 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7426942514584057 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:10.828 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7408450704225351 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:11.829 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7428563020687003 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:12.831 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.744157081323359 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:13.856 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7442141469983952 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:14.860 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7431163783855902 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:15.863 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7427523113089652 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:16.870 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7427523113089652 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:17.873 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7419949483953311 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:18.876 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7432436328525699 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:19.881 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7070422535211267 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:20.885 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7158774373259053 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:21.889 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7422860429885473 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:22.890 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7423095055308967 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:23.915 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.814516129032258 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:24.916 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7433014938680248 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:25.919 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7435514295945772 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:26.926 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7436619718309858 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:27.929 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7436629760882891 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:28.934 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7441696079349522 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:29.935 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7443193307687581 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:30.937 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7431462036512886 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:31.939 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7435218031639835 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:32.941 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7436309636364395 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:33.959 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7430434059021814 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:34.963 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7438683727513986 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:35.965 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.743917882692458 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:36.967 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.743917882692458 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:37.970 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7437570250927914 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:38.973 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7439936388673318 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:39.974 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7438723494534918 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:40.976 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7438723494534918 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:42.682 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7445237332563444 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:43.708 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7434621526325867 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:44.735 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7436860409604292 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:45.739 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7436971757262899 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:46.740 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7441473384032309 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:47.742 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7430927170081362 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:48.745 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7424906443112382 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:49.747 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7431666836670681 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:50.750 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7440051713034019 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:51.752 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7435068905311343 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:52.754 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7428417871060604 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:53.754 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7440300256914839 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:54.766 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7440294291861699 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:55.768 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7438327812676653 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:56.773 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7442338316737561 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:57.776 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7440373825903561 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:58.779 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7440373825903561 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:30:59.783 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.744463287384529 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:00.786 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7447134219461861 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:01.789 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.743794207257362 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:02.792 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7433975312235735 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:03.794 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7436456774341841 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:04.806 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.743848489240933 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:05.810 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7002801120448179 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3263] - [INFO] 2024-05-05 01:31:06.760 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3263, processInstanceId=943, status=9, startTime=1714842505551, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/943/3263.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263, endTime=1714842665300, processId=20088, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843566647)
[WI-0][TI-3263] - [INFO] 2024-05-05 01:31:06.763 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3263, processInstanceId=943, status=9, startTime=1714842505551, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/943/3263.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263, endTime=1714842665300, processId=20088, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843866760)
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:06.816 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7437256091462557 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:07.817 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7440135223777974 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:08.819 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7427821365746636 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:09.820 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7430797927263335 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:10.822 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7426874910648474 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:11.823 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7436333496576955 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:12.824 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7439214617243418 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:13.826 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7429233094989693 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:14.849 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7428789692706311 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:15.853 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.742026165506762 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:16.854 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7427628495695121 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:17.856 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.743249995575919 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:18.858 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7436878304763711 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:19.876 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7437363462419072 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:20.884 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7428547113878631 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:21.888 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7430151713173203 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:22.895 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7428636589675726 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:23.897 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7432138075868715 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:24.910 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7420038959750406 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:25.915 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7420156272462153 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:26.920 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7426071616825665 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:27.922 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7428831448078288 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:28.925 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.743621419551416 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:29.927 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.742795259691571 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:30.930 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7428954725843175 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:31.937 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7431688708532194 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:32.938 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7431813974648126 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:33.942 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7436790817317662 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:34.974 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7421571978407303 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:35.976 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7421428817131951 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:36.983 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7423560329453862 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:37.985 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.742805599117013 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:38.986 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7422834581321868 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:39.988 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.742304733488385 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:40.992 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7423170612648736 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:41.993 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7424421285457021 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:43.000 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7425896641933568 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:44.006 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7418384651679669 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:45.030 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7416656774620211 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:46.032 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7419283386352714 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:47.035 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7425045627685641 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:48.036 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.74222380760079 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:49.038 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7429940947962268 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:50.040 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7430056272322969 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:51.043 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7430933135134501 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:52.044 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7435808571900663 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:53.045 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7414290636874805 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:54.046 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7405247813411079 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:55.070 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7429002446268292 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:56.073 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7434138357021554 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:57.075 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7434995336322621 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:58.722 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7429509475785165 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:31:59.724 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7422248017763133 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:00.727 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7422248017763133 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:01.730 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7425880735125195 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:02.737 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7432151994326042 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:03.741 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7432261353633602 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:04.753 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7002724795640327 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:05.775 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7429891239186104 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:06.777 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7094972067039106 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:07.781 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7438773203311082 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:08.782 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.743051756976577 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:09.787 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7431253259652997 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:10.788 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7427282522613019 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:11.793 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7427678204471284 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:12.794 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7427678204471284 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:13.796 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7424904454761335 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:14.802 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7424904454761335 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:15.822 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7158774373259053 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:16.823 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7429221164883414 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:17.826 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7431088226516133 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:18.826 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7431452094757652 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:19.829 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7406096642211937 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:20.833 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7596685082872927 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:21.834 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7401853501078581 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:22.835 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7404971235519585 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:23.838 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7397495035584525 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:24.841 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.739124564824519 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:25.863 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7394417068164452 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:26.868 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7398188970099774 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:27.871 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7379273786593861 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:28.873 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7380033336693647 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:29.874 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7382222511195907 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:30.876 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7373571195792331 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:31.879 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7382586379437428 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:32.896 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7389221506879794 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:33.901 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7389336831240494 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:34.905 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7727353664853664 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:35.920 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7386304595894493 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:36.926 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7044198895027625 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:37.930 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7381894433273225 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:38.931 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7387893288380692 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:39.934 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7849162011173184 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:40.945 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.728813559322034 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:41.950 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8095238095238095 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:42.953 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384131328200604 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:43.955 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7381369508596934 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:44.958 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7394373324441428 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:45.988 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7394371336090381 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:46.991 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7399877080138302 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:47.992 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7399875091787255 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:48.997 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7396437232827755 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:50.002 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.739046621463494 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:51.009 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7390818152770181 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:52.010 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384608532451779 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:53.013 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7388344644068262 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:54.016 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7383811203682108 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:55.018 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7387044262483813 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:56.035 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7376776417679384 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:57.037 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7377776558255803 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:58.038 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7391414658084148 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:32:59.040 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7382610239649987 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:00.050 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7390094392989234 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:01.051 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.739360383258641 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:02.057 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7393727110351296 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:03.060 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.738333598778198 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:04.063 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7383333999430934 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:05.065 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7391585656274152 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:06.086 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7391583667923106 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:07.089 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7387280876258353 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:08.094 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7395655810866458 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:09.097 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7397656092019296 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:10.099 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7397656092019296 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:11.103 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7390257437775051 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:12.107 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7391088568512513 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:13.110 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7385026086171556 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3258] - [INFO] 2024-05-05 01:33:13.911 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3258, processInstanceId=938, status=9, startTime=1714841447434, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, endTime=1714842191466, processId=18118, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843693740)
[WI-0][TI-3258] - [INFO] 2024-05-05 01:33:13.920 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3258, processInstanceId=938, status=9, startTime=1714841447434, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, endTime=1714842191466, processId=18118, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843993911)
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:14.113 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7394045246518746 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:15.114 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7394045246518746 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:16.127 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7388205459495003 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:17.129 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7383190838155582 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:18.135 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7568306010928961 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:19.138 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7397216666438007 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:20.140 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7397216666438007 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:21.143 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7397546732711735 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:22.144 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.738850172380094 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:23.145 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7320441988950277 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:24.147 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7065527065527066 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:25.150 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7381739341891594 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:26.165 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384242675859212 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:27.166 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7188405797101449 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:28.172 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384153200062116 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:29.175 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7389147937891072 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:30.177 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7379220101115604 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:31.178 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384853099630505 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:32.181 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7389102205817001 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:33.182 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7389102205817001 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:34.183 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7394470753642709 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:35.190 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7394707367417249 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:36.220 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384421627453401 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:37.225 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7390112288148653 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:38.227 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7392353159778123 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:39.229 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7382826969914061 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:40.230 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.738291445736011 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:41.233 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7383674007459895 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:42.236 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7386282724032981 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:43.241 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7390152055169583 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:44.243 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.739040258740145 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:45.247 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7394317650612123 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:46.266 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7394317650612123 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3261] - [INFO] 2024-05-05 01:33:46.994 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3261, processInstanceId=941, status=9, startTime=1714842212366, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, endTime=1714842223865, processId=18773, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843726800)
[WI-0][TI-3261] - [INFO] 2024-05-05 01:33:47.002 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3261, processInstanceId=941, status=9, startTime=1714842212366, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, endTime=1714842223865, processId=18773, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844026994)
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:47.269 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.738689513615532 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:48.271 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.738697069349509 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:49.278 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7385220944574118 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3260] - [INFO] 2024-05-05 01:33:50.008 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3260, processInstanceId=940, status=9, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, endTime=1714842227024, processId=18772, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843729817)
[WI-0][TI-3260] - [INFO] 2024-05-05 01:33:50.015 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3260, processInstanceId=940, status=9, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, endTime=1714842227024, processId=18772, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844030008)
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:50.281 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7382431288055796 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:51.283 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7383161012889883 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:52.284 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7382616204703126 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3259] - [INFO] 2024-05-05 01:33:53.019 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3259, processInstanceId=939, status=9, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, endTime=1714842229451, processId=18762, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843732825)
[WI-0][TI-3259] - [INFO] 2024-05-05 01:33:53.026 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3259, processInstanceId=939, status=9, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, endTime=1714842229451, processId=18762, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844033019)
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:53.297 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7383224640123374 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:54.307 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7514124293785311 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:55.308 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384618474207011 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:56.332 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7388994834860486 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:57.336 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7389495899324219 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3191] - [INFO] 2024-05-05 01:33:58.036 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3191, processInstanceId=936, status=6, startTime=1714839942174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13377949373536/25/936/3191.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/936/3191, endTime=1714840005766, processId=14716, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843737857)
[WI-0][TI-3191] - [INFO] 2024-05-05 01:33:58.045 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3191, processInstanceId=936, status=6, startTime=1714839942174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13377949373536/25/936/3191.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/936/3191, endTime=1714840005766, processId=14716, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844038036)
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:58.337 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.739323399929175 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:33:59.339 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7383868865862458 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:00.340 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7386497465946009 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:01.342 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7389877662725158 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:02.345 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7389877662725158 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:03.348 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7389479992515847 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:04.349 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.73894780041648 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:05.351 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7380248078606675 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:06.365 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384934622023414 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:07.367 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7388312830451517 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:08.369 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7383884772670831 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:09.372 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7390649142931223 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:10.375 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384302326390608 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:11.376 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384427592506542 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:12.379 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384171095221536 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:13.383 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7385656393453315 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:14.384 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7383900679479203 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:15.388 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7385642474995988 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:16.405 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7378542073408728 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:17.408 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7387296783066726 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:18.413 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7396667881549156 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:19.414 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384634381015384 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:20.416 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7389265250602818 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:21.418 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7005649717514125 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:22.420 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7394043258167698 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:23.422 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7397411524840569 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:24.429 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7397530825903362 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:25.442 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7452054794520548 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:26.461 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7327725785711828 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:27.708 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7324126870317558 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:28.742 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.732414277712593 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:29.750 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7319529802697914 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:30.753 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7323534341705683 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:31.765 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7315246894543627 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:32.767 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7319505942485355 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:33.769 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7319496000730122 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:34.770 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8205128205128205 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:35.772 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7318605219461264 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:36.790 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7588075880758808 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:37.794 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7321102588375742 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:38.798 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.732109264662051 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:39.801 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.731905856349988 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:40.808 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7320561756891077 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:41.809 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7994350282485876 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:42.812 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7327952457731136 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:43.814 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7323928035212902 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:44.818 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8192419825072886 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:45.832 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7324800921322341 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:46.854 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7315332393638628 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:47.859 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7315332393638628 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:48.861 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7324337635528493 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:49.870 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7325586319985732 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:50.872 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.724233983286908 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:51.880 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.712707182320442 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:52.884 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7328069770442883 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:53.888 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8006329113924051 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:54.891 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7334394715121987 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:55.892 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7075208913649025 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:56.904 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7322170332887744 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:57.907 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7323367320217772 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3262] - [INFO] 2024-05-05 01:34:58.787 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3262, processInstanceId=942, status=9, startTime=1714842212466, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262, endTime=1714842296579, processId=18774, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843798033)
[WI-0][TI-3262] - [INFO] 2024-05-05 01:34:58.802 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3262, processInstanceId=942, status=9, startTime=1714842212466, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262, endTime=1714842296579, processId=18774, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844098787)
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:58.913 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7326612309125756 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:34:59.923 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7401129943502824 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:00.934 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7127071823204421 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:01.935 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7318098189944392 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:02.939 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7321979451187274 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:03.941 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7331109959193072 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:04.942 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7334364889856287 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:05.944 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7134831460674157 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:06.976 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7319450268656051 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:07.977 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7002724795640327 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:08.979 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7317430103992748 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:09.981 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.731891937892662 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:10.983 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7315972642675621 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:11.992 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7319478105570704 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:12.993 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7319647115409661 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:14.002 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.732002291375746 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:15.032 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.732376697877813 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:16.040 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7326129139821442 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:17.052 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7292971147943523 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:18.055 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7320058704076299 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:19.056 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7322249866929607 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:20.059 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7327258523215887 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:21.060 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7327254546513794 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:22.065 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7002724795640327 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:23.070 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7325630063708756 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:24.071 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7138728323699421 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:25.073 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7324212369412559 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:26.075 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7314566878485703 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:27.090 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.731456290178361 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:28.091 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7318062399625553 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:29.093 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313870955619408 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:30.095 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313972361522783 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:31.098 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313954466363364 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:32.099 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7314451554125003 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:33.104 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7309926067143035 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:34.105 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7316197326343882 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:35.106 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7319078447010345 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:36.108 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7321194052523884 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:37.120 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7309872381664779 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:38.124 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7312411505951234 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:39.128 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7316668565541916 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:40.130 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7312608352704844 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:41.131 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313232694933463 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:42.135 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7322766838201712 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:43.137 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7322754908095432 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:44.139 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7306003050528176 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:45.141 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7306502126640861 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:46.142 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7310792988199335 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:47.168 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.729936195803267 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:48.172 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7303617029272306 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:49.175 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7303396322306137 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:50.181 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7303772120653936 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:51.183 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7305905621326895 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:52.191 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7306016968985501 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:53.194 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7300199053823271 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:54.196 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7303322753317415 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:55.202 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7311202588514927 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:56.204 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7307309397165764 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:57.218 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7307547999291352 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:58.220 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7308778788589172 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:35:59.222 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7308902066354058 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:00.223 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.731083673192236 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:01.224 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7310961998038293 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:02.226 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.731321678812509 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:03.227 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313455390250677 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:04.229 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7306420604247953 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:05.231 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7310450991819327 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:06.233 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.73046887504864 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3263] - [INFO] 2024-05-05 01:36:06.915 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3263, processInstanceId=943, status=9, startTime=1714842505551, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/943/3263.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263, endTime=1714842665300, processId=20088, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843866760)
[WI-0][TI-3263] - [INFO] 2024-05-05 01:36:06.928 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3263, processInstanceId=943, status=9, startTime=1714842505551, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/943/3263.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263, endTime=1714842665300, processId=20088, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844166914)
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:07.247 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7304939282718267 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:08.248 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7302893269491358 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:09.250 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7310532514212236 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:10.255 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7310649826923983 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:11.258 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.729935002792639 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:12.259 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7302732213056586 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:13.260 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7053824362606232 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:14.261 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7304074350013013 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:15.262 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7296625986992605 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:16.263 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.730163265492784 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:17.294 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7306376860524929 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:18.296 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7103064066852368 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:19.297 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7305651112392935 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:20.301 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7303189533797295 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:21.301 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7303682644856841 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:22.303 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.730569485611596 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:23.306 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7298192807617293 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:24.310 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7787610619469026 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:25.313 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7310886440698524 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:26.314 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7033898305084746 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:27.329 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7315831469751315 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:28.331 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7319643138707568 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:29.332 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7314771678643499 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:30.334 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7297240387465992 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:31.339 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7293657378880093 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:32.341 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7298025786129383 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:33.343 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8689839572192513 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:34.347 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.818918918918919 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:35.349 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.730751419732356 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:36.378 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7303632936080677 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:37.453 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.75 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:38.457 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7442528735632183 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:39.470 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7310232273204206 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:40.473 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313990256682201 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:41.478 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7314358101625815 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:42.488 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7302537354654024 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:43.491 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7302535366302977 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:44.496 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7262247838616716 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:45.509 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.730777467131066 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:46.528 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7311156856440856 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:47.588 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7122905027932961 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:48.591 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7305287244151415 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:49.592 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7230320699708455 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:50.594 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7302169509710411 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:51.595 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7300173205259666 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:52.597 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7301831490032495 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:53.602 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7308116667690667 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:54.604 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7306209839037018 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:55.606 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.729216810394622 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:56.609 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7295353442322807 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:57.631 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7300358121906996 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:58.638 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7310862580485965 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:36:59.640 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7310860592134918 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:00.642 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7855153203342619 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:01.644 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7299381841543136 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:02.647 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7303758202196611 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:03.651 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7303968967407546 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:04.652 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7303941130492895 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:05.654 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7920227920227919 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:06.679 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7951070336391437 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:07.711 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8169014084507042 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:08.714 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8878787878787879 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:09.729 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8189655172413793 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:10.733 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7293253743617641 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:11.737 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.729751875661251 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:12.761 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7851002865329513 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:13.765 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7301085858390036 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:14.767 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313485215516375 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:15.768 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7303811887674868 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:16.776 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7180232558139534 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:17.800 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.860655737704918 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:18.805 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7304933317665127 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:19.809 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8521739130434782 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:20.820 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.827485380116959 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:21.840 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8839779005524862 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:22.842 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8567251461988303 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:23.863 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8413597733711048 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:24.871 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9162011173184358 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:25.881 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8472222222222223 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:26.892 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8163841807909605 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:27.943 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8555240793201133 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:28.970 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8151260504201681 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:29.971 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.904109589041096 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:30.980 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8500000000000001 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:31.982 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7521865889212829 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:33.003 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.733088527552481 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:34.014 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7336406926381104 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:35.016 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8767123287671232 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:36.018 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7342443560158456 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:37.043 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7338446974554873 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:38.068 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7340154968103867 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:39.070 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7348533879414064 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:40.075 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7336066918352142 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:41.077 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7342266596915312 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:42.080 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.734452138700211 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:43.082 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7328218896771376 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:44.089 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8221574344023324 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:45.101 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.73385463921072 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:46.103 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7336263765105752 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:47.105 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7337236068767519 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:48.119 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7883008356545962 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:49.123 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.742296918767507 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:50.126 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7819767441860466 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:51.127 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7345132743362832 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:52.130 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7343930846741281 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:53.131 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7037037037037037 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:54.132 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8132183908045978 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:55.138 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7954545454545454 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:56.150 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8027397260273973 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:57.153 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7040229885057472 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:58.170 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8922651933701659 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:37:59.173 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8264705882352942 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:00.174 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8484848484848485 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:01.195 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9292035398230087 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:02.209 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7350902005510516 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:03.210 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7456647398843931 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:04.221 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7357211043381247 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:05.234 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8017241379310345 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:06.236 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7349895899880957 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:07.237 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7834757834757834 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:08.279 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8194070080862533 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:09.287 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7126760563380281 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:10.287 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7349730866744093 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:11.291 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7441176470588236 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:12.296 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8792134831460674 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:13.306 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7685714285714286 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3258] - [INFO] 2024-05-05 01:38:14.249 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3258, processInstanceId=938, status=9, startTime=1714841447434, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, endTime=1714842191466, processId=18118, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714843993911)
[WI-0][TI-3258] - [INFO] 2024-05-05 01:38:14.257 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3258, processInstanceId=938, status=9, startTime=1714841447434, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, endTime=1714842191466, processId=18118, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844294249)
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:14.358 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8451086956521741 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:15.374 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.78125 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:16.380 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8053097345132744 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:17.383 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9263456090651558 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:18.423 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7181571815718157 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:19.425 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9014492753623189 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:20.426 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8522727272727272 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:21.449 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8742857142857143 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:22.457 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8693181818181819 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:23.460 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7653958944281525 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:24.464 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.737880055904478 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:25.467 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7382160872313465 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:26.470 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7565217391304347 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:27.476 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7597765363128491 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:28.529 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8726287262872628 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:29.537 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9055555555555556 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:30.553 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8264462809917354 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:31.577 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8291316526610644 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:32.579 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7380291822329699 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:33.581 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384161153466302 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:34.583 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384501161495264 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:35.606 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8689458689458689 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:36.609 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9220055710306406 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:37.612 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7380504575891681 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:38.641 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7831978319783197 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:39.643 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7636887608069165 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:40.644 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.847887323943662 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:41.646 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7587209302325582 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:42.661 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.807017543859649 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:43.664 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7768115942028985 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:44.670 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7982456140350876 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:45.672 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.922437673130194 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:46.676 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7298850574712643 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3261] - [INFO] 2024-05-05 01:38:47.384 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3261, processInstanceId=941, status=9, startTime=1714842212366, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, endTime=1714842223865, processId=18773, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844026994)
[WI-0][TI-3261] - [INFO] 2024-05-05 01:38:47.387 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3261, processInstanceId=941, status=9, startTime=1714842212366, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, endTime=1714842223865, processId=18773, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844327383)
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:47.678 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.741839762611276 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:48.690 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7891737891737891 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:49.691 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7373732252227102 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3260] - [INFO] 2024-05-05 01:38:50.392 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3260, processInstanceId=940, status=9, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, endTime=1714842227024, processId=18772, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844030008)
[WI-0][TI-3260] - [INFO] 2024-05-05 01:38:50.429 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3260, processInstanceId=940, status=9, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, endTime=1714842227024, processId=18772, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844330392)
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:50.714 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8455056179775281 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:51.718 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7369572621837701 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:52.719 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7375307026255977 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3259] - [INFO] 2024-05-05 01:38:53.437 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3259, processInstanceId=939, status=9, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, endTime=1714842229451, processId=18762, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844033019)
[WI-0][TI-3259] - [INFO] 2024-05-05 01:38:53.449 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3259, processInstanceId=939, status=9, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, endTime=1714842229451, processId=18762, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844333437)
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:53.743 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8440422651374863 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:54.768 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8662952646239553 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:55.782 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8455284552845528 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:56.784 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7528409090909091 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:57.786 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7372586962024285 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3191] - [INFO] 2024-05-05 01:38:58.463 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3191, processInstanceId=936, status=6, startTime=1714839942174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13377949373536/25/936/3191.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/936/3191, endTime=1714840005766, processId=14716, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844038036)
[WI-0][TI-3191] - [INFO] 2024-05-05 01:38:58.468 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3191, processInstanceId=936, status=6, startTime=1714839942174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13377949373536/25/936/3191.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/936/3191, endTime=1714840005766, processId=14716, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844338463)
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:58.811 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7373080073083831 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:38:59.814 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7742857142857142 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:00.815 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7593123209169055 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:01.817 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8715083798882681 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:02.822 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8017492711370263 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:03.823 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7632311977715878 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:04.825 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7372990597286736 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:05.827 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7655786350148368 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:06.839 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.737169419240438 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:07.846 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7375175795086903 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:08.879 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7379560109144566 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:09.880 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7457627118644068 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:10.883 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7372966737074177 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:11.888 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7376971276081946 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:12.889 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7372992585637782 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:13.894 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7373018434201388 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:14.895 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8235294117647058 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:15.898 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7373678566748845 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:16.900 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7382383567630679 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:17.903 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7382365672471259 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:18.920 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7385119538670744 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:19.923 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7380267962117141 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:20.926 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7380265973766094 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:21.929 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384902808406669 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:22.934 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7385135445479116 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:23.937 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.790518442013677 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:24.939 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7383721727885013 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:25.953 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7378337273250932 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:26.955 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7378693188088266 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:27.956 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7386441792116705 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:28.982 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7379301623508513 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:29.994 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7379925965737132 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:30.998 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7376875835231711 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:32.004 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7369835084175848 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:33.005 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8112676056338028 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:34.008 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7371654425383448 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:35.011 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7387811765987784 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:36.015 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7387811765987784 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:37.020 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7388813894915249 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:38.024 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7430939226519337 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:39.049 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.738402395724409 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:40.050 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7381101081205649 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:41.052 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.737597511220762 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:42.056 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7098591549295774 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:43.062 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7381971978964041 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:44.065 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384222792348746 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:45.067 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384222792348746 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:46.071 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.737690566049741 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:47.074 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7370737795550985 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:48.076 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.738010690568237 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:49.099 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7377828255383013 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:50.101 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7379707247122012 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:51.103 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7381653842796592 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:52.110 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7374050388394552 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:53.114 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.71671388101983 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:54.115 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7383630263736871 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:55.117 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.738500620266109 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:56.118 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7386507407701242 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:57.119 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7386755951582061 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:58.121 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7387873404870228 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:39:59.141 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7393009315623488 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3262] - [INFO] 2024-05-05 01:39:59.557 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3262, processInstanceId=942, status=9, startTime=1714842212466, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262, endTime=1714842296579, processId=18774, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844098787)
[WI-0][TI-3262] - [INFO] 2024-05-05 01:39:59.560 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3262, processInstanceId=942, status=9, startTime=1714842212466, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262, endTime=1714842296579, processId=18774, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844399557)
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:00.143 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7385978506322857 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:01.145 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7392363101533357 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:02.160 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.75 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:03.163 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7380102928980277 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:04.168 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7385990436429136 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:05.169 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7389243378741306 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:06.172 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7390368785433659 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:07.174 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.739274884163639 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:08.175 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7385304455318074 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:09.190 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7379021266010948 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:10.200 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7796143250688705 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:11.202 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7375054505673064 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:12.206 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7379174369041533 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:13.208 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7389310982676889 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:14.210 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7055555555555555 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:15.212 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7371724017670078 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:16.214 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7375354746681094 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:17.216 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384489231388984 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:18.219 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384479289633752 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:19.233 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.738151068152124 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:20.238 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7209944751381215 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:21.240 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7379128636967462 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:22.243 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7383886761021877 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:23.245 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7383882784319784 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:24.246 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7450424929178471 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:25.248 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384914738512948 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:26.250 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7385163282393767 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:27.253 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7388167680825118 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:28.255 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7392412810309521 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:29.282 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7091412742382271 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:30.283 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7380800840197618 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:31.284 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384433557559681 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:32.288 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.73854336981361 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:33.289 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7790055248618785 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:34.294 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7386089853981465 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:35.296 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7077363896848137 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:36.298 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7389742454853993 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:37.299 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7383870854213505 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:38.302 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7198879551820728 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:39.361 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7391561796061593 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:40.366 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7376987182890319 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:41.368 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7378490376281517 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:42.369 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7375322933064349 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:43.372 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7379027231064088 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:44.374 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7385827391643318 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:45.377 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7387058180941138 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:46.380 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7389688769375735 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:47.403 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384188990380954 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:48.404 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7389788186928063 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:49.418 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7391790456431948 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:50.420 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7395029480286792 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:51.422 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384296361337468 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:52.424 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.738928115741119 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:53.429 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384300338039561 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:54.437 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7386304595894493 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:55.439 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7166435826408125 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:56.442 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7382767319382665 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:57.445 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.814404432132964 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:58.449 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7388672721990944 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:40:59.481 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7391919699249974 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:00.488 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7385461535050751 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:01.489 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7375281177692371 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:02.496 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7111111111111111 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:03.497 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7370835224752267 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:04.501 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.737120306969588 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:05.503 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7371201081344834 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:06.504 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8695652173913044 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3263] - [INFO] 2024-05-05 01:41:07.030 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3263, processInstanceId=943, status=9, startTime=1714842505551, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/943/3263.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263, endTime=1714842665300, processId=20088, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844166914)
[WI-0][TI-3263] - [INFO] 2024-05-05 01:41:07.034 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3263, processInstanceId=943, status=9, startTime=1714842505551, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/943/3263.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263, endTime=1714842665300, processId=20088, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844467030)
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:07.506 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7374456012008049 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:08.507 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7370650308104937 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:09.519 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7366478747609256 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:10.522 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7370602587679819 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:11.526 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.707182320441989 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:12.527 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7362722752482307 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:13.528 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7361722611905888 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:14.531 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7365593931393538 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:15.536 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7367317831750904 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:16.539 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7360460008991323 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:17.542 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7357423796943229 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:18.546 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7360680715957492 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:19.559 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7602179836512262 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:20.561 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9362880886426593 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:21.565 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9549295774647888 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:22.568 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7787114845938375 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:23.569 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7357483447474625 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:24.589 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7355580595523069 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:25.596 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.717948717948718 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:26.603 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7362921587586962 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:27.612 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.736303293524557 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:28.613 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7363024981841384 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:29.643 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7356646351684024 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:30.644 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.734673442171693 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:31.645 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7356344122324947 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:32.649 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7362482162005674 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:33.651 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9178470254957507 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:34.655 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7471264367816093 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:35.658 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9162011173184358 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:36.665 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7564469914040114 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:37.666 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7857142857142858 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:38.667 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7138728323699421 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:39.685 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7358644644485816 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:40.687 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8085714285714286 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:41.690 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7097701149425287 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:42.695 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7692307692307692 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:43.706 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7845152085483578 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:44.717 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8823703445875392 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:45.719 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9387186629526463 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:46.721 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7353425222988599 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:47.724 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7897727272727272 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:48.729 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8357348703170029 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:49.750 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7393767705382437 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:50.766 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9322033898305084 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:51.775 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8270893371757926 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:52.777 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7361145990102386 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:53.778 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7361489974833441 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:54.780 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7348213754895568 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:55.782 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8575268817204301 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:56.783 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7351794775130421 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:57.784 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7352264025977409 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:58.785 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7347466134902063 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:41:59.818 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.734775046910172 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:00.823 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8523676880222841 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:01.824 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.803125 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:02.170 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:42:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
	
	[Stage 2:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:02.181 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:42:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
	
	[Stage 2:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:02.827 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7563739376770539 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:03.175 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:03.186 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:03.829 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8555555555555555 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:04.833 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.74104809562696 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:05.851 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.804093567251462 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:06.861 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8571428571428572 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:07.873 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8248587570621468 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:08.874 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7386363636363635 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:09.918 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9283819628647214 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:10.935 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9327731092436975 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:11.976 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9203296703296704 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:12.978 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7423629921740491 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:13.986 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7183098591549295 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:14.988 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8974358974358974 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:15.992 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7097701149425287 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:16.994 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7562326869806094 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:17.996 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7427049885540572 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:18.997 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7429179409511436 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:20.028 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7422546270420116 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:21.040 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8238962904911181 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:22.047 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8521739130434782 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:23.062 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7417841831843959 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:24.072 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7427459485856163 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:25.088 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7416457939515554 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:26.094 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7425703771882052 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:27.098 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7413475412945716 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:28.108 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7417312930465575 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:29.112 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7428312488455137 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:30.138 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7413246752575362 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:31.140 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7422391179038486 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:32.142 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7422766977386285 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:33.143 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7420058843260872 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:34.145 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7421754906703586 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:35.148 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7424013673492477 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:36.149 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7424123032800037 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:37.152 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7421864266011147 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:38.156 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7425095336461804 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:39.158 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7425204695769365 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:40.190 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7429561172912376 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:41.194 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8994413407821229 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:41.292 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:42:41 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:41.310 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 01:42:41 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:42.198 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7530864197530864 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:42.296 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 3:>                                                          (0 + 1) / 1]
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:42.311 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 3:>                                                          (0 + 1) / 1]
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:43.199 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7449076838434349 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:44.200 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7440357919095189 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:45.201 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7433917650055386 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:46.202 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7442751893755245 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:47.205 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7444092042360626 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:48.209 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7446257356650329 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:49.210 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.744739469344896 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:50.252 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7452635986807689 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:51.254 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7441531046212658 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:52.254 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7433525944899213 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:53.258 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8095238095238095 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:54.261 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7445002707139949 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:55.267 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7452518674095941 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:56.268 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.744139782669254 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:57.270 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.743868173916294 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:58.279 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7441389873288353 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:42:59.281 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7970149253731343 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:00.312 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7971428571428572 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:01.325 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9159663865546219 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:02.328 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8997214484679665 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:03.336 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8613569321533924 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:04.349 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8797814207650273 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:05.368 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8763440860215054 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:06.407 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9308607219603204 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:07.410 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8393351800554016 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:08.413 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.838526912181303 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:09.424 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.735632183908046 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:10.454 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7477802546003981 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:11.472 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8756906077348066 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:12.478 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8666666666666667 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:13.480 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7343283582089553 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3258] - [INFO] 2024-05-05 01:43:14.334 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3258, processInstanceId=938, status=9, startTime=1714841447434, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, endTime=1714842191466, processId=18118, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844294249)
[WI-0][TI-3258] - [INFO] 2024-05-05 01:43:14.341 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3258, processInstanceId=938, status=9, startTime=1714841447434, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, endTime=1714842191466, processId=18118, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844594334)
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:14.482 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8023255813953489 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:15.503 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9237057220708447 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:16.532 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8698224852071006 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:17.535 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9405099150141643 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:18.551 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8125 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:19.554 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7463074829802121 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:20.566 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7107438016528925 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:21.568 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7458342554311311 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:22.570 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7650429799426934 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:23.577 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7150997150997151 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:24.590 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7422096317280453 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:25.593 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9235127478753541 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:26.594 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7749287749287749 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:27.600 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7453628173979922 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:28.621 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7445259204424955 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:29.625 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7449760831194364 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:30.652 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8440111420612814 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:31.657 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8235294117647058 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:32.658 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7446559586009406 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:33.661 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7971830985915493 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:34.664 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8646408839779005 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:35.680 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8397626112759644 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:36.683 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7312138728323699 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:37.687 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7453936368392138 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:38.689 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7459424217280639 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:39.692 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.863905325443787 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:40.710 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9291553133514987 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:41.719 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8515406162464986 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:42.737 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.847887323943662 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:43.741 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7507246376811594 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:44.755 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8610354223433243 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:45.758 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7383427451930122 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:46.765 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8163265306122449 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3261] - [INFO] 2024-05-05 01:43:47.410 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3261, processInstanceId=941, status=9, startTime=1714842212366, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, endTime=1714842223865, processId=18773, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844327383)
[WI-0][TI-3261] - [INFO] 2024-05-05 01:43:47.414 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3261, processInstanceId=941, status=9, startTime=1714842212366, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, endTime=1714842223865, processId=18773, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844627410)
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:47.771 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8555240793201134 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:48.774 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7191011235955055 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:49.780 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7381999815878693 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3260] - [INFO] 2024-05-05 01:43:50.425 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3260, processInstanceId=940, status=9, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, endTime=1714842227024, processId=18772, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844330392)
[WI-0][TI-3260] - [INFO] 2024-05-05 01:43:50.435 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3260, processInstanceId=940, status=9, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, endTime=1714842227024, processId=18772, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844630425)
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:50.837 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.804945054945055 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:51.854 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7983193277310925 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:52.857 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9261363636363636 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3259] - [INFO] 2024-05-05 01:43:53.441 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3259, processInstanceId=939, status=9, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, endTime=1714842229451, processId=18762, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844333437)
[WI-0][TI-3259] - [INFO] 2024-05-05 01:43:53.485 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3259, processInstanceId=939, status=9, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, endTime=1714842229451, processId=18762, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844633441)
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:53.860 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9353932584269663 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:54.863 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.737365271818524 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:55.865 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7377424620120562 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:56.866 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7373175513934066 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:57.869 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7374046411692459 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3191] - [INFO] 2024-05-05 01:43:58.497 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3191, processInstanceId=936, status=6, startTime=1714839942174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13377949373536/25/936/3191.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/936/3191, endTime=1714840005766, processId=14716, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844338463)
[WI-0][TI-3191] - [INFO] 2024-05-05 01:43:58.507 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3191, processInstanceId=936, status=6, startTime=1714839942174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13377949373536/25/936/3191.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/936/3191, endTime=1714840005766, processId=14716, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844638497)
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:58.872 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7382912469009063 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:43:59.875 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384037875701416 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:00.896 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7386006343237509 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:01.897 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7376589512681007 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:02.898 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7372459707557305 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:03.899 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7364387002308277 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:04.901 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.737524141067144 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:05.902 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.737524141067144 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:06.903 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7374893449238292 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:07.905 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7374891460887246 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:08.906 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.736745303962207 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:09.908 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7369184893383622 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:10.924 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7377076658687414 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:11.925 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7377076658687414 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:12.926 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.737702098485811 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:13.927 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7381391380458446 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:14.929 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7071823204419889 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:15.933 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8567415730337078 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:16.936 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7407407407407407 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:17.938 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9182635755870222 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:18.952 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8785310734463276 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:19.954 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7372877261277082 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:20.973 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7374748299611894 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:21.976 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7437325905292479 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:22.977 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7374839763760035 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:23.979 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7374955088120736 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:24.980 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7078651685393258 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:25.983 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7371978526604037 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:26.987 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7561643835616438 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:27.996 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7368296100465811 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:28.998 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7372676437821379 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:30.000 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7372672461119286 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:31.020 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7647058823529411 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:32.021 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7036011080332409 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:33.024 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7371906945966361 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:34.026 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7372266837505789 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:35.027 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7376122250185065 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:36.030 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7282913165266107 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:37.031 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7371443660172513 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:38.034 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7579250720461095 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:39.036 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7369405600349791 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:40.040 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7356465411738787 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:41.062 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7961432506887053 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:42.064 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7372857377766616 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:43.072 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7373330605315698 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:44.076 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7374833798706896 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:45.081 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7374790054983872 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:46.085 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7384196185286103 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:47.086 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7366653722501354 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:48.090 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7370027954227364 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:49.092 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7363782543590123 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:50.095 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7082152974504249 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:51.112 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7363245688807553 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:52.118 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7366743198298449 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:53.120 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7368349785944068 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:54.123 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7371220964855298 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:55.126 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7369459285828048 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:56.143 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7374843740462128 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:57.146 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7368407448124419 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:58.147 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7371902969264268 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:44:59.149 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7058823529411765 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3262] - [INFO] 2024-05-05 01:44:59.583 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3262, processInstanceId=942, status=9, startTime=1714842212466, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262, endTime=1714842296579, processId=18774, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844399557)
[WI-0][TI-3262] - [INFO] 2024-05-05 01:44:59.591 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3262, processInstanceId=942, status=9, startTime=1714842212466, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262, endTime=1714842296579, processId=18774, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844699583)
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:00.153 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7364468524701185 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:01.177 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7364462559648046 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:02.179 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7369701864655728 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:03.183 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7369691922900495 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:04.185 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7033818731598631 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:05.188 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7768361581920904 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:06.189 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.737867926963094 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:07.192 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7375263282532952 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:08.196 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7375378606893653 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:09.197 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7619047619047619 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:10.200 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7364981519271198 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:11.220 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7359509577191068 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:12.222 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7366681559416005 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:13.224 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.702247191011236 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:14.229 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7368214578072902 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:15.230 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7368196682913483 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:16.232 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7362728717535447 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:17.233 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7367850709831382 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:18.235 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.718562874251497 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:19.242 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7368228496530228 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:20.249 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7406340057636888 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:21.280 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7309782608695653 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:22.283 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.842406876790831 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:23.308 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8756756756756757 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:24.309 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7921348314606742 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:25.319 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9055555555555554 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:26.321 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8498583569405099 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:27.326 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7138888888888888 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:28.338 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7388888888888889 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:29.346 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7403314917127072 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:30.354 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7368717630887681 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:31.377 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7050561797752809 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:32.379 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9136490250696379 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:33.381 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7367435144462651 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:34.383 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7360402346810974 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:35.384 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7367351633718696 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:36.386 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7370986339431805 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:37.388 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7372740065054869 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:38.390 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7378989452394203 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:39.391 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7369526889763631 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:40.394 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7373151653721507 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:41.410 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7368465110304768 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:42.414 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7369964326993874 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:43.415 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7374702567537823 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:44.416 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.737469859083573 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:45.417 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7368045568233944 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:46.418 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7375810079070756 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:47.420 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7376058622951576 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:48.429 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7378178205167207 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:49.431 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7380432995254005 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:50.437 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7381425182426238 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:51.454 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7372322511335092 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:52.456 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7372553160056493 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:53.457 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7366512549577048 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:54.461 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.736659208361891 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:55.469 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7374352617753629 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:56.479 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7366920161541592 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:57.483 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7370787504327149 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:58.487 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7377541932832309 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:45:59.488 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7380043278448879 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:00.492 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7370326206884348 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:01.501 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7375442234127143 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:02.504 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7379935907492365 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:03.509 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7380055208555159 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:04.511 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7377933637988481 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:05.513 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7378046973998135 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:06.514 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7373255047975928 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3263] - [INFO] 2024-05-05 01:46:07.043 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3263, processInstanceId=943, status=9, startTime=1714842505551, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/943/3263.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263, endTime=1714842665300, processId=20088, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844467030)
[WI-0][TI-3263] - [INFO] 2024-05-05 01:46:07.046 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3263, processInstanceId=943, status=9, startTime=1714842505551, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/943/3263.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263, endTime=1714842665300, processId=20088, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844767043)
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:07.518 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.737346183648477 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:08.519 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7387602989127895 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:09.521 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7379035184468274 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:10.522 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7380645748815987 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:11.544 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.738339961501547 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:12.545 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7405935585777166 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:13.546 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7507002801120448 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:14.547 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384051794158741 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:15.549 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.737792767293534 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:16.549 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7371729982723217 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:17.550 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7381118976365068 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:18.551 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7382113151888346 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:19.552 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7382488950236147 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:20.553 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.738584926350483 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:21.567 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7375917450027271 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:22.569 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7381037453972159 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:23.570 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384544905218288 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:24.572 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384540928516194 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:25.574 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384513091601543 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:26.576 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7374625021847007 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:27.578 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.737698121783718 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:28.579 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7374231328339789 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:29.581 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7374980936684341 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:30.583 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7372240988942184 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:31.594 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7377746732990104 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:32.594 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.737168822735124 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:33.595 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7371718052616938 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:34.598 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.738010690568237 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:35.599 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7373491661750469 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:36.600 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7387799835881504 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:37.602 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7382441229811029 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:38.604 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7385588789517732 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:39.606 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7385700137176339 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:40.611 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384059747562928 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:41.627 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7385407849572495 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:42.628 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7385656393453315 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:43.631 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7374603149985495 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:44.632 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.737886418627827 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:45.636 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7383365813047679 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:46.639 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7383988166925252 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:47.644 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.739249433270243 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:48.645 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.739249433270243 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:49.649 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7392866154348136 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:50.652 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7382962177785227 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:51.667 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7387463804554636 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:52.668 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7387443921044171 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:53.669 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7377104495602066 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:54.670 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7377168122835556 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:55.671 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7380798851846572 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:56.676 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.738489087830039 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:57.683 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9261213720316623 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:58.684 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9005376344086021 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:46:59.687 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7387917148593252 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:00.689 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7389823977246901 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:01.697 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7380888327643667 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:02.699 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7385397907817262 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:03.700 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7386638638870314 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:04.703 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7381665772902871 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:05.704 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7382349765662888 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:06.705 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.738334791788826 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:07.709 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7380500599189588 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:08.711 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384149223360023 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:09.713 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7386278747330888 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:10.715 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7385328315530633 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:11.726 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384053782509788 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:12.727 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7386298630841353 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:13.730 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7376098389972507 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:14.731 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7377710942671266 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:15.733 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7383747576448618 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:16.734 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7384542916867242 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:17.753 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7391197927820073 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:18.755 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7391599574731478 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:19.757 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7396357698785893 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:20.758 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7397737614412204 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:21.768 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7393422892641173 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:22.770 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7393617751043735 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:23.771 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7400924941139838 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:24.774 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7414175312514105 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:25.775 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7377355027833932 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:26.776 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7373891320310827 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:27.779 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7373891320310827 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:28.784 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.737501672700318 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:29.785 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7378271657666395 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:30.787 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7346911384960073 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:31.798 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7350723053916326 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:32.801 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7359901282347241 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:33.804 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7363277502424297 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:34.808 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7624309392265194 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:35.814 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9042253521126761 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:36.855 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8789135528324531 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:37.864 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8269691202652096 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:38.867 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8778409090909092 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:39.869 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7450980392156863 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:40.880 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7869237702807109 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:41.893 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7366687524469145 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:42.898 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7768361581920905 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:43.901 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9077809798270893 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:44.903 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7270194986072424 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:45.905 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8474496697700327 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:46.906 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7368190717860343 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:47.910 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7358765933899656 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:48.916 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7367904395309639 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:49.931 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7162534435261708 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:50.932 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7352683568048233 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:51.945 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7358066034331266 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:52.961 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7367530585312886 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:53.964 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8470588235294118 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:54.966 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7971428571428572 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:55.967 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7167630057803468 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:56.978 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7109826589595376 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:57.990 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8657142857142857 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:47:58.991 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.924409089478488 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:00.011 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9017341040462428 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:01.015 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8166189111747851 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:02.062 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7870619946091645 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:03.087 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8853868194842407 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:04.098 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9295774647887325 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:05.134 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.880952380952381 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:06.144 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9527777777777777 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:07.152 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9593023255813953 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:08.155 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7893258426966292 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:09.205 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9191374663072777 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:10.211 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8676056338028169 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:11.213 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7692307692307693 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:12.224 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9005681818181818 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:13.231 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9335180055401662 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:14.239 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9132947976878614 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:15.251 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8223332435366818 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3258] - [INFO] 2024-05-05 01:48:15.276 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3258, processInstanceId=938, status=9, startTime=1714841447434, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, endTime=1714842191466, processId=18118, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844594334)
[WI-0][TI-3258] - [INFO] 2024-05-05 01:48:15.297 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3258, processInstanceId=938, status=9, startTime=1714841447434, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, endTime=1714842191466, processId=18118, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844895276)
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:16.278 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8901250683018639 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:17.284 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.824712643678161 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:18.285 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8895184135977338 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:19.290 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9470752089136489 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:20.297 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9410112359550562 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:21.316 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9648648648648649 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:22.331 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.91005291005291 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:23.339 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7742857142857142 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:24.340 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9058171745152355 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:25.348 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8696883852691218 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:26.349 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8542274052478134 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:27.359 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8739002932551319 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:28.360 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9289772727272727 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:29.372 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.907514450867052 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:30.384 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8666666666666667 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:31.393 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8472222222222223 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:32.442 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8820375335120644 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:33.446 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8735632183908046 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:34.457 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.800561797752809 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:35.469 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8559556786703602 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:36.511 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.903729055258467 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:37.513 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8997134670487106 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:38.515 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8732782369146006 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:39.517 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7427395858622673 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:40.519 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7430018493653084 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:41.521 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7403314917127072 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:42.556 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7616438356164383 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:43.558 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7375014738652133 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:44.564 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7323872361383598 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:45.566 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.731488103795106 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:46.567 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7314863142791641 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:47.575 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7325987966897136 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3261] - [INFO] 2024-05-05 01:48:48.388 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3261, processInstanceId=941, status=9, startTime=1714842212366, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, endTime=1714842223865, processId=18773, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844627410)
[WI-0][TI-3261] - [INFO] 2024-05-05 01:48:48.395 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3261, processInstanceId=941, status=9, startTime=1714842212366, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, endTime=1714842223865, processId=18773, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844928387)
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:48.576 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.731567836672073 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:49.578 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7318205560900906 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:50.965 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.730984852145222 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3260] - [INFO] 2024-05-05 01:48:51.399 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3260, processInstanceId=940, status=9, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, endTime=1714842227024, processId=18772, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844630425)
[WI-0][TI-3260] - [INFO] 2024-05-05 01:48:51.406 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3260, processInstanceId=940, status=9, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, endTime=1714842227024, processId=18772, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844931399)
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:51.966 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7309331550180115 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:52.975 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7311884592923896 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:53.977 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7316139664163531 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3259] - [INFO] 2024-05-05 01:48:54.409 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3259, processInstanceId=939, status=9, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, endTime=1714842229451, processId=18762, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844633441)
[WI-0][TI-3259] - [INFO] 2024-05-05 01:48:54.412 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3259, processInstanceId=939, status=9, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, endTime=1714842229451, processId=18762, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844934409)
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:54.978 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7319897647641528 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:55.979 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.731382920024743 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:56.985 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.731519917411851 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:57.989 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7315324440234442 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:58.991 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7320309236308165 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3191] - [INFO] 2024-05-05 01:48:59.416 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3191, processInstanceId=936, status=6, startTime=1714839942174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13377949373536/25/936/3191.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/936/3191, endTime=1714840005766, processId=14716, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844638497)
[WI-0][TI-3191] - [INFO] 2024-05-05 01:48:59.421 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3191, processInstanceId=936, status=6, startTime=1714839942174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13377949373536/25/936/3191.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/936/3191, endTime=1714840005766, processId=14716, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844939416)
[WI-0][TI-0] - [INFO] 2024-05-05 01:48:59.993 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7318048481168228 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:00.994 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313155149242647 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:02.001 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313155149242647 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:03.010 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7307062841635992 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:04.016 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7315069931300483 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:05.017 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7309299736563369 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:06.038 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313481238814282 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:07.039 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.73135289592394 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:08.040 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.728813559322034 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:09.045 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7479224376731302 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:10.051 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8810198300283286 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:11.066 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8538011695906432 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:12.071 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.711484593837535 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:13.087 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7323498551386846 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:14.088 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7312248461165416 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:15.092 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7312862861638803 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:16.099 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8787878787878788 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:17.102 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9217877094972067 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:18.132 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8972222222222221 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:19.133 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7339315883962219 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:20.141 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7339427231620826 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:21.142 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7341176980541798 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:22.144 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7337959828548466 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:23.157 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7318957157596505 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:24.159 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7314364066678954 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:25.161 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7320627372475614 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:26.162 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7320627372475614 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:27.164 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7319871799077922 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:28.167 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7325978025141904 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:29.169 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.73302271313284 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:30.171 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7330602929676199 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:31.176 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7325156836159675 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:32.180 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7314809457313384 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:33.190 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7322365191290306 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:34.193 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7324781037811875 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:35.194 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7327280395077399 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:36.195 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7330537314091663 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:37.196 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7318933297383946 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:38.197 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7324067219786161 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:39.198 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7326061535885859 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:40.199 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7329944785479788 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:41.202 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7321549967361217 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:42.204 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7319215643232557 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:43.219 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7320450409232471 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:44.220 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7327958422784276 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:45.222 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7308812590556963 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:46.224 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7308536209761491 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:47.227 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7312147055262042 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:48.230 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7318648963184289 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:49.232 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7321645408211452 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:50.234 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7316700379158662 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:51.236 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7319521849293728 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:52.238 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7322625665277406 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:53.248 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7324124881966511 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:54.252 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7312777362543801 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:55.253 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7312896663606594 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:56.254 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313147195838461 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:57.256 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7311045508782248 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:58.257 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7321561897467497 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:49:59.260 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7317789995532176 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:00.262 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7318557499036147 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3262] - [INFO] 2024-05-05 01:50:00.510 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3262, processInstanceId=942, status=9, startTime=1714842212466, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262, endTime=1714842296579, processId=18774, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844699583)
[WI-0][TI-3262] - [INFO] 2024-05-05 01:50:00.514 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3262, processInstanceId=942, status=9, startTime=1714842212466, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262, endTime=1714842296579, processId=18774, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714845000510)
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:01.263 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7323301704633236 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:02.270 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7323301704633236 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:03.279 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313463343654864 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:04.280 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7312948360733805 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:05.287 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7318086259838112 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:06.289 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7318086259838112 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:07.291 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7316535346021796 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:08.293 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7317380395216584 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:09.294 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7322514317618799 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:10.296 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7322629641979499 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:11.296 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7325886560993762 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:12.297 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7320366898488515 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:13.306 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7312618294460076 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:14.307 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7312689875097752 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:15.308 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7316694414105521 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:16.309 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7310798953252475 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:17.310 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7310912289262129 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:18.321 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313662178759519 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:19.326 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313660190408473 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:20.328 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7321299435129351 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:21.331 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7321796522890991 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:22.336 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7326669971306106 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:23.346 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7317485777822051 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:24.348 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313157137593693 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:25.350 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7308754928376613 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:26.352 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7315642576401892 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:27.354 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7316268906981558 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:28.355 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313855048811035 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:29.357 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7316736169477499 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:31.033 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7320221748862117 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:32.034 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7310532514212236 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:33.036 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.731064585022189 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:34.051 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7315897085335852 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:35.052 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7310103030386179 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:36.053 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.731799479568997 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:37.056 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7318120061805904 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:38.058 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7325687725889106 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:39.060 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7320927613483644 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:40.061 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7328059828687651 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:41.063 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7328061817038697 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:42.064 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7318748380736616 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:43.066 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7327757599328574 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:44.077 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7312180857229833 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:45.083 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7316294755545163 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:46.084 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7324562319196753 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:47.089 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7332825906146252 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:48.092 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7321925765709018 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:49.093 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7323761013724991 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:50.094 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.73278868421466 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:51.095 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7320170051734906 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:52.097 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7322792686765317 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:53.098 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.731352299418626 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:54.106 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7317634904150544 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:55.108 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7323898209947204 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:56.109 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7317366476759258 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:57.112 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7314290497690232 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:58.114 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7314276579232906 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:50:59.117 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.732139288762854 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:00.118 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7325763283228875 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:01.119 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7320500118008635 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:02.120 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7320498129657588 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:03.121 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7310079170173621 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:04.131 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313811305088012 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:05.132 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7323955872127553 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:06.133 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7325572401528405 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:07.134 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7331459908977266 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3263] - [INFO] 2024-05-05 01:51:08.032 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3263, processInstanceId=943, status=9, startTime=1714842505551, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/943/3263.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263, endTime=1714842665300, processId=20088, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844767043)
[WI-0][TI-3263] - [INFO] 2024-05-05 01:51:08.035 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3263, processInstanceId=943, status=9, startTime=1714842505551, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/943/3263.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263, endTime=1714842665300, processId=20088, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714845068032)
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:08.136 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.733156131488064 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:09.140 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.733293128875172 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:10.146 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7331424118658427 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:11.147 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7323788850639643 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:12.149 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7314994373960714 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:13.149 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7314988408907575 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:14.161 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7329873204842112 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:15.161 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7331607046954711 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:16.162 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7335985395959234 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:17.164 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7335975454204 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:18.165 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7334448400600243 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:19.166 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7334663142513271 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:20.168 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7326590437264243 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:21.170 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7329761857183504 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:22.184 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7330137655531305 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:23.185 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7323983709042206 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:24.198 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.732396382553174 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:25.201 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7329330385006402 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:26.202 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7330831590046554 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:27.204 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.732530993919026 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:28.206 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7325299997435027 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:29.207 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7315837434804454 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:30.208 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7321044926195391 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:31.211 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7327682041988804 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:32.213 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7330555209251082 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:33.216 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7334422552036638 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:34.225 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7334420563685592 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:35.226 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7324699515418966 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:36.229 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7330819659940274 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:37.229 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7333732594223482 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:38.230 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7328334221132076 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:39.233 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7328334221132076 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:40.236 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7321683186881337 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:41.237 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7319895659290481 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:42.240 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7329161375167444 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:43.241 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7329161375167444 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:44.263 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7322973626710554 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:45.266 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7323930023563948 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:46.268 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7321031007738066 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:47.269 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7325154847808628 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:48.275 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7329151433412211 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:49.276 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7324516587122682 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:50.278 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7320961415451436 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:51.279 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.732095942710039 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:52.285 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7312705781906125 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:53.286 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313574691313471 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:54.296 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7307227874772856 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:55.297 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7309456816296048 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:56.298 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7305082443993619 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:57.302 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7307673265407285 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:58.302 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7307794554821125 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:51:59.304 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7314421728859305 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:00.307 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7314640447474426 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:01.312 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.731738834862077 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:02.313 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7318257258028117 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:03.314 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7311150891387715 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:04.326 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7311065392292714 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:05.327 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7315324440234442 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:06.329 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7318328838665793 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:07.330 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7315179290608044 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:08.332 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7310993811655038 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:09.333 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7316002467941319 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:10.339 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7316000479590272 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:11.341 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7317489754524145 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:12.344 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7309492606614886 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:13.345 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313248601741835 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:14.353 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7314739865026755 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:15.355 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7320001041895948 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:16.358 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.73215022469361 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:17.362 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7311445167342606 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:18.365 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7316940969635295 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:19.367 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7316940969635295 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:20.368 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.730425528995825 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:21.369 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7304072361661967 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:22.370 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313463343654864 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:23.371 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313574691313471 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:24.384 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7307880053916127 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:25.385 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7312385657387629 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:26.385 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.731082480181608 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:27.388 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7311568445107494 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:28.389 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7307050911529712 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:29.390 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7307176177645646 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:30.391 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7314045930511506 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:31.393 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.731642399836319 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:32.394 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7316771959796338 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:33.398 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7318817973023246 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:34.412 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7311578386862726 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:35.415 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7311618153883657 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:36.416 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.73154934500734 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:37.420 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7309096924756621 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:38.422 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.731530256837293 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:39.423 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7309303713265463 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:40.424 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7309301724914417 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:41.426 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7321327272044003 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:42.431 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7321198029225977 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:43.433 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7308116667690667 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:44.448 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7311866697764478 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:45.450 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7322365191290306 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:46.457 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.732324205410184 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:47.458 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7321774651029479 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:48.461 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7321774651029479 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:49.462 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7321774651029479 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:50.464 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7321774651029479 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:51.468 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7322397004907052 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:52.469 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7308570011729283 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:53.471 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7306675113181913 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:54.484 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7310792988199335 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:55.485 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313797386630686 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:56.486 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7314761736888267 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:57.492 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7316620845116799 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:58.493 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7318863705097317 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:52:59.494 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7318863705097317 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:00.496 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7317871517925084 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:01.497 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7317911284946015 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:02.498 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7317010561921924 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:03.500 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7318889553660922 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:04.510 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7319627231899195 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:05.512 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7325013674884323 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:06.513 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7325013674884323 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:07.516 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7315688308475963 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:08.517 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7318806042916967 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:09.518 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7308756916727659 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:10.525 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7315769830868871 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:11.525 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7315765854166778 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:12.527 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7306285396376787 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:13.528 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7309265934595578 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:14.537 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7318390477548236 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:15.539 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.731824930462393 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3258] - [INFO] 2024-05-05 01:53:16.177 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3258, processInstanceId=938, status=9, startTime=1714841447434, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, endTime=1714842191466, processId=18118, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844895276)
[WI-0][TI-3258] - [INFO] 2024-05-05 01:53:16.180 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3258, processInstanceId=938, status=9, startTime=1714841447434, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, endTime=1714842191466, processId=18118, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714845196177)
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:16.541 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.731824930462393 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:17.543 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7305026770164316 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:18.545 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7314149324765926 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:19.550 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7304559507668374 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:20.552 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7304684773784307 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:21.554 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7303062279330316 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:22.558 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.730906908784197 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:23.564 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7315403974276304 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:24.581 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7319535767751054 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:25.584 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.73195377561021 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:26.589 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7319285235519187 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:27.592 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7322404958311238 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:28.594 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.730996384581292 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:29.598 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7312749525629149 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:30.600 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7308271759072299 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:31.602 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7308518314602073 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:32.604 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.730771104407717 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:33.608 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.731021238969374 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:34.623 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7314847235983268 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:35.627 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7314843259281175 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:36.634 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7314843259281175 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:37.637 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7057142857142857 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:38.639 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7305824098933985 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:39.640 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7312572562386005 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:40.645 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7314071779075111 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:41.649 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7316451835277841 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:42.653 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7309482664859653 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:43.656 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313453401899631 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:44.669 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7318714578768825 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:45.670 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7318829903129526 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:46.674 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313767561364987 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:47.687 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7315730063847941 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:48.700 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7321114518482021 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3261] - [INFO] 2024-05-05 01:53:49.240 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3261, processInstanceId=941, status=9, startTime=1714842212366, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, endTime=1714842223865, processId=18773, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844928387)
[WI-0][TI-3261] - [INFO] 2024-05-05 01:53:49.248 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3261, processInstanceId=941, status=9, startTime=1714842212366, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, endTime=1714842223865, processId=18773, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714845229240)
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:49.701 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7322212088259722 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:50.703 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7322951754849042 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:51.704 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7478753541076487 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3260] - [INFO] 2024-05-05 01:53:52.254 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3260, processInstanceId=940, status=9, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, endTime=1714842227024, processId=18772, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844931399)
[WI-0][TI-3260] - [INFO] 2024-05-05 01:53:52.261 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3260, processInstanceId=940, status=9, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, endTime=1714842227024, processId=18772, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714845232254)
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:52.707 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313847095406849 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:54.141 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7315443741297236 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3259] - [INFO] 2024-05-05 01:53:55.174 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3259, processInstanceId=939, status=9, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, endTime=1714842229451, processId=18762, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844934409)
[WI-0][TI-3259] - [INFO] 2024-05-05 01:53:55.186 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3259, processInstanceId=939, status=9, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, endTime=1714842229451, processId=18762, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714845235174)
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:55.200 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.73184401863244 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:56.208 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7323568143673475 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:57.210 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7317575253619147 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:58.214 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7317271035909023 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:53:59.224 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.730114153221934 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3191] - [INFO] 2024-05-05 01:54:00.197 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3191, processInstanceId=936, status=6, startTime=1714839942174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13377949373536/25/936/3191.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/936/3191, endTime=1714840005766, processId=14716, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714844939416)
[WI-0][TI-3191] - [INFO] 2024-05-05 01:54:00.206 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3191, processInstanceId=936, status=6, startTime=1714839942174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13377949373536/25/936/3191.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/936/3191, endTime=1714840005766, processId=14716, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714845240196)
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:00.228 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8662790697674418 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:01.230 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7817109144542774 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:02.232 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7316630786872031 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:03.233 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7306581660682724 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:04.234 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7306581660682724 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:05.246 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7306693008341332 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:06.249 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7310142797407111 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:07.250 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7310142797407111 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:08.252 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313139242434274 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:09.254 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7314381961838373 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:10.255 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7309627814486052 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:11.265 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7313884874076734 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:12.272 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.730581018047666 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:13.273 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.730581018047666 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:14.277 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7303646854538004 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:15.290 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7309136691777552 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:16.293 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7304366637616858 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:17.295 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7301133578815153 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:18.296 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7301125625410967 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:19.300 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.730776274120438 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:20.301 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7312395599142861 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:21.302 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7311763303510056 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:22.303 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7306074631165852 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:23.306 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.731420499859523 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:24.308 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.731470208635687 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:25.319 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7303547436985676 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:26.325 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7305593450212584 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:27.334 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7276635105570505 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:28.335 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7276722593016554 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:29.336 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7269252358134632 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:30.338 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7265154366627675 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:31.342 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7272101665184351 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:32.343 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.727673054642074 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:33.344 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7268711526649968 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:34.347 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7276507851103525 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:35.359 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7279858222616976 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:36.360 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7275092145158375 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:37.361 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7276959206791094 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:38.364 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.727211359529063 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:39.365 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7275619058185713 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:40.365 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7284009899602191 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:41.367 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.728537987347327 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:43.162 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.728062970282304 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:44.164 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7282379451744012 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:45.165 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.727474816042732 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:46.180 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7279108614272424 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:47.185 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7282095117544355 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:48.193 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7280206184050124 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:49.194 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7279080777357771 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:50.195 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7279070835602539 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:51.199 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7277802267634834 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:52.201 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7283268244661825 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:53.215 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7278337134066358 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:54.236 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8611111111111112 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:55.237 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.825136612021858 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:56.249 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7285381861824316 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:57.252 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7288533398233111 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:58.253 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7288527433179972 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:54:59.255 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.729014595093187 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:00.259 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.729014595093187 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3262] - [INFO] 2024-05-05 01:55:01.152 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3262, processInstanceId=942, status=9, startTime=1714842212466, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262, endTime=1714842296579, processId=18774, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714845000510)
[WI-0][TI-3262] - [INFO] 2024-05-05 01:55:01.161 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3262, processInstanceId=942, status=9, startTime=1714842212466, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/942/3262.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/942/3262, endTime=1714842296579, processId=18774, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714845301152)
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:01.260 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.728309724647182 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:02.261 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7271672181358294 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:03.262 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7271674169709341 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:04.263 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7289251192960919 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:05.264 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7289251192960919 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:06.277 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7285825264107698 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:07.278 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7287569047975531 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:08.280 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.728083847968293 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:09.281 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7283321930140081 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:10.283 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7288593048764508 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:11.284 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7288591060413462 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:12.285 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7288205320310429 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:13.287 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7289193530780569 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:14.296 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7283061456152982 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:15.297 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7279597748629877 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:16.304 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7282101082597494 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:17.305 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7283944284017654 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:18.307 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7283942295666608 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:19.310 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7273594916820316 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:20.312 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7274716346810576 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:21.314 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7280715201918043 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:22.315 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7284337977524873 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:23.319 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.727935318145115 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:24.322 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7280416949261059 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:25.323 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7286051936127006 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:26.330 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.728604994777596 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:27.332 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.728641381601748 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:28.334 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7280428879367339 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:29.336 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7279726991447903 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:30.338 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7280343380272336 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:31.339 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7279935768307791 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:32.367 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7280687365003391 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:33.368 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7276138017808865 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:34.370 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7276134041106772 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:35.371 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7281164569254566 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:36.379 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7281136732339913 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:37.381 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.728297595705798 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:38.381 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.72844672203429 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:39.383 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7285719881502231 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:40.385 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7292732795643443 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:41.386 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7280912048671653 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:42.387 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7286045971073867 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:43.389 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7279772723521974 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:44.390 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7273364268098915 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:45.392 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7282005641747259 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:46.400 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.728726284191436 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:47.401 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7287871277334608 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:48.403 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7282264127383312 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:49.404 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7282264127383312 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:50.405 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7280548180430132 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:51.410 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7280607830961529 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:52.412 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7284477162098132 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:53.413 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7284964308104539 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:54.416 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7285799415544093 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:55.418 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7279899977988954 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:56.430 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7283280174768103 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:57.432 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.728340345253299 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:58.433 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7285002086774424 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:55:59.435 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7278245669918217 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:00.437 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.72817511328133 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:01.438 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7289998812954426 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:02.439 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7290004778007565 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:03.441 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.729260156447437 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:04.442 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7278693048903693 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:05.445 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7272105641886444 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:06.452 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7272105641886444 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:07.453 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7263269409835538 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3263] - [INFO] 2024-05-05 01:56:08.226 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3263, processInstanceId=943, status=9, startTime=1714842505551, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/943/3263.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263, endTime=1714842665300, processId=20088, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714845068032)
[WI-0][TI-3263] - [INFO] 2024-05-05 01:56:08.233 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3263, processInstanceId=943, status=9, startTime=1714842505551, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/943/3263.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/943/3263, endTime=1714842665300, processId=20088, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714845368226)
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:08.456 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7268242275802981 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:09.457 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7274255049367774 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:10.458 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7274253061016728 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:11.459 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7268331751600076 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:12.460 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7257141311910044 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:13.462 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7269771317757784 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:14.469 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7269771317757784 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:15.473 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7267429040224939 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:16.481 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7265824440930365 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:17.484 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7270180918073376 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:18.486 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.726861807415078 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:19.487 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7270371799773845 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:20.488 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7265188168595467 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:21.489 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7272950691081231 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:22.490 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7275201504465936 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:23.492 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7275195539412797 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:24.493 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7277541793647735 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:25.495 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7275690638823389 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:26.503 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7278480295341712 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:27.505 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7278476318639618 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:28.508 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7281596041431668 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:29.514 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7281596041431668 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:30.517 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7278040869760422 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:31.524 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7279325344536498 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:32.525 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7275471920208267 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:33.526 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.727543016483629 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:34.526 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.728206330392761 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:35.527 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7273024260069955 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:36.541 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.727439025723894 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:37.542 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.72806535630356 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:38.544 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7280643621280367 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:39.545 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.727928955421766 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:40.547 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7279291542568707 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:41.548 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.727377188006346 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:42.549 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.727377188006346 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:43.550 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7281427031592711 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:44.560 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7281520484091899 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:45.561 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7283268244661825 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:46.573 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7273451755544964 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:47.574 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7276082343979562 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:48.577 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7276072402224328 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:49.578 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7269463123345568 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:50.584 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7270087465574188 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:51.585 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7279213996877891 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:52.587 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7276466095731547 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:53.612 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7281582122974343 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:54.615 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7281826690153069 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:55.617 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7281818736748883 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:56.624 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7281868445525047 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:57.626 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7281989734938887 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:58.628 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7278343099119499 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:56:59.633 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7280820584523511 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:00.635 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7278808373264393 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:01.637 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7283063444504029 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:02.638 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7274420082504638 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:03.641 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7274420082504638 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:04.642 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7282148803022612 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:05.643 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7273984633625442 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:06.666 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7276647035676784 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:07.670 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7278020986249956 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:08.676 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7275227353029541 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:09.676 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7276893591206557 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:10.677 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7283433277798689 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:11.679 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.728340345253299 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:12.681 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7275658825206645 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:13.682 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7275605139728387 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:14.683 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.728573777666165 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:15.685 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7274626871013481 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:16.692 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7274626871013481 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:17.694 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7276875696047138 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:18.695 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7269693772066969 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:19.696 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7262776298775991 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:20.697 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7270401625039543 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:21.698 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7278042858111469 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:22.699 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7266285738373167 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:23.701 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7256493109468866 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:24.702 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.725666410765887 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:25.703 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7249154105756018 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:26.718 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.725874392285357 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:27.720 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8021978021978022 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:28.724 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.727352333618264 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:29.724 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7263931530734041 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:30.726 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8048780487804879 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:31.733 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7274189433783238 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:32.735 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7276631128868412 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:33.736 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7273612811979735 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:34.737 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7264957519874066 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:35.742 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7275764207812112 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:36.764 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7278750711084043 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:37.768 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7300275482093663 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:38.776 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8633879781420765 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:39.778 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8892045454545454 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:40.780 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8739726027397261 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:41.782 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7833333333333333 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:42.783 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7375140004768066 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:43.785 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7375939321888783 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:44.786 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7389559526557709 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:45.789 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.938375350140056 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:46.867 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8590078328981724 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:47.877 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9297297297297298 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:48.881 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9289617486338798 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:49.895 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8005464480874317 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:50.896 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7416692564939048 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:51.897 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7415626808778093 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:52.900 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.740628553556136 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:53.902 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7409099052292241 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:54.905 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7403973083294213 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:55.909 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7403342776012454 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:56.918 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7393945828966417 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:57.920 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7397196782927541 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:58.926 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7391480273668685 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:57:59.928 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7391889873984275 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:00.930 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.739550469618692 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:01.933 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7400006322956328 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:02.934 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7397244503352658 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:03.936 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7399980474392723 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:04.937 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7391522029040662 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:05.937 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7387901241784879 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:06.947 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7391267520106702 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:07.948 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7395647857462272 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:08.949 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7397880775687556 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:09.950 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7392392926799055 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:10.952 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7392390938448008 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:11.954 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7400531247632619 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:12.962 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7400531247632619 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:13.965 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7400286680453892 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:14.966 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7398781498711648 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:15.974 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9008264462809916 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3258] - [INFO] 2024-05-05 01:58:16.441 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3258, processInstanceId=938, status=9, startTime=1714841447434, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, endTime=1714842191466, processId=18118, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714845196177)
[WI-0][TI-3258] - [INFO] 2024-05-05 01:58:16.453 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3258, processInstanceId=938, status=9, startTime=1714841447434, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/938/3258.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/938/3258, endTime=1714842191466, processId=18118, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714845496441)
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:16.995 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8810810810810811 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:18.010 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7408327572086176 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:19.015 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9502762430939227 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:20.024 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8793103448275862 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:21.029 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7376086459866228 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:22.030 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7376209737631114 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:23.031 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7382590356139521 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:24.031 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7373390255847094 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:25.036 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7382655971724057 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:26.037 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7638888888888888 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:27.059 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8306010928961749 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:28.061 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8733153638814016 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:29.063 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8801089918256132 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:30.067 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.92090395480226 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:31.070 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7471264367816092 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:32.074 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8967391304347826 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:33.075 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9002849002849003 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:34.076 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7340685857833298 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:35.083 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7345461877047131 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:36.089 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7345461877047131 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:37.106 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7347841933249862 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:38.112 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7335591702452015 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:39.114 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.733934968593001 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:40.115 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7341473244847735 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:41.120 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7331515582806569 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:42.123 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.733339258619452 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:43.125 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7338888388487209 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:44.126 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7338890376838255 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:45.134 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7340266315762474 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:46.136 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7342000157875073 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:47.144 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.734525508853829 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:48.148 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7336379089466452 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:49.150 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7272966597889604 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3261] - [INFO] 2024-05-05 01:58:49.497 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3261, processInstanceId=941, status=9, startTime=1714842212366, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, endTime=1714842223865, processId=18773, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714845229240)
[WI-0][TI-3261] - [INFO] 2024-05-05 01:58:49.502 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3261, processInstanceId=941, status=9, startTime=1714842212366, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/941/3261.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/941/3261, endTime=1714842223865, processId=18773, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714845529497)
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:50.152 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7273328477780078 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:51.154 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7254437154486724 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:52.155 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7252043179826667 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3260] - [INFO] 2024-05-05 01:58:52.504 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3260, processInstanceId=940, status=9, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, endTime=1714842227024, processId=18772, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714845232254)
[WI-0][TI-3260] - [INFO] 2024-05-05 01:58:52.506 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3260, processInstanceId=940, status=9, startTime=1714842212222, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/940/3260.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/940/3260, endTime=1714842227024, processId=18772, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714845532504)
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:53.158 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7259415985507307 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:54.159 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.726077204092106 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:55.160 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7264649325461849 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3259] - [INFO] 2024-05-05 01:58:55.516 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3259, processInstanceId=939, status=9, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, endTime=1714842229451, processId=18762, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714845235174)
[WI-0][TI-3259] - [INFO] 2024-05-05 01:58:55.526 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3259, processInstanceId=939, status=9, startTime=1714842211991, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13386218435520/20/939/3259.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13386218435520_20/939/3259, endTime=1714842229451, processId=18762, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714845535515)
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:56.161 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7249261476712532 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:57.170 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7249261476712532 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:58.171 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7257393832492957 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:58:59.172 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7259879271301155 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:00.173 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.72659198817806 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-3191] - [INFO] 2024-05-05 01:59:00.530 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3191, processInstanceId=936, status=6, startTime=1714839942174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13377949373536/25/936/3191.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/936/3191, endTime=1714840005766, processId=14716, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714845240196)
[WI-0][TI-3191] - [INFO] 2024-05-05 01:59:00.533 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3191, processInstanceId=936, status=6, startTime=1714839942174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240505/13377949373536/25/936/3191.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_25/936/3191, endTime=1714840005766, processId=14716, appIds=null, varPool=[], eventCreateTime=0, eventSendTime=1714845540530)
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:01.175 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7265915905078507 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:02.176 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7267784955062272 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:03.177 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7267900279422973 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:04.178 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7262390558672959 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:05.180 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.726238459361982 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:06.182 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7269021709413231 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:07.193 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7269144987178118 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:08.194 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7269258323187772 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:09.197 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7261052398418625 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:10.198 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7270803271950949 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:11.201 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7271544926891315 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:12.202 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7263096423294487 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:13.204 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7267852558997855 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:14.206 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7267971860060649 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:15.209 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7267969871709602 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:16.211 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7268954105477649 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:17.230 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7259964770396157 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:18.232 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.725875585295985 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:19.235 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7247227393591903 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:20.236 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7247227393591903 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:21.238 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7259618797314056 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:22.239 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7249521950699631 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:23.240 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7249394696232652 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:24.243 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7253383328432048 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:25.244 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7258515262483216 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:26.248 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7261895459262365 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:27.258 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7265899998270134 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:28.259 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7265899998270134 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:29.260 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7249166035862297 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:30.261 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7249164047511251 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:31.263 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7261686682402477 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:32.264 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7263434442972402 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:33.266 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7269310020314983 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:34.268 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.726923645132626 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:35.268 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7263317130260655 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:36.273 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7256534864840843 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:37.289 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7259666517739173 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:38.290 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7254210482467416 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:39.291 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.725420849411637 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:40.292 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7260151675394534 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:41.294 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7264909799448949 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:42.295 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7257727875468779 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:43.296 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7257849164882618 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:44.299 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7266363284063982 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:45.303 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7270727714611179 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:46.305 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.726823631074984 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:47.328 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7259954828640924 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:48.329 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7268208473835189 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:49.330 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7269960211107207 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:50.331 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7267170554588885 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:51.332 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7269298090208703 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:52.333 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7277790337528556 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:53.334 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7283174792162636 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:54.336 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7275527594037572 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:55.337 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7265118576308837 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:56.338 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7265118576308837 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:57.352 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7263484151748566 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:58.354 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7267106927355396 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 01:59:59.356 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[65] - Worker OverLoad: the SystemMemoryUsedPercentage: 0.7273368244801008 is over then the MaxSystemMemoryUsagePercentageThresholds 0.7
