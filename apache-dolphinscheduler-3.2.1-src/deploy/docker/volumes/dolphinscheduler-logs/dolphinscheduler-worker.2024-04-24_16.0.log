[WI-0][TI-0] - [INFO] 2024-04-24 16:00:30.580 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7057057057057057 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:03:59.135 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7761194029850746 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:04:24.292 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7970149253731343 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [WARN] 2024-04-24 16:11:40.120 +0800 o.s.c.k.c.p.AbstractKubernetesProfileEnvironmentPostProcessor:[258] - Not running inside kubernetes. Skipping 'kubernetes' profile activation.
[WI-0][TI-0] - [WARN] 2024-04-24 16:11:43.755 +0800 o.s.c.k.c.p.AbstractKubernetesProfileEnvironmentPostProcessor:[258] - Not running inside kubernetes. Skipping 'kubernetes' profile activation.
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:43.756 +0800 o.a.d.s.w.WorkerServer:[634] - No active profile set, falling back to 1 default profile: "default"
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:51.734 +0800 o.s.c.c.s.GenericScope:[283] - BeanFactory id=7e7bf7c6-2cb3-34d2-a0d5-a56161c67d0e
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:53.308 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'org.springframework.cloud.commons.config.CommonsConfigAutoConfiguration' of type [org.springframework.cloud.commons.config.CommonsConfigAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:53.321 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'org.springframework.cloud.client.loadbalancer.LoadBalancerDefaultMappingsProviderAutoConfiguration' of type [org.springframework.cloud.client.loadbalancer.LoadBalancerDefaultMappingsProviderAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:53.341 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'loadBalancerClientsDefaultsMappingsProvider' of type [org.springframework.cloud.client.loadbalancer.LoadBalancerDefaultMappingsProviderAutoConfiguration$$Lambda$392/302905744] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:53.361 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'defaultsBindHandlerAdvisor' of type [org.springframework.cloud.commons.config.DefaultsBindHandlerAdvisor] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:53.441 +0800 o.a.d.c.u.NetUtils:[312] - Get all NetworkInterfaces: [name:eth0 (eth0), name:lo (lo)]
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:53.476 +0800 o.a.d.s.w.c.WorkerConfig:[98] - 
****************************Worker Configuration**************************************
  listen-port -> 1234
  exec-threads -> 100
  max-heartbeat-interval -> PT10S
  host-weight -> 100
  tenantConfig -> TenantConfig(autoCreateTenantEnabled=true, distributedTenantEnabled=false, defaultTenantEnabled=false)
  server-load-protection -> WorkerServerLoadProtection(enabled=true, maxCpuUsagePercentageThresholds=0.7, maxJVMMemoryUsagePercentageThresholds=0.7, maxSystemMemoryUsagePercentageThresholds=0.7, maxDiskUsagePercentageThresholds=0.7)
  registry-disconnect-strategy -> ConnectStrategyProperties(strategy=WAITING, maxWaitingTime=PT1M40S)
  task-execute-threads-full-policy: REJECT
  address -> 172.18.1.1:1234
  registry-path: /nodes/worker/172.18.1.1:1234
****************************Worker Configuration**************************************
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:53.477 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'workerConfig' of type [org.apache.dolphinscheduler.server.worker.config.WorkerConfig$$EnhancerBySpringCGLIB$$da954724] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:53.912 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'io.kubernetes.client.spring.extended.manifests.config.KubernetesManifestsAutoConfiguration' of type [io.kubernetes.client.spring.extended.manifests.config.KubernetesManifestsAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:53.939 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'kubernetes.manifests-io.kubernetes.client.spring.extended.manifests.config.KubernetesManifestsProperties' of type [io.kubernetes.client.spring.extended.manifests.config.KubernetesManifestsProperties] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:54.022 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'io.kubernetes.client.spring.extended.controller.config.KubernetesInformerAutoConfiguration' of type [io.kubernetes.client.spring.extended.controller.config.KubernetesInformerAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:54.080 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'defaultApiClient' of type [io.kubernetes.client.openapi.ApiClient] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:54.538 +0800 o.e.j.u.log:[170] - Logging initialized @27543ms to org.eclipse.jetty.util.log.Slf4jLog
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:56.808 +0800 o.s.b.w.e.j.JettyServletWebServerFactory:[166] - Server initialized with port: 1235
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:56.849 +0800 o.e.j.s.Server:[375] - jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 1.8.0_402-b06
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:57.400 +0800 o.e.j.s.h.C.application:[2368] - Initializing Spring embedded WebApplicationContext
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:57.415 +0800 o.s.b.w.s.c.ServletWebServerApplicationContext:[292] - Root WebApplicationContext: initialization completed in 13635 ms
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:58.935 +0800 o.e.j.s.session:[334] - DefaultSessionIdManager workerName=node0
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:58.937 +0800 o.e.j.s.session:[339] - No SessionScavenger set, using defaults
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:58.940 +0800 o.e.j.s.session:[132] - node0 Scavenging every 600000ms
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:58.978 +0800 o.e.j.s.h.ContextHandler:[921] - Started o.s.b.w.e.j.JettyEmbeddedWebAppContext@52433946{application,/,[file:///tmp/jetty-docbase.1235.3159466739913789096/],AVAILABLE}
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:58.987 +0800 o.e.j.s.Server:[415] - Started @31993ms
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.495 +0800 o.a.c.f.i.CuratorFrameworkImpl:[338] - Starting
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.504 +0800 o.a.z.ZooKeeper:[98] - Client environment:zookeeper.version=3.8.0-5a02a05eddb59aee6ac762f7ea82e92a68eb9c0f, built on 2022-02-25 08:49 UTC
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.514 +0800 o.a.z.ZooKeeper:[98] - Client environment:host.name=1036ec04ddf9
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.516 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.version=1.8.0_402
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.516 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.vendor=Temurin
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.516 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.home=/opt/java/openjdk
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.517 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.class.path=/opt/dolphinscheduler/conf:/opt/dolphinscheduler/libs/kerb-client-1.0.1.jar:/opt/dolphinscheduler/libs/spring-cloud-starter-kubernetes-client-config-2.1.3.jar:/opt/dolphinscheduler/libs/oauth2-oidc-sdk-9.35.jar:/opt/dolphinscheduler/libs/jsqlparser-4.4.jar:/opt/dolphinscheduler/libs/reactive-streams-1.0.4.jar:/opt/dolphinscheduler/libs/kubernetes-model-extensions-5.10.2.jar:/opt/dolphinscheduler/libs/zookeeper-3.8.0.jar:/opt/dolphinscheduler/libs/kubernetes-model-apps-5.10.2.jar:/opt/dolphinscheduler/libs/grpc-api-1.41.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-pigeon-3.2.1.jar:/opt/dolphinscheduler/libs/client-java-api-13.0.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-obs-3.2.1.jar:/opt/dolphinscheduler/libs/spring-web-5.3.22.jar:/opt/dolphinscheduler/libs/aws-java-sdk-emr-1.12.300.jar:/opt/dolphinscheduler/libs/spring-context-5.3.22.jar:/opt/dolphinscheduler/libs/gapic-google-cloud-storage-v2-2.18.0-alpha.jar:/opt/dolphinscheduler/libs/spring-cloud-kubernetes-client-config-2.1.3.jar:/opt/dolphinscheduler/libs/jetty-io-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/annotations-13.0.jar:/opt/dolphinscheduler/libs/jpam-1.1.jar:/opt/dolphinscheduler/libs/joda-time-2.10.13.jar:/opt/dolphinscheduler/libs/spring-cloud-kubernetes-client-autoconfig-2.1.3.jar:/opt/dolphinscheduler/libs/kerb-identity-1.0.1.jar:/opt/dolphinscheduler/libs/vertica-jdbc-12.0.4-0.jar:/opt/dolphinscheduler/libs/grpc-googleapis-1.52.1.jar:/opt/dolphinscheduler/libs/aliyun-java-sdk-kms-2.11.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-dvc-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-hana-3.2.1.jar:/opt/dolphinscheduler/libs/kubernetes-httpclient-okhttp-6.0.0.jar:/opt/dolphinscheduler/libs/jline-2.12.jar:/opt/dolphinscheduler/libs/sshd-core-2.8.0.jar:/opt/dolphinscheduler/libs/jna-platform-5.10.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-sqlserver-3.2.1.jar:/opt/dolphinscheduler/libs/commons-beanutils-1.9.4.jar:/opt/dolphinscheduler/libs/grpc-services-1.41.0.jar:/opt/dolphinscheduler/libs/jmespath-java-1.12.300.jar:/opt/dolphinscheduler/libs/curator-client-5.3.0.jar:/opt/dolphinscheduler/libs/proto-google-common-protos-2.0.1.jar:/opt/dolphinscheduler/libs/mybatis-3.5.10.jar:/opt/dolphinscheduler/libs/jackson-annotations-2.13.4.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-all-3.2.1.jar:/opt/dolphinscheduler/libs/jakarta.xml.bind-api-2.3.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-jupyter-3.2.1.jar:/opt/dolphinscheduler/libs/mybatis-plus-extension-3.5.2.jar:/opt/dolphinscheduler/libs/j2objc-annotations-1.3.jar:/opt/dolphinscheduler/libs/Java-WebSocket-1.5.1.jar:/opt/dolphinscheduler/libs/azure-storage-common-12.20.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-sagemaker-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-databend-3.2.1.jar:/opt/dolphinscheduler/libs/google-auth-library-oauth2-http-1.15.0.jar:/opt/dolphinscheduler/libs/jetty-security-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-python-3.2.1.jar:/opt/dolphinscheduler/libs/snappy-java-1.1.10.1.jar:/opt/dolphinscheduler/libs/kubernetes-model-batch-5.10.2.jar:/opt/dolphinscheduler/libs/netty-transport-native-epoll-4.1.53.Final-linux-x86_64.jar:/opt/dolphinscheduler/libs/jackson-core-asl-1.9.13.jar:/opt/dolphinscheduler/libs/gson-fire-1.8.5.jar:/opt/dolphinscheduler/libs/clickhouse-jdbc-0.4.6.jar:/opt/dolphinscheduler/libs/grpc-netty-shaded-1.41.0.jar:/opt/dolphinscheduler/libs/caffeine-2.9.3.jar:/opt/dolphinscheduler/libs/aliyun-java-sdk-core-4.5.10.jar:/opt/dolphinscheduler/libs/jackson-module-jaxb-annotations-2.13.3.jar:/opt/dolphinscheduler/libs/jetty-http-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/jersey-servlet-1.19.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-dinky-3.2.1.jar:/opt/dolphinscheduler/libs/simpleclient_tracer_common-0.15.0.jar:/opt/dolphinscheduler/libs/netty-handler-4.1.53.Final.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-datafactory-3.2.1.jar:/opt/dolphinscheduler/libs/json-path-2.7.0.jar:/opt/dolphinscheduler/libs/mybatis-plus-annotation-3.5.2.jar:/opt/dolphinscheduler/libs/azure-resourcemanager-datafactory-1.0.0-beta.19.jar:/opt/dolphinscheduler/libs/commons-codec-1.11.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-master-3.2.1.jar:/opt/dolphinscheduler/libs/spring-aop-5.3.22.jar:/opt/dolphinscheduler/libs/kubernetes-model-core-5.10.2.jar:/opt/dolphinscheduler/libs/kubernetes-model-networking-5.10.2.jar:/opt/dolphinscheduler/libs/kotlin-stdlib-jdk7-1.6.21.jar:/opt/dolphinscheduler/libs/kerby-xdr-1.0.1.jar:/opt/dolphinscheduler/libs/aliyun-java-sdk-ram-3.1.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-k8s-3.2.1.jar:/opt/dolphinscheduler/libs/guava-31.1-jre.jar:/opt/dolphinscheduler/libs/netty-codec-dns-4.1.53.Final.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-kubeflow-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-azure-sql-3.2.1.jar:/opt/dolphinscheduler/libs/HikariCP-4.0.3.jar:/opt/dolphinscheduler/libs/hive-metastore-2.3.9.jar:/opt/dolphinscheduler/libs/msal4j-1.13.3.jar:/opt/dolphinscheduler/libs/jackson-core-2.13.4.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-hive-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-mr-3.2.1.jar:/opt/dolphinscheduler/libs/kubernetes-model-node-5.10.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-common-3.2.1.jar:/opt/dolphinscheduler/libs/jose4j-0.7.8.jar:/opt/dolphinscheduler/libs/jul-to-slf4j-1.7.36.jar:/opt/dolphinscheduler/libs/azure-identity-1.7.1.jar:/opt/dolphinscheduler/libs/spring-boot-autoconfigure-2.7.3.jar:/opt/dolphinscheduler/libs/commons-math3-3.1.1.jar:/opt/dolphinscheduler/libs/jackson-jaxrs-json-provider-2.13.3.jar:/opt/dolphinscheduler/libs/perfmark-api-0.23.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-doris-3.2.1.jar:/opt/dolphinscheduler/libs/httpclient-4.5.13.jar:/opt/dolphinscheduler/libs/hive-service-2.3.9.jar:/opt/dolphinscheduler/libs/kerby-util-1.0.1.jar:/opt/dolphinscheduler/libs/commons-collections-3.2.2.jar:/opt/dolphinscheduler/libs/hadoop-yarn-client-3.2.4.jar:/opt/dolphinscheduler/libs/commons-text-1.8.jar:/opt/dolphinscheduler/libs/dolphinscheduler-worker-3.2.1.jar:/opt/dolphinscheduler/libs/hadoop-yarn-common-3.2.4.jar:/opt/dolphinscheduler/libs/zeppelin-client-0.10.1.jar:/opt/dolphinscheduler/libs/azure-core-management-1.10.1.jar:/opt/dolphinscheduler/libs/hadoop-mapreduce-client-jobclient-3.2.4.jar:/opt/dolphinscheduler/libs/jta-1.1.jar:/opt/dolphinscheduler/libs/annotations-4.1.1.4.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-alert-3.2.1.jar:/opt/dolphinscheduler/libs/curator-framework-5.3.0.jar:/opt/dolphinscheduler/libs/hive-jdbc-2.3.9.jar:/opt/dolphinscheduler/libs/kyuubi-hive-jdbc-shaded-1.7.0.jar:/opt/dolphinscheduler/libs/client-java-proto-13.0.2.jar:/opt/dolphinscheduler/libs/checker-qual-3.19.0.jar:/opt/dolphinscheduler/libs/jetty-servlet-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/google-cloud-core-grpc-2.10.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-shell-3.2.1.jar:/opt/dolphinscheduler/libs/spring-security-rsa-1.0.10.RELEASE.jar:/opt/dolphinscheduler/libs/avro-1.7.7.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-postgresql-3.2.1.jar:/opt/dolphinscheduler/libs/netty-nio-client-2.17.282.jar:/opt/dolphinscheduler/libs/spring-webmvc-5.3.22.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-mysql-3.2.1.jar:/opt/dolphinscheduler/libs/opentracing-util-0.33.0.jar:/opt/dolphinscheduler/libs/auto-value-1.10.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-all-3.2.1.jar:/opt/dolphinscheduler/libs/jetcd-core-0.5.11.jar:/opt/dolphinscheduler/libs/grpc-context-1.41.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-datasync-3.2.1.jar:/opt/dolphinscheduler/libs/jackson-dataformat-cbor-2.13.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-gcs-3.2.1.jar:/opt/dolphinscheduler/libs/client-java-extended-13.0.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-redshift-3.2.1.jar:/opt/dolphinscheduler/libs/opencsv-2.3.jar:/opt/dolphinscheduler/libs/jcip-annotations-1.0-1.jar:/opt/dolphinscheduler/libs/accessors-smart-2.4.8.jar:/opt/dolphinscheduler/libs/hadoop-client-3.2.4.jar:/opt/dolphinscheduler/libs/commons-collections4-4.3.jar:/opt/dolphinscheduler/libs/metrics-core-4.2.11.jar:/opt/dolphinscheduler/libs/netty-resolver-4.1.53.Final.jar:/opt/dolphinscheduler/libs/parquet-hadoop-bundle-1.8.1.jar:/opt/dolphinscheduler/libs/bucket4j-core-6.2.0.jar:/opt/dolphinscheduler/libs/spring-cloud-starter-3.1.3.jar:/opt/dolphinscheduler/libs/mssql-jdbc-11.2.1.jre8.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/dolphinscheduler/libs/kubernetes-model-coordination-5.10.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-linkis-3.2.1.jar:/opt/dolphinscheduler/libs/commons-net-3.6.jar:/opt/dolphinscheduler/libs/netty-transport-native-kqueue-4.1.53.Final-osx-x86_64.jar:/opt/dolphinscheduler/libs/dolphinscheduler-meter-3.2.1.jar:/opt/dolphinscheduler/libs/kubernetes-client-api-6.0.0.jar:/opt/dolphinscheduler/libs/kubernetes-model-certificates-5.10.2.jar:/opt/dolphinscheduler/libs/opencensus-api-0.31.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-mlflow-3.2.1.jar:/opt/dolphinscheduler/libs/kotlin-stdlib-jdk8-1.6.21.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-jdbc-3.2.1.jar:/opt/dolphinscheduler/libs/audience-annotations-0.12.0.jar:/opt/dolphinscheduler/libs/simpleclient_httpserver-0.15.0.jar:/opt/dolphinscheduler/libs/jetty-xml-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/kerby-asn1-1.0.1.jar:/opt/dolphinscheduler/libs/lang-tag-1.6.jar:/opt/dolphinscheduler/libs/api-common-2.6.0.jar:/opt/dolphinscheduler/libs/HdrHistogram-2.1.12.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-abs-3.2.1.jar:/opt/dolphinscheduler/libs/failsafe-2.4.4.jar:/opt/dolphinscheduler/libs/hbase-noop-htrace-4.1.1.jar:/opt/dolphinscheduler/libs/jamon-runtime-2.3.1.jar:/opt/dolphinscheduler/libs/jetty-util-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/threetenbp-1.6.5.jar:/opt/dolphinscheduler/libs/jakarta.activation-api-1.2.2.jar:/opt/dolphinscheduler/libs/kubernetes-model-common-5.10.2.jar:/opt/dolphinscheduler/libs/okhttp-4.9.3.jar:/opt/dolphinscheduler/libs/profiles-2.17.282.jar:/opt/dolphinscheduler/libs/hadoop-auth-3.2.4.jar:/opt/dolphinscheduler/libs/grpc-netty-1.41.0.jar:/opt/dolphinscheduler/libs/httpcore-nio-4.4.15.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-seatunnel-3.2.1.jar:/opt/dolphinscheduler/libs/aws-java-sdk-sagemaker-1.12.300.jar:/opt/dolphinscheduler/libs/oshi-core-6.1.1.jar:/opt/dolphinscheduler/libs/bonecp-0.8.0.RELEASE.jar:/opt/dolphinscheduler/libs/jackson-module-parameter-names-2.13.3.jar:/opt/dolphinscheduler/libs/bcutil-jdk15on-1.69.jar:/opt/dolphinscheduler/libs/aws-json-protocol-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-base-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-all-3.2.1.jar:/opt/dolphinscheduler/libs/derby-10.14.2.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-snowflake-3.2.1.jar:/opt/dolphinscheduler/libs/netty-codec-http2-4.1.53.Final.jar:/opt/dolphinscheduler/libs/jetty-continuation-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/jackson-mapper-asl-1.9.13.jar:/opt/dolphinscheduler/libs/datanucleus-core-4.1.17.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/dolphinscheduler/libs/failureaccess-1.0.1.jar:/opt/dolphinscheduler/libs/error_prone_annotations-2.5.1.jar:/opt/dolphinscheduler/libs/kerb-admin-1.0.1.jar:/opt/dolphinscheduler/libs/token-provider-1.0.1.jar:/opt/dolphinscheduler/libs/reactor-netty-core-1.0.22.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-all-3.2.1.jar:/opt/dolphinscheduler/libs/jakarta.servlet-api-4.0.4.jar:/opt/dolphinscheduler/libs/http-client-spi-2.17.282.jar:/opt/dolphinscheduler/libs/regions-2.17.282.jar:/opt/dolphinscheduler/libs/logback-core-1.2.11.jar:/opt/dolphinscheduler/libs/json-1.8.jar:/opt/dolphinscheduler/libs/gax-grpc-2.23.0.jar:/opt/dolphinscheduler/libs/google-http-client-1.42.3.jar:/opt/dolphinscheduler/libs/hive-serde-2.3.9.jar:/opt/dolphinscheduler/libs/jettison-1.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-http-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-zookeeper-3.2.1.jar:/opt/dolphinscheduler/libs/spring-security-crypto-5.7.3.jar:/opt/dolphinscheduler/libs/auth-2.17.282.jar:/opt/dolphinscheduler/libs/proto-google-cloud-storage-v2-2.18.0-alpha.jar:/opt/dolphinscheduler/libs/jetty-server-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/javax.annotation-api-1.3.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-starrocks-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-dataquality-3.2.1.jar:/opt/dolphinscheduler/libs/jcl-over-slf4j-1.7.36.jar:/opt/dolphinscheduler/libs/commons-lang3-3.12.0.jar:/opt/dolphinscheduler/libs/tomcat-embed-el-9.0.65.jar:/opt/dolphinscheduler/libs/opentracing-noop-0.33.0.jar:/opt/dolphinscheduler/libs/client-java-13.0.2.jar:/opt/dolphinscheduler/libs/spring-beans-5.3.22.jar:/opt/dolphinscheduler/libs/snakeyaml-1.33.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-ssh-3.2.1.jar:/opt/dolphinscheduler/libs/commons-pool-1.6.jar:/opt/dolphinscheduler/libs/javax.servlet-api-3.1.0.jar:/opt/dolphinscheduler/libs/hadoop-common-3.2.4.jar:/opt/dolphinscheduler/libs/kubernetes-model-apiextensions-5.10.2.jar:/opt/dolphinscheduler/libs/spring-boot-2.7.3.jar:/opt/dolphinscheduler/libs/bcprov-ext-jdk15on-1.69.jar:/opt/dolphinscheduler/libs/spring-boot-starter-2.7.3.jar:/opt/dolphinscheduler/libs/paranamer-2.3.jar:/opt/dolphinscheduler/libs/httpmime-4.5.13.jar:/opt/dolphinscheduler/libs/reactor-core-3.4.22.jar:/opt/dolphinscheduler/libs/azure-core-1.36.0.jar:/opt/dolphinscheduler/libs/bcpkix-jdk15on-1.69.jar:/opt/dolphinscheduler/libs/kubernetes-model-scheduling-5.10.2.jar:/opt/dolphinscheduler/libs/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/dolphinscheduler/libs/websocket-client-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/unirest-java-3.7.04-standalone.jar:/opt/dolphinscheduler/libs/hadoop-mapreduce-client-core-3.2.4.jar:/opt/dolphinscheduler/libs/google-auth-library-credentials-1.15.0.jar:/opt/dolphinscheduler/libs/azure-storage-internal-avro-12.6.0.jar:/opt/dolphinscheduler/libs/jackson-datatype-jdk8-2.13.3.jar:/opt/dolphinscheduler/libs/spring-expression-5.3.22.jar:/opt/dolphinscheduler/libs/aspectjweaver-1.9.7.jar:/opt/dolphinscheduler/libs/websocket-common-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/client-java-api-fluent-13.0.2.jar:/opt/dolphinscheduler/libs/mybatis-plus-3.5.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-athena-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-oss-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-openmldb-3.2.1.jar:/opt/dolphinscheduler/libs/curator-recipes-5.3.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-api-3.2.1.jar:/opt/dolphinscheduler/libs/netty-all-4.1.53.Final.jar:/opt/dolphinscheduler/libs/netty-resolver-dns-4.1.53.Final.jar:/opt/dolphinscheduler/libs/kubernetes-model-autoscaling-5.10.2.jar:/opt/dolphinscheduler/libs/kerb-util-1.0.1.jar:/opt/dolphinscheduler/libs/grpc-alts-1.41.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-presto-3.2.1.jar:/opt/dolphinscheduler/libs/kerb-simplekdc-1.0.1.jar:/opt/dolphinscheduler/libs/protobuf-java-util-3.17.2.jar:/opt/dolphinscheduler/libs/azure-core-http-netty-1.13.0.jar:/opt/dolphinscheduler/libs/transaction-api-1.1.jar:/opt/dolphinscheduler/libs/datanucleus-api-jdo-4.2.4.jar:/opt/dolphinscheduler/libs/zeppelin-common-0.10.1.jar:/opt/dolphinscheduler/libs/zt-zip-1.15.jar:/opt/dolphinscheduler/libs/animal-sniffer-annotations-1.19.jar:/opt/dolphinscheduler/libs/google-http-client-jackson2-1.42.3.jar:/opt/dolphinscheduler/libs/netty-buffer-4.1.53.Final.jar:/opt/dolphinscheduler/libs/jsr305-3.0.0.jar:/opt/dolphinscheduler/libs/netty-transport-classes-epoll-4.1.79.Final.jar:/opt/dolphinscheduler/libs/spring-boot-actuator-autoconfigure-2.7.3.jar:/opt/dolphinscheduler/libs/simpleclient-0.15.0.jar:/opt/dolphinscheduler/libs/aliyun-sdk-oss-3.15.1.jar:/opt/dolphinscheduler/libs/json-utils-2.17.282.jar:/opt/dolphinscheduler/libs/tephra-api-0.6.0.jar:/opt/dolphinscheduler/libs/spring-jdbc-5.3.22.jar:/opt/dolphinscheduler/libs/grpc-xds-1.41.0.jar:/opt/dolphinscheduler/libs/logback-classic-1.2.11.jar:/opt/dolphinscheduler/libs/google-cloud-storage-2.18.0.jar:/opt/dolphinscheduler/libs/micrometer-registry-prometheus-1.9.3.jar:/opt/dolphinscheduler/libs/grpc-core-1.41.0.jar:/opt/dolphinscheduler/libs/aws-java-sdk-kms-1.12.300.jar:/opt/dolphinscheduler/libs/kubernetes-model-rbac-5.10.2.jar:/opt/dolphinscheduler/libs/ini4j-0.5.4.jar:/opt/dolphinscheduler/libs/spring-boot-starter-web-2.7.3.jar:/opt/dolphinscheduler/libs/spring-cloud-commons-3.1.3.jar:/opt/dolphinscheduler/libs/netty-codec-http-4.1.53.Final.jar:/opt/dolphinscheduler/libs/jetty-client-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/jetcd-common-0.5.11.jar:/opt/dolphinscheduler/libs/metrics-spi-2.17.282.jar:/opt/dolphinscheduler/libs/utils-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-emr-3.2.1.jar:/opt/dolphinscheduler/libs/protocol-core-2.17.282.jar:/opt/dolphinscheduler/libs/micrometer-core-1.9.3.jar:/opt/dolphinscheduler/libs/netty-transport-native-unix-common-4.1.53.Final.jar:/opt/dolphinscheduler/libs/zjsonpatch-0.4.11.jar:/opt/dolphinscheduler/libs/annotations-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-sql-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-common-3.2.1.jar:/opt/dolphinscheduler/libs/apache-client-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-oceanbase-3.2.1.jar:/opt/dolphinscheduler/libs/jdo-api-3.0.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-datax-3.2.1.jar:/opt/dolphinscheduler/libs/postgresql-42.4.1.jar:/opt/dolphinscheduler/libs/jetty-util-ajax-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/gax-2.23.0.jar:/opt/dolphinscheduler/libs/grpc-stub-1.41.0.jar:/opt/dolphinscheduler/libs/libfb303-0.9.3.jar:/opt/dolphinscheduler/libs/spring-core-5.3.22.jar:/opt/dolphinscheduler/libs/jaxb-api-2.3.1.jar:/opt/dolphinscheduler/libs/hive-service-rpc-2.3.9.jar:/opt/dolphinscheduler/libs/kubernetes-model-flowcontrol-5.10.2.jar:/opt/dolphinscheduler/libs/commons-logging-1.1.1.jar:/opt/dolphinscheduler/libs/mybatis-spring-2.0.7.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-oracle-3.2.1.jar:/opt/dolphinscheduler/libs/opencensus-proto-0.2.0.jar:/opt/dolphinscheduler/libs/grpc-protobuf-1.41.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-clickhouse-3.2.1.jar:/opt/dolphinscheduler/libs/spring-cloud-starter-bootstrap-3.1.3.jar:/opt/dolphinscheduler/libs/kubernetes-model-admissionregistration-5.10.2.jar:/opt/dolphinscheduler/libs/grpc-google-cloud-storage-v2-2.18.0-alpha.jar:/opt/dolphinscheduler/libs/esdk-obs-java-bundle-3.23.3.jar:/opt/dolphinscheduler/libs/dnsjava-2.1.7.jar:/opt/dolphinscheduler/libs/asm-9.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-spark-3.2.1.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/dolphinscheduler/libs/re2j-1.6.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-vertica-3.2.1.jar:/opt/dolphinscheduler/libs/json-smart-2.4.8.jar:/opt/dolphinscheduler/libs/reactor-netty-http-1.0.22.jar:/opt/dolphinscheduler/libs/google-api-services-storage-v1-rev20220705-2.0.0.jar:/opt/dolphinscheduler/libs/kubernetes-client-6.0.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-kyuubi-3.2.1.jar:/opt/dolphinscheduler/libs/auto-value-annotations-1.10.1.jar:/opt/dolphinscheduler/libs/commons-cli-1.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-data-quality-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-chunjun-3.2.1.jar:/opt/dolphinscheduler/libs/kerb-server-1.0.1.jar:/opt/dolphinscheduler/libs/content-type-2.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-remoteshell-3.2.1.jar:/opt/dolphinscheduler/libs/opencensus-contrib-http-util-0.31.1.jar:/opt/dolphinscheduler/libs/druid-1.2.20.jar:/opt/dolphinscheduler/libs/javolution-5.5.1.jar:/opt/dolphinscheduler/libs/protobuf-java-3.17.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-db2-3.2.1.jar:/opt/dolphinscheduler/libs/aws-java-sdk-core-1.12.300.jar:/opt/dolphinscheduler/libs/spring-boot-starter-logging-2.7.3.jar:/opt/dolphinscheduler/libs/kerby-pkix-1.0.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-procedure-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-etcd-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-sqoop-3.2.1.jar:/opt/dolphinscheduler/libs/zookeeper-jute-3.8.0.jar:/opt/dolphinscheduler/libs/netty-resolver-dns-native-macos-4.1.53.Final-osx-x86_64.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-sagemaker-3.2.1.jar:/opt/dolphinscheduler/libs/spring-boot-configuration-processor-2.6.1.jar:/opt/dolphinscheduler/libs/hive-common-2.3.9.jar:/opt/dolphinscheduler/libs/kerb-common-1.0.1.jar:/opt/dolphinscheduler/libs/stax-api-1.0.1.jar:/opt/dolphinscheduler/libs/kubernetes-model-storageclass-5.10.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-s3-3.2.1.jar:/opt/dolphinscheduler/libs/spring-boot-starter-jetty-2.7.3.jar:/opt/dolphinscheduler/libs/okio-2.8.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-dms-3.2.1.jar:/opt/dolphinscheduler/libs/simpleclient_common-0.15.0.jar:/opt/dolphinscheduler/libs/sdk-core-2.17.282.jar:/opt/dolphinscheduler/libs/javax.activation-api-1.2.0.jar:/opt/dolphinscheduler/libs/netty-handler-proxy-4.1.53.Final.jar:/opt/dolphinscheduler/libs/kerby-config-1.0.1.jar:/opt/dolphinscheduler/libs/netty-tcnative-classes-2.0.53.Final.jar:/opt/dolphinscheduler/libs/jackson-jaxrs-base-2.13.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-trino-3.2.1.jar:/opt/dolphinscheduler/libs/presto-jdbc-0.238.1.jar:/opt/dolphinscheduler/libs/websocket-api-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-flink-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-api-3.2.1.jar:/opt/dolphinscheduler/libs/kubernetes-model-metrics-5.10.2.jar:/opt/dolphinscheduler/libs/datasync-2.17.282.jar:/opt/dolphinscheduler/libs/hadoop-yarn-api-3.2.4.jar:/opt/dolphinscheduler/libs/google-http-client-apache-v2-1.42.3.jar:/opt/dolphinscheduler/libs/hadoop-annotations-3.2.4.jar:/opt/dolphinscheduler/libs/google-oauth-client-1.34.1.jar:/opt/dolphinscheduler/libs/sshd-common-2.8.0.jar:/opt/dolphinscheduler/libs/LatencyUtils-2.0.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-spark-3.2.1.jar:/opt/dolphinscheduler/libs/slf4j-api-1.7.36.jar:/opt/dolphinscheduler/libs/snowflake-jdbc-3.13.29.jar:/opt/dolphinscheduler/libs/jackson-datatype-jsr310-2.13.3.jar:/opt/dolphinscheduler/libs/trino-jdbc-402.jar:/opt/dolphinscheduler/libs/logging-interceptor-4.9.3.jar:/opt/dolphinscheduler/libs/simpleclient_tracer_otel_agent-0.15.0.jar:/opt/dolphinscheduler/libs/janino-3.0.16.jar:/opt/dolphinscheduler/libs/jackson-dataformat-yaml-2.13.3.jar:/opt/dolphinscheduler/libs/ion-java-1.0.2.jar:/opt/dolphinscheduler/libs/commons-dbcp-1.4.jar:/opt/dolphinscheduler/libs/jsp-api-2.1.jar:/opt/dolphinscheduler/libs/google-http-client-gson-1.42.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-hivecli-3.2.1.jar:/opt/dolphinscheduler/libs/spring-boot-actuator-2.7.3.jar:/opt/dolphinscheduler/libs/google-cloud-core-http-2.10.0.jar:/opt/dolphinscheduler/libs/DmJdbcDriver18-8.1.2.79.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-java-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-dameng-3.2.1.jar:/opt/dolphinscheduler/libs/sshd-sftp-2.8.0.jar:/opt/dolphinscheduler/libs/kerb-crypto-1.0.1.jar:/opt/dolphinscheduler/libs/conscrypt-openjdk-uber-2.5.2.jar:/opt/dolphinscheduler/libs/httpcore-4.4.15.jar:/opt/dolphinscheduler/libs/lz4-java-1.4.0.jar:/opt/dolphinscheduler/libs/guava-retrying-2.0.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-flink-stream-3.2.1.jar:/opt/dolphinscheduler/libs/spring-tx-5.3.22.jar:/opt/dolphinscheduler/libs/grpc-auth-1.41.0.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/dolphinscheduler/libs/databend-jdbc-0.0.7.jar:/opt/dolphinscheduler/libs/bcprov-jdk15on-1.69.jar:/opt/dolphinscheduler/libs/commons-lang-2.6.jar:/opt/dolphinscheduler/libs/kubernetes-model-policy-5.10.2.jar:/opt/dolphinscheduler/libs/commons-io-2.11.0.jar:/opt/dolphinscheduler/libs/grpc-grpclb-1.41.0.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/dolphinscheduler/libs/opentracing-api-0.33.0.jar:/opt/dolphinscheduler/libs/sshd-scp-2.8.0.jar:/opt/dolphinscheduler/libs/reload4j-1.2.18.3.jar:/opt/dolphinscheduler/libs/netty-tcnative-2.0.48.Final.jar:/opt/dolphinscheduler/libs/jdom2-2.0.6.1.jar:/opt/dolphinscheduler/libs/spring-boot-starter-json-2.7.3.jar:/opt/dolphinscheduler/libs/netty-transport-native-epoll-4.1.53.Final.jar:/opt/dolphinscheduler/libs/google-cloud-core-2.10.0.jar:/opt/dolphinscheduler/libs/javax.jdo-3.2.0-m3.jar:/opt/dolphinscheduler/libs/gax-httpjson-0.108.0.jar:/opt/dolphinscheduler/libs/mybatis-plus-core-3.5.2.jar:/opt/dolphinscheduler/libs/auto-service-annotations-1.0.1.jar:/opt/dolphinscheduler/libs/nimbus-jose-jwt-9.22.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-zeppelin-3.2.1.jar:/opt/dolphinscheduler/libs/commons-compress-1.21.jar:/opt/dolphinscheduler/libs/hadoop-hdfs-client-3.2.4.jar:/opt/dolphinscheduler/libs/msal4j-persistence-extension-1.1.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-pytorch-3.2.1.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/dolphinscheduler/libs/jetty-servlets-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/woodstox-core-6.4.0.jar:/opt/dolphinscheduler/libs/spring-cloud-kubernetes-commons-2.1.3.jar:/opt/dolphinscheduler/libs/grpc-protobuf-lite-1.41.0.jar:/opt/dolphinscheduler/libs/jetty-webapp-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/eventstream-1.0.1.jar:/opt/dolphinscheduler/libs/commons-compiler-3.1.7.jar:/opt/dolphinscheduler/libs/netty-transport-4.1.53.Final.jar:/opt/dolphinscheduler/libs/spring-cloud-context-3.1.3.jar:/opt/dolphinscheduler/libs/aws-java-sdk-dms-1.12.300.jar:/opt/dolphinscheduler/libs/hive-storage-api-2.4.0.jar:/opt/dolphinscheduler/libs/client-java-spring-integration-13.0.2.jar:/opt/dolphinscheduler/libs/spring-boot-starter-actuator-2.7.3.jar:/opt/dolphinscheduler/libs/third-party-jackson-core-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-hdfs-3.2.1.jar:/opt/dolphinscheduler/libs/netty-codec-4.1.53.Final.jar:/opt/dolphinscheduler/libs/google-http-client-appengine-1.42.3.jar:/opt/dolphinscheduler/libs/commons-configuration2-2.1.1.jar:/opt/dolphinscheduler/libs/datanucleus-rdbms-4.1.19.jar:/opt/dolphinscheduler/libs/swagger-annotations-1.6.2.jar:/opt/dolphinscheduler/libs/simpleclient_tracer_otel-0.15.0.jar:/opt/dolphinscheduler/libs/kotlin-stdlib-common-1.6.21.jar:/opt/dolphinscheduler/libs/aws-core-2.17.282.jar:/opt/dolphinscheduler/libs/kerb-core-1.0.1.jar:/opt/dolphinscheduler/libs/httpasyncclient-4.1.5.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-worker-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-spi-3.2.1.jar:/opt/dolphinscheduler/libs/okhttp-2.7.5.jar:/opt/dolphinscheduler/libs/google-api-client-2.2.0.jar:/opt/dolphinscheduler/libs/kotlin-stdlib-1.6.21.jar:/opt/dolphinscheduler/libs/jakarta.websocket-api-1.1.2.jar:/opt/dolphinscheduler/libs/libthrift-0.9.3.jar:/opt/dolphinscheduler/libs/spring-boot-starter-aop-2.7.3.jar:/opt/dolphinscheduler/libs/netty-common-4.1.53.Final.jar:/opt/dolphinscheduler/libs/log4j-1.2-api-2.17.2.jar:/opt/dolphinscheduler/libs/jackson-databind-2.13.4.jar:/opt/dolphinscheduler/libs/jackson-dataformat-xml-2.13.3.jar:/opt/dolphinscheduler/libs/kubernetes-model-events-5.10.2.jar:/opt/dolphinscheduler/libs/gson-2.9.1.jar:/opt/dolphinscheduler/libs/proto-google-iam-v1-1.9.0.jar:/opt/dolphinscheduler/libs/netty-codec-socks-4.1.53.Final.jar:/opt/dolphinscheduler/libs/zjsonpatch-0.3.0.jar:/opt/dolphinscheduler/libs/hadoop-mapreduce-client-common-3.2.4.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-api-3.2.1.jar:/opt/dolphinscheduler/libs/azure-storage-blob-12.21.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-zeppelin-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-api-3.2.1.jar:/opt/dolphinscheduler/libs/aws-java-sdk-s3-1.12.300.jar:/opt/dolphinscheduler/libs/stax2-api-4.2.1.jar:/opt/dolphinscheduler/libs/jakarta.annotation-api-1.3.5.jar:/opt/dolphinscheduler/libs/kubernetes-model-discovery-5.10.2.jar:/opt/dolphinscheduler/libs/jna-5.10.0.jar:/opt/dolphinscheduler/libs/spring-jcl-5.3.22.jar
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.517 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.517 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.io.tmpdir=/tmp
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.517 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.compiler=<NA>
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.518 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.name=Linux
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.518 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.arch=amd64
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.518 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.version=6.5.0-28-generic
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.518 +0800 o.a.z.ZooKeeper:[98] - Client environment:user.name=root
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.518 +0800 o.a.z.ZooKeeper:[98] - Client environment:user.home=/root
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.518 +0800 o.a.z.ZooKeeper:[98] - Client environment:user.dir=/opt/dolphinscheduler
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.518 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.memory.free=3216MB
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.518 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.memory.max=3840MB
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.518 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.memory.total=3840MB
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.521 +0800 o.a.z.ZooKeeper:[637] - Initiating client connection, connectString=dolphinscheduler-zookeeper:2181 sessionTimeout=30000 watcher=org.apache.curator.ConnectionState@6996bbc4
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.526 +0800 o.a.z.c.X509Util:[77] - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.539 +0800 o.a.z.ClientCnxnSocket:[239] - jute.maxbuffer value is 1048575 Bytes
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.566 +0800 o.a.z.ClientCnxn:[1732] - zookeeper.request.timeout value is 0. feature enabled=false
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.601 +0800 o.a.c.f.i.CuratorFrameworkImpl:[386] - Default schema
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.614 +0800 o.a.z.ClientCnxn:[1171] - Opening socket connection to server dolphinscheduler-zookeeper/172.18.0.5:2181.
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.661 +0800 o.a.z.ClientCnxn:[1173] - SASL config status: Will not attempt to authenticate using SASL (unknown error)
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.668 +0800 o.a.z.ClientCnxn:[1005] - Socket connection established, initiating session, client: /172.18.1.1:40074, server: dolphinscheduler-zookeeper/172.18.0.5:2181
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.700 +0800 o.a.z.ClientCnxn:[1444] - Session establishment complete on server dolphinscheduler-zookeeper/172.18.0.5:2181, session id = 0x10001ca8dd20000, negotiated timeout = 30000
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.713 +0800 o.a.c.f.s.ConnectionStateManager:[252] - State change: CONNECTED
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.922 +0800 o.a.c.f.i.EnsembleTracker:[201] - New config event received: {}
[WI-0][TI-0] - [INFO] 2024-04-24 16:11:59.922 +0800 o.a.c.f.i.EnsembleTracker:[201] - New config event received: {}
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:00.127 +0800 o.a.d.s.w.r.WorkerRpcServer:[41] - WorkerRpcServer starting...
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.311 +0800 o.a.d.e.b.NettyRemotingServer:[112] - WorkerRpcServer bind success at port: 1234
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.312 +0800 o.a.d.s.w.r.WorkerRpcServer:[43] - WorkerRpcServer started...
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.634 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: JAVA - JavaTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.656 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: JAVA - JavaTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.657 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: JUPYTER - JupyterTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.670 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: JUPYTER - JupyterTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.677 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SPARK - SparkTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.692 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SPARK - SparkTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.710 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: FLINK_STREAM - FlinkStreamTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.719 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: FLINK_STREAM - FlinkStreamTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.720 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: PYTHON - PythonTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.740 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: PYTHON - PythonTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.740 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DATASYNC - DatasyncTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.757 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DATASYNC - DatasyncTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.758 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DATA_FACTORY - DatafactoryTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.761 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DATA_FACTORY - DatafactoryTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.762 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: CHUNJUN - ChunJunTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.776 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: CHUNJUN - ChunJunTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.777 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: REMOTESHELL - RemoteShellTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.784 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: REMOTESHELL - RemoteShellTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.796 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: PIGEON - PigeonTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.801 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: PIGEON - PigeonTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.801 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SHELL - ShellTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.806 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SHELL - ShellTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.807 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: PROCEDURE - ProcedureTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.825 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: PROCEDURE - ProcedureTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.826 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: MR - MapReduceTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.878 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: MR - MapReduceTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.878 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SQOOP - SqoopTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.883 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SQOOP - SqoopTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.883 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: PYTORCH - PytorchTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.931 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: PYTORCH - PytorchTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.932 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: K8S - K8sTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.935 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: K8S - K8sTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.936 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SEATUNNEL - SeatunnelTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.940 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SEATUNNEL - SeatunnelTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.940 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SAGEMAKER - SagemakerTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.943 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SAGEMAKER - SagemakerTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.943 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: HTTP - HttpTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.945 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: HTTP - HttpTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.946 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: EMR - EmrTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.950 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: EMR - EmrTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.951 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DMS - DmsTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.952 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DMS - DmsTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.953 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DATA_QUALITY - DataQualityTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.954 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DATA_QUALITY - DataQualityTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.955 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: KUBEFLOW - KubeflowTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.956 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: KUBEFLOW - KubeflowTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.957 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SQL - SqlTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.959 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SQL - SqlTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.959 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DVC - DvcTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.961 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DVC - DvcTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.961 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DATAX - DataxTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.963 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DATAX - DataxTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.963 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: ZEPPELIN - ZeppelinTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.965 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: ZEPPELIN - ZeppelinTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.965 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DINKY - DinkyTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.966 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DINKY - DinkyTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.991 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: MLFLOW - MlflowTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.995 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: MLFLOW - MlflowTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:01.996 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: OPENMLDB - OpenmldbTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:02.002 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: OPENMLDB - OpenmldbTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:02.003 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: LINKIS - LinkisTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:02.005 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: LINKIS - LinkisTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:02.006 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: FLINK - FlinkTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:02.006 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: FLINK - FlinkTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:02.007 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: HIVECLI - HiveCliTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:02.008 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: HIVECLI - HiveCliTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:02.161 +0800 o.a.d.c.u.JSONUtils:[72] - init timezone: sun.util.calendar.ZoneInfo[id="Asia/Shanghai",offset=28800000,dstSavings=0,useDaylight=false,transitions=31,lastRule=null]
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:02.323 +0800 o.a.d.s.w.r.WorkerRegistryClient:[104] - Worker node: 172.18.1.1:1234 registry to ZK /nodes/worker/172.18.1.1:1234 successfully
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:04.187 +0800 o.a.d.c.m.BaseHeartBeatTask:[48] - Starting WorkerHeartBeatTask...
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:04.189 +0800 o.a.d.c.m.BaseHeartBeatTask:[50] - Started WorkerHeartBeatTask, heartBeatInterval: 10000...
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:04.190 +0800 o.a.d.s.w.r.WorkerRegistryClient:[114] - Worker node: 172.18.1.1:1234 registry finished
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:04.214 +0800 o.a.d.s.w.m.MessageRetryRunner:[69] - Message retry runner staring
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:04.214 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.393700787401575 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:04.349 +0800 o.a.d.s.w.m.MessageRetryRunner:[72] - Injected message sender: TaskInstanceExecutionFinishEventSender
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:04.388 +0800 o.a.d.s.w.m.MessageRetryRunner:[72] - Injected message sender: TaskInstanceExecutionInfoUpdateEventSender
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:04.389 +0800 o.a.d.s.w.m.MessageRetryRunner:[72] - Injected message sender: TaskInstanceExecutionRunningEventSender
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:04.390 +0800 o.a.d.s.w.m.MessageRetryRunner:[75] - Message retry runner started
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:05.475 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.4754901960784315 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:06.790 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.2671375787872203 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:07.900 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.4049079754601228 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:09.625 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.2331838565022422 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:11.192 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.7323232323232325 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:12.865 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.3232323232323233 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:13.897 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.3243243243243243 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:15.185 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.4 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:16.356 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.900709219858156 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:17.363 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.3493975903614457 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:18.575 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.2884615384615385 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:19.592 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.404040404040404 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:20.703 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.4846625766871164 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:21.722 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.590277777777778 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:22.158 +0800 o.s.b.a.e.w.EndpointLinksResolver:[58] - Exposing 3 endpoint(s) beneath base path '/actuator'
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:22.628 +0800 o.e.j.s.h.C.application:[2368] - Initializing Spring DispatcherServlet 'dispatcherServlet'
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:22.628 +0800 o.s.w.s.DispatcherServlet:[525] - Initializing Servlet 'dispatcherServlet'
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:22.634 +0800 o.s.w.s.DispatcherServlet:[547] - Completed initialization in 5 ms
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:22.813 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.5656565656565657 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:23.558 +0800 o.e.j.s.AbstractConnector:[333] - Started ServerConnector@acb5508{HTTP/1.1, (http/1.1)}{0.0.0.0:1235}
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:23.573 +0800 o.s.b.w.e.j.JettyWebServer:[172] - Jetty started on port(s) 1235 (http/1.1) with context path '/'
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:23.664 +0800 o.a.d.s.w.WorkerServer:[61] - Started WorkerServer in 52.775 seconds (JVM running for 56.67)
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:23.844 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.2885966180083828 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:24.846 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.7386946386946387 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:25.851 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0625 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:27.890 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8282208588957054 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:28.909 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.1818181818181817 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:29.929 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7007874015748031 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:31.167 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9715909090909092 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:33.198 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.73 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:35.327 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7379310344827587 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:12:37.399 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7575757575757575 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:14:03.104 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.75 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:14:13.202 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7507002801120448 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:14:14.244 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8328611898016998 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:14:15.255 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7842312812821247 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:14:16.257 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7379162982611259 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:14:28.461 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7198879551820727 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:14:29.539 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8421052631578948 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:15:06.057 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7424657534246575 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:15:10.802 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1477, taskName=extract from preprocessed queue, firstSubmitTime=1713946510286, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=5, appIds=null, processInstanceId=584, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"message","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n#     --bootstrap-server kafka:9092 \\\n#     --topic reddit_preprocessed \\\n#     --group preprocessed_consumers \\\n#     --max-messages 1 \\\n#     --timeout-ms 10000)\n\n# echo \"#{setValue(message=${message})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract from preprocessed queue'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1477'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13375898458848'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424161510'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='584'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:10.907 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract from preprocessed queue to wait queue success
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:10.941 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:10.953 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:10.956 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:10.961 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:10.963 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713946510963
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:10.963 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 584_1477
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:10.976 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1477,
  "taskName" : "extract from preprocessed queue",
  "firstSubmitTime" : 1713946510286,
  "startTime" : 1713946510963,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/5/584/1477.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 5,
  "processInstanceId" : 584,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"message\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\\\\n#     --bootstrap-server kafka:9092 \\\\\\n#     --topic reddit_preprocessed \\\\\\n#     --group preprocessed_consumers \\\\\\n#     --max-messages 1 \\\\\\n#     --timeout-ms 10000)\\n\\n# echo \\\"#{setValue(message=${message})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract from preprocessed queue"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1477"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13375898458848"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424161510"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "584"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "584_1477",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:10.998 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:11.015 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:11.015 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-0][TI-0] - [INFO] 2024-04-24 16:15:11.158 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9939393939393939 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:11.175 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:11.195 +0800 o.a.d.c.u.OSUtils:[231] - create linux os user: default
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:11.196 +0800 o.a.d.c.u.OSUtils:[233] - execute cmd: sudo useradd -g root
 default
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:11.269 +0800 o.a.d.c.u.OSUtils:[190] - create user default success
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:11.271 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:11.275 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1477 check successfully
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:11.277 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:11.284 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:11.302 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:11.305 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:11.308 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "message",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n#     --bootstrap-server kafka:9092 \\\n#     --topic reddit_preprocessed \\\n#     --group preprocessed_consumers \\\n#     --max-messages 1 \\\n#     --timeout-ms 10000)\n\n# echo \"#{setValue(message=${message})}\"",
  "resourceList" : [ ]
}
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:11.309 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:11.309 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:11.313 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:11.315 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:11.315 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:11.325 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:11.326 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:11.326 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \
#     --bootstrap-server kafka:9092 \
#     --topic reddit_preprocessed \
#     --group preprocessed_consumers \
#     --max-messages 1 \
#     --timeout-ms 10000)

# echo "#{setValue(message=${message})}"
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:11.326 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:11.328 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1477/584_1477.sh
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:11.359 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 114
[WI-0][TI-1477] - [INFO] 2024-04-24 16:15:12.094 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1477, success=true)
[WI-0][TI-1477] - [INFO] 2024-04-24 16:15:12.136 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1477)
[WI-0][TI-0] - [INFO] 2024-04-24 16:15:12.368 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:12.375 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1477, processId:114 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:12.377 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:12.377 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:12.378 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:12.382 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:12.415 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:12.416 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:12.417 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1477
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:12.440 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1477
[WI-584][TI-1477] - [INFO] 2024-04-24 16:15:12.443 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1477] - [INFO] 2024-04-24 16:15:13.095 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1477, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 16:15:13.176 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1478, taskName=keyword filtering, firstSubmitTime=1713946513148, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=5, appIds=null, processInstanceId=584, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is stupid\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [${keywords}]\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\n# pass whether the data is relevant to singapore or not to the downstream tasks\n\nprint('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))\n\n\n\n\n# stop Spark session\nspark.stop()\n","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1478'}, message=Property{prop='message', direct=IN, type=VARCHAR, value=''}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424161513'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='584'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"message","direct":"IN","type":"VARCHAR","value":""}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.177 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.177 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.183 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.184 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.184 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.184 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713946513184
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.184 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 584_1478
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.187 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1478,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713946513148,
  "startTime" : 1713946513184,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/5/584/1478.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 5,
  "processInstanceId" : 584,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is stupid\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [${keywords}]\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\n# pass whether the data is relevant to singapore or not to the downstream tasks\\n\\nprint('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))\\n\\n\\n\\n\\n# stop Spark session\\nspark.stop()\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1478"
    },
    "message" : {
      "prop" : "message",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : ""
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424161513"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "584"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "584_1478",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"message\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.188 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.189 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.189 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.195 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.196 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.197 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1478 check successfully
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.197 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.200 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.201 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.202 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.203 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is stupid\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [${keywords}]\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\n# pass whether the data is relevant to singapore or not to the downstream tasks\n\nprint('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))\n\n\n\n\n# stop Spark session\nspark.stop()\n",
  "resourceList" : [ ]
}
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.204 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.206 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"message","direct":"IN","type":"VARCHAR","value":""}] successfully
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.206 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.206 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.207 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.207 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is stupid",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [${keywords}]

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

# pass whether the data is relevant to singapore or not to the downstream tasks

print('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))




# stop Spark session
spark.stop()

[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.209 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1478
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.209 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1478/py_584_1478.py
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.210 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# message = 
message = {
  "user": "premiumplatinum",
  "content": "singapore is stupid",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = ["singapore", "lol"]

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

# pass whether the data is relevant to singapore or not to the downstream tasks

print('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))




# stop Spark session
spark.stop()

[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.221 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.222 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.222 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1478/py_584_1478.py
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.223 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.223 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1478/584_1478.sh
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:13.227 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 128
[WI-0][TI-1478] - [INFO] 2024-04-24 16:15:14.111 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1478, success=true)
[WI-0][TI-1478] - [INFO] 2024-04-24 16:15:14.139 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1478)
[WI-0][TI-0] - [INFO] 2024-04-24 16:15:14.227 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	JAVA_HOME is not set
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1478/py_584_1478.py", line 15, in <module>
	    .getOrCreate()
	     ^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py", line 497, in getOrCreate
	    sc = SparkContext.getOrCreate(sparkConf)
	         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 515, in getOrCreate
	    SparkContext(conf=conf or SparkConf())
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 201, in __init__
	    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 436, in _ensure_initialized
	    SparkContext._gateway = gateway or launch_gateway(conf)
	                                       ^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/java_gateway.py", line 107, in launch_gateway
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:14.230 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1478, processId:128 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:14.231 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:14.231 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:14.231 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:14.231 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:14.241 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:14.242 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:14.242 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1478
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:14.243 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1478
[WI-584][TI-1478] - [INFO] 2024-04-24 16:15:14.244 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1478] - [INFO] 2024-04-24 16:15:15.112 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1478, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 16:15:15.193 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1479, taskName=keyword filtering, firstSubmitTime=1713946515174, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=5, appIds=null, processInstanceId=584, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is stupid\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [${keywords}]\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\n# pass whether the data is relevant to singapore or not to the downstream tasks\n\nprint('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))\n\n\n\n\n# stop Spark session\nspark.stop()\n","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1479'}, message=Property{prop='message', direct=IN, type=VARCHAR, value=''}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424161515'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='584'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"message","direct":"IN","type":"VARCHAR","value":""}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.193 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.194 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.195 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.195 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.195 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.195 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713946515195
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.196 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 584_1479
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.196 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1479,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713946515174,
  "startTime" : 1713946515195,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/5/584/1479.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 5,
  "processInstanceId" : 584,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is stupid\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [${keywords}]\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\n# pass whether the data is relevant to singapore or not to the downstream tasks\\n\\nprint('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))\\n\\n\\n\\n\\n# stop Spark session\\nspark.stop()\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1479"
    },
    "message" : {
      "prop" : "message",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : ""
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424161515"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "584"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "584_1479",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"message\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.198 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.198 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.198 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.205 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.206 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.207 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1479 check successfully
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.208 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.208 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.210 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.210 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.211 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is stupid\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [${keywords}]\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\n# pass whether the data is relevant to singapore or not to the downstream tasks\n\nprint('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))\n\n\n\n\n# stop Spark session\nspark.stop()\n",
  "resourceList" : [ ]
}
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.225 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.225 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"message","direct":"IN","type":"VARCHAR","value":""}] successfully
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.225 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.226 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.227 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.227 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is stupid",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [${keywords}]

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

# pass whether the data is relevant to singapore or not to the downstream tasks

print('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))




# stop Spark session
spark.stop()

[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.229 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1479
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.229 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1479/py_584_1479.py
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.229 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# message = 
message = {
  "user": "premiumplatinum",
  "content": "singapore is stupid",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = ["singapore", "lol"]

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

# pass whether the data is relevant to singapore or not to the downstream tasks

print('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))




# stop Spark session
spark.stop()

[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.230 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.231 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.231 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1479/py_584_1479.py
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.232 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.232 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1479/584_1479.sh
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:15.235 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 150
[WI-0][TI-1479] - [INFO] 2024-04-24 16:15:16.118 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1479, success=true)
[WI-0][TI-1479] - [INFO] 2024-04-24 16:15:16.150 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1479)
[WI-0][TI-0] - [INFO] 2024-04-24 16:15:16.238 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	JAVA_HOME is not set
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1479/py_584_1479.py", line 15, in <module>
	    .getOrCreate()
	     ^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py", line 497, in getOrCreate
	    sc = SparkContext.getOrCreate(sparkConf)
	         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 515, in getOrCreate
	    SparkContext(conf=conf or SparkConf())
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 201, in __init__
	    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 436, in _ensure_initialized
	    SparkContext._gateway = gateway or launch_gateway(conf)
	                                       ^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/java_gateway.py", line 107, in launch_gateway
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:16.266 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1479, processId:150 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:16.274 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:16.277 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:16.277 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:16.277 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:16.290 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:16.291 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:16.291 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1479
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:16.293 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1479
[WI-584][TI-1479] - [INFO] 2024-04-24 16:15:16.294 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1479] - [INFO] 2024-04-24 16:15:17.135 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1479, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 16:15:17.274 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1480, taskName=keyword filtering, firstSubmitTime=1713946517241, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=5, appIds=null, processInstanceId=584, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is stupid\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [${keywords}]\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\n# pass whether the data is relevant to singapore or not to the downstream tasks\n\nprint('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))\n\n\n\n\n# stop Spark session\nspark.stop()\n","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1480'}, message=Property{prop='message', direct=IN, type=VARCHAR, value=''}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424161517'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='584'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"message","direct":"IN","type":"VARCHAR","value":""}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.279 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.282 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.283 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.304 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.304 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.304 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713946517304
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.304 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 584_1480
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.305 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1480,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713946517241,
  "startTime" : 1713946517304,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/5/584/1480.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 5,
  "processInstanceId" : 584,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is stupid\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [${keywords}]\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\n# pass whether the data is relevant to singapore or not to the downstream tasks\\n\\nprint('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))\\n\\n\\n\\n\\n# stop Spark session\\nspark.stop()\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1480"
    },
    "message" : {
      "prop" : "message",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : ""
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424161517"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "584"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "584_1480",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"message\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.306 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.307 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.307 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.312 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.313 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.314 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1480 check successfully
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.314 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.315 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.315 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.316 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.316 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is stupid\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [${keywords}]\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\n# pass whether the data is relevant to singapore or not to the downstream tasks\n\nprint('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))\n\n\n\n\n# stop Spark session\nspark.stop()\n",
  "resourceList" : [ ]
}
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.317 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.317 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"message","direct":"IN","type":"VARCHAR","value":""}] successfully
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.317 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.317 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.318 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.318 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is stupid",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [${keywords}]

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

# pass whether the data is relevant to singapore or not to the downstream tasks

print('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))




# stop Spark session
spark.stop()

[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.319 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1480
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.319 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1480/py_584_1480.py
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.319 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# message = 
message = {
  "user": "premiumplatinum",
  "content": "singapore is stupid",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = ["singapore", "lol"]

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

# pass whether the data is relevant to singapore or not to the downstream tasks

print('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))




# stop Spark session
spark.stop()

[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.320 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.320 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.321 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1480/py_584_1480.py
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.321 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.321 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1480/584_1480.sh
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:17.328 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 169
[WI-0][TI-0] - [INFO] 2024-04-24 16:15:17.351 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1480] - [INFO] 2024-04-24 16:15:18.123 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1480, success=true)
[WI-0][TI-1480] - [INFO] 2024-04-24 16:15:18.165 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1480)
[WI-0][TI-0] - [INFO] 2024-04-24 16:15:18.355 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	JAVA_HOME is not set
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1480/py_584_1480.py", line 15, in <module>
	    .getOrCreate()
	     ^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py", line 497, in getOrCreate
	    sc = SparkContext.getOrCreate(sparkConf)
	         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 515, in getOrCreate
	    SparkContext(conf=conf or SparkConf())
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 201, in __init__
	    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 436, in _ensure_initialized
	    SparkContext._gateway = gateway or launch_gateway(conf)
	                                       ^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/java_gateway.py", line 107, in launch_gateway
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:18.358 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1480, processId:169 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:18.358 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:18.358 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:18.359 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:18.359 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:18.366 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:18.366 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:18.367 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1480
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:18.369 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1480
[WI-584][TI-1480] - [INFO] 2024-04-24 16:15:18.371 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1480] - [INFO] 2024-04-24 16:15:19.112 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1480, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 16:15:19.229 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1481, taskName=keyword filtering, firstSubmitTime=1713946519196, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=5, appIds=null, processInstanceId=584, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is stupid\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [${keywords}]\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\n# pass whether the data is relevant to singapore or not to the downstream tasks\n\nprint('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))\n\n\n\n\n# stop Spark session\nspark.stop()\n","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1481'}, message=Property{prop='message', direct=IN, type=VARCHAR, value=''}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424161519'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='584'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"message","direct":"IN","type":"VARCHAR","value":""}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-0][TI-0] - [INFO] 2024-04-24 16:15:19.258 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7748538011695906 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.258 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.259 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.259 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.259 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.259 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713946519259
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.259 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 584_1481
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.260 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1481,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713946519196,
  "startTime" : 1713946519259,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/5/584/1481.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 5,
  "processInstanceId" : 584,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is stupid\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [${keywords}]\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\n# pass whether the data is relevant to singapore or not to the downstream tasks\\n\\nprint('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))\\n\\n\\n\\n\\n# stop Spark session\\nspark.stop()\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1481"
    },
    "message" : {
      "prop" : "message",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : ""
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424161519"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "584"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "584_1481",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"message\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.260 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.261 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.261 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.258 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.335 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.336 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.337 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1481 check successfully
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.337 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.337 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.337 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.338 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.352 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is stupid\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [${keywords}]\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\n# pass whether the data is relevant to singapore or not to the downstream tasks\n\nprint('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))\n\n\n\n\n# stop Spark session\nspark.stop()\n",
  "resourceList" : [ ]
}
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.353 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.353 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"message","direct":"IN","type":"VARCHAR","value":""}] successfully
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.353 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.353 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.353 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.353 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is stupid",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [${keywords}]

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

# pass whether the data is relevant to singapore or not to the downstream tasks

print('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))




# stop Spark session
spark.stop()

[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.354 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1481
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.354 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1481/py_584_1481.py
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.354 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# message = 
message = {
  "user": "premiumplatinum",
  "content": "singapore is stupid",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = ["singapore", "lol"]

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

# pass whether the data is relevant to singapore or not to the downstream tasks

print('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))




# stop Spark session
spark.stop()

[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.355 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.355 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.355 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1481/py_584_1481.py
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.355 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.355 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1481/584_1481.sh
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:19.384 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 188
[WI-0][TI-1481] - [INFO] 2024-04-24 16:15:20.140 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1481, success=true)
[WI-0][TI-1481] - [INFO] 2024-04-24 16:15:20.181 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1481)
[WI-0][TI-0] - [INFO] 2024-04-24 16:15:20.358 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9007832898172323 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:15:20.387 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	JAVA_HOME is not set
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1481/py_584_1481.py", line 15, in <module>
	    .getOrCreate()
	     ^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py", line 497, in getOrCreate
	    sc = SparkContext.getOrCreate(sparkConf)
	         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 515, in getOrCreate
	    SparkContext(conf=conf or SparkConf())
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 201, in __init__
	    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/context.py", line 436, in _ensure_initialized
	    SparkContext._gateway = gateway or launch_gateway(conf)
	                                       ^^^^^^^^^^^^^^^^^^^^
	  File "/usr/local/lib/python3.11/dist-packages/pyspark/java_gateway.py", line 107, in launch_gateway
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:20.391 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1481, processId:188 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:20.393 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:20.393 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:20.394 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:20.395 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:20.407 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:20.412 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:20.412 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1481
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:20.415 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_5/584/1481
[WI-584][TI-1481] - [INFO] 2024-04-24 16:15:20.417 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1481] - [INFO] 2024-04-24 16:15:21.127 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1481, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 16:16:11.678 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7361963190184049 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:17:08.019 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:17:21.125 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.744807121661721 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:17:24.117 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1482, taskName=extract from preprocessed queue, firstSubmitTime=1713946644106, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=6, appIds=null, processInstanceId=585, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"message","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n#     --bootstrap-server kafka:9092 \\\n#     --topic reddit_preprocessed \\\n#     --group preprocessed_consumers \\\n#     --max-messages 1 \\\n#     --timeout-ms 10000)\n\n# echo \"#{setValue(message=${message})}\"","resourceList":[]}, environmentConfig=null, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract from preprocessed queue'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1482'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13375898458848'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424161724'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='585'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.118 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract from preprocessed queue to wait queue success
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.119 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.149 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.150 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.150 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.151 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713946644151
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.157 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 585_1482
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.158 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1482,
  "taskName" : "extract from preprocessed queue",
  "firstSubmitTime" : 1713946644106,
  "startTime" : 1713946644151,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/6/585/1482.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 6,
  "processInstanceId" : 585,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"message\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\\\\n#     --bootstrap-server kafka:9092 \\\\\\n#     --topic reddit_preprocessed \\\\\\n#     --group preprocessed_consumers \\\\\\n#     --max-messages 1 \\\\\\n#     --timeout-ms 10000)\\n\\n# echo \\\"#{setValue(message=${message})}\\\"\",\"resourceList\":[]}",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract from preprocessed queue"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1482"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13375898458848"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424161724"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "585"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "585_1482",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.158 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.158 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.158 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.170 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-0][TI-0] - [INFO] 2024-04-24 16:17:24.171 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7210682492581602 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.171 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.173 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_6/585/1482 check successfully
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.173 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.175 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.176 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.176 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.179 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "message",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n#     --bootstrap-server kafka:9092 \\\n#     --topic reddit_preprocessed \\\n#     --group preprocessed_consumers \\\n#     --max-messages 1 \\\n#     --timeout-ms 10000)\n\n# echo \"#{setValue(message=${message})}\"",
  "resourceList" : [ ]
}
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.179 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.180 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.180 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.180 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.180 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.181 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.182 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.183 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
# KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \
#     --bootstrap-server kafka:9092 \
#     --topic reddit_preprocessed \
#     --group preprocessed_consumers \
#     --max-messages 1 \
#     --timeout-ms 10000)

# echo "#{setValue(message=${message})}"
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.183 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.184 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_6/585/1482/585_1482.sh
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:24.191 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 232
[WI-0][TI-1482] - [INFO] 2024-04-24 16:17:25.086 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1482, success=true)
[WI-0][TI-1482] - [INFO] 2024-04-24 16:17:25.119 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1482)
[WI-0][TI-0] - [INFO] 2024-04-24 16:17:25.192 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:25.215 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_6/585/1482, processId:232 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:25.216 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:25.216 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:25.216 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:25.216 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:25.224 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:25.225 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:25.225 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_6/585/1482
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:25.226 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_6/585/1482
[WI-585][TI-1482] - [INFO] 2024-04-24 16:17:25.226 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1482] - [INFO] 2024-04-24 16:17:26.076 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1482, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 16:17:26.218 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1483, taskName=keyword filtering, firstSubmitTime=1713946646208, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=6, appIds=null, processInstanceId=585, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is stupid\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [${keywords}]\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\n# pass whether the data is relevant to singapore or not to the downstream tasks\n\nprint('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))\n\n\n\n\n# stop Spark session\nspark.stop()\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1483'}, message=Property{prop='message', direct=IN, type=VARCHAR, value=''}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424161726'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='585'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"message","direct":"IN","type":"VARCHAR","value":""}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.220 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.220 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.222 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.223 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.223 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.223 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713946646223
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.223 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 585_1483
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.226 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1483,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713946646208,
  "startTime" : 1713946646223,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/6/585/1483.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 6,
  "processInstanceId" : 585,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is stupid\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [${keywords}]\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\n# pass whether the data is relevant to singapore or not to the downstream tasks\\n\\nprint('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))\\n\\n\\n\\n\\n# stop Spark session\\nspark.stop()\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1483"
    },
    "message" : {
      "prop" : "message",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : ""
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424161726"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "585"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "585_1483",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"message\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.227 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.227 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.227 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.233 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.233 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.234 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_6/585/1483 check successfully
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.234 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.234 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.235 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.235 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.236 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is stupid\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [${keywords}]\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\n# pass whether the data is relevant to singapore or not to the downstream tasks\n\nprint('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))\n\n\n\n\n# stop Spark session\nspark.stop()\n",
  "resourceList" : [ ]
}
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.236 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.237 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"message","direct":"IN","type":"VARCHAR","value":""}] successfully
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.237 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.237 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.237 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.237 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is stupid",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [${keywords}]

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

# pass whether the data is relevant to singapore or not to the downstream tasks

print('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))




# stop Spark session
spark.stop()

[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.238 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_6/585/1483
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.238 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_6/585/1483/py_585_1483.py
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.238 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# message = 
message = {
  "user": "premiumplatinum",
  "content": "singapore is stupid",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = ["singapore", "lol"]

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

# pass whether the data is relevant to singapore or not to the downstream tasks

print('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))




# stop Spark session
spark.stop()

[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.239 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.240 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.240 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_6/585/1483/py_585_1483.py
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.240 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.241 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_6/585/1483/585_1483.sh
[WI-585][TI-1483] - [INFO] 2024-04-24 16:17:26.255 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 243
[WI-0][TI-1483] - [INFO] 2024-04-24 16:17:27.087 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1483, success=true)
[WI-0][TI-1483] - [INFO] 2024-04-24 16:17:27.111 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1483)
[WI-0][TI-0] - [INFO] 2024-04-24 16:17:27.256 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-24 16:17:28.260 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3-scala2.13/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-3303d157-abc4-4b20-a2c5-1f308e90b90e;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-04-24 16:17:31.241 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7234042553191489 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:17:34.282 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central
[WI-0][TI-0] - [INFO] 2024-04-24 16:17:35.283 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
[WI-0][TI-0] - [INFO] 2024-04-24 16:17:37.287 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.slf4j#slf4j-api;2.0.7 in central
[WI-0][TI-0] - [INFO] 2024-04-24 16:17:40.336 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[WI-0][TI-0] - [INFO] 2024-04-24 16:17:40.336 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7032640949554896 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:17:41.350 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[WI-0][TI-0] - [INFO] 2024-04-24 16:17:42.351 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found commons-logging#commons-logging;1.1.3 in central
[WI-0][TI-0] - [INFO] 2024-04-24 16:17:43.354 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central
[WI-0][TI-0] - [INFO] 2024-04-24 16:17:46.359 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.commons#commons-pool2;2.11.1 in central
[WI-0][TI-0] - [INFO] 2024-04-24 16:17:47.369 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.13/3.5.1/spark-sql-kafka-0-10_2.13-3.5.1.jar ...
[WI-0][TI-0] - [INFO] 2024-04-24 16:17:48.378 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1!spark-sql-kafka-0-10_2.13.jar (1563ms)
	downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.13/3.5.1/spark-token-provider-kafka-0-10_2.13-3.5.1.jar ...
[WI-0][TI-0] - [INFO] 2024-04-24 16:17:49.380 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-parallel-collections_2.13/1.0.4/scala-parallel-collections_2.13-1.0.4.jar ...
[WI-0][TI-0] - [INFO] 2024-04-24 16:17:53.387 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4!scala-parallel-collections_2.13.jar (4829ms)
[WI-0][TI-0] - [INFO] 2024-04-24 16:17:54.391 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...
[WI-0][TI-0] - [INFO] 2024-04-24 16:18:33.487 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar (39731ms)
	downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...
[WI-0][TI-0] - [INFO] 2024-04-24 16:18:34.490 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (622ms)
	downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...
[WI-0][TI-0] - [INFO] 2024-04-24 16:18:35.491 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (1568ms)
	downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...
[WI-0][TI-0] - [INFO] 2024-04-24 16:18:37.642 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7292307692307692 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:19:41.967 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7160493827160493 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:19:59.726 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[55] - Receive TaskInstanceKillRequest: TaskInstanceKillRequest(taskInstanceId=1483)
[WI-0][TI-1483] - [ERROR] 2024-04-24 16:19:59.734 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[139] - kill task error
java.io.IOException: Cannot run program "pstree": error=2, No such file or directory
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.runCommand(AbstractShell.java:138)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.run(AbstractShell.java:118)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execute(ShellExecutor.java:125)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:103)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:86)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeShell(OSUtils.java:345)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeCmd(OSUtils.java:334)
	at org.apache.dolphinscheduler.plugin.task.api.utils.ProcessUtils.getPidsStr(ProcessUtils.java:129)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.killProcess(TaskInstanceKillOperationFunction.java:130)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.doKill(TaskInstanceKillOperationFunction.java:96)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.operate(TaskInstanceKillOperationFunction.java:69)
	at org.apache.dolphinscheduler.server.worker.rpc.TaskInstanceOperatorImpl.killTask(TaskInstanceOperatorImpl.java:49)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.dolphinscheduler.extract.base.server.ServerMethodInvokerImpl.invoke(ServerMethodInvokerImpl.java:41)
	at org.apache.dolphinscheduler.extract.base.server.JdkDynamicServerHandler.lambda$processReceived$0(JdkDynamicServerHandler.java:108)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: error=2, No such file or directory
	at java.lang.UNIXProcess.forkAndExec(Native Method)
	at java.lang.UNIXProcess.<init>(UNIXProcess.java:247)
	at java.lang.ProcessImpl.start(ProcessImpl.java:134)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	... 21 common frames omitted
[WI-0][TI-1483] - [INFO] 2024-04-24 16:19:59.737 +0800 o.a.d.p.t.a.u.ProcessUtils:[182] - Get appIds from worker 172.18.1.1:1234, taskLogPath: /opt/dolphinscheduler/logs/20240424/13377949373536/6/585/1483.log
[WI-0][TI-1483] - [INFO] 2024-04-24 16:19:59.738 +0800 o.a.d.p.t.a.u.LogUtils:[79] - Start finding appId in /opt/dolphinscheduler/logs/20240424/13377949373536/6/585/1483.log, fetch way: log 
[WI-0][TI-1483] - [INFO] 2024-04-24 16:19:59.745 +0800 o.a.d.p.t.a.u.ProcessUtils:[188] - The appId is empty
[WI-0][TI-1483] - [INFO] 2024-04-24 16:19:59.747 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[220] - Begin to kill process process, pid is : 243
[WI-0][TI-0] - [INFO] 2024-04-24 16:20:00.059 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8926553672316385 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:20:01.105 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7421203438395416 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1483] - [INFO] 2024-04-24 16:20:04.752 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[225] - Success kill task: 585_1483, pid: 243
[WI-0][TI-1483] - [INFO] 2024-04-24 16:20:04.753 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[119] - kill task by cancelApplication, taskInstanceId: 1483
[WI-0][TI-0] - [INFO] 2024-04-24 16:20:07.111 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1484, taskName=extract from preprocessed queue, firstSubmitTime=1713946807056, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=6, appIds=null, processInstanceId=585, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"message","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n#     --bootstrap-server kafka:9092 \\\n#     --topic reddit_preprocessed \\\n#     --group preprocessed_consumers \\\n#     --max-messages 1 \\\n#     --timeout-ms 10000)\n\n# echo \"#{setValue(message=${message})}\"","resourceList":[]}, environmentConfig=null, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract from preprocessed queue'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1484'}, message=Property{prop='message', direct=OUT, type=VARCHAR, value=''}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13375898458848'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424162007'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='585'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"message","direct":"OUT","type":"VARCHAR","value":""}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.128 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract from preprocessed queue to wait queue success
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.129 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.135 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.139 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.139 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.140 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713946807140
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.140 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 585_1484
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.153 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1484,
  "taskName" : "extract from preprocessed queue",
  "firstSubmitTime" : 1713946807056,
  "startTime" : 1713946807140,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/6/585/1484.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 6,
  "processInstanceId" : 585,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"message\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\\\\n#     --bootstrap-server kafka:9092 \\\\\\n#     --topic reddit_preprocessed \\\\\\n#     --group preprocessed_consumers \\\\\\n#     --max-messages 1 \\\\\\n#     --timeout-ms 10000)\\n\\n# echo \\\"#{setValue(message=${message})}\\\"\",\"resourceList\":[]}",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract from preprocessed queue"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1484"
    },
    "message" : {
      "prop" : "message",
      "direct" : "OUT",
      "type" : "VARCHAR",
      "value" : ""
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13375898458848"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424162007"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "585"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "585_1484",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"message\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.154 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.156 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.156 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.161 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.186 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.203 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_6/585/1484 check successfully
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.203 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.203 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.205 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.205 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.206 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "message",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n#     --bootstrap-server kafka:9092 \\\n#     --topic reddit_preprocessed \\\n#     --group preprocessed_consumers \\\n#     --max-messages 1 \\\n#     --timeout-ms 10000)\n\n# echo \"#{setValue(message=${message})}\"",
  "resourceList" : [ ]
}
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.206 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.207 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"message","direct":"OUT","type":"VARCHAR","value":""}] successfully
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.207 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.207 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.208 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.208 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.209 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.209 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
# KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \
#     --bootstrap-server kafka:9092 \
#     --topic reddit_preprocessed \
#     --group preprocessed_consumers \
#     --max-messages 1 \
#     --timeout-ms 10000)

# echo "#{setValue(message=)}"
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.209 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.209 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_6/585/1484/585_1484.sh
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:07.234 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 329
[WI-0][TI-1484] - [INFO] 2024-04-24 16:20:07.415 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1484, success=true)
[WI-0][TI-1484] - [INFO] 2024-04-24 16:20:07.424 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1484)
[WI-0][TI-0] - [INFO] 2024-04-24 16:20:08.240 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:08.242 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_6/585/1484, processId:329 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:08.243 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:08.243 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:08.243 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:08.244 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:08.261 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:08.262 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:08.263 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_6/585/1484
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:08.263 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_6/585/1484
[WI-585][TI-1484] - [INFO] 2024-04-24 16:20:08.263 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1484] - [INFO] 2024-04-24 16:20:08.405 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1484, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 16:20:08.462 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1485, taskName=keyword filtering, firstSubmitTime=1713946808443, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=6, appIds=null, processInstanceId=585, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is stupid\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [${keywords}]\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\n# pass whether the data is relevant to singapore or not to the downstream tasks\n\nprint('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))\n\n\n\n\n# stop Spark session\nspark.stop()\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1485'}, message=Property{prop='message', direct=IN, type=VARCHAR, value=''}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424162008'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='585'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"message","direct":"IN","type":"VARCHAR","value":""}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.464 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.464 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.470 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.470 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.470 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.470 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713946808470
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.471 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 585_1485
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.471 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1485,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713946808443,
  "startTime" : 1713946808470,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/6/585/1485.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 6,
  "processInstanceId" : 585,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# message = ${message}\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is stupid\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [${keywords}]\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\n# pass whether the data is relevant to singapore or not to the downstream tasks\\n\\nprint('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))\\n\\n\\n\\n\\n# stop Spark session\\nspark.stop()\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1485"
    },
    "message" : {
      "prop" : "message",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : ""
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424162008"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "585"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "585_1485",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"message\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.472 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.473 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.473 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.478 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.479 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.480 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_6/585/1485 check successfully
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.480 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.481 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.481 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.482 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.483 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# message = ${message}\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is stupid\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [${keywords}]\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\n# pass whether the data is relevant to singapore or not to the downstream tasks\n\nprint('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))\n\n\n\n\n# stop Spark session\nspark.stop()\n",
  "resourceList" : [ ]
}
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.485 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.485 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"message","direct":"IN","type":"VARCHAR","value":""}] successfully
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.485 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.485 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.485 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.487 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# message = ${message}
message = {
  "user": "premiumplatinum",
  "content": "singapore is stupid",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [${keywords}]

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

# pass whether the data is relevant to singapore or not to the downstream tasks

print('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))




# stop Spark session
spark.stop()

[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.488 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_6/585/1485
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.490 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_6/585/1485/py_585_1485.py
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.490 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# message = 
message = {
  "user": "premiumplatinum",
  "content": "singapore is stupid",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = ["singapore", "lol"]

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

# pass whether the data is relevant to singapore or not to the downstream tasks

print('#{setValue(is_relevant=%s)}' % str(True if filtered_df.count()==1 else False))




# stop Spark session
spark.stop()

[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.492 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.492 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.492 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_6/585/1485/py_585_1485.py
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.492 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.492 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_6/585/1485/585_1485.sh
[WI-585][TI-1485] - [INFO] 2024-04-24 16:20:08.497 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 340
[WI-0][TI-1485] - [INFO] 2024-04-24 16:20:09.473 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1485, success=true)
[WI-0][TI-1485] - [INFO] 2024-04-24 16:20:09.489 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1485)
[WI-0][TI-0] - [INFO] 2024-04-24 16:20:09.500 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-24 16:20:10.531 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3-scala2.13/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[WI-0][TI-0] - [INFO] 2024-04-24 16:20:11.534 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-21452fe6-e46a-40c3-913c-d6b84c6010bf;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
[WI-0][TI-0] - [INFO] 2024-04-24 16:20:12.535 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...
[WI-0][TI-0] - [INFO] 2024-04-24 16:20:53.398 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7738095238095237 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:21:15.671 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (64495ms)
	downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...
[WI-0][TI-0] - [INFO] 2024-04-24 16:21:16.673 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (724ms)
	downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.3/snappy-java-1.1.10.3.jar ...
[WI-0][TI-0] - [INFO] 2024-04-24 16:21:17.681 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.3!snappy-java.jar(bundle) (1363ms)
[WI-0][TI-0] - [INFO] 2024-04-24 16:21:18.684 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...
		[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (413ms)
	downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...
[WI-0][TI-0] - [ERROR] 2024-04-24 16:21:38.533 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[279] - Parse var pool error
java.io.IOException: Stream closed
	at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:170)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:283)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.io.BufferedReader.readLine(BufferedReader.java:324)
	at java.io.BufferedReader.readLine(BufferedReader.java:389)
	at org.apache.dolphinscheduler.plugin.task.api.AbstractCommandExecutor.lambda$parseProcessOutput$1(AbstractCommandExecutor.java:273)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[WI-585][TI-1483] - [INFO] 2024-04-24 16:21:38.934 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has killed. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_6/585/1483, processId:243 ,exitStatusCode:137 ,processWaitForStatus:true ,processExitValue:137
[WI-585][TI-1483] - [INFO] 2024-04-24 16:21:38.935 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1483] - [INFO] 2024-04-24 16:21:38.936 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-585][TI-1483] - [INFO] 2024-04-24 16:21:38.936 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1483] - [INFO] 2024-04-24 16:21:38.937 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-585][TI-1483] - [INFO] 2024-04-24 16:21:38.956 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: KILL to master : 172.18.1.1:1234
[WI-585][TI-1483] - [INFO] 2024-04-24 16:21:38.956 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-585][TI-1483] - [INFO] 2024-04-24 16:21:38.957 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_6/585/1483
[WI-585][TI-1483] - [INFO] 2024-04-24 16:21:38.957 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_6/585/1483
[WI-585][TI-1483] - [INFO] 2024-04-24 16:21:38.958 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1483] - [INFO] 2024-04-24 16:21:39.571 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1483, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 16:21:42.714 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (23837ms)
	downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...
		[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (421ms)
	:: resolution report :: resolve 346ms :: artifacts dl 91292ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   12  |   0   |   0   |   0   ||   12  |   6   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-21452fe6-e46a-40c3-913c-d6b84c6010bf
		confs: [default]
		12 artifacts copied, 0 already retrieved (57876kB/78ms)
	24/04/24 16:21:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-24 16:21:43.727 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-04-24 16:21:52.741 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-24 16:21:53.744 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-24 16:21:55.746 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	+-------------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+
	|            content|             context|           datetime|     id|score|subreddit|type|                 url|           user|
	+-------------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+
	|singapore is stupid|21apr2024 orchard...|2024-04-21 22:10:42|1c9itat|  131|singapore|post|https://www.reddi...|premiumplatinum|
	+-------------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+
	
	#{setValue(is_relevant=True)}
[WI-585][TI-1485] - [INFO] 2024-04-24 16:21:56.748 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_6/585/1485, processId:340 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-585][TI-1485] - [INFO] 2024-04-24 16:21:56.751 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1485] - [INFO] 2024-04-24 16:21:56.751 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-585][TI-1485] - [INFO] 2024-04-24 16:21:56.751 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-585][TI-1485] - [INFO] 2024-04-24 16:21:56.753 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-585][TI-1485] - [INFO] 2024-04-24 16:21:56.772 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-585][TI-1485] - [INFO] 2024-04-24 16:21:56.773 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-585][TI-1485] - [INFO] 2024-04-24 16:21:56.773 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_6/585/1485
[WI-585][TI-1485] - [INFO] 2024-04-24 16:21:56.774 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_6/585/1485
[WI-585][TI-1485] - [INFO] 2024-04-24 16:21:56.775 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1485] - [INFO] 2024-04-24 16:21:57.592 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1485, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 16:23:44.176 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.849858895039618 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:23:58.311 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7416413373860181 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:24:00.379 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7379518072289157 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:38:55.204 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8921282798833818 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:38:56.221 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9761194029850746 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:38:57.223 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9716713881019831 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:38:58.226 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9578651685393258 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:38:59.228 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9283819628647214 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:39:00.229 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8089887640449438 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:39:01.231 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8271954674220963 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:39:22.291 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8184357541899441 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:39:23.313 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7891737891737891 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:39:57.470 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8079268292682927 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:39:59.508 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7632311977715878 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:40:01.538 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7705382436260624 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:40:13.613 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7052023121387283 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:40:14.640 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7687861271676301 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:40:15.641 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7151335311572701 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:47:22.316 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7722222222222223 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:47:24.338 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8176470588235294 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:47:25.309 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1486, taskName=extract from preprocessed queue, firstSubmitTime=1713948445285, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=7, appIds=null, processInstanceId=586, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"message","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n#     --bootstrap-server kafka:9092 \\\n#     --topic reddit_preprocessed \\\n#     --group preprocessed_consumers \\\n#     --max-messages 1 \\\n#     --timeout-ms 10000)\n\n# echo \"#{setValue(message=${message})}\"","resourceList":[]}, environmentConfig=null, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract from preprocessed queue'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1486'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13375898458848'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424164725'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='586'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.314 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract from preprocessed queue to wait queue success
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.314 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.322 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.322 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.322 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.322 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713948445322
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.322 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 586_1486
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.323 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1486,
  "taskName" : "extract from preprocessed queue",
  "firstSubmitTime" : 1713948445285,
  "startTime" : 1713948445322,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/7/586/1486.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 7,
  "processInstanceId" : 586,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"message\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\\\\n#     --bootstrap-server kafka:9092 \\\\\\n#     --topic reddit_preprocessed \\\\\\n#     --group preprocessed_consumers \\\\\\n#     --max-messages 1 \\\\\\n#     --timeout-ms 10000)\\n\\n# echo \\\"#{setValue(message=${message})}\\\"\",\"resourceList\":[]}",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract from preprocessed queue"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1486"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13375898458848"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424164725"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "586"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "586_1486",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.324 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.324 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.324 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.337 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.338 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.339 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_7/586/1486 check successfully
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.339 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.339 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.340 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.340 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.341 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "message",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n#     --bootstrap-server kafka:9092 \\\n#     --topic reddit_preprocessed \\\n#     --group preprocessed_consumers \\\n#     --max-messages 1 \\\n#     --timeout-ms 10000)\n\n# echo \"#{setValue(message=${message})}\"",
  "resourceList" : [ ]
}
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.341 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.341 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.341 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.341 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.341 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.342 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.342 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.342 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
# KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \
#     --bootstrap-server kafka:9092 \
#     --topic reddit_preprocessed \
#     --group preprocessed_consumers \
#     --max-messages 1 \
#     --timeout-ms 10000)

# echo "#{setValue(message=${message})}"
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.342 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.342 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_7/586/1486/586_1486.sh
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:25.352 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 874
[WI-0][TI-1486] - [INFO] 2024-04-24 16:47:26.074 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1486, success=true)
[WI-0][TI-1486] - [INFO] 2024-04-24 16:47:26.085 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1486)
[WI-0][TI-0] - [INFO] 2024-04-24 16:47:26.353 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:26.355 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_7/586/1486, processId:874 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:26.356 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:26.357 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:26.357 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:26.357 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:26.362 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:26.362 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:26.362 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_7/586/1486
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:26.363 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_7/586/1486
[WI-586][TI-1486] - [INFO] 2024-04-24 16:47:26.363 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1486] - [INFO] 2024-04-24 16:47:27.038 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1486, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 16:47:27.184 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1487, taskName=keyword filtering, firstSubmitTime=1713948447163, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=7, appIds=null, processInstanceId=586, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is stupid\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame([message], StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1487'}, message=Property{prop='message', direct=IN, type=VARCHAR, value=''}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424164727'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='586'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"message","direct":"IN","type":"VARCHAR","value":""}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.195 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.195 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.197 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.198 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.198 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.198 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713948447198
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.198 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 586_1487
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.198 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1487,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713948447163,
  "startTime" : 1713948447198,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/7/586/1487.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 7,
  "processInstanceId" : 586,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is stupid\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame([message], StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1487"
    },
    "message" : {
      "prop" : "message",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : ""
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424164727"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "586"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "586_1487",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"message\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.200 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.200 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.200 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.207 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.208 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.211 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_7/586/1487 check successfully
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.211 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.211 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.212 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.212 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.212 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is stupid\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame([message], StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n",
  "resourceList" : [ ]
}
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.212 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.213 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"message","direct":"IN","type":"VARCHAR","value":""}] successfully
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.213 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.213 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.213 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.213 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = {
  "user": "premiumplatinum",
  "content": "singapore is stupid",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame([message], StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()

[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.216 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_7/586/1487
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.216 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_7/586/1487/py_586_1487.py
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.216 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = {
  "user": "premiumplatinum",
  "content": "singapore is stupid",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame([message], StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()

[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.217 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.217 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.217 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_7/586/1487/py_586_1487.py
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.217 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.217 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_7/586/1487/586_1487.sh
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:27.231 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 885
[WI-0][TI-0] - [INFO] 2024-04-24 16:47:27.231 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1487] - [INFO] 2024-04-24 16:47:28.087 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1487, success=true)
[WI-0][TI-1487] - [INFO] 2024-04-24 16:47:28.143 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1487)
[WI-0][TI-0] - [INFO] 2024-04-24 16:47:30.242 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3-scala2.13/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-fbec7194-028e-4c49-84b6-2ef7db2e3e60;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 541ms :: artifacts dl 19ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-fbec7194-028e-4c49-84b6-2ef7db2e3e60
		confs: [default]
		1 artifacts copied, 11 already retrieved (19002kB/65ms)
[WI-0][TI-0] - [INFO] 2024-04-24 16:47:30.444 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7161716171617162 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:47:31.251 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 16:47:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-04-24 16:47:41.287 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-24 16:47:42.361 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-24 16:47:44.754 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8717948717948718 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:47:46.447 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 5:>                                                          (0 + 1) / 1]
	24/04/24 16:47:46 WARN NetworkClient: [Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {reddit_filtered=LEADER_NOT_AVAILABLE}
[WI-0][TI-0] - [INFO] 2024-04-24 16:47:46.813 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8519736842105263 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:47:47.448 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 16:47:46 WARN NetworkClient: [Producer clientId=producer-1] Error while fetching metadata with correlation id 4 : {reddit_filtered=LEADER_NOT_AVAILABLE}
	24/04/24 16:47:46 WARN NetworkClient: [Producer clientId=producer-1] Error while fetching metadata with correlation id 5 : {reddit_filtered=LEADER_NOT_AVAILABLE}
	24/04/24 16:47:46 WARN NetworkClient: [Producer clientId=producer-1] Error while fetching metadata with correlation id 6 : {reddit_filtered=LEADER_NOT_AVAILABLE}
	
	                                                                                
	+-------------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+
	|            content|             context|           datetime|     id|score|subreddit|type|                 url|           user|
	+-------------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+
	|singapore is stupid|21apr2024 orchard...|2024-04-21 22:10:42|1c9itat|  131|singapore|post|https://www.reddi...|premiumplatinum|
	+-------------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+
	
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:48.459 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_7/586/1487, processId:885 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:48.460 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:48.461 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:48.461 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:48.462 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:48.466 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:48.466 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:48.469 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_7/586/1487
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:48.470 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_7/586/1487
[WI-586][TI-1487] - [INFO] 2024-04-24 16:47:48.470 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1487] - [INFO] 2024-04-24 16:47:49.502 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1487, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 16:47:49.864 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7189349112426036 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:47:51.917 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9075630252100839 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:47:57.958 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8042168674698795 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:48:59.238 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7245508982035929 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:49:00.310 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7432432432432432 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:49:01.313 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8448753462603877 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:53:03.361 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7097701149425287 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:53:04.371 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7954545454545455 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:53:12.429 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9131652661064424 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:55:12.436 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8353293413173652 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:55:27.179 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7851002865329513 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:55:38.247 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7722222222222221 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:55:39.303 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1488, taskName=extract from preprocessed queue, firstSubmitTime=1713948939271, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=8, appIds=null, processInstanceId=587, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"message","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n#     --bootstrap-server kafka:9092 \\\n#     --topic reddit_preprocessed \\\n#     --group preprocessed_consumers \\\n#     --max-messages 1 \\\n#     --timeout-ms 10000)\n\n# echo \"#{setValue(message=${message})}\"","resourceList":[]}, environmentConfig=null, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract from preprocessed queue'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1488'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13375898458848'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424165539'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='587'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.335 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract from preprocessed queue to wait queue success
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.335 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.366 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.366 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.367 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.367 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713948939367
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.371 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 587_1488
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.372 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1488,
  "taskName" : "extract from preprocessed queue",
  "firstSubmitTime" : 1713948939271,
  "startTime" : 1713948939367,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/8/587/1488.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 8,
  "processInstanceId" : 587,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"message\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\\\\n#     --bootstrap-server kafka:9092 \\\\\\n#     --topic reddit_preprocessed \\\\\\n#     --group preprocessed_consumers \\\\\\n#     --max-messages 1 \\\\\\n#     --timeout-ms 10000)\\n\\n# echo \\\"#{setValue(message=${message})}\\\"\",\"resourceList\":[]}",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract from preprocessed queue"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1488"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13375898458848"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424165539"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "587"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "587_1488",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.372 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.372 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.372 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.389 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.390 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.390 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_8/587/1488 check successfully
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.391 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.391 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.393 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.397 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.402 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "message",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n#     --bootstrap-server kafka:9092 \\\n#     --topic reddit_preprocessed \\\n#     --group preprocessed_consumers \\\n#     --max-messages 1 \\\n#     --timeout-ms 10000)\n\n# echo \"#{setValue(message=${message})}\"",
  "resourceList" : [ ]
}
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.407 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.407 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.407 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.408 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.411 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.419 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.426 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.426 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
# KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \
#     --bootstrap-server kafka:9092 \
#     --topic reddit_preprocessed \
#     --group preprocessed_consumers \
#     --max-messages 1 \
#     --timeout-ms 10000)

# echo "#{setValue(message=${message})}"
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.426 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.426 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_8/587/1488/587_1488.sh
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:39.446 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 1211
[WI-0][TI-1488] - [INFO] 2024-04-24 16:55:40.174 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1488, success=true)
[WI-0][TI-1488] - [INFO] 2024-04-24 16:55:40.189 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1488)
[WI-0][TI-0] - [INFO] 2024-04-24 16:55:40.272 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7859154929577464 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:55:40.450 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:40.453 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_8/587/1488, processId:1211 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:40.454 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:40.454 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:40.454 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:40.456 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:40.460 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:40.461 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:40.462 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_8/587/1488
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:40.462 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_8/587/1488
[WI-587][TI-1488] - [INFO] 2024-04-24 16:55:40.463 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1488] - [INFO] 2024-04-24 16:55:41.173 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1488, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 16:55:41.297 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1489, taskName=keyword filtering, firstSubmitTime=1713948941288, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=8, appIds=null, processInstanceId=587, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is stupid\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame([filtered_json], StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1489'}, message=Property{prop='message', direct=IN, type=VARCHAR, value=''}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424165541'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='587'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"message","direct":"IN","type":"VARCHAR","value":""}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.298 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.298 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.301 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.301 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.301 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.301 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713948941301
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.301 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 587_1489
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.301 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1489,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713948941288,
  "startTime" : 1713948941301,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/8/587/1489.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 8,
  "processInstanceId" : 587,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is stupid\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame([filtered_json], StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1489"
    },
    "message" : {
      "prop" : "message",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : ""
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424165541"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "587"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "587_1489",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"message\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.302 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.302 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.302 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.307 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.308 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.309 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_8/587/1489 check successfully
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.309 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.310 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.310 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.311 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.312 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is stupid\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame([filtered_json], StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.313 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.314 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"message","direct":"IN","type":"VARCHAR","value":""}] successfully
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.314 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.315 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.315 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.315 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = {
  "user": "premiumplatinum",
  "content": "singapore is stupid",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame([filtered_json], StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.316 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_8/587/1489
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.317 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_8/587/1489/py_587_1489.py
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.317 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = {
  "user": "premiumplatinum",
  "content": "singapore is stupid",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame([filtered_json], StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.319 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.319 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.320 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_8/587/1489/py_587_1489.py
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.321 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.322 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_8/587/1489/587_1489.sh
[WI-587][TI-1489] - [INFO] 2024-04-24 16:55:41.325 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 1222
[WI-0][TI-0] - [INFO] 2024-04-24 16:55:41.335 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1489] - [INFO] 2024-04-24 16:55:42.196 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1489, success=true)
[WI-0][TI-1489] - [INFO] 2024-04-24 16:55:42.225 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1489)
[WI-0][TI-0] - [INFO] 2024-04-24 16:55:43.341 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3-scala2.13/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[WI-0][TI-0] - [INFO] 2024-04-24 16:55:44.344 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-53e570bf-323b-4886-9189-5aae59bb3282;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 354ms :: artifacts dl 19ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-53e570bf-323b-4886-9189-5aae59bb3282
		confs: [default]
		0 artifacts copied, 12 already retrieved (0kB/8ms)
	24/04/24 16:55:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-24 16:55:44.346 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7058823529411764 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:55:45.348 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-04-24 16:55:47.378 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7230320699708455 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:55:48.410 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8147138964577657 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:55:49.411 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8366013071895425 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:55:50.417 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7085714285714286 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:55:56.432 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-24 16:55:57.435 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-24 16:55:58.513 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7397260273972602 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:55:59.398 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1490, taskName=extract from preprocessed queue, firstSubmitTime=1713948959387, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=8, appIds=null, processInstanceId=588, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"message","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n#     --bootstrap-server kafka:9092 \\\n#     --topic reddit_preprocessed \\\n#     --group preprocessed_consumers \\\n#     --max-messages 1 \\\n#     --timeout-ms 10000)\n\n# echo \"#{setValue(message=${message})}\"","resourceList":[]}, environmentConfig=null, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract from preprocessed queue'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1490'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13375898458848'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424165559'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='588'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.401 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract from preprocessed queue to wait queue success
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.403 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.407 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.407 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.408 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.409 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713948959409
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.409 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 588_1490
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.410 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1490,
  "taskName" : "extract from preprocessed queue",
  "firstSubmitTime" : 1713948959387,
  "startTime" : 1713948959409,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/8/588/1490.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 8,
  "processInstanceId" : 588,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"message\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\\\\n#     --bootstrap-server kafka:9092 \\\\\\n#     --topic reddit_preprocessed \\\\\\n#     --group preprocessed_consumers \\\\\\n#     --max-messages 1 \\\\\\n#     --timeout-ms 10000)\\n\\n# echo \\\"#{setValue(message=${message})}\\\"\",\"resourceList\":[]}",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract from preprocessed queue"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1490"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13375898458848"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424165559"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "588"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "588_1490",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.412 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.413 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.413 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.416 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.417 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.419 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_8/588/1490 check successfully
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.420 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.421 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.426 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.427 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.428 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "message",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n#     --bootstrap-server kafka:9092 \\\n#     --topic reddit_preprocessed \\\n#     --group preprocessed_consumers \\\n#     --max-messages 1 \\\n#     --timeout-ms 10000)\n\n# echo \"#{setValue(message=${message})}\"",
  "resourceList" : [ ]
}
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.429 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.429 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.430 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.430 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.431 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.432 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.432 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.433 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
# KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \
#     --bootstrap-server kafka:9092 \
#     --topic reddit_preprocessed \
#     --group preprocessed_consumers \
#     --max-messages 1 \
#     --timeout-ms 10000)

# echo "#{setValue(message=${message})}"
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.435 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.435 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_8/588/1490/588_1490.sh
[WI-588][TI-1490] - [INFO] 2024-04-24 16:55:59.477 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 1424
[WI-0][TI-0] - [INFO] 2024-04-24 16:55:59.492 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1490] - [INFO] 2024-04-24 16:56:00.256 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1490, success=true)
[WI-0][TI-1490] - [INFO] 2024-04-24 16:56:00.266 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1490)
[WI-588][TI-1490] - [INFO] 2024-04-24 16:56:00.521 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_8/588/1490, processId:1424 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-588][TI-1490] - [INFO] 2024-04-24 16:56:00.531 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-588][TI-1490] - [INFO] 2024-04-24 16:56:00.532 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-588][TI-1490] - [INFO] 2024-04-24 16:56:00.534 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-588][TI-1490] - [INFO] 2024-04-24 16:56:00.538 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-588][TI-1490] - [INFO] 2024-04-24 16:56:00.550 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-588][TI-1490] - [INFO] 2024-04-24 16:56:00.551 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-588][TI-1490] - [INFO] 2024-04-24 16:56:00.551 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_8/588/1490
[WI-588][TI-1490] - [INFO] 2024-04-24 16:56:00.553 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_8/588/1490
[WI-588][TI-1490] - [INFO] 2024-04-24 16:56:00.554 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1490] - [INFO] 2024-04-24 16:56:01.258 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1490, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 16:56:01.402 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1491, taskName=keyword filtering, firstSubmitTime=1713948961377, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=8, appIds=null, processInstanceId=588, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is stupid\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame([filtered_json], StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1491'}, message=Property{prop='message', direct=IN, type=VARCHAR, value=''}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424165601'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='588'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"message","direct":"IN","type":"VARCHAR","value":""}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.430 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.436 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.437 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.437 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.437 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.437 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713948961437
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.437 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 588_1491
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.437 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1491,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713948961377,
  "startTime" : 1713948961437,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/8/588/1491.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 8,
  "processInstanceId" : 588,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is stupid\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame([filtered_json], StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1491"
    },
    "message" : {
      "prop" : "message",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : ""
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424165601"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "588"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "588_1491",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"message\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.438 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.438 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.438 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.448 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.449 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.453 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_8/588/1491 check successfully
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.454 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.456 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.483 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-0][TI-0] - [INFO] 2024-04-24 16:56:01.480 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 6:>                                                          (0 + 1) / 1]
	
	                                                                                
	+-------------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+
	|            content|             context|           datetime|     id|score|subreddit|type|                 url|           user|
	+-------------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+
	|singapore is stupid|21apr2024 orchard...|2024-04-21 22:10:42|1c9itat|  131|singapore|post|https://www.reddi...|premiumplatinum|
	+-------------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+
	
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.483 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.510 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is stupid\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame([filtered_json], StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.513 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.514 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"message","direct":"IN","type":"VARCHAR","value":""}] successfully
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.524 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.524 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.524 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.524 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = {
  "user": "premiumplatinum",
  "content": "singapore is stupid",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame([filtered_json], StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.525 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_8/588/1491
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.527 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_8/588/1491/py_588_1491.py
[WI-0][TI-0] - [INFO] 2024-04-24 16:56:01.556 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9103855721393035 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.527 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = {
  "user": "premiumplatinum",
  "content": "singapore is stupid",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame([filtered_json], StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.559 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.559 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.559 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_8/588/1491/py_588_1491.py
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.560 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.560 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_8/588/1491/588_1491.sh
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:01.596 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 1469
[WI-0][TI-1491] - [INFO] 2024-04-24 16:56:02.280 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1491, success=true)
[WI-0][TI-1491] - [INFO] 2024-04-24 16:56:02.314 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1491)
[WI-587][TI-1489] - [INFO] 2024-04-24 16:56:02.510 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_8/587/1489, processId:1222 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-587][TI-1489] - [INFO] 2024-04-24 16:56:02.510 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-587][TI-1489] - [INFO] 2024-04-24 16:56:02.511 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-587][TI-1489] - [INFO] 2024-04-24 16:56:02.511 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-587][TI-1489] - [INFO] 2024-04-24 16:56:02.511 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-587][TI-1489] - [INFO] 2024-04-24 16:56:02.517 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-587][TI-1489] - [INFO] 2024-04-24 16:56:02.518 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-587][TI-1489] - [INFO] 2024-04-24 16:56:02.518 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_8/587/1489
[WI-587][TI-1489] - [INFO] 2024-04-24 16:56:02.519 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_8/587/1489
[WI-587][TI-1489] - [INFO] 2024-04-24 16:56:02.519 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-04-24 16:56:02.596 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-24 16:56:02.600 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8182176599618644 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1489] - [INFO] 2024-04-24 16:56:03.269 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1489, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 16:56:03.602 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8742690058479532 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:56:04.602 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3-scala2.13/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-d41c7a45-ac37-488e-b174-5006ff565e87;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 350ms :: artifacts dl 11ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-d41c7a45-ac37-488e-b174-5006ff565e87
		confs: [default]
		0 artifacts copied, 12 already retrieved (0kB/10ms)
[WI-0][TI-0] - [INFO] 2024-04-24 16:56:05.635 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 16:56:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-04-24 16:56:12.698 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-24 16:56:13.701 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.877094972067039 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:56:13.701 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-24 16:56:14.717 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7890173410404625 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:56:15.720 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7033639143730888 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:56:16.724 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9568733153638813 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:56:17.736 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8833333333333333 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:56:18.723 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 6:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-24 16:56:18.749 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8122977346278317 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:56:19.725 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
	+-------------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+
	|            content|             context|           datetime|     id|score|subreddit|type|                 url|           user|
	+-------------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+
	|singapore is stupid|21apr2024 orchard...|2024-04-21 22:10:42|1c9itat|  131|singapore|post|https://www.reddi...|premiumplatinum|
	+-------------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+
	
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:19.726 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_8/588/1491, processId:1469 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:19.727 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:19.729 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:19.729 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:19.730 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:19.736 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:19.737 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:19.737 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_8/588/1491
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:19.738 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_8/588/1491
[WI-588][TI-1491] - [INFO] 2024-04-24 16:56:19.738 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1491] - [INFO] 2024-04-24 16:56:20.315 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1491, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 16:56:29.794 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7900874635568513 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:56:34.841 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9113573407202216 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:56:35.880 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7661538461538462 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:57:42.161 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7020648967551623 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:57:44.206 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8211143695014662 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:57:47.990 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1492, taskName=extract from preprocessed queue, firstSubmitTime=1713949067940, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=9, appIds=null, processInstanceId=589, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"message","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n#     --bootstrap-server kafka:9092 \\\n#     --topic reddit_preprocessed \\\n#     --group preprocessed_consumers \\\n#     --max-messages 1 \\\n#     --timeout-ms 10000)\n\n# echo \"#{setValue(message=${message})}\"","resourceList":[]}, environmentConfig=null, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract from preprocessed queue'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1492'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13375898458848'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424165747'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='589'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.012 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.014 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract from preprocessed queue to wait queue success
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.039 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.039 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.040 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.040 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713949068040
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.040 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 589_1492
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.064 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1492,
  "taskName" : "extract from preprocessed queue",
  "firstSubmitTime" : 1713949067940,
  "startTime" : 1713949068040,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/9/589/1492.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 9,
  "processInstanceId" : 589,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"message\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\\\\n#     --bootstrap-server kafka:9092 \\\\\\n#     --topic reddit_preprocessed \\\\\\n#     --group preprocessed_consumers \\\\\\n#     --max-messages 1 \\\\\\n#     --timeout-ms 10000)\\n\\n# echo \\\"#{setValue(message=${message})}\\\"\",\"resourceList\":[]}",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract from preprocessed queue"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1492"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13375898458848"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424165747"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "589"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "589_1492",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.065 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.065 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.066 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.079 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.079 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.080 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_9/589/1492 check successfully
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.080 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.080 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.080 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.081 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.082 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "message",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\n# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n#     --bootstrap-server kafka:9092 \\\n#     --topic reddit_preprocessed \\\n#     --group preprocessed_consumers \\\n#     --max-messages 1 \\\n#     --timeout-ms 10000)\n\n# echo \"#{setValue(message=${message})}\"",
  "resourceList" : [ ]
}
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.082 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.082 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.082 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.082 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.082 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.083 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.083 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.083 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
# KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

# message=$(${KAFKA_DIR}/kafka-console-consumer.sh \
#     --bootstrap-server kafka:9092 \
#     --topic reddit_preprocessed \
#     --group preprocessed_consumers \
#     --max-messages 1 \
#     --timeout-ms 10000)

# echo "#{setValue(message=${message})}"
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.083 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.083 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_9/589/1492/589_1492.sh
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:48.087 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 1738
[WI-0][TI-0] - [INFO] 2024-04-24 16:57:48.257 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7535410764872521 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1492] - [INFO] 2024-04-24 16:57:48.563 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1492, success=true)
[WI-0][TI-1492] - [INFO] 2024-04-24 16:57:48.597 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1492)
[WI-0][TI-0] - [INFO] 2024-04-24 16:57:49.094 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:49.097 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_9/589/1492, processId:1738 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:49.099 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:49.099 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:49.124 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:49.125 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:49.131 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:49.132 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:49.132 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_9/589/1492
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:49.132 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_9/589/1492
[WI-589][TI-1492] - [INFO] 2024-04-24 16:57:49.133 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1492] - [INFO] 2024-04-24 16:57:49.559 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1492, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 16:57:49.656 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1493, taskName=keyword filtering, firstSubmitTime=1713949069646, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=9, appIds=null, processInstanceId=589, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is stupid\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1493'}, message=Property{prop='message', direct=IN, type=VARCHAR, value=''}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424165749'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='589'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"message","direct":"IN","type":"VARCHAR","value":""}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.657 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.658 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.659 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.659 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.659 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.659 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713949069659
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.660 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 589_1493
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.660 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1493,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713949069646,
  "startTime" : 1713949069659,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/9/589/1493.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 9,
  "processInstanceId" : 589,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = {\\n  \\\"user\\\": \\\"premiumplatinum\\\",\\n  \\\"content\\\": \\\"singapore is stupid\\\",\\n  \\\"url\\\": \\\"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\\\",\\n  \\\"context\\\": \\\"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\\\",\\n  \\\"score\\\": 131,\\n  \\\"subreddit\\\": \\\"singapore\\\",\\n  \\\"type\\\": \\\"post\\\",\\n  \\\"id\\\": \\\"1c9itat\\\",\\n  \\\"datetime\\\": \\\"2024-04-21 22:10:42\\\"\\n}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1493"
    },
    "message" : {
      "prop" : "message",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : ""
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424165749"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "589"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "589_1493",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"message\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.661 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.661 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.661 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.665 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.666 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.667 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_9/589/1493 check successfully
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.668 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.668 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.669 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.669 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.669 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = {\n  \"user\": \"premiumplatinum\",\n  \"content\": \"singapore is stupid\",\n  \"url\": \"https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/\",\n  \"context\": \"21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist\",\n  \"score\": 131,\n  \"subreddit\": \"singapore\",\n  \"type\": \"post\",\n  \"id\": \"1c9itat\",\n  \"datetime\": \"2024-04-21 22:10:42\"\n}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.670 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.670 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"message","direct":"IN","type":"VARCHAR","value":""}] successfully
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.670 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.670 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.670 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.670 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = {
  "user": "premiumplatinum",
  "content": "singapore is stupid",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.671 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_9/589/1493
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.672 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_9/589/1493/py_589_1493.py
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.672 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = {
  "user": "premiumplatinum",
  "content": "singapore is stupid",
  "url": "https://www.reddit.com/r/singapore/comments/1c9itat/21apr2024_orchard_road_camcar_turning_right_into/",
  "context": "21apr2024 orchard road camcar turning right into centrepoint get tbone by cyclist",
  "score": 131,
  "subreddit": "singapore",
  "type": "post",
  "id": "1c9itat",
  "datetime": "2024-04-21 22:10:42"
}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.673 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.673 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.674 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_9/589/1493/py_589_1493.py
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.674 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.674 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_9/589/1493/589_1493.sh
[WI-589][TI-1493] - [INFO] 2024-04-24 16:57:49.677 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 1749
[WI-0][TI-1493] - [INFO] 2024-04-24 16:57:50.587 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1493, success=true)
[WI-0][TI-1493] - [INFO] 2024-04-24 16:57:50.616 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1493)
[WI-0][TI-0] - [INFO] 2024-04-24 16:57:50.679 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-24 16:57:51.287 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9063444108761329 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:57:52.304 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8555240793201134 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:57:52.700 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3-scala2.13/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-411522d1-956c-4397-9fe4-36c2d9fc6d3e;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central
[WI-0][TI-0] - [INFO] 2024-04-24 16:57:53.305 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8600823045267489 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:57:53.734 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 486ms :: artifacts dl 13ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-411522d1-956c-4397-9fe4-36c2d9fc6d3e
		confs: [default]
		0 artifacts copied, 12 already retrieved (0kB/13ms)
	24/04/24 16:57:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-04-24 16:57:54.309 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7615062761506276 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:57:55.313 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8778625954198473 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:57:56.315 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7558139534883721 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:58:01.384 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7883435582822086 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:58:03.786 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:>                                                          (0 + 1) / 1]
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-24 16:58:04.551 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.752906976744186 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 16:58:06.794 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 6:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-24 16:58:07.796 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
	+-------------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+
	|            content|             context|           datetime|     id|score|subreddit|type|                 url|           user|
	+-------------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+
	|singapore is stupid|21apr2024 orchard...|2024-04-21 22:10:42|1c9itat|  131|singapore|post|https://www.reddi...|premiumplatinum|
	+-------------------+--------------------+-------------------+-------+-----+---------+----+--------------------+---------------+
	
[WI-589][TI-1493] - [INFO] 2024-04-24 16:58:07.799 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_9/589/1493, processId:1749 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-589][TI-1493] - [INFO] 2024-04-24 16:58:07.799 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-589][TI-1493] - [INFO] 2024-04-24 16:58:07.804 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-589][TI-1493] - [INFO] 2024-04-24 16:58:07.804 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-589][TI-1493] - [INFO] 2024-04-24 16:58:07.805 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-589][TI-1493] - [INFO] 2024-04-24 16:58:07.810 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-589][TI-1493] - [INFO] 2024-04-24 16:58:07.810 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-589][TI-1493] - [INFO] 2024-04-24 16:58:07.811 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_9/589/1493
[WI-589][TI-1493] - [INFO] 2024-04-24 16:58:07.811 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_9/589/1493
[WI-589][TI-1493] - [INFO] 2024-04-24 16:58:07.812 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1493] - [INFO] 2024-04-24 16:58:08.658 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1493, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 16:58:16.630 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7582417582417582 is over then the MaxCpuUsagePercentageThresholds 0.7
