[WI-0][TI-0] - [INFO] 2024-04-24 17:20:32.616 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1494, taskName=extract from preprocessed queue, firstSubmitTime=1713950432604, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=10, appIds=null, processInstanceId=590, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"message","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n    --bootstrap-server kafka:9092 \\\n    --topic reddit_preprocessed \\\n    --group preprocessed_consumers \\\n    --max-messages 1 \\\n    --timeout-ms 10000)\n\necho \"#{setValue(message=${message})}\"","resourceList":[]}, environmentConfig=null, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract from preprocessed queue'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1494'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13375898458848'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424172032'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='590'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.618 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract from preprocessed queue to wait queue success
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.618 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.626 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.627 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.638 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.639 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713950432638
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.639 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 590_1494
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.639 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1494,
  "taskName" : "extract from preprocessed queue",
  "firstSubmitTime" : 1713950432604,
  "startTime" : 1713950432638,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/10/590/1494.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 10,
  "processInstanceId" : 590,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"message\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\\\\n    --bootstrap-server kafka:9092 \\\\\\n    --topic reddit_preprocessed \\\\\\n    --group preprocessed_consumers \\\\\\n    --max-messages 1 \\\\\\n    --timeout-ms 10000)\\n\\necho \\\"#{setValue(message=${message})}\\\"\",\"resourceList\":[]}",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract from preprocessed queue"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1494"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13375898458848"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424172032"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "590"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "590_1494",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.640 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.640 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.640 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.644 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.645 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.647 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1494 check successfully
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.647 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.648 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.648 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.649 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.649 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "message",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n    --bootstrap-server kafka:9092 \\\n    --topic reddit_preprocessed \\\n    --group preprocessed_consumers \\\n    --max-messages 1 \\\n    --timeout-ms 10000)\n\necho \"#{setValue(message=${message})}\"",
  "resourceList" : [ ]
}
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.650 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.650 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.650 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.650 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.650 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.651 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.651 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.651 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

message=$(${KAFKA_DIR}/kafka-console-consumer.sh \
    --bootstrap-server kafka:9092 \
    --topic reddit_preprocessed \
    --group preprocessed_consumers \
    --max-messages 1 \
    --timeout-ms 10000)

echo "#{setValue(message=${message})}"
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.651 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.651 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1494/590_1494.sh
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:32.683 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2254
[WI-0][TI-0] - [INFO] 2024-04-24 17:20:32.683 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1494] - [INFO] 2024-04-24 17:20:33.331 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1494, success=true)
[WI-0][TI-1494] - [INFO] 2024-04-24 17:20:33.350 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1494)
[WI-0][TI-0] - [INFO] 2024-04-24 17:20:33.687 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	/opt/kafka_2.13-3.7.0/bin/kafka-run-class.sh: line 347: exec: java: not found
	#{setValue(message=)}
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:33.689 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1494, processId:2254 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-590][TI-1494] - [WARN] 2024-04-24 17:20:33.691 +0800 o.a.d.p.t.a.p.AbstractParameters:[152] - Cannot find the output parameter message in the task output parameters
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:33.692 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:33.692 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:33.692 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:33.693 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:33.696 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:33.697 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:33.697 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1494
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:33.698 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1494
[WI-590][TI-1494] - [INFO] 2024-04-24 17:20:33.698 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1494] - [INFO] 2024-04-24 17:20:34.329 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1494, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:20:34.445 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1495, taskName=keyword filtering, firstSubmitTime=1713950434438, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=10, appIds=null, processInstanceId=590, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1495'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424172034'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='590'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.446 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.447 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.448 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.449 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.449 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.449 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713950434449
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.449 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 590_1495
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.449 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1495,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713950434438,
  "startTime" : 1713950434449,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/10/590/1495.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 10,
  "processInstanceId" : 590,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1495"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424172034"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "590"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "590_1495",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.450 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.450 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.450 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.453 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.454 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.454 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1495 check successfully
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.454 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.455 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.455 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.455 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.455 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.455 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.456 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.456 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.456 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.456 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.456 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.471 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1495
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.472 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1495/py_590_1495.py
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.472 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.473 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.473 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.473 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1495/py_590_1495.py
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.473 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.473 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1495/590_1495.sh
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:34.476 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2634
[WI-0][TI-1495] - [INFO] 2024-04-24 17:20:35.350 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1495, success=true)
[WI-0][TI-1495] - [INFO] 2024-04-24 17:20:35.357 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1495)
[WI-0][TI-0] - [INFO] 2024-04-24 17:20:35.477 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1495/py_590_1495.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:35.481 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1495, processId:2634 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:35.482 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:35.482 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:35.482 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:35.482 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:35.487 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:35.487 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:35.487 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1495
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:35.488 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1495
[WI-590][TI-1495] - [INFO] 2024-04-24 17:20:35.488 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1495] - [INFO] 2024-04-24 17:20:36.348 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1495, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:20:36.512 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1496, taskName=keyword filtering, firstSubmitTime=1713950436457, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=10, appIds=null, processInstanceId=590, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1496'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424172036'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='590'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.514 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.518 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.534 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.536 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.536 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.536 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713950436536
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.537 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 590_1496
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.539 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1496,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713950436457,
  "startTime" : 1713950436536,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/10/590/1496.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 10,
  "processInstanceId" : 590,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1496"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424172036"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "590"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "590_1496",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.540 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.545 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.545 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.564 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.565 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.566 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1496 check successfully
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.566 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.566 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.566 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.566 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.567 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.567 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.567 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.567 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.567 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.567 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.568 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.572 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1496
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.573 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1496/py_590_1496.py
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.573 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.573 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.574 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.580 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1496/py_590_1496.py
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.581 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.581 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1496/590_1496.sh
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:36.602 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2645
[WI-0][TI-1496] - [INFO] 2024-04-24 17:20:37.342 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1496, success=true)
[WI-0][TI-1496] - [INFO] 2024-04-24 17:20:37.351 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1496)
[WI-0][TI-0] - [INFO] 2024-04-24 17:20:37.604 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1496/py_590_1496.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:37.607 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1496, processId:2645 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:37.608 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:37.609 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:37.609 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:37.610 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:37.613 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:37.614 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:37.614 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1496
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:37.615 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1496
[WI-590][TI-1496] - [INFO] 2024-04-24 17:20:37.616 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1496] - [INFO] 2024-04-24 17:20:38.345 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1496, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:20:38.395 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1497, taskName=keyword filtering, firstSubmitTime=1713950438381, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=10, appIds=null, processInstanceId=590, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1497'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424172038'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='590'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.396 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.398 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.399 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.400 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.400 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.400 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713950438400
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.400 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 590_1497
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.412 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1497,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713950438381,
  "startTime" : 1713950438400,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/10/590/1497.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 10,
  "processInstanceId" : 590,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1497"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424172038"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "590"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "590_1497",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.414 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.414 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.414 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.419 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.420 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.421 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1497 check successfully
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.423 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.423 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.424 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.424 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.424 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.425 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.425 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.425 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.426 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.426 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.426 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.426 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1497
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.427 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1497/py_590_1497.py
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.427 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.428 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.428 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.428 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1497/py_590_1497.py
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.428 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.428 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1497/590_1497.sh
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:38.434 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2656
[WI-0][TI-1497] - [INFO] 2024-04-24 17:20:39.355 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1497, success=true)
[WI-0][TI-1497] - [INFO] 2024-04-24 17:20:39.365 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1497)
[WI-0][TI-0] - [INFO] 2024-04-24 17:20:39.434 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1497/py_590_1497.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:39.437 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1497, processId:2656 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:39.438 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:39.439 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:39.439 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:39.439 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:39.444 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:39.445 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:39.445 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1497
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:39.448 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1497
[WI-590][TI-1497] - [INFO] 2024-04-24 17:20:39.448 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1497] - [INFO] 2024-04-24 17:20:40.356 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1497, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:20:40.440 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1498, taskName=keyword filtering, firstSubmitTime=1713950440425, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=10, appIds=null, processInstanceId=590, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1498'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424172040'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='590'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.441 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.442 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.443 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.443 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.443 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.443 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713950440443
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.443 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 590_1498
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.444 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1498,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713950440425,
  "startTime" : 1713950440443,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/10/590/1498.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 10,
  "processInstanceId" : 590,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1498"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424172040"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "590"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "590_1498",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.445 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.445 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.445 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.454 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.455 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.457 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1498 check successfully
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.457 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.457 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.458 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.458 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.460 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.461 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.461 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.462 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.462 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.462 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.462 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.463 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1498
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.464 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1498/py_590_1498.py
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.465 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.466 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.466 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.466 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1498/py_590_1498.py
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.466 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.466 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1498/590_1498.sh
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:40.471 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2667
[WI-0][TI-1498] - [INFO] 2024-04-24 17:20:41.375 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1498, success=true)
[WI-0][TI-1498] - [INFO] 2024-04-24 17:20:41.381 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1498)
[WI-0][TI-0] - [INFO] 2024-04-24 17:20:41.487 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1498/py_590_1498.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:41.489 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1498, processId:2667 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:41.490 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:41.490 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:41.490 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:41.491 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:41.530 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:41.530 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:41.530 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1498
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:41.531 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/590/1498
[WI-590][TI-1498] - [INFO] 2024-04-24 17:20:41.531 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1498] - [INFO] 2024-04-24 17:20:42.372 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1498, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:21:05.871 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7287671232876712 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:22:57.387 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1499, taskName=Extract Reddit Data, firstSubmitTime=1713950577372, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13045665829504, processDefineVersion=14, appIds=null, processInstanceId=591, scheduleTime=0, globalParams=[{"prop":"subreddit","direct":"IN","type":"VARCHAR","value":"singapore"},{"prop":"limit","direct":"IN","type":"VARCHAR","value":"3"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"data","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"import praw\nimport json\n\nclass RedditPostDataExtractor:\n    def __init__(self):\n        self.data = []\n\n    def extract_data(self, hot_posts):\n        if hot_posts:\n            for post in hot_posts:\n                # initialise a dictionary to contain the data of the post\n                post_data = {}\n\n                # use permalink as the primary key\n                permalink = post.permalink\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the poster's name\n                if post.author is not None:\n                    post_data['author'] = post.author.name\n                else:\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\n\n\n                # get the unix time in which the post was created\n                post_data['unix_time'] = post.created_utc\n                post_data['permalink'] = post.permalink\n                post_data['score'] = post.score\n                post_data['subreddit'] = post.subreddit.display_name\n                post_data['post_title'] = post.title\n                post_data['body'] = post.selftext\n\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\n                # t3 represents that it is a post\n                post_data['name'] = post.name\n\n                # store the posts data in the dictionary where the name is used as the key\n                self.data.append(post_data)\n\n                # extract the comments from the post\n                comments = post.comments.list()\n\n                # extract the data in the comments and store them inside the dictionary\n                self.extract_comments_data(comments)\n        \n        return self.data\n\n    def extract_comments_data(self, comments):\n        if comments:\n            for comment in comments:\n                # initialise a dictionary to contain the data of the comment\n                comment_data = {}\n\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the commenter name\n                if comment.author is not None:\n                    comment_data['author'] = comment.author.name\n                else:\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the comment was created\n                comment_data['unix_time'] = comment.created_utc\n                comment_data['permalink'] = comment.permalink\n                comment_data['score'] = comment.score\n                comment_data['subreddit'] = comment.subreddit.display_name\n                comment_data['post_title'] = comment.submission.title\n                comment_data['body'] = comment.body\n\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\n                # t1 represents that it is a post\n                comment_data['name'] = comment.name\n\n                # store the comment's data in the dictionary where the name is used as the key\n                self.data.append(comment_data)\n\n                # extract the comment's replies from the post\n                # replies = comment.replies\n\n                # extract the data in the comments and store them inside the dictionary\n                # self.extract_comments_data(replies)\n        \n        return\n    \ndef get_hot_posts(reddit_instance, subreddit_name, limit):\n    # Define the subreddit\n    subreddit = reddit_instance.subreddit(subreddit_name)\n\n    # Get hot posts from the subreddit\n    hot_posts = subreddit.hot(limit=limit)\n\n    # Filter out posts that are not pinned\n    filtered_posts = [post for post in hot_posts if not post.stickied]\n    return filtered_posts\n\n# initialize Reddit instance\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\n                     user_agent='Prawlingv1')\n\n\n# choose the subreddit to extract data from\nsubreddit = '${subreddit}'\nlimit = ${limit}\n\n# get hot posts from the subreddit\nhot_posts = get_hot_posts(reddit, subreddit, limit)\n\n# initialise a reddit data extractor object\ndata_extractor = RedditPostDataExtractor()\n\n# extract data from the hot posts from r/singapore\ndata = data_extractor.extract_data(hot_posts)\n\nprint(str(data))\n\nwith open(\"${REDDIT_HOME}/in/${system.workflow.instance.id}-${system.biz.curdate}.json\", \"w\") as fp:\n    json.dump(data, fp, indent=4) \n","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='Extract Reddit Data'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1499'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13045662268160'}, subreddit=Property{prop='subreddit', direct=IN, type=VARCHAR, value='singapore'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424172257'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='591'}, limit=Property{prop='limit', direct=IN, type=VARCHAR, value='3'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13045665829504'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.394 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: Extract Reddit Data to wait queue success
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.396 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.407 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.411 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.413 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.423 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713950577423
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.430 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 591_1499
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.431 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1499,
  "taskName" : "Extract Reddit Data",
  "firstSubmitTime" : 1713950577372,
  "startTime" : 1713950577423,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13045665829504/14/591/1499.log",
  "processId" : 0,
  "processDefineCode" : 13045665829504,
  "processDefineVersion" : 14,
  "processInstanceId" : 591,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"subreddit\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"singapore\"},{\"prop\":\"limit\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"3\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"data\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"import praw\\nimport json\\n\\nclass RedditPostDataExtractor:\\n    def __init__(self):\\n        self.data = []\\n\\n    def extract_data(self, hot_posts):\\n        if hot_posts:\\n            for post in hot_posts:\\n                # initialise a dictionary to contain the data of the post\\n                post_data = {}\\n\\n                # use permalink as the primary key\\n                permalink = post.permalink\\n\\n                # get the relevant post's data and store it inside post_data\\n\\n                # get the poster's name\\n                if post.author is not None:\\n                    post_data['author'] = post.author.name\\n                else:\\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\\n\\n\\n                # get the unix time in which the post was created\\n                post_data['unix_time'] = post.created_utc\\n                post_data['permalink'] = post.permalink\\n                post_data['score'] = post.score\\n                post_data['subreddit'] = post.subreddit.display_name\\n                post_data['post_title'] = post.title\\n                post_data['body'] = post.selftext\\n\\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\\n                # t3 represents that it is a post\\n                post_data['name'] = post.name\\n\\n                # store the posts data in the dictionary where the name is used as the key\\n                self.data.append(post_data)\\n\\n                # extract the comments from the post\\n                comments = post.comments.list()\\n\\n                # extract the data in the comments and store them inside the dictionary\\n                self.extract_comments_data(comments)\\n        \\n        return self.data\\n\\n    def extract_comments_data(self, comments):\\n        if comments:\\n            for comment in comments:\\n                # initialise a dictionary to contain the data of the comment\\n                comment_data = {}\\n\\n\\n                # get the relevant post's data and store it inside post_data\\n\\n                # get the commenter name\\n                if comment.author is not None:\\n                    comment_data['author'] = comment.author.name\\n                else:\\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\\n\\n                # get the unix time in which the comment was created\\n                comment_data['unix_time'] = comment.created_utc\\n                comment_data['permalink'] = comment.permalink\\n                comment_data['score'] = comment.score\\n                comment_data['subreddit'] = comment.subreddit.display_name\\n                comment_data['post_title'] = comment.submission.title\\n                comment_data['body'] = comment.body\\n\\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\\n                # t1 represents that it is a post\\n                comment_data['name'] = comment.name\\n\\n                # store the comment's data in the dictionary where the name is used as the key\\n                self.data.append(comment_data)\\n\\n                # extract the comment's replies from the post\\n                # replies = comment.replies\\n\\n                # extract the data in the comments and store them inside the dictionary\\n                # self.extract_comments_data(replies)\\n        \\n        return\\n    \\ndef get_hot_posts(reddit_instance, subreddit_name, limit):\\n    # Define the subreddit\\n    subreddit = reddit_instance.subreddit(subreddit_name)\\n\\n    # Get hot posts from the subreddit\\n    hot_posts = subreddit.hot(limit=limit)\\n\\n    # Filter out posts that are not pinned\\n    filtered_posts = [post for post in hot_posts if not post.stickied]\\n    return filtered_posts\\n\\n# initialize Reddit instance\\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\\n                     user_agent='Prawlingv1')\\n\\n\\n# choose the subreddit to extract data from\\nsubreddit = '${subreddit}'\\nlimit = ${limit}\\n\\n# get hot posts from the subreddit\\nhot_posts = get_hot_posts(reddit, subreddit, limit)\\n\\n# initialise a reddit data extractor object\\ndata_extractor = RedditPostDataExtractor()\\n\\n# extract data from the hot posts from r/singapore\\ndata = data_extractor.extract_data(hot_posts)\\n\\nprint(str(data))\\n\\nwith open(\\\"${REDDIT_HOME}/in/${system.workflow.instance.id}-${system.biz.curdate}.json\\\", \\\"w\\\") as fp:\\n    json.dump(data, fp, indent=4) \\n\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "Extract Reddit Data"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1499"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13045662268160"
    },
    "subreddit" : {
      "prop" : "subreddit",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "singapore"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424172257"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "591"
    },
    "limit" : {
      "prop" : "limit",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13045665829504"
    }
  },
  "taskAppId" : "591_1499",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.435 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.435 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.435 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.442 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.443 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.445 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_14/591/1499 check successfully
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.445 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.446 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.446 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.446 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.447 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "data",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "import praw\nimport json\n\nclass RedditPostDataExtractor:\n    def __init__(self):\n        self.data = []\n\n    def extract_data(self, hot_posts):\n        if hot_posts:\n            for post in hot_posts:\n                # initialise a dictionary to contain the data of the post\n                post_data = {}\n\n                # use permalink as the primary key\n                permalink = post.permalink\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the poster's name\n                if post.author is not None:\n                    post_data['author'] = post.author.name\n                else:\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\n\n\n                # get the unix time in which the post was created\n                post_data['unix_time'] = post.created_utc\n                post_data['permalink'] = post.permalink\n                post_data['score'] = post.score\n                post_data['subreddit'] = post.subreddit.display_name\n                post_data['post_title'] = post.title\n                post_data['body'] = post.selftext\n\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\n                # t3 represents that it is a post\n                post_data['name'] = post.name\n\n                # store the posts data in the dictionary where the name is used as the key\n                self.data.append(post_data)\n\n                # extract the comments from the post\n                comments = post.comments.list()\n\n                # extract the data in the comments and store them inside the dictionary\n                self.extract_comments_data(comments)\n        \n        return self.data\n\n    def extract_comments_data(self, comments):\n        if comments:\n            for comment in comments:\n                # initialise a dictionary to contain the data of the comment\n                comment_data = {}\n\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the commenter name\n                if comment.author is not None:\n                    comment_data['author'] = comment.author.name\n                else:\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the comment was created\n                comment_data['unix_time'] = comment.created_utc\n                comment_data['permalink'] = comment.permalink\n                comment_data['score'] = comment.score\n                comment_data['subreddit'] = comment.subreddit.display_name\n                comment_data['post_title'] = comment.submission.title\n                comment_data['body'] = comment.body\n\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\n                # t1 represents that it is a post\n                comment_data['name'] = comment.name\n\n                # store the comment's data in the dictionary where the name is used as the key\n                self.data.append(comment_data)\n\n                # extract the comment's replies from the post\n                # replies = comment.replies\n\n                # extract the data in the comments and store them inside the dictionary\n                # self.extract_comments_data(replies)\n        \n        return\n    \ndef get_hot_posts(reddit_instance, subreddit_name, limit):\n    # Define the subreddit\n    subreddit = reddit_instance.subreddit(subreddit_name)\n\n    # Get hot posts from the subreddit\n    hot_posts = subreddit.hot(limit=limit)\n\n    # Filter out posts that are not pinned\n    filtered_posts = [post for post in hot_posts if not post.stickied]\n    return filtered_posts\n\n# initialize Reddit instance\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\n                     user_agent='Prawlingv1')\n\n\n# choose the subreddit to extract data from\nsubreddit = '${subreddit}'\nlimit = ${limit}\n\n# get hot posts from the subreddit\nhot_posts = get_hot_posts(reddit, subreddit, limit)\n\n# initialise a reddit data extractor object\ndata_extractor = RedditPostDataExtractor()\n\n# extract data from the hot posts from r/singapore\ndata = data_extractor.extract_data(hot_posts)\n\nprint(str(data))\n\nwith open(\"${REDDIT_HOME}/in/${system.workflow.instance.id}-${system.biz.curdate}.json\", \"w\") as fp:\n    json.dump(data, fp, indent=4) \n",
  "resourceList" : [ ]
}
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.447 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.447 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.447 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.447 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.447 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.447 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import praw
import json

class RedditPostDataExtractor:
    def __init__(self):
        self.data = []

    def extract_data(self, hot_posts):
        if hot_posts:
            for post in hot_posts:
                # initialise a dictionary to contain the data of the post
                post_data = {}

                # use permalink as the primary key
                permalink = post.permalink

                # get the relevant post's data and store it inside post_data

                # get the poster's name
                if post.author is not None:
                    post_data['author'] = post.author.name
                else:
                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'


                # get the unix time in which the post was created
                post_data['unix_time'] = post.created_utc
                post_data['permalink'] = post.permalink
                post_data['score'] = post.score
                post_data['subreddit'] = post.subreddit.display_name
                post_data['post_title'] = post.title
                post_data['body'] = post.selftext

                # get the unique name of the post (name refers to the id of the post prefixed with t3_)
                # t3 represents that it is a post
                post_data['name'] = post.name

                # store the posts data in the dictionary where the name is used as the key
                self.data.append(post_data)

                # extract the comments from the post
                comments = post.comments.list()

                # extract the data in the comments and store them inside the dictionary
                self.extract_comments_data(comments)
        
        return self.data

    def extract_comments_data(self, comments):
        if comments:
            for comment in comments:
                # initialise a dictionary to contain the data of the comment
                comment_data = {}


                # get the relevant post's data and store it inside post_data

                # get the commenter name
                if comment.author is not None:
                    comment_data['author'] = comment.author.name
                else:
                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the comment was created
                comment_data['unix_time'] = comment.created_utc
                comment_data['permalink'] = comment.permalink
                comment_data['score'] = comment.score
                comment_data['subreddit'] = comment.subreddit.display_name
                comment_data['post_title'] = comment.submission.title
                comment_data['body'] = comment.body

                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)
                # t1 represents that it is a post
                comment_data['name'] = comment.name

                # store the comment's data in the dictionary where the name is used as the key
                self.data.append(comment_data)

                # extract the comment's replies from the post
                # replies = comment.replies

                # extract the data in the comments and store them inside the dictionary
                # self.extract_comments_data(replies)
        
        return
    
def get_hot_posts(reddit_instance, subreddit_name, limit):
    # Define the subreddit
    subreddit = reddit_instance.subreddit(subreddit_name)

    # Get hot posts from the subreddit
    hot_posts = subreddit.hot(limit=limit)

    # Filter out posts that are not pinned
    filtered_posts = [post for post in hot_posts if not post.stickied]
    return filtered_posts

# initialize Reddit instance
reddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',
                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',
                     user_agent='Prawlingv1')


# choose the subreddit to extract data from
subreddit = '${subreddit}'
limit = ${limit}

# get hot posts from the subreddit
hot_posts = get_hot_posts(reddit, subreddit, limit)

# initialise a reddit data extractor object
data_extractor = RedditPostDataExtractor()

# extract data from the hot posts from r/singapore
data = data_extractor.extract_data(hot_posts)

print(str(data))

with open("${REDDIT_HOME}/in/${system.workflow.instance.id}-${system.biz.curdate}.json", "w") as fp:
    json.dump(data, fp, indent=4) 

[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.448 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_14/591/1499
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.448 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_14/591/1499/py_591_1499.py
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.448 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import praw
import json

class RedditPostDataExtractor:
    def __init__(self):
        self.data = []

    def extract_data(self, hot_posts):
        if hot_posts:
            for post in hot_posts:
                # initialise a dictionary to contain the data of the post
                post_data = {}

                # use permalink as the primary key
                permalink = post.permalink

                # get the relevant post's data and store it inside post_data

                # get the poster's name
                if post.author is not None:
                    post_data['author'] = post.author.name
                else:
                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'


                # get the unix time in which the post was created
                post_data['unix_time'] = post.created_utc
                post_data['permalink'] = post.permalink
                post_data['score'] = post.score
                post_data['subreddit'] = post.subreddit.display_name
                post_data['post_title'] = post.title
                post_data['body'] = post.selftext

                # get the unique name of the post (name refers to the id of the post prefixed with t3_)
                # t3 represents that it is a post
                post_data['name'] = post.name

                # store the posts data in the dictionary where the name is used as the key
                self.data.append(post_data)

                # extract the comments from the post
                comments = post.comments.list()

                # extract the data in the comments and store them inside the dictionary
                self.extract_comments_data(comments)
        
        return self.data

    def extract_comments_data(self, comments):
        if comments:
            for comment in comments:
                # initialise a dictionary to contain the data of the comment
                comment_data = {}


                # get the relevant post's data and store it inside post_data

                # get the commenter name
                if comment.author is not None:
                    comment_data['author'] = comment.author.name
                else:
                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the comment was created
                comment_data['unix_time'] = comment.created_utc
                comment_data['permalink'] = comment.permalink
                comment_data['score'] = comment.score
                comment_data['subreddit'] = comment.subreddit.display_name
                comment_data['post_title'] = comment.submission.title
                comment_data['body'] = comment.body

                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)
                # t1 represents that it is a post
                comment_data['name'] = comment.name

                # store the comment's data in the dictionary where the name is used as the key
                self.data.append(comment_data)

                # extract the comment's replies from the post
                # replies = comment.replies

                # extract the data in the comments and store them inside the dictionary
                # self.extract_comments_data(replies)
        
        return
    
def get_hot_posts(reddit_instance, subreddit_name, limit):
    # Define the subreddit
    subreddit = reddit_instance.subreddit(subreddit_name)

    # Get hot posts from the subreddit
    hot_posts = subreddit.hot(limit=limit)

    # Filter out posts that are not pinned
    filtered_posts = [post for post in hot_posts if not post.stickied]
    return filtered_posts

# initialize Reddit instance
reddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',
                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',
                     user_agent='Prawlingv1')


# choose the subreddit to extract data from
subreddit = 'singapore'
limit = 3

# get hot posts from the subreddit
hot_posts = get_hot_posts(reddit, subreddit, limit)

# initialise a reddit data extractor object
data_extractor = RedditPostDataExtractor()

# extract data from the hot posts from r/singapore
data = data_extractor.extract_data(hot_posts)

print(str(data))

with open("/local_storage/reddit/in/591-20240424.json", "w") as fp:
    json.dump(data, fp, indent=4) 

[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.450 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.450 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.450 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_14/591/1499/py_591_1499.py
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.450 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.450 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_14/591/1499/591_1499.sh
[WI-591][TI-1499] - [INFO] 2024-04-24 17:22:57.461 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2707
[WI-0][TI-1499] - [INFO] 2024-04-24 17:22:57.604 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1499, success=true)
[WI-0][TI-1499] - [INFO] 2024-04-24 17:22:57.626 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1499)
[WI-0][TI-0] - [INFO] 2024-04-24 17:22:58.466 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:00.476 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_14/591/1499/py_591_1499.py", line 117, in <module>
	    data = data_extractor.extract_data(hot_posts)
	           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_14/591/1499/py_591_1499.py", line 47, in extract_data
	    self.extract_comments_data(comments)
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_14/591/1499/py_591_1499.py", line 61, in extract_comments_data
	    if comment.author is not None:
	       ^^^^^^^^^^^^^^
	AttributeError: 'MoreComments' object has no attribute 'author'
[WI-591][TI-1499] - [INFO] 2024-04-24 17:23:00.479 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_14/591/1499, processId:2707 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-591][TI-1499] - [INFO] 2024-04-24 17:23:00.480 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-591][TI-1499] - [INFO] 2024-04-24 17:23:00.480 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-591][TI-1499] - [INFO] 2024-04-24 17:23:00.480 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-591][TI-1499] - [INFO] 2024-04-24 17:23:00.481 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-591][TI-1499] - [INFO] 2024-04-24 17:23:00.486 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-591][TI-1499] - [INFO] 2024-04-24 17:23:00.487 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-591][TI-1499] - [INFO] 2024-04-24 17:23:00.487 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_14/591/1499
[WI-591][TI-1499] - [INFO] 2024-04-24 17:23:00.489 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_14/591/1499
[WI-591][TI-1499] - [INFO] 2024-04-24 17:23:00.489 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1499] - [INFO] 2024-04-24 17:23:00.611 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1499, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:01.438 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1500, taskName=search for intake JSON files, firstSubmitTime=1713950581430, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=113, appIds=null, processInstanceId=592, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1500'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340113777664'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424172301'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='592'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.440 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake JSON files to wait queue success
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.440 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.442 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.442 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.442 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.442 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713950581442
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.442 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 592_1500
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.445 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1500,
  "taskName" : "search for intake JSON files",
  "firstSubmitTime" : 1713950581430,
  "startTime" : 1713950581442,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/113/592/1500.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 113,
  "processInstanceId" : 592,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set raw_file_dir to null\\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1500"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340113777664"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424172301"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "592"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "592_1500",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.446 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.446 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.446 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.452 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.453 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.453 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1500 check successfully
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.453 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.454 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.454 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.454 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.454 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"",
  "resourceList" : [ ]
}
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.454 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.454 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.454 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.454 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.454 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.455 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.455 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.455 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/reddit/processing -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set raw_file_dir to null
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
fi

echo "#{setValue(raw_file_dir=${raw_file_dir})}"
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.455 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.456 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1500/592_1500.sh
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:01.477 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2719
[WI-0][TI-1500] - [INFO] 2024-04-24 17:23:01.611 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1500, success=true)
[WI-0][TI-1500] - [INFO] 2024-04-24 17:23:01.619 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1500)
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:02.478 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	#{setValue(raw_file_dir=/local_storage/reddit/processing/505-20240423.json)}
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:02.480 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1500, processId:2719 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:02.480 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:02.480 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:02.481 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:02.481 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:02.502 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:02.503 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:02.503 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1500
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:02.503 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1500
[WI-592][TI-1500] - [INFO] 2024-04-24 17:23:02.504 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1500] - [INFO] 2024-04-24 17:23:02.622 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1500, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:03.393 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7245508982035929 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:03.814 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1502, taskName=move to processing, firstSubmitTime=1713950583807, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=113, appIds=null, processInstanceId=592, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to processing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1502'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340390094208'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424172303'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='592'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/505-20240423.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/505-20240423.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.817 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to processing to wait queue success
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.818 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.819 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.819 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.819 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.819 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713950583819
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.820 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 592_1502
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.820 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1502,
  "taskName" : "move to processing",
  "firstSubmitTime" : 1713950583807,
  "startTime" : 1713950583819,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/113/592/1502.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 113,
  "processInstanceId" : 592,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the reddit processing folder\\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\\nfilename=$(basename \\\"${raw_file_dir}\\\")\\nif ! grep -q \\\"${REDDIT_HOME}/processing\\\" <<< \\\"${raw_file_dir}\\\"; then\\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\\nelse\\n    echo JSON is already in processing\\nfi\\n\\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\\necho \\\"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to processing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1502"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340390094208"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424172303"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "592"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/505-20240423.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "592_1502",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/505-20240423.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.821 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.821 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.821 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.827 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.829 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.829 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1502 check successfully
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.830 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.830 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.831 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.831 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.831 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"",
  "resourceList" : [ ]
}
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.831 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.832 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/505-20240423.json"}] successfully
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.832 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.832 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.832 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.833 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.833 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.833 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the reddit processing folder
# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error
filename=$(basename "/local_storage/reddit/processing/505-20240423.json")
if ! grep -q "/local_storage/reddit/processing" <<< "/local_storage/reddit/processing/505-20240423.json"; then
    mv /local_storage/reddit/processing/505-20240423.json /local_storage/reddit/processing
else
    echo JSON is already in processing
fi

# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing
echo "#{setValue(raw_file_dir=/local_storage/reddit/processing/${filename})}"
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.833 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.833 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1502/592_1502.sh
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:03.849 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2732
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:04.401 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8222891566265059 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1502] - [INFO] 2024-04-24 17:23:04.629 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1502, success=true)
[WI-0][TI-1502] - [INFO] 2024-04-24 17:23:04.647 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1502)
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:04.849 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	JSON is already in processing
	#{setValue(raw_file_dir=/local_storage/reddit/processing/505-20240423.json)}
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:04.857 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1502, processId:2732 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:04.858 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:04.858 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:04.858 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:04.858 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:04.873 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:04.873 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:04.874 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1502
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:04.874 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1502
[WI-592][TI-1502] - [INFO] 2024-04-24 17:23:04.878 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1502] - [INFO] 2024-04-24 17:23:05.635 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1502, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:05.753 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1503, taskName=preprocessing and kafka, firstSubmitTime=1713950585734, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=113, appIds=null, processInstanceId=592, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_reddit_preprocessing.py\n\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='preprocessing and kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1503'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13210058002752'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424172305'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='592'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/505-20240423.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/505-20240423.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.754 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: preprocessing and kafka to wait queue success
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.755 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.756 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.756 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.756 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.756 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713950585756
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.756 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 592_1503
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.757 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1503,
  "taskName" : "preprocessing and kafka",
  "firstSubmitTime" : 1713950585734,
  "startTime" : 1713950585756,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/113/592/1503.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 113,
  "processInstanceId" : 592,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name reddit_preprocessing \\\\\\n    --files ${raw_file_dir} \\\\\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\\\\n    --conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n    spark_reddit_preprocessing.py\\n\\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\\\\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocessing and kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1503"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13210058002752"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424172305"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "592"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/505-20240423.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "592_1503",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/505-20240423.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.758 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.758 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.758 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.763 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.763 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.764 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1503 check successfully
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.764 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.819 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py, resourceRelativePath=spark_reddit_preprocessing.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1503/spark_reddit_preprocessing.py)})
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.823 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.824 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.827 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_reddit_preprocessing.py\n\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py",
    "res" : null
  } ]
}
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.829 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.829 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/505-20240423.json"}] successfully
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.829 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.829 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.829 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.830 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.830 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.830 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master local \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name reddit_preprocessing \
    --files /local_storage/reddit/processing/505-20240423.json \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \
    --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
    spark_reddit_preprocessing.py

#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.831 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.831 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1503/592_1503.sh
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:05.893 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2744
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:06.420 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7463126843657817 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1503] - [INFO] 2024-04-24 17:23:06.659 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1503, success=true)
[WI-0][TI-1503] - [INFO] 2024-04-24 17:23:06.682 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1503)
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:06.893 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:07.494 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1504, taskName=extract from preprocessed queue, firstSubmitTime=1713950587475, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=10, appIds=null, processInstanceId=593, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"message","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n    --bootstrap-server kafka:9092 \\\n    --topic reddit_preprocessed \\\n    --group preprocessed_consumers \\\n    --max-messages 1 \\\n    --timeout-ms 10000)\n\necho \"#{setValue(message=${message})}\"","resourceList":[]}, environmentConfig=null, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract from preprocessed queue'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1504'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13375898458848'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424172307'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='593'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.495 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract from preprocessed queue to wait queue success
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.495 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.497 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.498 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.498 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.498 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713950587498
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.498 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 593_1504
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.498 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1504,
  "taskName" : "extract from preprocessed queue",
  "firstSubmitTime" : 1713950587475,
  "startTime" : 1713950587498,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/10/593/1504.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 10,
  "processInstanceId" : 593,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"message\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\\\\n    --bootstrap-server kafka:9092 \\\\\\n    --topic reddit_preprocessed \\\\\\n    --group preprocessed_consumers \\\\\\n    --max-messages 1 \\\\\\n    --timeout-ms 10000)\\n\\necho \\\"#{setValue(message=${message})}\\\"\",\"resourceList\":[]}",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract from preprocessed queue"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1504"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13375898458848"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424172307"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "593"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "593_1504",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.499 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.499 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.499 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.511 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.512 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.513 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1504 check successfully
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.513 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.514 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.516 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.517 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.522 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "message",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n    --bootstrap-server kafka:9092 \\\n    --topic reddit_preprocessed \\\n    --group preprocessed_consumers \\\n    --max-messages 1 \\\n    --timeout-ms 10000)\n\necho \"#{setValue(message=${message})}\"",
  "resourceList" : [ ]
}
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.522 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.522 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.522 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.522 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.522 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.524 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.525 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.525 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

message=$(${KAFKA_DIR}/kafka-console-consumer.sh \
    --bootstrap-server kafka:9092 \
    --topic reddit_preprocessed \
    --group preprocessed_consumers \
    --max-messages 1 \
    --timeout-ms 10000)

echo "#{setValue(message=${message})}"
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.525 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.525 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1504/593_1504.sh
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:07.536 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2786
[WI-0][TI-1504] - [INFO] 2024-04-24 17:23:07.642 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1504, success=true)
[WI-0][TI-1504] - [INFO] 2024-04-24 17:23:07.652 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1504)
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:08.452 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8297872340425532 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:08.537 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:08.896 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3-scala2.13/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-80e3cb05-5a79-4ada-99f5-879a519658b0;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 670ms :: artifacts dl 12ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-80e3cb05-5a79-4ada-99f5-879a519658b0
		confs: [default]
		0 artifacts copied, 12 already retrieved (0kB/11ms)
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:09.471 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8116883116883116 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:09.539 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	/opt/kafka_2.13-3.7.0/bin/kafka-run-class.sh: line 347: exec: java: not found
	#{setValue(message=)}
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:09.541 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1504, processId:2786 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-593][TI-1504] - [WARN] 2024-04-24 17:23:09.541 +0800 o.a.d.p.t.a.p.AbstractParameters:[152] - Cannot find the output parameter message in the task output parameters
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:09.542 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:09.542 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:09.542 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:09.543 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:09.548 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:09.548 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:09.548 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1504
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:09.549 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1504
[WI-593][TI-1504] - [INFO] 2024-04-24 17:23:09.549 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1504] - [INFO] 2024-04-24 17:23:09.654 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1504, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:09.750 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1505, taskName=keyword filtering, firstSubmitTime=1713950589716, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=10, appIds=null, processInstanceId=593, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1505'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424172309'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='593'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.750 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.753 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.755 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.755 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.755 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.755 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713950589755
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.755 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 593_1505
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.755 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1505,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713950589716,
  "startTime" : 1713950589755,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/10/593/1505.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 10,
  "processInstanceId" : 593,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1505"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424172309"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "593"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "593_1505",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.763 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.763 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.764 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.775 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.787 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.794 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1505 check successfully
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.795 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.795 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.795 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.795 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.802 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.803 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.803 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.803 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.803 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.803 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.803 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.804 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1505
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.804 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1505/py_593_1505.py
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.804 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.805 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.806 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.806 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1505/py_593_1505.py
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.806 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.806 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1505/593_1505.sh
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:09.814 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3174
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:09.902 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 17:23:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-1505] - [INFO] 2024-04-24 17:23:10.745 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1505, success=true)
[WI-0][TI-1505] - [INFO] 2024-04-24 17:23:10.772 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1505)
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:10.817 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1505/py_593_1505.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:10.823 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1505, processId:3174 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:10.823 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:10.823 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:10.824 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:10.824 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:10.830 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:10.831 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:10.831 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1505
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:10.831 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1505
[WI-593][TI-1505] - [INFO] 2024-04-24 17:23:10.831 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:10.904 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 17:23:10 INFO SparkContext: Running Spark version 3.5.1
	24/04/24 17:23:10 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/04/24 17:23:10 INFO SparkContext: Java version 1.8.0_402
	24/04/24 17:23:10 INFO ResourceUtils: ==============================================================
	24/04/24 17:23:10 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/04/24 17:23:10 INFO ResourceUtils: ==============================================================
	24/04/24 17:23:10 INFO SparkContext: Submitted application: reddit_preprocessing
	24/04/24 17:23:10 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/04/24 17:23:10 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
	24/04/24 17:23:10 INFO ResourceProfileManager: Added ResourceProfile id: 0
	24/04/24 17:23:10 INFO SecurityManager: Changing view acls to: default
	24/04/24 17:23:10 INFO SecurityManager: Changing modify acls to: default
	24/04/24 17:23:10 INFO SecurityManager: Changing view acls groups to: 
	24/04/24 17:23:10 INFO SecurityManager: Changing modify acls groups to: 
	24/04/24 17:23:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
	24/04/24 17:23:10 INFO Utils: Successfully started service 'sparkDriver' on port 41423.
	24/04/24 17:23:10 INFO SparkEnv: Registering MapOutputTracker
	24/04/24 17:23:10 INFO SparkEnv: Registering BlockManagerMaster
[WI-0][TI-1505] - [INFO] 2024-04-24 17:23:11.681 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1505, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:11.794 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1506, taskName=keyword filtering, firstSubmitTime=1713950591782, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=10, appIds=null, processInstanceId=593, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1506'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424172311'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='593'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.796 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.796 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.797 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.797 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.797 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.797 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713950591797
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.798 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 593_1506
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.798 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1506,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713950591782,
  "startTime" : 1713950591797,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/10/593/1506.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 10,
  "processInstanceId" : 593,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1506"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424172311"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "593"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "593_1506",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.799 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.800 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.800 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.806 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.806 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.807 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1506 check successfully
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.807 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.807 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.808 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.808 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.809 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.809 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.809 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.809 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.810 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.810 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.810 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.810 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1506
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.811 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1506/py_593_1506.py
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.811 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.811 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.812 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.813 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1506/py_593_1506.py
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.813 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.813 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1506/593_1506.sh
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:11.816 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3264
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:11.905 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 17:23:10 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/04/24 17:23:10 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/04/24 17:23:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9e6f4180-925d-427f-b8ea-f774216064c2
	24/04/24 17:23:10 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
	24/04/24 17:23:10 INFO SparkEnv: Registering OutputCommitCoordinator
	24/04/24 17:23:11 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/04/24 17:23:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/04/24 17:23:11 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar at spark://1036ec04ddf9:41423/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar at spark://1036ec04ddf9:41423/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO SparkContext: Added JAR file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar at spark://1036ec04ddf9:41423/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://1036ec04ddf9:41423/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO SparkContext: Added JAR file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://1036ec04ddf9:41423/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://1036ec04ddf9:41423/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://1036ec04ddf9:41423/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO SparkContext: Added JAR file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at spark://1036ec04ddf9:41423/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO SparkContext: Added JAR file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://1036ec04ddf9:41423/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO SparkContext: Added JAR file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://1036ec04ddf9:41423/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://1036ec04ddf9:41423/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO SparkContext: Added JAR file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at spark://1036ec04ddf9:41423/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO SparkContext: Added file file:///local_storage/reddit/processing/505-20240423.json at file:///local_storage/reddit/processing/505-20240423.json with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: Copying /local_storage/reddit/processing/505-20240423.json to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/505-20240423.json
	24/04/24 17:23:11 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar
	24/04/24 17:23:11 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar
	24/04/24 17:23:11 INFO SparkContext: Added file file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar at file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: Copying /tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar
	24/04/24 17:23:11 INFO SparkContext: Added file file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: Copying /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/24 17:23:11 INFO SparkContext: Added file file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: Copying /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/24 17:23:11 INFO SparkContext: Added file file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: Copying /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/24 17:23:11 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/24 17:23:11 INFO SparkContext: Added file file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: Copying /tmp/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.lz4_lz4-java-1.8.0.jar
	24/04/24 17:23:11 INFO SparkContext: Added file file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: Copying /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/24 17:23:11 INFO SparkContext: Added file file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: Copying /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.slf4j_slf4j-api-2.0.7.jar
	24/04/24 17:23:11 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/24 17:23:11 INFO SparkContext: Added file file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: Copying /tmp/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/commons-logging_commons-logging-1.1.3.jar
	24/04/24 17:23:11 INFO Executor: Starting executor ID driver on host 1036ec04ddf9
	24/04/24 17:23:11 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/04/24 17:23:11 INFO Executor: Java version 1.8.0_402
	24/04/24 17:23:11 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/04/24 17:23:11 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1de83575 for default.
	24/04/24 17:23:11 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar has been previously copied to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar
	24/04/24 17:23:11 INFO Executor: Fetching file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/24 17:23:11 INFO Executor: Fetching file:///local_storage/reddit/processing/505-20240423.json with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: /local_storage/reddit/processing/505-20240423.json has been previously copied to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/505-20240423.json
	24/04/24 17:23:11 INFO Executor: Fetching file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: /tmp/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.lz4_lz4-java-1.8.0.jar
	24/04/24 17:23:11 INFO Executor: Fetching file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/24 17:23:11 INFO Executor: Fetching file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.slf4j_slf4j-api-2.0.7.jar
	24/04/24 17:23:11 INFO Executor: Fetching file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/24 17:23:11 INFO Executor: Fetching file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: /tmp/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/commons-logging_commons-logging-1.1.3.jar
	24/04/24 17:23:11 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/24 17:23:11 INFO Executor: Fetching file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: /tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar has been previously copied to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar
	24/04/24 17:23:11 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/24 17:23:11 INFO Executor: Fetching file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/24 17:23:11 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar has been previously copied to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar
	24/04/24 17:23:11 INFO Executor: Fetching spark://1036ec04ddf9:41423/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1713950590284
[WI-0][TI-1506] - [INFO] 2024-04-24 17:23:12.701 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1506, success=true)
[WI-0][TI-1506] - [INFO] 2024-04-24 17:23:12.711 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1506)
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:12.820 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1506/py_593_1506.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:12.823 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1506, processId:3264 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:12.823 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:12.823 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:12.824 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:12.824 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:12.826 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:12.827 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:12.827 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1506
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:12.827 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1506
[WI-593][TI-1506] - [INFO] 2024-04-24 17:23:12.828 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:12.912 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 17:23:11 INFO TransportClientFactory: Successfully created connection to 1036ec04ddf9/172.18.1.1:41423 after 46 ms (0 ms spent in bootstraps)
	24/04/24 17:23:11 INFO Utils: Fetching spark://1036ec04ddf9:41423/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/fetchFileTemp4973848047182514520.tmp
	24/04/24 17:23:11 INFO Utils: /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/fetchFileTemp4973848047182514520.tmp has been previously copied to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar
	24/04/24 17:23:11 INFO Executor: Adding file:/tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar to class loader default
	24/04/24 17:23:11 INFO Executor: Fetching spark://1036ec04ddf9:41423/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1713950590284
	24/04/24 17:23:11 INFO Utils: Fetching spark://1036ec04ddf9:41423/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/fetchFileTemp576436674173950075.tmp
	24/04/24 17:23:11 INFO Utils: /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/fetchFileTemp576436674173950075.tmp has been previously copied to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar
	24/04/24 17:23:12 INFO Executor: Adding file:/tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar to class loader default
	24/04/24 17:23:12 INFO Executor: Fetching spark://1036ec04ddf9:41423/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1713950590284
	24/04/24 17:23:12 INFO Utils: Fetching spark://1036ec04ddf9:41423/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/fetchFileTemp2487312739748800900.tmp
	24/04/24 17:23:12 INFO Utils: /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/fetchFileTemp2487312739748800900.tmp has been previously copied to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar
	24/04/24 17:23:12 INFO Executor: Adding file:/tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar to class loader default
	24/04/24 17:23:12 INFO Executor: Fetching spark://1036ec04ddf9:41423/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1713950590284
	24/04/24 17:23:12 INFO Utils: Fetching spark://1036ec04ddf9:41423/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/fetchFileTemp5064778948999281455.tmp
	24/04/24 17:23:12 INFO Utils: /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/fetchFileTemp5064778948999281455.tmp has been previously copied to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.lz4_lz4-java-1.8.0.jar
	24/04/24 17:23:12 INFO Executor: Adding file:/tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.lz4_lz4-java-1.8.0.jar to class loader default
	24/04/24 17:23:12 INFO Executor: Fetching spark://1036ec04ddf9:41423/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1713950590284
	24/04/24 17:23:12 INFO Utils: Fetching spark://1036ec04ddf9:41423/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/fetchFileTemp6267203747046956745.tmp
	24/04/24 17:23:12 INFO Utils: /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/fetchFileTemp6267203747046956745.tmp has been previously copied to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/24 17:23:12 INFO Executor: Adding file:/tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
	24/04/24 17:23:12 INFO Executor: Fetching spark://1036ec04ddf9:41423/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1713950590284
	24/04/24 17:23:12 INFO Utils: Fetching spark://1036ec04ddf9:41423/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/fetchFileTemp2699007265059801082.tmp
	24/04/24 17:23:12 INFO Utils: /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/fetchFileTemp2699007265059801082.tmp has been previously copied to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/commons-logging_commons-logging-1.1.3.jar
	24/04/24 17:23:12 INFO Executor: Adding file:/tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/commons-logging_commons-logging-1.1.3.jar to class loader default
	24/04/24 17:23:12 INFO Executor: Fetching spark://1036ec04ddf9:41423/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1713950590284
	24/04/24 17:23:12 INFO Utils: Fetching spark://1036ec04ddf9:41423/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/fetchFileTemp2129180929122036061.tmp
	24/04/24 17:23:12 INFO Utils: /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/fetchFileTemp2129180929122036061.tmp has been previously copied to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/24 17:23:12 INFO Executor: Adding file:/tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
	24/04/24 17:23:12 INFO Executor: Fetching spark://1036ec04ddf9:41423/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1713950590284
	24/04/24 17:23:12 INFO Utils: Fetching spark://1036ec04ddf9:41423/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/fetchFileTemp2034807078702923116.tmp
	24/04/24 17:23:12 INFO Utils: /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/fetchFileTemp2034807078702923116.tmp has been previously copied to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/24 17:23:12 INFO Executor: Adding file:/tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
	24/04/24 17:23:12 INFO Executor: Fetching spark://1036ec04ddf9:41423/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1713950590284
	24/04/24 17:23:12 INFO Utils: Fetching spark://1036ec04ddf9:41423/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/fetchFileTemp137963665181953622.tmp
	24/04/24 17:23:12 INFO Utils: /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/fetchFileTemp137963665181953622.tmp has been previously copied to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/24 17:23:12 INFO Executor: Adding file:/tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
	24/04/24 17:23:12 INFO Executor: Fetching spark://1036ec04ddf9:41423/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1713950590284
	24/04/24 17:23:12 INFO Utils: Fetching spark://1036ec04ddf9:41423/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/fetchFileTemp2033684782922939227.tmp
	24/04/24 17:23:12 INFO Utils: /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/fetchFileTemp2033684782922939227.tmp has been previously copied to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/24 17:23:12 INFO Executor: Adding file:/tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
	24/04/24 17:23:12 INFO Executor: Fetching spark://1036ec04ddf9:41423/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1713950590284
	24/04/24 17:23:12 INFO Utils: Fetching spark://1036ec04ddf9:41423/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/fetchFileTemp3694537399345041217.tmp
	24/04/24 17:23:12 INFO Utils: /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/fetchFileTemp3694537399345041217.tmp has been previously copied to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/24 17:23:12 INFO Executor: Adding file:/tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
	24/04/24 17:23:12 INFO Executor: Fetching spark://1036ec04ddf9:41423/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1713950590284
	24/04/24 17:23:12 INFO Utils: Fetching spark://1036ec04ddf9:41423/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/fetchFileTemp6149700817603573542.tmp
	24/04/24 17:23:12 INFO Utils: /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/fetchFileTemp6149700817603573542.tmp has been previously copied to /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.slf4j_slf4j-api-2.0.7.jar
	24/04/24 17:23:12 INFO Executor: Adding file:/tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/org.slf4j_slf4j-api-2.0.7.jar to class loader default
	24/04/24 17:23:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34943.
	24/04/24 17:23:12 INFO NettyBlockTransferService: Server created on 1036ec04ddf9:34943
	24/04/24 17:23:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/04/24 17:23:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1036ec04ddf9, 34943, None)
	24/04/24 17:23:12 INFO BlockManagerMasterEndpoint: Registering block manager 1036ec04ddf9:34943 with 366.3 MiB RAM, BlockManagerId(driver, 1036ec04ddf9, 34943, None)
	24/04/24 17:23:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 1036ec04ddf9, 34943, None)
	24/04/24 17:23:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 1036ec04ddf9, 34943, None)
[WI-0][TI-1506] - [INFO] 2024-04-24 17:23:13.714 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1506, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:13.734 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1507, taskName=keyword filtering, firstSubmitTime=1713950593724, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=10, appIds=null, processInstanceId=593, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1507'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424172313'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='593'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.736 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.737 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.743 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.743 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.743 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.743 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713950593743
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.743 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 593_1507
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.743 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1507,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713950593724,
  "startTime" : 1713950593743,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/10/593/1507.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 10,
  "processInstanceId" : 593,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1507"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424172313"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "593"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "593_1507",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.744 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.744 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.744 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.748 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.749 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.749 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1507 check successfully
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.750 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.750 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.750 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.751 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.751 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.751 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.751 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.751 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.751 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.751 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.751 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.752 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1507
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.753 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1507/py_593_1507.py
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.753 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.753 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.753 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.753 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1507/py_593_1507.py
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.753 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.754 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1507/593_1507.sh
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:13.762 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3316
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:13.916 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	
	
	 /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/505-20240423.json 
	
	
	
	24/04/24 17:23:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 345.6 KiB, free 366.0 MiB)
	24/04/24 17:23:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 365.9 MiB)
	24/04/24 17:23:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 1036ec04ddf9:34943 (size: 32.7 KiB, free: 366.3 MiB)
	24/04/24 17:23:13 INFO SparkContext: Created broadcast 0 from wholeTextFiles at NativeMethodAccessorImpl.java:0
	24/04/24 17:23:13 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/04/24 17:23:13 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1503/spark-warehouse'.
[WI-0][TI-1507] - [INFO] 2024-04-24 17:23:14.747 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1507, success=true)
[WI-0][TI-1507] - [INFO] 2024-04-24 17:23:14.765 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1507)
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:14.765 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1507/py_593_1507.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:14.784 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1507, processId:3316 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:14.785 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:14.785 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:14.785 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:14.785 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:14.790 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:14.791 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:14.791 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1507
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:14.800 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1507
[WI-593][TI-1507] - [INFO] 2024-04-24 17:23:14.801 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1507] - [INFO] 2024-04-24 17:23:15.729 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1507, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:15.819 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1508, taskName=keyword filtering, firstSubmitTime=1713950595795, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=10, appIds=null, processInstanceId=593, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1508'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424172315'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='593'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.831 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.831 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.837 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.838 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.838 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.838 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713950595838
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.838 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 593_1508
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.838 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1508,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713950595795,
  "startTime" : 1713950595838,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/10/593/1508.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 10,
  "processInstanceId" : 593,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1508"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424172315"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "593"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "593_1508",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.839 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.839 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.839 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.848 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.849 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.855 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1508 check successfully
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.856 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.858 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.859 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.859 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.859 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.859 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.859 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.859 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.860 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.860 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.860 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.868 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1508
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.869 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1508/py_593_1508.py
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.870 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.871 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.871 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.871 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1508/py_593_1508.py
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.879 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.879 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1508/593_1508.sh
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:15.915 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3327
[WI-0][TI-1508] - [INFO] 2024-04-24 17:23:16.738 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1508, success=true)
[WI-0][TI-1508] - [INFO] 2024-04-24 17:23:16.754 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1508)
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:16.915 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1508/py_593_1508.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:16.921 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1508, processId:3327 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:16.922 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:16.923 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:16.923 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:16.924 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:16.936 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:16.936 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:16.937 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1508
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:16.938 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/593/1508
[WI-593][TI-1508] - [INFO] 2024-04-24 17:23:16.939 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:17.521 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.774390243902439 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1508] - [INFO] 2024-04-24 17:23:17.755 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1508, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:18.950 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 17:23:18 INFO CodeGenerator: Code generated in 286.540192 ms
	24/04/24 17:23:18 INFO FileInputFormat: Total input files to process : 1
	24/04/24 17:23:18 INFO FileInputFormat: Total input files to process : 1
	24/04/24 17:23:18 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
	24/04/24 17:23:18 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/24 17:23:18 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
	24/04/24 17:23:18 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 17:23:18 INFO DAGScheduler: Missing parents: List()
	24/04/24 17:23:18 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:19.589 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9057239057239057 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:19.957 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 17:23:19 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 28.8 KiB, free 365.9 MiB)
	24/04/24 17:23:19 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 365.9 MiB)
	24/04/24 17:23:19 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 1036ec04ddf9:34943 (size: 12.3 KiB, free: 366.3 MiB)
	24/04/24 17:23:19 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
	24/04/24 17:23:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 17:23:19 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
	24/04/24 17:23:19 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (1036ec04ddf9, executor driver, partition 0, PROCESS_LOCAL, 10558 bytes) 
	24/04/24 17:23:19 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:20.958 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 17:23:20 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/505-20240423.json:0+192788
	24/04/24 17:23:20 INFO CodeGenerator: Code generated in 112.207439 ms
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:21.960 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 17:23:21 INFO PythonRunner: Times: total = 847, boot = 488, init = 308, finish = 51
	24/04/24 17:23:21 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2990 bytes result sent to driver
	24/04/24 17:23:21 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1699 ms on 1036ec04ddf9 (executor driver) (1/1)
	24/04/24 17:23:21 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 47781
	24/04/24 17:23:21 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
	24/04/24 17:23:21 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 2.148 s
	24/04/24 17:23:21 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 17:23:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
	24/04/24 17:23:21 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 2.368252 s
	
	
	
	 1 
	
	
	
	
	
	
	 2 
	
	
	
	
	
	
	 3 
	
	
	
	
	
	
	 4 
	
	
	
	24/04/24 17:23:21 INFO CodeGenerator: Code generated in 36.223635 ms
	24/04/24 17:23:21 INFO CodeGenerator: Code generated in 15.339429 ms
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:22.965 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 17:23:22 INFO SparkContext: Starting job: collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1503/spark_reddit_preprocessing.py:84
	24/04/24 17:23:22 INFO DAGScheduler: Got job 1 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1503/spark_reddit_preprocessing.py:84) with 1 output partitions
	24/04/24 17:23:22 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1503/spark_reddit_preprocessing.py:84)
	24/04/24 17:23:22 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 17:23:22 INFO DAGScheduler: Missing parents: List()
	24/04/24 17:23:22 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[19] at toJavaRDD at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 17:23:22 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 68.1 KiB, free 365.8 MiB)
	24/04/24 17:23:22 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 365.8 MiB)
	24/04/24 17:23:22 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 1036ec04ddf9:34943 (size: 25.4 KiB, free: 366.2 MiB)
	24/04/24 17:23:22 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
	24/04/24 17:23:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[19] at toJavaRDD at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 17:23:22 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
	24/04/24 17:23:22 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (1036ec04ddf9, executor driver, partition 0, PROCESS_LOCAL, 10558 bytes) 
	24/04/24 17:23:22 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
	24/04/24 17:23:22 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/userFiles-b6664997-1a89-4a88-a715-a57e38abba58/505-20240423.json:0+192788
	24/04/24 17:23:22 INFO CodeGenerator: Code generated in 48.423207 ms
	24/04/24 17:23:22 INFO CodeGenerator: Code generated in 10.455581 ms
	24/04/24 17:23:22 INFO CodeGenerator: Code generated in 27.009907 ms
	24/04/24 17:23:22 INFO CodeGenerator: Code generated in 20.288234 ms
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:23.969 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 17:23:22 INFO CodeGenerator: Code generated in 20.769882 ms
	24/04/24 17:23:23 INFO PythonRunner: Times: total = 144, boot = -1152, init = 1280, finish = 16
	24/04/24 17:23:23 INFO CodeGenerator: Code generated in 83.619565 ms
	24/04/24 17:23:23 INFO PythonUDFRunner: Times: total = 635, boot = 467, init = 166, finish = 2
	24/04/24 17:23:23 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 177203 bytes result sent to driver
	24/04/24 17:23:23 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1247 ms on 1036ec04ddf9 (executor driver) (1/1)
	24/04/24 17:23:23 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
	24/04/24 17:23:23 INFO DAGScheduler: ResultStage 1 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1503/spark_reddit_preprocessing.py:84) finished in 1.278 s
	24/04/24 17:23:23 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 17:23:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
	24/04/24 17:23:23 INFO DAGScheduler: Job 1 finished: collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1503/spark_reddit_preprocessing.py:84, took 1.292500 s
	24/04/24 17:23:23 INFO CodeGenerator: Code generated in 7.922406 ms
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:24.706 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8185654008438819 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:24.976 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 17:23:23 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 1036ec04ddf9:34943 in memory (size: 12.3 KiB, free: 366.2 MiB)
	24/04/24 17:23:24 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/24 17:23:24 INFO DAGScheduler: Got job 2 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/24 17:23:24 INFO DAGScheduler: Final stage: ResultStage 2 (save at NativeMethodAccessorImpl.java:0)
	24/04/24 17:23:24 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 17:23:24 INFO DAGScheduler: Missing parents: List()
	24/04/24 17:23:24 INFO DAGScheduler: Submitting ResultStage 2 (SQLExecutionRDD[26] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 17:23:24 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 21.7 KiB, free 365.8 MiB)
	24/04/24 17:23:24 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 365.9 MiB)
	24/04/24 17:23:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 1036ec04ddf9:34943 (size: 9.2 KiB, free: 366.2 MiB)
	24/04/24 17:23:24 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 1036ec04ddf9:34943 in memory (size: 25.4 KiB, free: 366.3 MiB)
	24/04/24 17:23:24 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
	24/04/24 17:23:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (SQLExecutionRDD[26] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 17:23:24 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
	24/04/24 17:23:24 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (1036ec04ddf9, executor driver, partition 0, PROCESS_LOCAL, 11577 bytes) 
	24/04/24 17:23:24 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
	24/04/24 17:23:24 INFO CodeGenerator: Code generated in 7.018154 ms
	24/04/24 17:23:24 INFO CodeGenerator: Code generated in 18.139394 ms
	24/04/24 17:23:24 INFO ProducerConfig: ProducerConfig values: 
		acks = -1
		auto.include.jmx.reporter = true
		batch.size = 16384
		bootstrap.servers = [kafka:9092]
		buffer.memory = 33554432
		client.dns.lookup = use_all_dns_ips
		client.id = producer-1
		compression.type = none
		connections.max.idle.ms = 540000
		delivery.timeout.ms = 120000
		enable.idempotence = true
		interceptor.classes = []
		key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
		linger.ms = 0
		max.block.ms = 60000
		max.in.flight.requests.per.connection = 5
		max.request.size = 1048576
		metadata.max.age.ms = 300000
		metadata.max.idle.ms = 300000
		metric.reporters = []
		metrics.num.samples = 2
		metrics.recording.level = INFO
		metrics.sample.window.ms = 30000
		partitioner.adaptive.partitioning.enable = true
		partitioner.availability.timeout.ms = 0
		partitioner.class = null
		partitioner.ignore.keys = false
		receive.buffer.bytes = 32768
		reconnect.backoff.max.ms = 1000
		reconnect.backoff.ms = 50
		request.timeout.ms = 30000
		retries = 2147483647
		retry.backoff.ms = 100
		sasl.client.callback.handler.class = null
		sasl.jaas.config = null
		sasl.kerberos.kinit.cmd = /usr/bin/kinit
		sasl.kerberos.min.time.before.relogin = 60000
		sasl.kerberos.service.name = null
		sasl.kerberos.ticket.renew.jitter = 0.05
		sasl.kerberos.ticket.renew.window.factor = 0.8
		sasl.login.callback.handler.class = null
		sasl.login.class = null
		sasl.login.connect.timeout.ms = null
		sasl.login.read.timeout.ms = null
		sasl.login.refresh.buffer.seconds = 300
		sasl.login.refresh.min.period.seconds = 60
		sasl.login.refresh.window.factor = 0.8
		sasl.login.refresh.window.jitter = 0.05
		sasl.login.retry.backoff.max.ms = 10000
		sasl.login.retry.backoff.ms = 100
		sasl.mechanism = GSSAPI
		sasl.oauthbearer.clock.skew.seconds = 30
		sasl.oauthbearer.expected.audience = null
		sasl.oauthbearer.expected.issuer = null
		sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
		sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
		sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
		sasl.oauthbearer.jwks.endpoint.url = null
		sasl.oauthbearer.scope.claim.name = scope
		sasl.oauthbearer.sub.claim.name = sub
		sasl.oauthbearer.token.endpoint.url = null
		security.protocol = PLAINTEXT
		security.providers = null
		send.buffer.bytes = 131072
		socket.connection.setup.timeout.max.ms = 30000
		socket.connection.setup.timeout.ms = 10000
		ssl.cipher.suites = null
		ssl.enabled.protocols = [TLSv1.2]
		ssl.endpoint.identification.algorithm = https
		ssl.engine.factory.class = null
		ssl.key.password = null
		ssl.keymanager.algorithm = SunX509
		ssl.keystore.certificate.chain = null
		ssl.keystore.key = null
		ssl.keystore.location = null
		ssl.keystore.password = null
		ssl.keystore.type = JKS
		ssl.protocol = TLSv1.2
		ssl.provider = null
		ssl.secure.random.implementation = null
		ssl.trustmanager.algorithm = PKIX
		ssl.truststore.certificates = null
		ssl.truststore.location = null
		ssl.truststore.password = null
		ssl.truststore.type = JKS
		transaction.timeout.ms = 60000
		transactional.id = null
		value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	
	24/04/24 17:23:24 INFO KafkaProducer: [Producer clientId=producer-1] Instantiated an idempotent producer.
	24/04/24 17:23:24 INFO AppInfoParser: Kafka version: 3.4.1
	24/04/24 17:23:24 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
	24/04/24 17:23:24 INFO AppInfoParser: Kafka startTimeMs: 1713950604477
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:25.729 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8584337349397589 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:25.980 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 17:23:25 WARN NetworkClient: [Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {reddit_preprocessed=LEADER_NOT_AVAILABLE}
	24/04/24 17:23:25 INFO Metadata: [Producer clientId=producer-1] Cluster ID: sLCJhaksSWePQOqq3_wHlg
	24/04/24 17:23:25 INFO TransactionManager: [Producer clientId=producer-1] ProducerId set to 4 with epoch 0
	24/04/24 17:23:25 WARN NetworkClient: [Producer clientId=producer-1] Error while fetching metadata with correlation id 4 : {reddit_preprocessed=LEADER_NOT_AVAILABLE}
	24/04/24 17:23:25 INFO Metadata: [Producer clientId=producer-1] Resetting the last seen epoch of partition reddit_preprocessed-0 to 0 since the associated topicId changed from null to _vusU7u1TOyQxdPV0BN5Lw
	24/04/24 17:23:25 INFO PythonRunner: Times: total = 126, boot = -1749, init = 1875, finish = 0
	24/04/24 17:23:25 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1710 bytes result sent to driver
	24/04/24 17:23:25 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1301 ms on 1036ec04ddf9 (executor driver) (1/1)
	24/04/24 17:23:25 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
	24/04/24 17:23:25 INFO DAGScheduler: ResultStage 2 (save at NativeMethodAccessorImpl.java:0) finished in 1.347 s
	24/04/24 17:23:25 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 17:23:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
	24/04/24 17:23:25 INFO DAGScheduler: Job 2 finished: save at NativeMethodAccessorImpl.java:0, took 1.376963 s
	24/04/24 17:23:25 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/24 17:23:25 INFO DAGScheduler: Got job 3 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/24 17:23:25 INFO DAGScheduler: Final stage: ResultStage 3 (save at NativeMethodAccessorImpl.java:0)
	24/04/24 17:23:25 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 17:23:25 INFO DAGScheduler: Missing parents: List()
	24/04/24 17:23:25 INFO DAGScheduler: Submitting ResultStage 3 (SQLExecutionRDD[33] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 17:23:25 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 21.7 KiB, free 365.9 MiB)
	24/04/24 17:23:25 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 365.9 MiB)
	24/04/24 17:23:25 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 1036ec04ddf9:34943 (size: 9.2 KiB, free: 366.3 MiB)
	24/04/24 17:23:25 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
	24/04/24 17:23:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (SQLExecutionRDD[33] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 17:23:25 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
	24/04/24 17:23:25 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (1036ec04ddf9, executor driver, partition 0, PROCESS_LOCAL, 10770 bytes) 
	24/04/24 17:23:25 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
	24/04/24 17:23:25 INFO PythonRunner: Times: total = 95, boot = -1386, init = 1481, finish = 0
	24/04/24 17:23:25 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1710 bytes result sent to driver
	24/04/24 17:23:25 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 134 ms on 1036ec04ddf9 (executor driver) (1/1)
	24/04/24 17:23:25 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
	24/04/24 17:23:25 INFO DAGScheduler: ResultStage 3 (save at NativeMethodAccessorImpl.java:0) finished in 0.158 s
	24/04/24 17:23:25 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 17:23:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
	24/04/24 17:23:25 INFO DAGScheduler: Job 3 finished: save at NativeMethodAccessorImpl.java:0, took 0.184066 s
	24/04/24 17:23:25 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/24 17:23:25 INFO DAGScheduler: Got job 4 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/24 17:23:25 INFO DAGScheduler: Final stage: ResultStage 4 (save at NativeMethodAccessorImpl.java:0)
	24/04/24 17:23:25 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 17:23:25 INFO DAGScheduler: Missing parents: List()
	24/04/24 17:23:25 INFO DAGScheduler: Submitting ResultStage 4 (SQLExecutionRDD[40] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 17:23:25 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 21.7 KiB, free 365.8 MiB)
	24/04/24 17:23:25 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 365.8 MiB)
	24/04/24 17:23:25 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 1036ec04ddf9:34943 (size: 9.2 KiB, free: 366.2 MiB)
	24/04/24 17:23:25 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
	24/04/24 17:23:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (SQLExecutionRDD[40] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 17:23:25 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
	24/04/24 17:23:25 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (1036ec04ddf9, executor driver, partition 0, PROCESS_LOCAL, 10868 bytes) 
	24/04/24 17:23:25 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:27.010 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 17:23:26 INFO PythonRunner: Times: total = 78, boot = -175, init = 253, finish = 0
	24/04/24 17:23:26 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1710 bytes result sent to driver
	24/04/24 17:23:26 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 100 ms on 1036ec04ddf9 (executor driver) (1/1)
	24/04/24 17:23:26 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
	24/04/24 17:23:26 INFO DAGScheduler: ResultStage 4 (save at NativeMethodAccessorImpl.java:0) finished in 0.110 s
	24/04/24 17:23:26 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 17:23:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
	24/04/24 17:23:26 INFO DAGScheduler: Job 4 finished: save at NativeMethodAccessorImpl.java:0, took 0.129522 s
	24/04/24 17:23:26 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/24 17:23:26 INFO DAGScheduler: Got job 5 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/24 17:23:26 INFO DAGScheduler: Final stage: ResultStage 5 (save at NativeMethodAccessorImpl.java:0)
	24/04/24 17:23:26 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 17:23:26 INFO DAGScheduler: Missing parents: List()
	24/04/24 17:23:26 INFO DAGScheduler: Submitting ResultStage 5 (SQLExecutionRDD[47] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 17:23:26 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 21.7 KiB, free 365.8 MiB)
	24/04/24 17:23:26 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 365.8 MiB)
	24/04/24 17:23:26 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 1036ec04ddf9:34943 (size: 9.2 KiB, free: 366.2 MiB)
	24/04/24 17:23:26 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
	24/04/24 17:23:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (SQLExecutionRDD[47] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 17:23:26 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
	24/04/24 17:23:26 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (1036ec04ddf9, executor driver, partition 0, PROCESS_LOCAL, 10847 bytes) 
	24/04/24 17:23:26 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
	24/04/24 17:23:26 INFO PythonRunner: Times: total = 79, boot = -108, init = 187, finish = 0
	24/04/24 17:23:26 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1710 bytes result sent to driver
	24/04/24 17:23:26 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 94 ms on 1036ec04ddf9 (executor driver) (1/1)
	24/04/24 17:23:26 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
	24/04/24 17:23:26 INFO DAGScheduler: ResultStage 5 (save at NativeMethodAccessorImpl.java:0) finished in 0.110 s
	24/04/24 17:23:26 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 17:23:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
	24/04/24 17:23:26 INFO DAGScheduler: Job 5 finished: save at NativeMethodAccessorImpl.java:0, took 0.126124 s
	24/04/24 17:23:26 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/24 17:23:26 INFO DAGScheduler: Got job 6 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/24 17:23:26 INFO DAGScheduler: Final stage: ResultStage 6 (save at NativeMethodAccessorImpl.java:0)
	24/04/24 17:23:26 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 17:23:26 INFO DAGScheduler: Missing parents: List()
	24/04/24 17:23:26 INFO DAGScheduler: Submitting ResultStage 6 (SQLExecutionRDD[54] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 17:23:26 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 21.7 KiB, free 365.8 MiB)
	24/04/24 17:23:26 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 365.8 MiB)
	24/04/24 17:23:26 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 1036ec04ddf9:34943 (size: 9.2 KiB, free: 366.2 MiB)
	24/04/24 17:23:26 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
	24/04/24 17:23:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (SQLExecutionRDD[54] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 17:23:26 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
	24/04/24 17:23:26 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (1036ec04ddf9, executor driver, partition 0, PROCESS_LOCAL, 11155 bytes) 
	24/04/24 17:23:26 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
	24/04/24 17:23:26 INFO PythonRunner: Times: total = 69, boot = -101, init = 170, finish = 0
	24/04/24 17:23:26 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1710 bytes result sent to driver
	24/04/24 17:23:26 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 85 ms on 1036ec04ddf9 (executor driver) (1/1)
	24/04/24 17:23:26 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
	24/04/24 17:23:26 INFO DAGScheduler: ResultStage 6 (save at NativeMethodAccessorImpl.java:0) finished in 0.103 s
	24/04/24 17:23:26 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 17:23:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
	24/04/24 17:23:26 INFO DAGScheduler: Job 6 finished: save at NativeMethodAccessorImpl.java:0, took 0.107648 s
	24/04/24 17:23:26 INFO SparkContext: Invoking stop() from shutdown hook
	24/04/24 17:23:26 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/04/24 17:23:26 INFO SparkUI: Stopped Spark web UI at http://1036ec04ddf9:4040
	24/04/24 17:23:26 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/04/24 17:23:26 INFO MemoryStore: MemoryStore cleared
	24/04/24 17:23:26 INFO BlockManager: BlockManager stopped
	24/04/24 17:23:26 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/04/24 17:23:26 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/04/24 17:23:26 INFO SparkContext: Successfully stopped SparkContext
	24/04/24 17:23:26 INFO ShutdownHookManager: Shutdown hook called
	24/04/24 17:23:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a
	24/04/24 17:23:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-feaf9680-6ae2-420a-b0a9-de5fc2aee54a/pyspark-35c1880f-e4ac-4770-8dc9-0fb7eef3d3dd
	24/04/24 17:23:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-9d8de362-3758-473a-b926-9dbd4272d60b
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:28.013 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1503, processId:2744 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:28.014 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:28.015 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:28.016 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:28.017 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:28.027 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:28.028 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:28.028 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1503
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:28.029 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1503
[WI-592][TI-1503] - [INFO] 2024-04-24 17:23:28.029 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1503] - [INFO] 2024-04-24 17:23:28.760 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1503, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:28.792 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1509, taskName=move to archives, firstSubmitTime=1713950608782, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=113, appIds=null, processInstanceId=592, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the reddit archive folder\nmv ${raw_file_dir} ${REDDIT_HOME}/archive\n","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to archives'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1509'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13375839065312'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424172328'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='592'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/505-20240423.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/505-20240423.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.793 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to archives to wait queue success
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.793 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.794 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.797 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.797 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.797 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713950608797
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.797 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 592_1509
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.797 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1509,
  "taskName" : "move to archives",
  "firstSubmitTime" : 1713950608782,
  "startTime" : 1713950608797,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/113/592/1509.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 113,
  "processInstanceId" : 592,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the reddit archive folder\\nmv ${raw_file_dir} ${REDDIT_HOME}/archive\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to archives"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1509"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13375839065312"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424172328"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "592"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/505-20240423.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "592_1509",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/505-20240423.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.798 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.798 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.798 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.801 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.802 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.802 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1509 check successfully
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.802 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.802 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.802 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.803 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.803 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the reddit archive folder\nmv ${raw_file_dir} ${REDDIT_HOME}/archive\n",
  "resourceList" : [ ]
}
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.803 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.803 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/505-20240423.json"}] successfully
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.803 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.803 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.803 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.807 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.807 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.807 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the reddit archive folder
mv /local_storage/reddit/processing/505-20240423.json /local_storage/reddit/archive

[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.808 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.808 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1509/592_1509.sh
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:28.810 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3408
[WI-0][TI-1509] - [INFO] 2024-04-24 17:23:29.786 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1509, success=true)
[WI-0][TI-1509] - [INFO] 2024-04-24 17:23:29.796 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1509)
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:29.811 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:29.813 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1509, processId:3408 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:29.814 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:29.814 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:29.814 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:29.814 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:29.817 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:29.818 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:29.818 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1509
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:29.818 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/592/1509
[WI-592][TI-1509] - [INFO] 2024-04-24 17:23:29.818 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1509] - [INFO] 2024-04-24 17:23:30.777 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1509, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:37.767 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7211267605633803 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:23:42.795 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7031700288184439 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:24:24.008 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7767857142857142 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:24:25.028 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1510, taskName=Extract Reddit Data, firstSubmitTime=1713950664979, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13045665829504, processDefineVersion=14, appIds=null, processInstanceId=594, scheduleTime=0, globalParams=[{"prop":"subreddit","direct":"IN","type":"VARCHAR","value":"singapore"},{"prop":"limit","direct":"IN","type":"VARCHAR","value":"3"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"data","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"import praw\nimport json\n\nclass RedditPostDataExtractor:\n    def __init__(self):\n        self.data = []\n\n    def extract_data(self, hot_posts):\n        if hot_posts:\n            for post in hot_posts:\n                # initialise a dictionary to contain the data of the post\n                post_data = {}\n\n                # use permalink as the primary key\n                permalink = post.permalink\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the poster's name\n                if post.author is not None:\n                    post_data['author'] = post.author.name\n                else:\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\n\n\n                # get the unix time in which the post was created\n                post_data['unix_time'] = post.created_utc\n                post_data['permalink'] = post.permalink\n                post_data['score'] = post.score\n                post_data['subreddit'] = post.subreddit.display_name\n                post_data['post_title'] = post.title\n                post_data['body'] = post.selftext\n\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\n                # t3 represents that it is a post\n                post_data['name'] = post.name\n\n                # store the posts data in the dictionary where the name is used as the key\n                self.data.append(post_data)\n\n                # extract the comments from the post\n                comments = post.comments.list()\n\n                # extract the data in the comments and store them inside the dictionary\n                self.extract_comments_data(comments)\n        \n        return self.data\n\n    def extract_comments_data(self, comments):\n        if comments:\n            for comment in comments:\n                # initialise a dictionary to contain the data of the comment\n                comment_data = {}\n\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the commenter name\n                if comment.author is not None:\n                    comment_data['author'] = comment.author.name\n                else:\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the comment was created\n                comment_data['unix_time'] = comment.created_utc\n                comment_data['permalink'] = comment.permalink\n                comment_data['score'] = comment.score\n                comment_data['subreddit'] = comment.subreddit.display_name\n                comment_data['post_title'] = comment.submission.title\n                comment_data['body'] = comment.body\n\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\n                # t1 represents that it is a post\n                comment_data['name'] = comment.name\n\n                # store the comment's data in the dictionary where the name is used as the key\n                self.data.append(comment_data)\n\n                # extract the comment's replies from the post\n                # replies = comment.replies\n\n                # extract the data in the comments and store them inside the dictionary\n                # self.extract_comments_data(replies)\n        \n        return\n    \ndef get_hot_posts(reddit_instance, subreddit_name, limit):\n    # Define the subreddit\n    subreddit = reddit_instance.subreddit(subreddit_name)\n\n    # Get hot posts from the subreddit\n    hot_posts = subreddit.hot(limit=limit)\n\n    # Filter out posts that are not pinned\n    filtered_posts = [post for post in hot_posts if not post.stickied]\n    return filtered_posts\n\n# initialize Reddit instance\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\n                     user_agent='Prawlingv1')\n\n\n# choose the subreddit to extract data from\nsubreddit = '${subreddit}'\nlimit = ${limit}\n\n# get hot posts from the subreddit\nhot_posts = get_hot_posts(reddit, subreddit, limit)\n\n# initialise a reddit data extractor object\ndata_extractor = RedditPostDataExtractor()\n\n# extract data from the hot posts from r/singapore\ndata = data_extractor.extract_data(hot_posts)\n\nprint(str(data))\n\nwith open(\"${REDDIT_HOME}/in/${system.workflow.instance.id}-${system.biz.curdate}.json\", \"w\") as fp:\n    json.dump(data, fp, indent=4) \n","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='Extract Reddit Data'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1510'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13045662268160'}, subreddit=Property{prop='subreddit', direct=IN, type=VARCHAR, value='singapore'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424172425'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='594'}, limit=Property{prop='limit', direct=IN, type=VARCHAR, value='3'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13045665829504'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.031 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: Extract Reddit Data to wait queue success
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.031 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.032 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.033 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.033 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.033 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713950665033
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.033 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 594_1510
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.034 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1510,
  "taskName" : "Extract Reddit Data",
  "firstSubmitTime" : 1713950664979,
  "startTime" : 1713950665033,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13045665829504/14/594/1510.log",
  "processId" : 0,
  "processDefineCode" : 13045665829504,
  "processDefineVersion" : 14,
  "processInstanceId" : 594,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"subreddit\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"singapore\"},{\"prop\":\"limit\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"3\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"data\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"import praw\\nimport json\\n\\nclass RedditPostDataExtractor:\\n    def __init__(self):\\n        self.data = []\\n\\n    def extract_data(self, hot_posts):\\n        if hot_posts:\\n            for post in hot_posts:\\n                # initialise a dictionary to contain the data of the post\\n                post_data = {}\\n\\n                # use permalink as the primary key\\n                permalink = post.permalink\\n\\n                # get the relevant post's data and store it inside post_data\\n\\n                # get the poster's name\\n                if post.author is not None:\\n                    post_data['author'] = post.author.name\\n                else:\\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\\n\\n\\n                # get the unix time in which the post was created\\n                post_data['unix_time'] = post.created_utc\\n                post_data['permalink'] = post.permalink\\n                post_data['score'] = post.score\\n                post_data['subreddit'] = post.subreddit.display_name\\n                post_data['post_title'] = post.title\\n                post_data['body'] = post.selftext\\n\\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\\n                # t3 represents that it is a post\\n                post_data['name'] = post.name\\n\\n                # store the posts data in the dictionary where the name is used as the key\\n                self.data.append(post_data)\\n\\n                # extract the comments from the post\\n                comments = post.comments.list()\\n\\n                # extract the data in the comments and store them inside the dictionary\\n                self.extract_comments_data(comments)\\n        \\n        return self.data\\n\\n    def extract_comments_data(self, comments):\\n        if comments:\\n            for comment in comments:\\n                # initialise a dictionary to contain the data of the comment\\n                comment_data = {}\\n\\n\\n                # get the relevant post's data and store it inside post_data\\n\\n                # get the commenter name\\n                if comment.author is not None:\\n                    comment_data['author'] = comment.author.name\\n                else:\\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\\n\\n                # get the unix time in which the comment was created\\n                comment_data['unix_time'] = comment.created_utc\\n                comment_data['permalink'] = comment.permalink\\n                comment_data['score'] = comment.score\\n                comment_data['subreddit'] = comment.subreddit.display_name\\n                comment_data['post_title'] = comment.submission.title\\n                comment_data['body'] = comment.body\\n\\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\\n                # t1 represents that it is a post\\n                comment_data['name'] = comment.name\\n\\n                # store the comment's data in the dictionary where the name is used as the key\\n                self.data.append(comment_data)\\n\\n                # extract the comment's replies from the post\\n                # replies = comment.replies\\n\\n                # extract the data in the comments and store them inside the dictionary\\n                # self.extract_comments_data(replies)\\n        \\n        return\\n    \\ndef get_hot_posts(reddit_instance, subreddit_name, limit):\\n    # Define the subreddit\\n    subreddit = reddit_instance.subreddit(subreddit_name)\\n\\n    # Get hot posts from the subreddit\\n    hot_posts = subreddit.hot(limit=limit)\\n\\n    # Filter out posts that are not pinned\\n    filtered_posts = [post for post in hot_posts if not post.stickied]\\n    return filtered_posts\\n\\n# initialize Reddit instance\\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\\n                     user_agent='Prawlingv1')\\n\\n\\n# choose the subreddit to extract data from\\nsubreddit = '${subreddit}'\\nlimit = ${limit}\\n\\n# get hot posts from the subreddit\\nhot_posts = get_hot_posts(reddit, subreddit, limit)\\n\\n# initialise a reddit data extractor object\\ndata_extractor = RedditPostDataExtractor()\\n\\n# extract data from the hot posts from r/singapore\\ndata = data_extractor.extract_data(hot_posts)\\n\\nprint(str(data))\\n\\nwith open(\\\"${REDDIT_HOME}/in/${system.workflow.instance.id}-${system.biz.curdate}.json\\\", \\\"w\\\") as fp:\\n    json.dump(data, fp, indent=4) \\n\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "Extract Reddit Data"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1510"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13045662268160"
    },
    "subreddit" : {
      "prop" : "subreddit",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "singapore"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424172425"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "594"
    },
    "limit" : {
      "prop" : "limit",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13045665829504"
    }
  },
  "taskAppId" : "594_1510",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.035 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.036 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.036 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.050 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.051 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.052 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_14/594/1510 check successfully
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.054 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.054 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.054 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.069 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.070 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "data",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "import praw\nimport json\n\nclass RedditPostDataExtractor:\n    def __init__(self):\n        self.data = []\n\n    def extract_data(self, hot_posts):\n        if hot_posts:\n            for post in hot_posts:\n                # initialise a dictionary to contain the data of the post\n                post_data = {}\n\n                # use permalink as the primary key\n                permalink = post.permalink\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the poster's name\n                if post.author is not None:\n                    post_data['author'] = post.author.name\n                else:\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\n\n\n                # get the unix time in which the post was created\n                post_data['unix_time'] = post.created_utc\n                post_data['permalink'] = post.permalink\n                post_data['score'] = post.score\n                post_data['subreddit'] = post.subreddit.display_name\n                post_data['post_title'] = post.title\n                post_data['body'] = post.selftext\n\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\n                # t3 represents that it is a post\n                post_data['name'] = post.name\n\n                # store the posts data in the dictionary where the name is used as the key\n                self.data.append(post_data)\n\n                # extract the comments from the post\n                comments = post.comments.list()\n\n                # extract the data in the comments and store them inside the dictionary\n                self.extract_comments_data(comments)\n        \n        return self.data\n\n    def extract_comments_data(self, comments):\n        if comments:\n            for comment in comments:\n                # initialise a dictionary to contain the data of the comment\n                comment_data = {}\n\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the commenter name\n                if comment.author is not None:\n                    comment_data['author'] = comment.author.name\n                else:\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the comment was created\n                comment_data['unix_time'] = comment.created_utc\n                comment_data['permalink'] = comment.permalink\n                comment_data['score'] = comment.score\n                comment_data['subreddit'] = comment.subreddit.display_name\n                comment_data['post_title'] = comment.submission.title\n                comment_data['body'] = comment.body\n\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\n                # t1 represents that it is a post\n                comment_data['name'] = comment.name\n\n                # store the comment's data in the dictionary where the name is used as the key\n                self.data.append(comment_data)\n\n                # extract the comment's replies from the post\n                # replies = comment.replies\n\n                # extract the data in the comments and store them inside the dictionary\n                # self.extract_comments_data(replies)\n        \n        return\n    \ndef get_hot_posts(reddit_instance, subreddit_name, limit):\n    # Define the subreddit\n    subreddit = reddit_instance.subreddit(subreddit_name)\n\n    # Get hot posts from the subreddit\n    hot_posts = subreddit.hot(limit=limit)\n\n    # Filter out posts that are not pinned\n    filtered_posts = [post for post in hot_posts if not post.stickied]\n    return filtered_posts\n\n# initialize Reddit instance\nreddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',\n                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',\n                     user_agent='Prawlingv1')\n\n\n# choose the subreddit to extract data from\nsubreddit = '${subreddit}'\nlimit = ${limit}\n\n# get hot posts from the subreddit\nhot_posts = get_hot_posts(reddit, subreddit, limit)\n\n# initialise a reddit data extractor object\ndata_extractor = RedditPostDataExtractor()\n\n# extract data from the hot posts from r/singapore\ndata = data_extractor.extract_data(hot_posts)\n\nprint(str(data))\n\nwith open(\"${REDDIT_HOME}/in/${system.workflow.instance.id}-${system.biz.curdate}.json\", \"w\") as fp:\n    json.dump(data, fp, indent=4) \n",
  "resourceList" : [ ]
}
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.070 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.071 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.071 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.071 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.071 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.071 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import praw
import json

class RedditPostDataExtractor:
    def __init__(self):
        self.data = []

    def extract_data(self, hot_posts):
        if hot_posts:
            for post in hot_posts:
                # initialise a dictionary to contain the data of the post
                post_data = {}

                # use permalink as the primary key
                permalink = post.permalink

                # get the relevant post's data and store it inside post_data

                # get the poster's name
                if post.author is not None:
                    post_data['author'] = post.author.name
                else:
                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'


                # get the unix time in which the post was created
                post_data['unix_time'] = post.created_utc
                post_data['permalink'] = post.permalink
                post_data['score'] = post.score
                post_data['subreddit'] = post.subreddit.display_name
                post_data['post_title'] = post.title
                post_data['body'] = post.selftext

                # get the unique name of the post (name refers to the id of the post prefixed with t3_)
                # t3 represents that it is a post
                post_data['name'] = post.name

                # store the posts data in the dictionary where the name is used as the key
                self.data.append(post_data)

                # extract the comments from the post
                comments = post.comments.list()

                # extract the data in the comments and store them inside the dictionary
                self.extract_comments_data(comments)
        
        return self.data

    def extract_comments_data(self, comments):
        if comments:
            for comment in comments:
                # initialise a dictionary to contain the data of the comment
                comment_data = {}


                # get the relevant post's data and store it inside post_data

                # get the commenter name
                if comment.author is not None:
                    comment_data['author'] = comment.author.name
                else:
                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the comment was created
                comment_data['unix_time'] = comment.created_utc
                comment_data['permalink'] = comment.permalink
                comment_data['score'] = comment.score
                comment_data['subreddit'] = comment.subreddit.display_name
                comment_data['post_title'] = comment.submission.title
                comment_data['body'] = comment.body

                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)
                # t1 represents that it is a post
                comment_data['name'] = comment.name

                # store the comment's data in the dictionary where the name is used as the key
                self.data.append(comment_data)

                # extract the comment's replies from the post
                # replies = comment.replies

                # extract the data in the comments and store them inside the dictionary
                # self.extract_comments_data(replies)
        
        return
    
def get_hot_posts(reddit_instance, subreddit_name, limit):
    # Define the subreddit
    subreddit = reddit_instance.subreddit(subreddit_name)

    # Get hot posts from the subreddit
    hot_posts = subreddit.hot(limit=limit)

    # Filter out posts that are not pinned
    filtered_posts = [post for post in hot_posts if not post.stickied]
    return filtered_posts

# initialize Reddit instance
reddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',
                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',
                     user_agent='Prawlingv1')


# choose the subreddit to extract data from
subreddit = '${subreddit}'
limit = ${limit}

# get hot posts from the subreddit
hot_posts = get_hot_posts(reddit, subreddit, limit)

# initialise a reddit data extractor object
data_extractor = RedditPostDataExtractor()

# extract data from the hot posts from r/singapore
data = data_extractor.extract_data(hot_posts)

print(str(data))

with open("${REDDIT_HOME}/in/${system.workflow.instance.id}-${system.biz.curdate}.json", "w") as fp:
    json.dump(data, fp, indent=4) 

[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.077 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_14/594/1510
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.079 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_14/594/1510/py_594_1510.py
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.081 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import praw
import json

class RedditPostDataExtractor:
    def __init__(self):
        self.data = []

    def extract_data(self, hot_posts):
        if hot_posts:
            for post in hot_posts:
                # initialise a dictionary to contain the data of the post
                post_data = {}

                # use permalink as the primary key
                permalink = post.permalink

                # get the relevant post's data and store it inside post_data

                # get the poster's name
                if post.author is not None:
                    post_data['author'] = post.author.name
                else:
                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'


                # get the unix time in which the post was created
                post_data['unix_time'] = post.created_utc
                post_data['permalink'] = post.permalink
                post_data['score'] = post.score
                post_data['subreddit'] = post.subreddit.display_name
                post_data['post_title'] = post.title
                post_data['body'] = post.selftext

                # get the unique name of the post (name refers to the id of the post prefixed with t3_)
                # t3 represents that it is a post
                post_data['name'] = post.name

                # store the posts data in the dictionary where the name is used as the key
                self.data.append(post_data)

                # extract the comments from the post
                comments = post.comments.list()

                # extract the data in the comments and store them inside the dictionary
                self.extract_comments_data(comments)
        
        return self.data

    def extract_comments_data(self, comments):
        if comments:
            for comment in comments:
                # initialise a dictionary to contain the data of the comment
                comment_data = {}


                # get the relevant post's data and store it inside post_data

                # get the commenter name
                if comment.author is not None:
                    comment_data['author'] = comment.author.name
                else:
                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the comment was created
                comment_data['unix_time'] = comment.created_utc
                comment_data['permalink'] = comment.permalink
                comment_data['score'] = comment.score
                comment_data['subreddit'] = comment.subreddit.display_name
                comment_data['post_title'] = comment.submission.title
                comment_data['body'] = comment.body

                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)
                # t1 represents that it is a post
                comment_data['name'] = comment.name

                # store the comment's data in the dictionary where the name is used as the key
                self.data.append(comment_data)

                # extract the comment's replies from the post
                # replies = comment.replies

                # extract the data in the comments and store them inside the dictionary
                # self.extract_comments_data(replies)
        
        return
    
def get_hot_posts(reddit_instance, subreddit_name, limit):
    # Define the subreddit
    subreddit = reddit_instance.subreddit(subreddit_name)

    # Get hot posts from the subreddit
    hot_posts = subreddit.hot(limit=limit)

    # Filter out posts that are not pinned
    filtered_posts = [post for post in hot_posts if not post.stickied]
    return filtered_posts

# initialize Reddit instance
reddit = praw.Reddit(client_id='hM2BAD8XKakgkVdR2V2S-Q',
                     client_secret='dWBfOf7Ed2WxqVavqQIcZ6V72QyBLg',
                     user_agent='Prawlingv1')


# choose the subreddit to extract data from
subreddit = 'singapore'
limit = 3

# get hot posts from the subreddit
hot_posts = get_hot_posts(reddit, subreddit, limit)

# initialise a reddit data extractor object
data_extractor = RedditPostDataExtractor()

# extract data from the hot posts from r/singapore
data = data_extractor.extract_data(hot_posts)

print(str(data))

with open("/local_storage/reddit/in/594-20240424.json", "w") as fp:
    json.dump(data, fp, indent=4) 

[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.093 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.099 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.099 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_14/594/1510/py_594_1510.py
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.099 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.099 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_14/594/1510/594_1510.sh
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:25.103 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3432
[WI-0][TI-1510] - [INFO] 2024-04-24 17:24:25.846 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1510, success=true)
[WI-0][TI-1510] - [INFO] 2024-04-24 17:24:25.854 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1510)
[WI-0][TI-0] - [INFO] 2024-04-24 17:24:26.108 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-24 17:24:28.120 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_14/594/1510/py_594_1510.py", line 117, in <module>
	    data = data_extractor.extract_data(hot_posts)
	           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_14/594/1510/py_594_1510.py", line 47, in extract_data
	    self.extract_comments_data(comments)
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_14/594/1510/py_594_1510.py", line 61, in extract_comments_data
	    if comment.author is not None:
	       ^^^^^^^^^^^^^^
	AttributeError: 'MoreComments' object has no attribute 'author'
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:28.121 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_14/594/1510, processId:3432 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:28.121 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:28.121 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:28.121 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:28.122 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:28.127 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:28.127 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:28.127 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_14/594/1510
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:28.128 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_14/594/1510
[WI-594][TI-1510] - [INFO] 2024-04-24 17:24:28.128 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1510] - [INFO] 2024-04-24 17:24:28.844 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1510, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:33:35.425 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8106312292358805 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:33:40.458 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7257142857142858 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:33:48.494 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7075208913649025 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:33:51.479 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1511, taskName=Extract Reddit Data, firstSubmitTime=1713951231461, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13045665829504, processDefineVersion=15, appIds=null, processInstanceId=595, scheduleTime=0, globalParams=[{"prop":"subreddit","direct":"IN","type":"VARCHAR","value":"singapore"},{"prop":"limit","direct":"IN","type":"VARCHAR","value":"3"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"data","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"import praw\nimport json\n\nclass RedditPostDataExtractor:\n    def __init__(self):\n        self.data = []\n\n    def extract_data(self, hot_posts):\n        if hot_posts:\n            for post in hot_posts:\n                post.comments.replace_more(limit=None)\n                # initialise a dictionary to contain the data of the post\n                post_data = {}\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the poster's name\n                if post.author is not None:\n                    post_data['author'] = post.author.name\n                else:\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the post was created\n                post_data['unix_time'] = post.created_utc\n                post_data['permalink'] = post.permalink\n                post_data['score'] = post.score\n                post_data['subreddit'] = post.subreddit.display_name\n                post_data['post_title'] = post.title\n                post_data['body'] = post.selftext\n\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\n                # t3 represents that it is a post\n                post_data['name'] = post.name\n\n                # store the posts data in the dictionary where the name is used as the key\n                self.data.append(post_data)\n\n                # extract the comments from the post\n                comments = post.comments.list()\n\n                # extract the data in the comments and store them inside the dictionary\n                self.extract_comments_data(comments)\n        \n        return self.data\n\n    def extract_comments_data(self, comments):\n        if comments:\n            for comment in comments:\n                # initialise a dictionary to contain the data of the comment\n                comment_data = {}\n\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the commenter name\n                if comment.author is not None:\n                    comment_data['author'] = comment.author.name\n                else:\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the comment was created\n                comment_data['unix_time'] = comment.created_utc\n                comment_data['permalink'] = comment.permalink\n                comment_data['score'] = comment.score\n                comment_data['subreddit'] = comment.subreddit.display_name\n                comment_data['post_title'] = comment.submission.title\n                comment_data['body'] = comment.body\n\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\n                # t1 represents that it is a post\n                comment_data['name'] = comment.name\n\n                # store the comment's data in the dictionary where the name is used as the key\n                self.data.append(comment_data)\n\n                # extract the comment's replies from the post\n                # replies = comment.replies\n\n                # extract the data in the comments and store them inside the dictionary\n                # self.extract_comments_data(replies)\n        \n        return\n    \ndef get_hot_posts(reddit_instance, subreddit_name, limit):\n    # Define the subreddit\n    subreddit = reddit_instance.subreddit(subreddit_name)\n\n    # Get hot posts from the subreddit\n    hot_posts = subreddit.hot(limit=limit)\n\n    # Filter out posts that are not pinned\n    filtered_posts = [post for post in hot_posts if not post.stickied]\n    return filtered_posts","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='Extract Reddit Data'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1511'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13045662268160'}, subreddit=Property{prop='subreddit', direct=IN, type=VARCHAR, value='singapore'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424173351'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='595'}, limit=Property{prop='limit', direct=IN, type=VARCHAR, value='3'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='extract_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13045665829504'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.482 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: Extract Reddit Data to wait queue success
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.482 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.484 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.484 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.484 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.484 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713951231484
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.484 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 595_1511
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.485 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1511,
  "taskName" : "Extract Reddit Data",
  "firstSubmitTime" : 1713951231461,
  "startTime" : 1713951231484,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13045665829504/15/595/1511.log",
  "processId" : 0,
  "processDefineCode" : 13045665829504,
  "processDefineVersion" : 15,
  "processInstanceId" : 595,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"subreddit\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"singapore\"},{\"prop\":\"limit\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"3\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"data\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"import praw\\nimport json\\n\\nclass RedditPostDataExtractor:\\n    def __init__(self):\\n        self.data = []\\n\\n    def extract_data(self, hot_posts):\\n        if hot_posts:\\n            for post in hot_posts:\\n                post.comments.replace_more(limit=None)\\n                # initialise a dictionary to contain the data of the post\\n                post_data = {}\\n\\n                # get the relevant post's data and store it inside post_data\\n\\n                # get the poster's name\\n                if post.author is not None:\\n                    post_data['author'] = post.author.name\\n                else:\\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\\n\\n                # get the unix time in which the post was created\\n                post_data['unix_time'] = post.created_utc\\n                post_data['permalink'] = post.permalink\\n                post_data['score'] = post.score\\n                post_data['subreddit'] = post.subreddit.display_name\\n                post_data['post_title'] = post.title\\n                post_data['body'] = post.selftext\\n\\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\\n                # t3 represents that it is a post\\n                post_data['name'] = post.name\\n\\n                # store the posts data in the dictionary where the name is used as the key\\n                self.data.append(post_data)\\n\\n                # extract the comments from the post\\n                comments = post.comments.list()\\n\\n                # extract the data in the comments and store them inside the dictionary\\n                self.extract_comments_data(comments)\\n        \\n        return self.data\\n\\n    def extract_comments_data(self, comments):\\n        if comments:\\n            for comment in comments:\\n                # initialise a dictionary to contain the data of the comment\\n                comment_data = {}\\n\\n\\n                # get the relevant post's data and store it inside post_data\\n\\n                # get the commenter name\\n                if comment.author is not None:\\n                    comment_data['author'] = comment.author.name\\n                else:\\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\\n\\n                # get the unix time in which the comment was created\\n                comment_data['unix_time'] = comment.created_utc\\n                comment_data['permalink'] = comment.permalink\\n                comment_data['score'] = comment.score\\n                comment_data['subreddit'] = comment.subreddit.display_name\\n                comment_data['post_title'] = comment.submission.title\\n                comment_data['body'] = comment.body\\n\\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\\n                # t1 represents that it is a post\\n                comment_data['name'] = comment.name\\n\\n                # store the comment's data in the dictionary where the name is used as the key\\n                self.data.append(comment_data)\\n\\n                # extract the comment's replies from the post\\n                # replies = comment.replies\\n\\n                # extract the data in the comments and store them inside the dictionary\\n                # self.extract_comments_data(replies)\\n        \\n        return\\n    \\ndef get_hot_posts(reddit_instance, subreddit_name, limit):\\n    # Define the subreddit\\n    subreddit = reddit_instance.subreddit(subreddit_name)\\n\\n    # Get hot posts from the subreddit\\n    hot_posts = subreddit.hot(limit=limit)\\n\\n    # Filter out posts that are not pinned\\n    filtered_posts = [post for post in hot_posts if not post.stickied]\\n    return filtered_posts\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "Extract Reddit Data"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1511"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13045662268160"
    },
    "subreddit" : {
      "prop" : "subreddit",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "singapore"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424173351"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "595"
    },
    "limit" : {
      "prop" : "limit",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13045665829504"
    }
  },
  "taskAppId" : "595_1511",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.485 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.485 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.485 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.491 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.492 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.492 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_15/595/1511 check successfully
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.493 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.493 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.493 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.493 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.493 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ {
    "prop" : "data",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "import praw\nimport json\n\nclass RedditPostDataExtractor:\n    def __init__(self):\n        self.data = []\n\n    def extract_data(self, hot_posts):\n        if hot_posts:\n            for post in hot_posts:\n                post.comments.replace_more(limit=None)\n                # initialise a dictionary to contain the data of the post\n                post_data = {}\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the poster's name\n                if post.author is not None:\n                    post_data['author'] = post.author.name\n                else:\n                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the post was created\n                post_data['unix_time'] = post.created_utc\n                post_data['permalink'] = post.permalink\n                post_data['score'] = post.score\n                post_data['subreddit'] = post.subreddit.display_name\n                post_data['post_title'] = post.title\n                post_data['body'] = post.selftext\n\n                # get the unique name of the post (name refers to the id of the post prefixed with t3_)\n                # t3 represents that it is a post\n                post_data['name'] = post.name\n\n                # store the posts data in the dictionary where the name is used as the key\n                self.data.append(post_data)\n\n                # extract the comments from the post\n                comments = post.comments.list()\n\n                # extract the data in the comments and store them inside the dictionary\n                self.extract_comments_data(comments)\n        \n        return self.data\n\n    def extract_comments_data(self, comments):\n        if comments:\n            for comment in comments:\n                # initialise a dictionary to contain the data of the comment\n                comment_data = {}\n\n\n                # get the relevant post's data and store it inside post_data\n\n                # get the commenter name\n                if comment.author is not None:\n                    comment_data['author'] = comment.author.name\n                else:\n                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'\n\n                # get the unix time in which the comment was created\n                comment_data['unix_time'] = comment.created_utc\n                comment_data['permalink'] = comment.permalink\n                comment_data['score'] = comment.score\n                comment_data['subreddit'] = comment.subreddit.display_name\n                comment_data['post_title'] = comment.submission.title\n                comment_data['body'] = comment.body\n\n                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)\n                # t1 represents that it is a post\n                comment_data['name'] = comment.name\n\n                # store the comment's data in the dictionary where the name is used as the key\n                self.data.append(comment_data)\n\n                # extract the comment's replies from the post\n                # replies = comment.replies\n\n                # extract the data in the comments and store them inside the dictionary\n                # self.extract_comments_data(replies)\n        \n        return\n    \ndef get_hot_posts(reddit_instance, subreddit_name, limit):\n    # Define the subreddit\n    subreddit = reddit_instance.subreddit(subreddit_name)\n\n    # Get hot posts from the subreddit\n    hot_posts = subreddit.hot(limit=limit)\n\n    # Filter out posts that are not pinned\n    filtered_posts = [post for post in hot_posts if not post.stickied]\n    return filtered_posts",
  "resourceList" : [ ]
}
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.494 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.494 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.494 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.494 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.494 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.494 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import praw
import json

class RedditPostDataExtractor:
    def __init__(self):
        self.data = []

    def extract_data(self, hot_posts):
        if hot_posts:
            for post in hot_posts:
                post.comments.replace_more(limit=None)
                # initialise a dictionary to contain the data of the post
                post_data = {}

                # get the relevant post's data and store it inside post_data

                # get the poster's name
                if post.author is not None:
                    post_data['author'] = post.author.name
                else:
                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the post was created
                post_data['unix_time'] = post.created_utc
                post_data['permalink'] = post.permalink
                post_data['score'] = post.score
                post_data['subreddit'] = post.subreddit.display_name
                post_data['post_title'] = post.title
                post_data['body'] = post.selftext

                # get the unique name of the post (name refers to the id of the post prefixed with t3_)
                # t3 represents that it is a post
                post_data['name'] = post.name

                # store the posts data in the dictionary where the name is used as the key
                self.data.append(post_data)

                # extract the comments from the post
                comments = post.comments.list()

                # extract the data in the comments and store them inside the dictionary
                self.extract_comments_data(comments)
        
        return self.data

    def extract_comments_data(self, comments):
        if comments:
            for comment in comments:
                # initialise a dictionary to contain the data of the comment
                comment_data = {}


                # get the relevant post's data and store it inside post_data

                # get the commenter name
                if comment.author is not None:
                    comment_data['author'] = comment.author.name
                else:
                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the comment was created
                comment_data['unix_time'] = comment.created_utc
                comment_data['permalink'] = comment.permalink
                comment_data['score'] = comment.score
                comment_data['subreddit'] = comment.subreddit.display_name
                comment_data['post_title'] = comment.submission.title
                comment_data['body'] = comment.body

                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)
                # t1 represents that it is a post
                comment_data['name'] = comment.name

                # store the comment's data in the dictionary where the name is used as the key
                self.data.append(comment_data)

                # extract the comment's replies from the post
                # replies = comment.replies

                # extract the data in the comments and store them inside the dictionary
                # self.extract_comments_data(replies)
        
        return
    
def get_hot_posts(reddit_instance, subreddit_name, limit):
    # Define the subreddit
    subreddit = reddit_instance.subreddit(subreddit_name)

    # Get hot posts from the subreddit
    hot_posts = subreddit.hot(limit=limit)

    # Filter out posts that are not pinned
    filtered_posts = [post for post in hot_posts if not post.stickied]
    return filtered_posts
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.495 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_15/595/1511
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.512 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_15/595/1511/py_595_1511.py
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.512 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import praw
import json

class RedditPostDataExtractor:
    def __init__(self):
        self.data = []

    def extract_data(self, hot_posts):
        if hot_posts:
            for post in hot_posts:
                post.comments.replace_more(limit=None)
                # initialise a dictionary to contain the data of the post
                post_data = {}

                # get the relevant post's data and store it inside post_data

                # get the poster's name
                if post.author is not None:
                    post_data['author'] = post.author.name
                else:
                    post_data['author'] = '[deleted]' # if the user was deleted, post.author will return None. This line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the post was created
                post_data['unix_time'] = post.created_utc
                post_data['permalink'] = post.permalink
                post_data['score'] = post.score
                post_data['subreddit'] = post.subreddit.display_name
                post_data['post_title'] = post.title
                post_data['body'] = post.selftext

                # get the unique name of the post (name refers to the id of the post prefixed with t3_)
                # t3 represents that it is a post
                post_data['name'] = post.name

                # store the posts data in the dictionary where the name is used as the key
                self.data.append(post_data)

                # extract the comments from the post
                comments = post.comments.list()

                # extract the data in the comments and store them inside the dictionary
                self.extract_comments_data(comments)
        
        return self.data

    def extract_comments_data(self, comments):
        if comments:
            for comment in comments:
                # initialise a dictionary to contain the data of the comment
                comment_data = {}


                # get the relevant post's data and store it inside post_data

                # get the commenter name
                if comment.author is not None:
                    comment_data['author'] = comment.author.name
                else:
                    comment_data['author'] = '[deleted]' # if the user was deleted, comment.author will return None, this line will catch that scenario and instead provide the value 'deleted'

                # get the unix time in which the comment was created
                comment_data['unix_time'] = comment.created_utc
                comment_data['permalink'] = comment.permalink
                comment_data['score'] = comment.score
                comment_data['subreddit'] = comment.subreddit.display_name
                comment_data['post_title'] = comment.submission.title
                comment_data['body'] = comment.body

                # get the unique name of the comment (name refers to the id of the post prefixed with t1_)
                # t1 represents that it is a post
                comment_data['name'] = comment.name

                # store the comment's data in the dictionary where the name is used as the key
                self.data.append(comment_data)

                # extract the comment's replies from the post
                # replies = comment.replies

                # extract the data in the comments and store them inside the dictionary
                # self.extract_comments_data(replies)
        
        return
    
def get_hot_posts(reddit_instance, subreddit_name, limit):
    # Define the subreddit
    subreddit = reddit_instance.subreddit(subreddit_name)

    # Get hot posts from the subreddit
    hot_posts = subreddit.hot(limit=limit)

    # Filter out posts that are not pinned
    filtered_posts = [post for post in hot_posts if not post.stickied]
    return filtered_posts
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.516 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.517 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.517 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_15/595/1511/py_595_1511.py
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.517 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.517 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_15/595/1511/595_1511.sh
[WI-0][TI-0] - [INFO] 2024-04-24 17:33:51.563 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:51.563 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3549
[WI-0][TI-1511] - [INFO] 2024-04-24 17:33:52.307 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1511, success=true)
[WI-0][TI-1511] - [INFO] 2024-04-24 17:33:52.318 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1511)
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:52.569 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_15/595/1511, processId:3549 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:52.570 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:52.570 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:52.570 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:52.571 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:52.575 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:52.575 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:52.575 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_15/595/1511
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:52.576 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13045665829504_15/595/1511
[WI-595][TI-1511] - [INFO] 2024-04-24 17:33:52.576 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1511] - [INFO] 2024-04-24 17:33:53.302 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1511, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:34:08.597 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1512, taskName=search for intake JSON files, firstSubmitTime=1713951248590, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=113, appIds=null, processInstanceId=596, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1512'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340113777664'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424173408'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='596'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.598 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake JSON files to wait queue success
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.598 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.600 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.600 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.600 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.600 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713951248600
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.600 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 596_1512
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.600 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1512,
  "taskName" : "search for intake JSON files",
  "firstSubmitTime" : 1713951248590,
  "startTime" : 1713951248600,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/113/596/1512.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 113,
  "processInstanceId" : 596,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set raw_file_dir to null\\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1512"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340113777664"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424173408"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "596"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "596_1512",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.600 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.600 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.601 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.610 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.617 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.619 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/596/1512 check successfully
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.619 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.621 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.621 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.621 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.621 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"",
  "resourceList" : [ ]
}
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.621 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.621 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.626 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.626 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.626 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.626 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.626 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.629 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/reddit/processing -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set raw_file_dir to null
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
fi

echo "#{setValue(raw_file_dir=${raw_file_dir})}"
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.629 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.629 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/596/1512/596_1512.sh
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:08.646 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3567
[WI-0][TI-1512] - [INFO] 2024-04-24 17:34:09.358 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1512, success=true)
[WI-0][TI-1512] - [INFO] 2024-04-24 17:34:09.370 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1512)
[WI-0][TI-0] - [INFO] 2024-04-24 17:34:09.656 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	#{setValue(raw_file_dir=null)}
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:09.658 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/596/1512, processId:3567 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:09.658 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:09.658 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:09.658 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:09.659 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:09.662 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:09.663 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:09.663 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/596/1512
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:09.663 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/596/1512
[WI-596][TI-1512] - [INFO] 2024-04-24 17:34:09.664 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1512] - [INFO] 2024-04-24 17:34:10.343 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1512, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:34:11.395 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1514, taskName=end workflow, firstSubmitTime=1713951251379, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=113, appIds=null, processInstanceId=596, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"echo No Files Detected - End Workflow","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='end workflow'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1514'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13348985859488'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424173411'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='596'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='null'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"null"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.397 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: end workflow to wait queue success
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.397 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.400 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.400 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.400 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.400 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713951251400
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.400 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 596_1514
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1514,
  "taskName" : "end workflow",
  "firstSubmitTime" : 1713951251379,
  "startTime" : 1713951251400,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/113/596/1514.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 113,
  "processInstanceId" : 596,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"echo No Files Detected - End Workflow\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "end workflow"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1514"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13348985859488"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424173411"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "596"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "null"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "596_1514",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"null\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.401 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.401 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.402 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.407 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.408 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.408 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/596/1514 check successfully
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.409 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.409 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.409 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.409 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.409 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "echo No Files Detected - End Workflow",
  "resourceList" : [ ]
}
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.409 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.410 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"null"}] successfully
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.410 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.410 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.410 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.410 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.410 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.411 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
echo No Files Detected - End Workflow
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.411 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.411 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/596/1514/596_1514.sh
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:11.420 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3580
[WI-0][TI-1514] - [INFO] 2024-04-24 17:34:12.358 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1514, success=true)
[WI-0][TI-1514] - [INFO] 2024-04-24 17:34:12.365 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1514)
[WI-0][TI-0] - [INFO] 2024-04-24 17:34:12.423 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	No Files Detected - End Workflow
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:12.435 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/596/1514, processId:3580 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:12.435 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:12.436 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:12.436 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:12.437 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:12.440 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:12.440 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:12.441 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/596/1514
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:12.441 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/596/1514
[WI-596][TI-1514] - [INFO] 2024-04-24 17:34:12.441 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1514] - [INFO] 2024-04-24 17:34:13.363 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1514, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:34:17.652 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1515, taskName=search for intake JSON files, firstSubmitTime=1713951257641, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=113, appIds=null, processInstanceId=597, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1515'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340113777664'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424173417'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='597'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.654 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake JSON files to wait queue success
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.654 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.656 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.656 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.657 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.657 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713951257657
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.657 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 597_1515
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.657 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1515,
  "taskName" : "search for intake JSON files",
  "firstSubmitTime" : 1713951257641,
  "startTime" : 1713951257657,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/113/597/1515.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 113,
  "processInstanceId" : 597,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set raw_file_dir to null\\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1515"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340113777664"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424173417"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "597"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "597_1515",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.658 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.658 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.658 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.683 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.690 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.691 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/597/1515 check successfully
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.691 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.692 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.693 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.693 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.695 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"",
  "resourceList" : [ ]
}
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.695 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.696 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.696 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.696 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.696 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.697 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.697 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.698 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/reddit/in -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set raw_file_dir to null
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
fi

echo "#{setValue(raw_file_dir=${raw_file_dir})}"
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.698 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.699 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/597/1515/597_1515.sh
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:17.702 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3590
[WI-0][TI-1515] - [INFO] 2024-04-24 17:34:18.381 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1515, success=true)
[WI-0][TI-1515] - [INFO] 2024-04-24 17:34:18.387 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1515)
[WI-0][TI-0] - [INFO] 2024-04-24 17:34:18.703 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	#{setValue(raw_file_dir=null)}
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:18.707 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/597/1515, processId:3590 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:18.710 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:18.711 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:18.711 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:18.711 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:18.733 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:18.736 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:18.736 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/597/1515
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:18.739 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/597/1515
[WI-597][TI-1515] - [INFO] 2024-04-24 17:34:18.741 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1515] - [INFO] 2024-04-24 17:34:19.402 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1515, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:34:20.586 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1517, taskName=end workflow, firstSubmitTime=1713951260576, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=113, appIds=null, processInstanceId=597, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"echo No Files Detected - End Workflow","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='end workflow'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1517'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13348985859488'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424173420'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='597'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='null'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"null"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.601 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: end workflow to wait queue success
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.602 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.607 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.608 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.609 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.609 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713951260609
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.609 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 597_1517
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.610 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1517,
  "taskName" : "end workflow",
  "firstSubmitTime" : 1713951260576,
  "startTime" : 1713951260609,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/113/597/1517.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 113,
  "processInstanceId" : 597,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"echo No Files Detected - End Workflow\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "end workflow"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1517"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13348985859488"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424173420"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "597"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "null"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "597_1517",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"null\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.610 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.610 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.611 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.615 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.617 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.618 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/597/1517 check successfully
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.619 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.624 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.625 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.625 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.625 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "echo No Files Detected - End Workflow",
  "resourceList" : [ ]
}
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.626 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.626 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"null"}] successfully
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.626 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.626 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.626 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.626 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.627 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.628 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
echo No Files Detected - End Workflow
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.628 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.628 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/597/1517/597_1517.sh
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:20.638 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3603
[WI-0][TI-1517] - [INFO] 2024-04-24 17:34:21.415 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1517, success=true)
[WI-0][TI-1517] - [INFO] 2024-04-24 17:34:21.424 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1517)
[WI-0][TI-0] - [INFO] 2024-04-24 17:34:21.639 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	No Files Detected - End Workflow
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:21.645 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/597/1517, processId:3603 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:21.646 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:21.646 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:21.646 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:21.647 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:21.650 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:21.651 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:21.651 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/597/1517
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:21.651 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/597/1517
[WI-597][TI-1517] - [INFO] 2024-04-24 17:34:21.651 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1517] - [INFO] 2024-04-24 17:34:22.413 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1517, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:35:03.820 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1518, taskName=extract from preprocessed queue, firstSubmitTime=1713951303805, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=10, appIds=null, processInstanceId=598, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"message","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n    --bootstrap-server kafka:9092 \\\n    --topic reddit_preprocessed \\\n    --group preprocessed_consumers \\\n    --max-messages 1 \\\n    --timeout-ms 10000)\n\necho \"#{setValue(message=${message})}\"","resourceList":[]}, environmentConfig=null, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract from preprocessed queue'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1518'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13375898458848'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424173503'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='598'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.824 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract from preprocessed queue to wait queue success
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.824 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.826 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.826 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.826 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.826 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713951303826
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.826 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 598_1518
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.827 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1518,
  "taskName" : "extract from preprocessed queue",
  "firstSubmitTime" : 1713951303805,
  "startTime" : 1713951303826,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/10/598/1518.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 10,
  "processInstanceId" : 598,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"message\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\\\\n    --bootstrap-server kafka:9092 \\\\\\n    --topic reddit_preprocessed \\\\\\n    --group preprocessed_consumers \\\\\\n    --max-messages 1 \\\\\\n    --timeout-ms 10000)\\n\\necho \\\"#{setValue(message=${message})}\\\"\",\"resourceList\":[]}",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract from preprocessed queue"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1518"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13375898458848"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424173503"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "598"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "598_1518",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.827 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.827 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.827 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.836 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.836 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.837 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1518 check successfully
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.837 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.838 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.838 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.838 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.839 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "message",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n    --bootstrap-server kafka:9092 \\\n    --topic reddit_preprocessed \\\n    --group preprocessed_consumers \\\n    --max-messages 1 \\\n    --timeout-ms 10000)\n\necho \"#{setValue(message=${message})}\"",
  "resourceList" : [ ]
}
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.840 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.840 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.840 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.840 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.840 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.841 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.841 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.841 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

message=$(${KAFKA_DIR}/kafka-console-consumer.sh \
    --bootstrap-server kafka:9092 \
    --topic reddit_preprocessed \
    --group preprocessed_consumers \
    --max-messages 1 \
    --timeout-ms 10000)

echo "#{setValue(message=${message})}"
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.841 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.842 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1518/598_1518.sh
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:03.847 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3625
[WI-0][TI-1518] - [INFO] 2024-04-24 17:35:04.477 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1518, success=true)
[WI-0][TI-1518] - [INFO] 2024-04-24 17:35:04.485 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1518)
[WI-0][TI-0] - [INFO] 2024-04-24 17:35:04.849 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	/opt/kafka_2.13-3.7.0/bin/kafka-run-class.sh: line 347: exec: java: not found
	#{setValue(message=)}
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:04.855 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1518, processId:3625 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-598][TI-1518] - [WARN] 2024-04-24 17:35:04.856 +0800 o.a.d.p.t.a.p.AbstractParameters:[152] - Cannot find the output parameter message in the task output parameters
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:04.856 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:04.856 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:04.857 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:04.857 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:04.861 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:04.861 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:04.861 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1518
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:04.862 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1518
[WI-598][TI-1518] - [INFO] 2024-04-24 17:35:04.862 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1518] - [INFO] 2024-04-24 17:35:05.478 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1518, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:35:05.550 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1519, taskName=keyword filtering, firstSubmitTime=1713951305525, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=10, appIds=null, processInstanceId=598, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1519'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424173505'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='598'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.551 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.552 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.559 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.559 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.559 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.559 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713951305559
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.559 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 598_1519
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.559 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1519,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713951305525,
  "startTime" : 1713951305559,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/10/598/1519.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 10,
  "processInstanceId" : 598,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1519"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424173505"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "598"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "598_1519",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.560 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.561 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.561 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.563 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.564 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.565 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1519 check successfully
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.565 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.565 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.566 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.566 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.566 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.567 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.567 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.567 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.567 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.568 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.568 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.569 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1519
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.569 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1519/py_598_1519.py
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.569 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.570 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.570 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.571 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1519/py_598_1519.py
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.571 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.571 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1519/598_1519.sh
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:05.575 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 4004
[WI-0][TI-1519] - [INFO] 2024-04-24 17:35:06.497 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1519, success=true)
[WI-0][TI-1519] - [INFO] 2024-04-24 17:35:06.512 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1519)
[WI-0][TI-0] - [INFO] 2024-04-24 17:35:06.577 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1519/py_598_1519.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:06.589 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1519, processId:4004 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:06.589 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:06.589 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:06.589 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:06.590 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:06.593 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:06.594 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:06.594 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1519
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:06.595 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1519
[WI-598][TI-1519] - [INFO] 2024-04-24 17:35:06.595 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1519] - [INFO] 2024-04-24 17:35:07.494 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1519, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:35:07.591 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1520, taskName=keyword filtering, firstSubmitTime=1713951307582, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=10, appIds=null, processInstanceId=598, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1520'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424173507'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='598'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.598 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.600 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.603 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.603 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.603 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.603 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713951307603
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.603 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 598_1520
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.605 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1520,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713951307582,
  "startTime" : 1713951307603,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/10/598/1520.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 10,
  "processInstanceId" : 598,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1520"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424173507"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "598"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "598_1520",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.606 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.606 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.606 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.615 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.615 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.616 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1520 check successfully
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.617 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.617 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.617 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.617 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.618 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.618 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.618 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.618 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.618 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.618 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.618 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.619 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1520
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.619 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1520/py_598_1520.py
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.619 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.620 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.620 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.620 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1520/py_598_1520.py
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.620 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.620 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1520/598_1520.sh
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:07.637 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 4015
[WI-0][TI-1520] - [INFO] 2024-04-24 17:35:08.530 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1520, success=true)
[WI-0][TI-1520] - [INFO] 2024-04-24 17:35:08.568 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1520)
[WI-0][TI-0] - [INFO] 2024-04-24 17:35:08.639 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1520/py_598_1520.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:08.663 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1520, processId:4015 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:08.663 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:08.664 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:08.665 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:08.665 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:08.670 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:08.670 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:08.670 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1520
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:08.671 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1520
[WI-598][TI-1520] - [INFO] 2024-04-24 17:35:08.671 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1520] - [INFO] 2024-04-24 17:35:09.513 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1520, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:35:09.659 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1521, taskName=keyword filtering, firstSubmitTime=1713951309618, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=10, appIds=null, processInstanceId=598, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1521'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424173509'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='598'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.661 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.661 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.664 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.665 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.665 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.665 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713951309665
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.665 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 598_1521
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.666 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1521,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713951309618,
  "startTime" : 1713951309665,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/10/598/1521.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 10,
  "processInstanceId" : 598,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1521"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424173509"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "598"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "598_1521",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.667 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.667 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.667 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.673 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.674 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.675 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1521 check successfully
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.675 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.675 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.675 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.676 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.676 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.676 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.676 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.677 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.677 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.677 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.677 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.678 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1521
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.678 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1521/py_598_1521.py
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.678 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.679 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.680 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.680 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1521/py_598_1521.py
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.680 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.681 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1521/598_1521.sh
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:09.710 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 4026
[WI-0][TI-0] - [INFO] 2024-04-24 17:35:10.039 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7514450867052024 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1521] - [INFO] 2024-04-24 17:35:10.523 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1521, success=true)
[WI-0][TI-1521] - [INFO] 2024-04-24 17:35:10.534 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1521)
[WI-0][TI-0] - [INFO] 2024-04-24 17:35:10.720 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1521/py_598_1521.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:10.724 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1521, processId:4026 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:10.726 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:10.726 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:10.727 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:10.728 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:10.730 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:10.731 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:10.731 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1521
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:10.731 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1521
[WI-598][TI-1521] - [INFO] 2024-04-24 17:35:10.732 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-04-24 17:35:11.058 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8664850136239782 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1521] - [INFO] 2024-04-24 17:35:11.502 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1521, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:35:11.606 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1522, taskName=keyword filtering, firstSubmitTime=1713951311580, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=10, appIds=null, processInstanceId=598, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1522'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424173511'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='598'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.607 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.607 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.610 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.610 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.610 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.610 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713951311610
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.610 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 598_1522
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.612 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1522,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713951311580,
  "startTime" : 1713951311610,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/10/598/1522.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 10,
  "processInstanceId" : 598,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1522"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424173511"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "598"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "598_1522",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.614 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.614 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.614 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.618 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.619 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.619 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1522 check successfully
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.619 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.619 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.620 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.620 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.621 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.622 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.622 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.622 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.622 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.622 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.622 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.623 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1522
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.624 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1522/py_598_1522.py
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.624 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.624 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.624 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.624 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1522/py_598_1522.py
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.624 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.624 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1522/598_1522.sh
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:11.643 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 4037
[WI-0][TI-1522] - [INFO] 2024-04-24 17:35:12.518 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1522, success=true)
[WI-0][TI-1522] - [INFO] 2024-04-24 17:35:12.529 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1522)
[WI-0][TI-0] - [INFO] 2024-04-24 17:35:12.647 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1522/py_598_1522.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:12.648 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1522, processId:4037 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:12.648 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:12.649 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:12.649 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:12.649 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:12.652 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:12.652 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:12.653 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1522
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:12.653 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_10/598/1522
[WI-598][TI-1522] - [INFO] 2024-04-24 17:35:12.654 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1522] - [INFO] 2024-04-24 17:35:13.514 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1522, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:36:17.388 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8647058823529412 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:37:04.565 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7655367231638418 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:37:10.605 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7298050139275766 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:37:52.768 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7325581395348837 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:38:12.926 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7767857142857142 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:38:16.806 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1523, taskName=extract from preprocessed queue, firstSubmitTime=1713951496796, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=11, appIds=null, processInstanceId=599, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"message","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n    --bootstrap-server kafka:9092 \\\n    --topic reddit_preprocessed \\\n    --group preprocessed_consumers \\\n    --max-messages 1 \\\n    --timeout-ms 10000)\n\necho \"#{setValue(message=${message})}\"","resourceList":[]}, environmentConfig=export JAVA_HOME=/opt/java/openjdk, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract from preprocessed queue'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1523'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13375898458848'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424173816'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='599'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.808 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract from preprocessed queue to wait queue success
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.808 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.809 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.809 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.809 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.810 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713951496809
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.810 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 599_1523
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.810 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1523,
  "taskName" : "extract from preprocessed queue",
  "firstSubmitTime" : 1713951496796,
  "startTime" : 1713951496809,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/11/599/1523.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 11,
  "processInstanceId" : 599,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"message\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\\\\n    --bootstrap-server kafka:9092 \\\\\\n    --topic reddit_preprocessed \\\\\\n    --group preprocessed_consumers \\\\\\n    --max-messages 1 \\\\\\n    --timeout-ms 10000)\\n\\necho \\\"#{setValue(message=${message})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export JAVA_HOME=/opt/java/openjdk",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract from preprocessed queue"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1523"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13375898458848"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424173816"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "599"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "599_1523",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.810 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.810 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.810 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.814 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.814 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.815 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1523 check successfully
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.815 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.815 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.815 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.815 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.816 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "message",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n    --bootstrap-server kafka:9092 \\\n    --topic reddit_preprocessed \\\n    --group preprocessed_consumers \\\n    --max-messages 1 \\\n    --timeout-ms 10000)\n\necho \"#{setValue(message=${message})}\"",
  "resourceList" : [ ]
}
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.816 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.816 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.816 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.816 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.816 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.816 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.816 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.817 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export JAVA_HOME=/opt/java/openjdk
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

message=$(${KAFKA_DIR}/kafka-console-consumer.sh \
    --bootstrap-server kafka:9092 \
    --topic reddit_preprocessed \
    --group preprocessed_consumers \
    --max-messages 1 \
    --timeout-ms 10000)

echo "#{setValue(message=${message})}"
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.817 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.817 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1523/599_1523.sh
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:16.846 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 4084
[WI-0][TI-0] - [INFO] 2024-04-24 17:38:16.847 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1523] - [INFO] 2024-04-24 17:38:17.317 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1523, success=true)
[WI-0][TI-1523] - [INFO] 2024-04-24 17:38:17.333 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1523)
[WI-0][TI-0] - [INFO] 2024-04-24 17:38:17.968 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7478005865102639 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:38:18.992 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8204334365325077 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:38:29.894 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	[2024-04-24 17:38:29,226] ERROR Error processing message, terminating consumer process:  (kafka.tools.ConsoleConsumer$)
	org.apache.kafka.common.errors.TimeoutException
	Processed a total of 0 messages
	#{setValue(message=)}
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:29.894 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1523, processId:4084 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-599][TI-1523] - [WARN] 2024-04-24 17:38:29.895 +0800 o.a.d.p.t.a.p.AbstractParameters:[152] - Cannot find the output parameter message in the task output parameters
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:29.895 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:29.896 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:29.896 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:29.896 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:29.908 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:29.908 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:29.908 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1523
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:29.908 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1523
[WI-599][TI-1523] - [INFO] 2024-04-24 17:38:29.908 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1523] - [INFO] 2024-04-24 17:38:30.339 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1523, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:38:30.450 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1524, taskName=keyword filtering, firstSubmitTime=1713951510437, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=11, appIds=null, processInstanceId=599, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1524'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424173830'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='599'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.453 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.453 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.455 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.455 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.455 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.456 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713951510456
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.456 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 599_1524
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.457 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1524,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713951510437,
  "startTime" : 1713951510456,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/11/599/1524.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 11,
  "processInstanceId" : 599,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1524"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424173830"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "599"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "599_1524",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.458 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.458 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.458 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.468 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.468 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.469 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1524 check successfully
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.469 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.470 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.470 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.470 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.471 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.472 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.473 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.473 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.473 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.473 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.473 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.474 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1524
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.474 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1524/py_599_1524.py
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.474 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.475 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.477 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.478 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1524/py_599_1524.py
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.478 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.478 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1524/599_1524.sh
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:30.485 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 4496
[WI-0][TI-1524] - [INFO] 2024-04-24 17:38:31.341 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1524, success=true)
[WI-0][TI-1524] - [INFO] 2024-04-24 17:38:31.349 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1524)
[WI-0][TI-0] - [INFO] 2024-04-24 17:38:31.486 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1524/py_599_1524.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:31.499 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1524, processId:4496 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:31.500 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:31.500 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:31.500 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:31.501 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:31.505 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:31.505 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:31.505 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1524
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:31.506 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1524
[WI-599][TI-1524] - [INFO] 2024-04-24 17:38:31.506 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1524] - [INFO] 2024-04-24 17:38:32.329 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1524, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:38:32.384 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1525, taskName=keyword filtering, firstSubmitTime=1713951512372, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=11, appIds=null, processInstanceId=599, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1525'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424173832'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='599'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.387 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.387 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.388 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.388 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.388 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.388 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713951512388
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.388 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 599_1525
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.388 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1525,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713951512372,
  "startTime" : 1713951512388,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/11/599/1525.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 11,
  "processInstanceId" : 599,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1525"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424173832"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "599"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "599_1525",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.389 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.389 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.389 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.392 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.393 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.393 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1525 check successfully
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.393 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.393 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.393 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.393 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.394 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.394 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.394 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.394 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.394 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.394 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.394 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.394 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1525
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.395 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1525/py_599_1525.py
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.395 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.398 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.398 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.398 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1525/py_599_1525.py
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.399 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.399 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1525/599_1525.sh
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:32.432 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 4507
[WI-0][TI-1525] - [INFO] 2024-04-24 17:38:33.339 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1525, success=true)
[WI-0][TI-1525] - [INFO] 2024-04-24 17:38:33.351 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1525)
[WI-0][TI-0] - [INFO] 2024-04-24 17:38:33.433 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1525/py_599_1525.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:33.435 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1525, processId:4507 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:33.436 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:33.436 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:33.437 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:33.438 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:33.442 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:33.442 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:33.443 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1525
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:33.444 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1525
[WI-599][TI-1525] - [INFO] 2024-04-24 17:38:33.444 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1525] - [INFO] 2024-04-24 17:38:34.331 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1525, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:38:34.422 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1526, taskName=keyword filtering, firstSubmitTime=1713951514403, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=11, appIds=null, processInstanceId=599, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1526'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424173834'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='599'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.423 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.424 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.426 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.427 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.427 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.427 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713951514427
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.427 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 599_1526
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.427 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1526,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713951514403,
  "startTime" : 1713951514427,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/11/599/1526.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 11,
  "processInstanceId" : 599,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1526"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424173834"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "599"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "599_1526",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.428 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.428 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.428 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.433 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.434 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.435 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1526 check successfully
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.435 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.436 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.436 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.437 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.437 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.437 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.438 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.438 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.438 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.442 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.443 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.443 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1526
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.443 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1526/py_599_1526.py
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.444 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.445 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.445 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.445 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1526/py_599_1526.py
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.445 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.445 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1526/599_1526.sh
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:34.450 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 4518
[WI-0][TI-1526] - [INFO] 2024-04-24 17:38:35.336 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1526, success=true)
[WI-0][TI-1526] - [INFO] 2024-04-24 17:38:35.344 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1526)
[WI-0][TI-0] - [INFO] 2024-04-24 17:38:35.451 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1526/py_599_1526.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:35.454 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1526, processId:4518 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:35.455 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:35.455 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:35.456 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:35.456 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:35.460 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:35.460 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:35.461 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1526
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:35.461 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1526
[WI-599][TI-1526] - [INFO] 2024-04-24 17:38:35.462 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1526] - [INFO] 2024-04-24 17:38:36.347 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1526, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:38:36.467 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1527, taskName=keyword filtering, firstSubmitTime=1713951516455, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=11, appIds=null, processInstanceId=599, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1527'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424173836'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='599'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.469 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.472 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.473 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.474 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.474 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.475 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713951516475
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.475 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 599_1527
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.475 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1527,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713951516455,
  "startTime" : 1713951516475,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/11/599/1527.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 11,
  "processInstanceId" : 599,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1527"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424173836"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "599"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "599_1527",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.475 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.475 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.475 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.498 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.499 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.501 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1527 check successfully
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.501 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.501 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.502 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.502 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.502 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.502 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.502 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.502 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.503 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.503 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.503 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.503 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1527
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.503 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1527/py_599_1527.py
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.503 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.504 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.504 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.504 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1527/py_599_1527.py
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.504 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.504 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1527/599_1527.sh
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:36.525 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 4529
[WI-0][TI-0] - [INFO] 2024-04-24 17:38:36.525 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1527] - [INFO] 2024-04-24 17:38:37.340 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1527, success=true)
[WI-0][TI-1527] - [INFO] 2024-04-24 17:38:37.357 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1527)
[WI-0][TI-0] - [INFO] 2024-04-24 17:38:37.534 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1527/py_599_1527.py", line 17
	    message = ${message}
	              ^
	SyntaxError: invalid syntax
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:37.552 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1527, processId:4529 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:37.557 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:37.557 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:37.557 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:37.558 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:37.569 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:37.569 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:37.570 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1527
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:37.571 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1527
[WI-599][TI-1527] - [INFO] 2024-04-24 17:38:37.571 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1527] - [INFO] 2024-04-24 17:38:38.357 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1527, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:38:59.172 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7163742690058479 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:39:35.396 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.793002915451895 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:44:20.374 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1528, taskName=extract from preprocessed queue, firstSubmitTime=1713951860356, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=11, appIds=null, processInstanceId=599, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"message","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n    --bootstrap-server kafka:9092 \\\n    --topic reddit_preprocessed \\\n    --group preprocessed_consumers \\\n    --max-messages 1 \\\n    --timeout-ms 10000)\n\necho \"#{setValue(message=${message})}\"","resourceList":[]}, environmentConfig=export JAVA_HOME=/opt/java/openjdk, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='extract from preprocessed queue'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1528'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13375898458848'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424174420'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='599'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.376 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: extract from preprocessed queue to wait queue success
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.377 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.378 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.379 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.379 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.379 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713951860379
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.379 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 599_1528
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.379 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1528,
  "taskName" : "extract from preprocessed queue",
  "firstSubmitTime" : 1713951860356,
  "startTime" : 1713951860379,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/11/599/1528.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 11,
  "processInstanceId" : 599,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"message\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"KAFKA_DIR=\\\"/opt/kafka_2.13-3.7.0/bin\\\"\\n\\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\\\\n    --bootstrap-server kafka:9092 \\\\\\n    --topic reddit_preprocessed \\\\\\n    --group preprocessed_consumers \\\\\\n    --max-messages 1 \\\\\\n    --timeout-ms 10000)\\n\\necho \\\"#{setValue(message=${message})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export JAVA_HOME=/opt/java/openjdk",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "extract from preprocessed queue"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1528"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13375898458848"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424174420"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "599"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "599_1528",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.379 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.380 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.380 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.383 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.383 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.384 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1528 check successfully
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.385 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.385 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.385 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.385 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.386 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "message",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "KAFKA_DIR=\"/opt/kafka_2.13-3.7.0/bin\"\n\nmessage=$(${KAFKA_DIR}/kafka-console-consumer.sh \\\n    --bootstrap-server kafka:9092 \\\n    --topic reddit_preprocessed \\\n    --group preprocessed_consumers \\\n    --max-messages 1 \\\n    --timeout-ms 10000)\n\necho \"#{setValue(message=${message})}\"",
  "resourceList" : [ ]
}
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.387 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.387 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [] successfully
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.387 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.387 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.387 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.388 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.388 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.388 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export JAVA_HOME=/opt/java/openjdk
KAFKA_DIR="/opt/kafka_2.13-3.7.0/bin"

message=$(${KAFKA_DIR}/kafka-console-consumer.sh \
    --bootstrap-server kafka:9092 \
    --topic reddit_preprocessed \
    --group preprocessed_consumers \
    --max-messages 1 \
    --timeout-ms 10000)

echo "#{setValue(message=${message})}"
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.388 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.389 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1528/599_1528.sh
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:20.395 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 5804
[WI-0][TI-1528] - [INFO] 2024-04-24 17:44:21.307 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1528, success=true)
[WI-0][TI-1528] - [INFO] 2024-04-24 17:44:21.366 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1528)
[WI-0][TI-0] - [INFO] 2024-04-24 17:44:21.406 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-24 17:44:27.401 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8076923076923077 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:44:27.417 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Processed a total of 1 messages
	#{setValue(message={"user":"NekoIdo","content":"https://preview.redd.it/u68bjywya5wc1.png?width=808&format=png&auto=webp&s=d42529d6566d93e3df2f8be398afd887a258d020\n\nFound the deleted post [here](https://www.reddit.com/r/SGExams/comments/1capzon/hi_i_am_the_driver_of_snh7z_i_would_like_to_clear/).  \nMain content has been deleted by mods but the comments are definitely not buying his excuse.\n\nAlso someone else posted another captured instance of this same driver being a road menace here:  \n[https://forums.hardwarezone.com.sg/threads/gvgt-snh7z-mercedes-glc43-tailgating-cambike-try-to-force-cambike-off-the-road.7022346/](https://forums.hardwarezone.com.sg/threads/gvgt-snh7z-mercedes-glc43-tailgating-cambike-try-to-force-cambike-off-the-road.7022346/)\n\nPersonally I think it's really lucky for this Benz driver that the SAAB driver survived, or the spotlight would shine solely on him.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":432,"subreddit":"singapore","type":"post","id":"1cau62k","datetime":"2024-04-23 11:06:00"})}
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:27.418 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1528, processId:5804 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:27.418 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:27.420 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:27.420 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:27.422 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:27.427 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:27.427 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:27.427 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1528
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:27.428 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1528
[WI-599][TI-1528] - [INFO] 2024-04-24 17:44:27.428 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1528] - [INFO] 2024-04-24 17:44:28.290 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1528, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:44:28.338 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1529, taskName=keyword filtering, firstSubmitTime=1713951868330, startTime=0, taskType=PYTHON, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13377949373536, processDefineVersion=11, appIds=null, processInstanceId=599, scheduleTime=0, globalParams=[{"prop":"keywords","direct":"IN","type":"VARCHAR","value":"\"singapore\", \"lol\""}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n","resourceList":[]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='keyword filtering'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, keywords=Property{prop='keywords', direct=IN, type=VARCHAR, value='"singapore", "lol"'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1529'}, message=Property{prop='message', direct=IN, type=VARCHAR, value='{"user":"NekoIdo","content":"https://preview.redd.it/u68bjywya5wc1.png?width=808&format=png&auto=webp&s=d42529d6566d93e3df2f8be398afd887a258d020\n\nFound the deleted post [here](https://www.reddit.com/r/SGExams/comments/1capzon/hi_i_am_the_driver_of_snh7z_i_would_like_to_clear/).  \nMain content has been deleted by mods but the comments are definitely not buying his excuse.\n\nAlso someone else posted another captured instance of this same driver being a road menace here:  \n[https://forums.hardwarezone.com.sg/threads/gvgt-snh7z-mercedes-glc43-tailgating-cambike-try-to-force-cambike-off-the-road.7022346/](https://forums.hardwarezone.com.sg/threads/gvgt-snh7z-mercedes-glc43-tailgating-cambike-try-to-force-cambike-off-the-road.7022346/)\n\nPersonally I think it's really lucky for this Benz driver that the SAAB driver survived, or the spotlight would shine solely on him.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":432,"subreddit":"singapore","type":"post","id":"1cau62k","datetime":"2024-04-23 11:06:00"}'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13377752024032'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424174428'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='599'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='filter_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13377949373536'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"message","direct":"IN","type":"VARCHAR","value":"{\"user\":\"NekoIdo\",\"content\":\"https://preview.redd.it/u68bjywya5wc1.png?width=808&format=png&auto=webp&s=d42529d6566d93e3df2f8be398afd887a258d020\\n\\nFound the deleted post [here](https://www.reddit.com/r/SGExams/comments/1capzon/hi_i_am_the_driver_of_snh7z_i_would_like_to_clear/).  \\nMain content has been deleted by mods but the comments are definitely not buying his excuse.\\n\\nAlso someone else posted another captured instance of this same driver being a road menace here:  \\n[https://forums.hardwarezone.com.sg/threads/gvgt-snh7z-mercedes-glc43-tailgating-cambike-try-to-force-cambike-off-the-road.7022346/](https://forums.hardwarezone.com.sg/threads/gvgt-snh7z-mercedes-glc43-tailgating-cambike-try-to-force-cambike-off-the-road.7022346/)\\n\\nPersonally I think it's really lucky for this Benz driver that the SAAB driver survived, or the spotlight would shine solely on him.\",\"url\":\"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/\",\"context\":\"Screenshot of SNH7Z Driver's alleged \\\"Self Justification\\\"\",\"score\":432,\"subreddit\":\"singapore\",\"type\":\"post\",\"id\":\"1cau62k\",\"datetime\":\"2024-04-23 11:06:00\"}"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.340 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: keyword filtering to wait queue success
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.340 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.341 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.341 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.341 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.342 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713951868342
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.342 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 599_1529
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.342 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1529,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1713951868330,
  "startTime" : 1713951868342,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13377949373536/11/599/1529.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 11,
  "processInstanceId" : 599,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"singapore\\\", \\\"lol\\\"\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\\nfrom pyspark.sql.types import StringType\\nfrom pyspark import SparkFiles\\n\\n# Initialize Spark session in local mode\\nspark = SparkSession.builder \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .master(\\\"local\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\nmessage = ${message}\\n\\nrdd = spark.sparkContext.parallelize([message])\\ndf = spark.read.json(rdd)\\n\\n# filter rows containing specific keywords\\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = df.filter(filter_condition)\\nfiltered_df.show()\\n\\nis_relevant = True if filtered_df.count()==1 else False\\n\\nfiltered_json = filtered_df.toJSON().collect()\\n\\n# define the parameters for the kafka broker and topic\\nkafka_params = {\\n    \\\"kafka.bootstrap.servers\\\": \\\"kafka:9092\\\",  # Kafka broker address\\n    \\\"topic\\\": \\\"reddit_filtered\\\"  # Kafka topic name\\n}\\n\\n# write data to Kafka\\nif is_relevant:\\n    spark \\\\\\n        .createDataFrame(filtered_json, StringType()) \\\\\\n        .write \\\\\\n        .format(\\\"kafka\\\") \\\\\\n        .options(**kafka_params) \\\\\\n        .save()\\n\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"singapore\", \"lol\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1529"
    },
    "message" : {
      "prop" : "message",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "{\"user\":\"NekoIdo\",\"content\":\"https://preview.redd.it/u68bjywya5wc1.png?width=808&format=png&auto=webp&s=d42529d6566d93e3df2f8be398afd887a258d020\\n\\nFound the deleted post [here](https://www.reddit.com/r/SGExams/comments/1capzon/hi_i_am_the_driver_of_snh7z_i_would_like_to_clear/).  \\nMain content has been deleted by mods but the comments are definitely not buying his excuse.\\n\\nAlso someone else posted another captured instance of this same driver being a road menace here:  \\n[https://forums.hardwarezone.com.sg/threads/gvgt-snh7z-mercedes-glc43-tailgating-cambike-try-to-force-cambike-off-the-road.7022346/](https://forums.hardwarezone.com.sg/threads/gvgt-snh7z-mercedes-glc43-tailgating-cambike-try-to-force-cambike-off-the-road.7022346/)\\n\\nPersonally I think it's really lucky for this Benz driver that the SAAB driver survived, or the spotlight would shine solely on him.\",\"url\":\"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/\",\"context\":\"Screenshot of SNH7Z Driver's alleged \\\"Self Justification\\\"\",\"score\":432,\"subreddit\":\"singapore\",\"type\":\"post\",\"id\":\"1cau62k\",\"datetime\":\"2024-04-23 11:06:00\"}"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424174428"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "599"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "599_1529",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"message\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"{\\\"user\\\":\\\"NekoIdo\\\",\\\"content\\\":\\\"https://preview.redd.it/u68bjywya5wc1.png?width=808&format=png&auto=webp&s=d42529d6566d93e3df2f8be398afd887a258d020\\\\n\\\\nFound the deleted post [here](https://www.reddit.com/r/SGExams/comments/1capzon/hi_i_am_the_driver_of_snh7z_i_would_like_to_clear/).  \\\\nMain content has been deleted by mods but the comments are definitely not buying his excuse.\\\\n\\\\nAlso someone else posted another captured instance of this same driver being a road menace here:  \\\\n[https://forums.hardwarezone.com.sg/threads/gvgt-snh7z-mercedes-glc43-tailgating-cambike-try-to-force-cambike-off-the-road.7022346/](https://forums.hardwarezone.com.sg/threads/gvgt-snh7z-mercedes-glc43-tailgating-cambike-try-to-force-cambike-off-the-road.7022346/)\\\\n\\\\nPersonally I think it's really lucky for this Benz driver that the SAAB driver survived, or the spotlight would shine solely on him.\\\",\\\"url\\\":\\\"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/\\\",\\\"context\\\":\\\"Screenshot of SNH7Z Driver's alleged \\\\\\\"Self Justification\\\\\\\"\\\",\\\"score\\\":432,\\\"subreddit\\\":\\\"singapore\\\",\\\"type\\\":\\\"post\\\",\\\"id\\\":\\\"1cau62k\\\",\\\"datetime\\\":\\\"2024-04-23 11:06:00\\\"}\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.343 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.344 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.344 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.352 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.353 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.353 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1529 check successfully
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.353 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.353 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.354 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.354 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: PYTHON create successfully
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.354 +0800 o.a.d.p.t.p.PythonTask:[75] - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col\nfrom pyspark.sql.types import StringType\nfrom pyspark import SparkFiles\n\n# Initialize Spark session in local mode\nspark = SparkSession.builder \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .master(\"local\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\nmessage = ${message}\n\nrdd = spark.sparkContext.parallelize([message])\ndf = spark.read.json(rdd)\n\n# filter rows containing specific keywords\nkeywords = [' ', 'sdf', 'sdfsdfsdf']\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = df.filter(filter_condition)\nfiltered_df.show()\n\nis_relevant = True if filtered_df.count()==1 else False\n\nfiltered_json = filtered_df.toJSON().collect()\n\n# define the parameters for the kafka broker and topic\nkafka_params = {\n    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker address\n    \"topic\": \"reddit_filtered\"  # Kafka topic name\n}\n\n# write data to Kafka\nif is_relevant:\n    spark \\\n        .createDataFrame(filtered_json, StringType()) \\\n        .write \\\n        .format(\"kafka\") \\\n        .options(**kafka_params) \\\n        .save()\n\n",
  "resourceList" : [ ]
}
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.354 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.354 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"message","direct":"IN","type":"VARCHAR","value":"{\"user\":\"NekoIdo\",\"content\":\"https://preview.redd.it/u68bjywya5wc1.png?width=808&format=png&auto=webp&s=d42529d6566d93e3df2f8be398afd887a258d020\\n\\nFound the deleted post [here](https://www.reddit.com/r/SGExams/comments/1capzon/hi_i_am_the_driver_of_snh7z_i_would_like_to_clear/).  \\nMain content has been deleted by mods but the comments are definitely not buying his excuse.\\n\\nAlso someone else posted another captured instance of this same driver being a road menace here:  \\n[https://forums.hardwarezone.com.sg/threads/gvgt-snh7z-mercedes-glc43-tailgating-cambike-try-to-force-cambike-off-the-road.7022346/](https://forums.hardwarezone.com.sg/threads/gvgt-snh7z-mercedes-glc43-tailgating-cambike-try-to-force-cambike-off-the-road.7022346/)\\n\\nPersonally I think it's really lucky for this Benz driver that the SAAB driver survived, or the spotlight would shine solely on him.\",\"url\":\"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/\",\"context\":\"Screenshot of SNH7Z Driver's alleged \\\"Self Justification\\\"\",\"score\":432,\"subreddit\":\"singapore\",\"type\":\"post\",\"id\":\"1cau62k\",\"datetime\":\"2024-04-23 11:06:00\"}"}] successfully
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.355 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.355 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.355 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.355 +0800 o.a.d.p.t.p.PythonTask:[165] - raw python script : import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = ${message}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.355 +0800 o.a.d.p.t.p.PythonTask:[131] - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1529
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.355 +0800 o.a.d.p.t.p.PythonTask:[134] - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1529/py_599_1529.py
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.355 +0800 o.a.d.p.t.p.PythonTask:[141] - #-*- encoding=utf8 -*-

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat, lit, split, udf, from_unixtime, col
from pyspark.sql.types import StringType
from pyspark import SparkFiles

# Initialize Spark session in local mode
spark = SparkSession.builder \
    .appName("Reddit Keyword Filtering") \
    .master("local") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

message = {"user":"NekoIdo","content":"https://preview.redd.it/u68bjywya5wc1.png?width=808&format=png&auto=webp&s=d42529d6566d93e3df2f8be398afd887a258d020\n\nFound the deleted post [here](https://www.reddit.com/r/SGExams/comments/1capzon/hi_i_am_the_driver_of_snh7z_i_would_like_to_clear/).  \nMain content has been deleted by mods but the comments are definitely not buying his excuse.\n\nAlso someone else posted another captured instance of this same driver being a road menace here:  \n[https://forums.hardwarezone.com.sg/threads/gvgt-snh7z-mercedes-glc43-tailgating-cambike-try-to-force-cambike-off-the-road.7022346/](https://forums.hardwarezone.com.sg/threads/gvgt-snh7z-mercedes-glc43-tailgating-cambike-try-to-force-cambike-off-the-road.7022346/)\n\nPersonally I think it's really lucky for this Benz driver that the SAAB driver survived, or the spotlight would shine solely on him.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":432,"subreddit":"singapore","type":"post","id":"1cau62k","datetime":"2024-04-23 11:06:00"}

rdd = spark.sparkContext.parallelize([message])
df = spark.read.json(rdd)

# filter rows containing specific keywords
keywords = [' ', 'sdf', 'sdfsdfsdf']

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = df.filter(filter_condition)
filtered_df.show()

is_relevant = True if filtered_df.count()==1 else False

filtered_json = filtered_df.toJSON().collect()

# define the parameters for the kafka broker and topic
kafka_params = {
    "kafka.bootstrap.servers": "kafka:9092",  # Kafka broker address
    "topic": "reddit_filtered"  # Kafka topic name
}

# write data to Kafka
if is_relevant:
    spark \
        .createDataFrame(filtered_json, StringType()) \
        .write \
        .format("kafka") \
        .options(**kafka_params) \
        .save()


[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.356 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.356 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.356 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1529/py_599_1529.py
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.356 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.356 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1529/599_1529.sh
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:28.374 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 6210
[WI-0][TI-0] - [INFO] 2024-04-24 17:44:28.412 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7331460674157303 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1529] - [INFO] 2024-04-24 17:44:29.310 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1529, success=true)
[WI-0][TI-1529] - [INFO] 2024-04-24 17:44:29.323 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1529)
[WI-0][TI-0] - [INFO] 2024-04-24 17:44:29.384 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-24 17:44:29.414 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7417417417417418 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:44:31.421 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3-scala2.13/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-421e0a9f-fb7e-4149-ad44-3a36a98e9d82;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 609ms :: artifacts dl 17ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-421e0a9f-fb7e-4149-ad44-3a36a98e9d82
		confs: [default]
		0 artifacts copied, 12 already retrieved (0kB/14ms)
[WI-0][TI-0] - [INFO] 2024-04-24 17:44:31.435 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.743859649122807 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:44:32.423 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 17:44:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[WI-0][TI-0] - [INFO] 2024-04-24 17:44:33.457 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7374631268436578 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:44:38.507 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8118811881188119 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:44:40.555 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7275747508305648 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:44:41.436 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 0:>                                                          (0 + 1) / 1]
[WI-0][TI-0] - [INFO] 2024-04-24 17:44:42.440 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	                                                                                
[WI-0][TI-0] - [INFO] 2024-04-24 17:44:46.454 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	[Stage 6:>                                                          (0 + 1) / 1]
	
	                                                                                
	+--------------------+--------------------+-------------------+-------+-----+---------+----+--------------------+-------+
	|             content|             context|           datetime|     id|score|subreddit|type|                 url|   user|
	+--------------------+--------------------+-------------------+-------+-----+---------+----+--------------------+-------+
	|https://preview.r...|Screenshot of SNH...|2024-04-23 11:06:00|1cau62k|  432|singapore|post|https://www.reddi...|NekoIdo|
	+--------------------+--------------------+-------------------+-------+-----+---------+----+--------------------+-------+
	
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:47.456 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1529, processId:6210 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:47.458 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:47.458 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:47.458 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:47.459 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:47.462 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:47.463 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:47.464 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1529
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:47.465 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_11/599/1529
[WI-599][TI-1529] - [INFO] 2024-04-24 17:44:47.466 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1529] - [INFO] 2024-04-24 17:44:48.374 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1529, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 17:44:54.624 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7670807453416149 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:44:57.689 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7687687687687688 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:44:58.708 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7954545454545454 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:44:59.709 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.78125 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:45:01.721 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7701492537313432 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:45:52.975 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8450292397660819 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:47:05.334 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.717391304347826 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 17:47:06.367 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7971014492753622 is over then the MaxCpuUsagePercentageThresholds 0.7
