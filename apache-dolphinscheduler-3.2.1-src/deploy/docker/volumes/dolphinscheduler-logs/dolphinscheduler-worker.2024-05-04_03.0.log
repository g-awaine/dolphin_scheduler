[WI-0][TI-0] - [WARN] 2024-05-04 03:00:25.403 +0800 o.s.c.k.c.p.AbstractKubernetesProfileEnvironmentPostProcessor:[258] - Not running inside kubernetes. Skipping 'kubernetes' profile activation.
[WI-0][TI-0] - [WARN] 2024-05-04 03:00:35.158 +0800 o.s.c.k.c.p.AbstractKubernetesProfileEnvironmentPostProcessor:[258] - Not running inside kubernetes. Skipping 'kubernetes' profile activation.
[WI-0][TI-0] - [INFO] 2024-05-04 03:00:35.168 +0800 o.a.d.s.w.WorkerServer:[634] - No active profile set, falling back to 1 default profile: "default"
[WI-0][TI-0] - [INFO] 2024-05-04 03:00:51.714 +0800 o.s.c.c.s.GenericScope:[283] - BeanFactory id=7e7bf7c6-2cb3-34d2-a0d5-a56161c67d0e
[WI-0][TI-0] - [INFO] 2024-05-04 03:00:56.309 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'org.springframework.cloud.commons.config.CommonsConfigAutoConfiguration' of type [org.springframework.cloud.commons.config.CommonsConfigAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-04 03:00:56.320 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'org.springframework.cloud.client.loadbalancer.LoadBalancerDefaultMappingsProviderAutoConfiguration' of type [org.springframework.cloud.client.loadbalancer.LoadBalancerDefaultMappingsProviderAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-04 03:00:56.325 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'loadBalancerClientsDefaultsMappingsProvider' of type [org.springframework.cloud.client.loadbalancer.LoadBalancerDefaultMappingsProviderAutoConfiguration$$Lambda$392/189820624] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-04 03:00:56.339 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'defaultsBindHandlerAdvisor' of type [org.springframework.cloud.commons.config.DefaultsBindHandlerAdvisor] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-04 03:00:56.406 +0800 o.a.d.c.u.NetUtils:[312] - Get all NetworkInterfaces: [name:eth0 (eth0), name:lo (lo)]
[WI-0][TI-0] - [INFO] 2024-05-04 03:00:56.515 +0800 o.a.d.s.w.c.WorkerConfig:[98] - 
****************************Worker Configuration**************************************
  listen-port -> 1234
  exec-threads -> 100
  max-heartbeat-interval -> PT10S
  host-weight -> 100
  tenantConfig -> TenantConfig(autoCreateTenantEnabled=true, distributedTenantEnabled=false, defaultTenantEnabled=false)
  server-load-protection -> WorkerServerLoadProtection(enabled=true, maxCpuUsagePercentageThresholds=0.7, maxJVMMemoryUsagePercentageThresholds=0.7, maxSystemMemoryUsagePercentageThresholds=0.7, maxDiskUsagePercentageThresholds=0.7)
  registry-disconnect-strategy -> ConnectStrategyProperties(strategy=WAITING, maxWaitingTime=PT1M40S)
  task-execute-threads-full-policy: REJECT
  address -> 172.18.1.1:1234
  registry-path: /nodes/worker/172.18.1.1:1234
****************************Worker Configuration**************************************
[WI-0][TI-0] - [INFO] 2024-05-04 03:00:56.538 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'workerConfig' of type [org.apache.dolphinscheduler.server.worker.config.WorkerConfig$$EnhancerBySpringCGLIB$$153d90bb] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-04 03:00:58.288 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'io.kubernetes.client.spring.extended.manifests.config.KubernetesManifestsAutoConfiguration' of type [io.kubernetes.client.spring.extended.manifests.config.KubernetesManifestsAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-04 03:00:58.612 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'kubernetes.manifests-io.kubernetes.client.spring.extended.manifests.config.KubernetesManifestsProperties' of type [io.kubernetes.client.spring.extended.manifests.config.KubernetesManifestsProperties] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-04 03:00:58.622 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'io.kubernetes.client.spring.extended.controller.config.KubernetesInformerAutoConfiguration' of type [io.kubernetes.client.spring.extended.controller.config.KubernetesInformerAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-04 03:00:59.021 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'defaultApiClient' of type [io.kubernetes.client.openapi.ApiClient] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:00.364 +0800 o.e.j.u.log:[170] - Logging initialized @67518ms to org.eclipse.jetty.util.log.Slf4jLog
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:02.964 +0800 o.s.b.w.e.j.JettyServletWebServerFactory:[166] - Server initialized with port: 1235
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:03.065 +0800 o.e.j.s.Server:[375] - jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 1.8.0_402-b06
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:03.762 +0800 o.e.j.s.h.C.application:[2368] - Initializing Spring embedded WebApplicationContext
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:03.763 +0800 o.s.b.w.s.c.ServletWebServerApplicationContext:[292] - Root WebApplicationContext: initialization completed in 28420 ms
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:06.717 +0800 o.e.j.s.session:[334] - DefaultSessionIdManager workerName=node0
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:06.718 +0800 o.e.j.s.session:[339] - No SessionScavenger set, using defaults
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:06.726 +0800 o.e.j.s.session:[132] - node0 Scavenging every 600000ms
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:06.755 +0800 o.e.j.s.h.ContextHandler:[921] - Started o.s.b.w.e.j.JettyEmbeddedWebAppContext@5403431a{application,/,[file:///tmp/jetty-docbase.1235.3216478358203662833/],AVAILABLE}
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:06.870 +0800 o.e.j.s.Server:[415] - Started @74023ms
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:08.604 +0800 o.a.c.f.i.CuratorFrameworkImpl:[338] - Starting
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:08.644 +0800 o.a.z.ZooKeeper:[98] - Client environment:zookeeper.version=3.8.0-5a02a05eddb59aee6ac762f7ea82e92a68eb9c0f, built on 2022-02-25 08:49 UTC
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:08.645 +0800 o.a.z.ZooKeeper:[98] - Client environment:host.name=bfad91e62d55
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:08.645 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.version=1.8.0_402
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:08.646 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.vendor=Temurin
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:08.646 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.home=/opt/java/openjdk
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:08.646 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.class.path=/opt/dolphinscheduler/conf:/opt/dolphinscheduler/libs/kerb-client-1.0.1.jar:/opt/dolphinscheduler/libs/spring-cloud-starter-kubernetes-client-config-2.1.3.jar:/opt/dolphinscheduler/libs/oauth2-oidc-sdk-9.35.jar:/opt/dolphinscheduler/libs/jsqlparser-4.4.jar:/opt/dolphinscheduler/libs/reactive-streams-1.0.4.jar:/opt/dolphinscheduler/libs/kubernetes-model-extensions-5.10.2.jar:/opt/dolphinscheduler/libs/zookeeper-3.8.0.jar:/opt/dolphinscheduler/libs/kubernetes-model-apps-5.10.2.jar:/opt/dolphinscheduler/libs/grpc-api-1.41.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-pigeon-3.2.1.jar:/opt/dolphinscheduler/libs/client-java-api-13.0.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-obs-3.2.1.jar:/opt/dolphinscheduler/libs/spring-web-5.3.22.jar:/opt/dolphinscheduler/libs/aws-java-sdk-emr-1.12.300.jar:/opt/dolphinscheduler/libs/spring-context-5.3.22.jar:/opt/dolphinscheduler/libs/gapic-google-cloud-storage-v2-2.18.0-alpha.jar:/opt/dolphinscheduler/libs/spring-cloud-kubernetes-client-config-2.1.3.jar:/opt/dolphinscheduler/libs/jetty-io-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/annotations-13.0.jar:/opt/dolphinscheduler/libs/jpam-1.1.jar:/opt/dolphinscheduler/libs/joda-time-2.10.13.jar:/opt/dolphinscheduler/libs/spring-cloud-kubernetes-client-autoconfig-2.1.3.jar:/opt/dolphinscheduler/libs/kerb-identity-1.0.1.jar:/opt/dolphinscheduler/libs/vertica-jdbc-12.0.4-0.jar:/opt/dolphinscheduler/libs/grpc-googleapis-1.52.1.jar:/opt/dolphinscheduler/libs/aliyun-java-sdk-kms-2.11.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-dvc-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-hana-3.2.1.jar:/opt/dolphinscheduler/libs/kubernetes-httpclient-okhttp-6.0.0.jar:/opt/dolphinscheduler/libs/jline-2.12.jar:/opt/dolphinscheduler/libs/sshd-core-2.8.0.jar:/opt/dolphinscheduler/libs/jna-platform-5.10.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-sqlserver-3.2.1.jar:/opt/dolphinscheduler/libs/commons-beanutils-1.9.4.jar:/opt/dolphinscheduler/libs/grpc-services-1.41.0.jar:/opt/dolphinscheduler/libs/jmespath-java-1.12.300.jar:/opt/dolphinscheduler/libs/curator-client-5.3.0.jar:/opt/dolphinscheduler/libs/proto-google-common-protos-2.0.1.jar:/opt/dolphinscheduler/libs/mybatis-3.5.10.jar:/opt/dolphinscheduler/libs/jackson-annotations-2.13.4.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-all-3.2.1.jar:/opt/dolphinscheduler/libs/jakarta.xml.bind-api-2.3.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-jupyter-3.2.1.jar:/opt/dolphinscheduler/libs/mybatis-plus-extension-3.5.2.jar:/opt/dolphinscheduler/libs/j2objc-annotations-1.3.jar:/opt/dolphinscheduler/libs/Java-WebSocket-1.5.1.jar:/opt/dolphinscheduler/libs/azure-storage-common-12.20.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-sagemaker-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-databend-3.2.1.jar:/opt/dolphinscheduler/libs/google-auth-library-oauth2-http-1.15.0.jar:/opt/dolphinscheduler/libs/jetty-security-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-python-3.2.1.jar:/opt/dolphinscheduler/libs/snappy-java-1.1.10.1.jar:/opt/dolphinscheduler/libs/kubernetes-model-batch-5.10.2.jar:/opt/dolphinscheduler/libs/netty-transport-native-epoll-4.1.53.Final-linux-x86_64.jar:/opt/dolphinscheduler/libs/jackson-core-asl-1.9.13.jar:/opt/dolphinscheduler/libs/gson-fire-1.8.5.jar:/opt/dolphinscheduler/libs/clickhouse-jdbc-0.4.6.jar:/opt/dolphinscheduler/libs/grpc-netty-shaded-1.41.0.jar:/opt/dolphinscheduler/libs/caffeine-2.9.3.jar:/opt/dolphinscheduler/libs/aliyun-java-sdk-core-4.5.10.jar:/opt/dolphinscheduler/libs/jackson-module-jaxb-annotations-2.13.3.jar:/opt/dolphinscheduler/libs/jetty-http-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/jersey-servlet-1.19.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-dinky-3.2.1.jar:/opt/dolphinscheduler/libs/simpleclient_tracer_common-0.15.0.jar:/opt/dolphinscheduler/libs/netty-handler-4.1.53.Final.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-datafactory-3.2.1.jar:/opt/dolphinscheduler/libs/json-path-2.7.0.jar:/opt/dolphinscheduler/libs/mybatis-plus-annotation-3.5.2.jar:/opt/dolphinscheduler/libs/azure-resourcemanager-datafactory-1.0.0-beta.19.jar:/opt/dolphinscheduler/libs/commons-codec-1.11.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-master-3.2.1.jar:/opt/dolphinscheduler/libs/spring-aop-5.3.22.jar:/opt/dolphinscheduler/libs/kubernetes-model-core-5.10.2.jar:/opt/dolphinscheduler/libs/kubernetes-model-networking-5.10.2.jar:/opt/dolphinscheduler/libs/kotlin-stdlib-jdk7-1.6.21.jar:/opt/dolphinscheduler/libs/kerby-xdr-1.0.1.jar:/opt/dolphinscheduler/libs/aliyun-java-sdk-ram-3.1.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-k8s-3.2.1.jar:/opt/dolphinscheduler/libs/guava-31.1-jre.jar:/opt/dolphinscheduler/libs/netty-codec-dns-4.1.53.Final.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-kubeflow-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-azure-sql-3.2.1.jar:/opt/dolphinscheduler/libs/HikariCP-4.0.3.jar:/opt/dolphinscheduler/libs/hive-metastore-2.3.9.jar:/opt/dolphinscheduler/libs/msal4j-1.13.3.jar:/opt/dolphinscheduler/libs/jackson-core-2.13.4.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-hive-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-mr-3.2.1.jar:/opt/dolphinscheduler/libs/kubernetes-model-node-5.10.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-common-3.2.1.jar:/opt/dolphinscheduler/libs/jose4j-0.7.8.jar:/opt/dolphinscheduler/libs/jul-to-slf4j-1.7.36.jar:/opt/dolphinscheduler/libs/azure-identity-1.7.1.jar:/opt/dolphinscheduler/libs/spring-boot-autoconfigure-2.7.3.jar:/opt/dolphinscheduler/libs/commons-math3-3.1.1.jar:/opt/dolphinscheduler/libs/jackson-jaxrs-json-provider-2.13.3.jar:/opt/dolphinscheduler/libs/perfmark-api-0.23.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-doris-3.2.1.jar:/opt/dolphinscheduler/libs/httpclient-4.5.13.jar:/opt/dolphinscheduler/libs/hive-service-2.3.9.jar:/opt/dolphinscheduler/libs/kerby-util-1.0.1.jar:/opt/dolphinscheduler/libs/commons-collections-3.2.2.jar:/opt/dolphinscheduler/libs/hadoop-yarn-client-3.2.4.jar:/opt/dolphinscheduler/libs/commons-text-1.8.jar:/opt/dolphinscheduler/libs/dolphinscheduler-worker-3.2.1.jar:/opt/dolphinscheduler/libs/hadoop-yarn-common-3.2.4.jar:/opt/dolphinscheduler/libs/zeppelin-client-0.10.1.jar:/opt/dolphinscheduler/libs/azure-core-management-1.10.1.jar:/opt/dolphinscheduler/libs/hadoop-mapreduce-client-jobclient-3.2.4.jar:/opt/dolphinscheduler/libs/jta-1.1.jar:/opt/dolphinscheduler/libs/annotations-4.1.1.4.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-alert-3.2.1.jar:/opt/dolphinscheduler/libs/curator-framework-5.3.0.jar:/opt/dolphinscheduler/libs/hive-jdbc-2.3.9.jar:/opt/dolphinscheduler/libs/kyuubi-hive-jdbc-shaded-1.7.0.jar:/opt/dolphinscheduler/libs/client-java-proto-13.0.2.jar:/opt/dolphinscheduler/libs/checker-qual-3.19.0.jar:/opt/dolphinscheduler/libs/jetty-servlet-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/google-cloud-core-grpc-2.10.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-shell-3.2.1.jar:/opt/dolphinscheduler/libs/spring-security-rsa-1.0.10.RELEASE.jar:/opt/dolphinscheduler/libs/avro-1.7.7.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-postgresql-3.2.1.jar:/opt/dolphinscheduler/libs/netty-nio-client-2.17.282.jar:/opt/dolphinscheduler/libs/spring-webmvc-5.3.22.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-mysql-3.2.1.jar:/opt/dolphinscheduler/libs/opentracing-util-0.33.0.jar:/opt/dolphinscheduler/libs/auto-value-1.10.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-all-3.2.1.jar:/opt/dolphinscheduler/libs/jetcd-core-0.5.11.jar:/opt/dolphinscheduler/libs/grpc-context-1.41.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-datasync-3.2.1.jar:/opt/dolphinscheduler/libs/jackson-dataformat-cbor-2.13.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-gcs-3.2.1.jar:/opt/dolphinscheduler/libs/client-java-extended-13.0.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-redshift-3.2.1.jar:/opt/dolphinscheduler/libs/opencsv-2.3.jar:/opt/dolphinscheduler/libs/jcip-annotations-1.0-1.jar:/opt/dolphinscheduler/libs/accessors-smart-2.4.8.jar:/opt/dolphinscheduler/libs/hadoop-client-3.2.4.jar:/opt/dolphinscheduler/libs/commons-collections4-4.3.jar:/opt/dolphinscheduler/libs/metrics-core-4.2.11.jar:/opt/dolphinscheduler/libs/netty-resolver-4.1.53.Final.jar:/opt/dolphinscheduler/libs/parquet-hadoop-bundle-1.8.1.jar:/opt/dolphinscheduler/libs/bucket4j-core-6.2.0.jar:/opt/dolphinscheduler/libs/spring-cloud-starter-3.1.3.jar:/opt/dolphinscheduler/libs/mssql-jdbc-11.2.1.jre8.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/dolphinscheduler/libs/kubernetes-model-coordination-5.10.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-linkis-3.2.1.jar:/opt/dolphinscheduler/libs/commons-net-3.6.jar:/opt/dolphinscheduler/libs/netty-transport-native-kqueue-4.1.53.Final-osx-x86_64.jar:/opt/dolphinscheduler/libs/dolphinscheduler-meter-3.2.1.jar:/opt/dolphinscheduler/libs/kubernetes-client-api-6.0.0.jar:/opt/dolphinscheduler/libs/kubernetes-model-certificates-5.10.2.jar:/opt/dolphinscheduler/libs/opencensus-api-0.31.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-mlflow-3.2.1.jar:/opt/dolphinscheduler/libs/kotlin-stdlib-jdk8-1.6.21.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-jdbc-3.2.1.jar:/opt/dolphinscheduler/libs/audience-annotations-0.12.0.jar:/opt/dolphinscheduler/libs/simpleclient_httpserver-0.15.0.jar:/opt/dolphinscheduler/libs/jetty-xml-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/kerby-asn1-1.0.1.jar:/opt/dolphinscheduler/libs/lang-tag-1.6.jar:/opt/dolphinscheduler/libs/api-common-2.6.0.jar:/opt/dolphinscheduler/libs/HdrHistogram-2.1.12.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-abs-3.2.1.jar:/opt/dolphinscheduler/libs/failsafe-2.4.4.jar:/opt/dolphinscheduler/libs/hbase-noop-htrace-4.1.1.jar:/opt/dolphinscheduler/libs/jamon-runtime-2.3.1.jar:/opt/dolphinscheduler/libs/jetty-util-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/threetenbp-1.6.5.jar:/opt/dolphinscheduler/libs/jakarta.activation-api-1.2.2.jar:/opt/dolphinscheduler/libs/kubernetes-model-common-5.10.2.jar:/opt/dolphinscheduler/libs/okhttp-4.9.3.jar:/opt/dolphinscheduler/libs/profiles-2.17.282.jar:/opt/dolphinscheduler/libs/hadoop-auth-3.2.4.jar:/opt/dolphinscheduler/libs/grpc-netty-1.41.0.jar:/opt/dolphinscheduler/libs/httpcore-nio-4.4.15.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-seatunnel-3.2.1.jar:/opt/dolphinscheduler/libs/aws-java-sdk-sagemaker-1.12.300.jar:/opt/dolphinscheduler/libs/oshi-core-6.1.1.jar:/opt/dolphinscheduler/libs/bonecp-0.8.0.RELEASE.jar:/opt/dolphinscheduler/libs/jackson-module-parameter-names-2.13.3.jar:/opt/dolphinscheduler/libs/bcutil-jdk15on-1.69.jar:/opt/dolphinscheduler/libs/aws-json-protocol-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-base-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-all-3.2.1.jar:/opt/dolphinscheduler/libs/derby-10.14.2.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-snowflake-3.2.1.jar:/opt/dolphinscheduler/libs/netty-codec-http2-4.1.53.Final.jar:/opt/dolphinscheduler/libs/jetty-continuation-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/jackson-mapper-asl-1.9.13.jar:/opt/dolphinscheduler/libs/datanucleus-core-4.1.17.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/dolphinscheduler/libs/failureaccess-1.0.1.jar:/opt/dolphinscheduler/libs/error_prone_annotations-2.5.1.jar:/opt/dolphinscheduler/libs/kerb-admin-1.0.1.jar:/opt/dolphinscheduler/libs/token-provider-1.0.1.jar:/opt/dolphinscheduler/libs/reactor-netty-core-1.0.22.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-all-3.2.1.jar:/opt/dolphinscheduler/libs/jakarta.servlet-api-4.0.4.jar:/opt/dolphinscheduler/libs/http-client-spi-2.17.282.jar:/opt/dolphinscheduler/libs/regions-2.17.282.jar:/opt/dolphinscheduler/libs/logback-core-1.2.11.jar:/opt/dolphinscheduler/libs/json-1.8.jar:/opt/dolphinscheduler/libs/gax-grpc-2.23.0.jar:/opt/dolphinscheduler/libs/google-http-client-1.42.3.jar:/opt/dolphinscheduler/libs/hive-serde-2.3.9.jar:/opt/dolphinscheduler/libs/jettison-1.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-http-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-zookeeper-3.2.1.jar:/opt/dolphinscheduler/libs/spring-security-crypto-5.7.3.jar:/opt/dolphinscheduler/libs/auth-2.17.282.jar:/opt/dolphinscheduler/libs/proto-google-cloud-storage-v2-2.18.0-alpha.jar:/opt/dolphinscheduler/libs/jetty-server-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/javax.annotation-api-1.3.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-starrocks-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-dataquality-3.2.1.jar:/opt/dolphinscheduler/libs/jcl-over-slf4j-1.7.36.jar:/opt/dolphinscheduler/libs/commons-lang3-3.12.0.jar:/opt/dolphinscheduler/libs/tomcat-embed-el-9.0.65.jar:/opt/dolphinscheduler/libs/opentracing-noop-0.33.0.jar:/opt/dolphinscheduler/libs/client-java-13.0.2.jar:/opt/dolphinscheduler/libs/spring-beans-5.3.22.jar:/opt/dolphinscheduler/libs/snakeyaml-1.33.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-ssh-3.2.1.jar:/opt/dolphinscheduler/libs/commons-pool-1.6.jar:/opt/dolphinscheduler/libs/javax.servlet-api-3.1.0.jar:/opt/dolphinscheduler/libs/hadoop-common-3.2.4.jar:/opt/dolphinscheduler/libs/kubernetes-model-apiextensions-5.10.2.jar:/opt/dolphinscheduler/libs/spring-boot-2.7.3.jar:/opt/dolphinscheduler/libs/bcprov-ext-jdk15on-1.69.jar:/opt/dolphinscheduler/libs/spring-boot-starter-2.7.3.jar:/opt/dolphinscheduler/libs/paranamer-2.3.jar:/opt/dolphinscheduler/libs/httpmime-4.5.13.jar:/opt/dolphinscheduler/libs/reactor-core-3.4.22.jar:/opt/dolphinscheduler/libs/azure-core-1.36.0.jar:/opt/dolphinscheduler/libs/bcpkix-jdk15on-1.69.jar:/opt/dolphinscheduler/libs/kubernetes-model-scheduling-5.10.2.jar:/opt/dolphinscheduler/libs/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/dolphinscheduler/libs/websocket-client-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/unirest-java-3.7.04-standalone.jar:/opt/dolphinscheduler/libs/hadoop-mapreduce-client-core-3.2.4.jar:/opt/dolphinscheduler/libs/google-auth-library-credentials-1.15.0.jar:/opt/dolphinscheduler/libs/azure-storage-internal-avro-12.6.0.jar:/opt/dolphinscheduler/libs/jackson-datatype-jdk8-2.13.3.jar:/opt/dolphinscheduler/libs/spring-expression-5.3.22.jar:/opt/dolphinscheduler/libs/aspectjweaver-1.9.7.jar:/opt/dolphinscheduler/libs/websocket-common-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/client-java-api-fluent-13.0.2.jar:/opt/dolphinscheduler/libs/mybatis-plus-3.5.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-athena-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-oss-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-openmldb-3.2.1.jar:/opt/dolphinscheduler/libs/curator-recipes-5.3.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-api-3.2.1.jar:/opt/dolphinscheduler/libs/netty-all-4.1.53.Final.jar:/opt/dolphinscheduler/libs/netty-resolver-dns-4.1.53.Final.jar:/opt/dolphinscheduler/libs/kubernetes-model-autoscaling-5.10.2.jar:/opt/dolphinscheduler/libs/kerb-util-1.0.1.jar:/opt/dolphinscheduler/libs/grpc-alts-1.41.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-presto-3.2.1.jar:/opt/dolphinscheduler/libs/kerb-simplekdc-1.0.1.jar:/opt/dolphinscheduler/libs/protobuf-java-util-3.17.2.jar:/opt/dolphinscheduler/libs/azure-core-http-netty-1.13.0.jar:/opt/dolphinscheduler/libs/transaction-api-1.1.jar:/opt/dolphinscheduler/libs/datanucleus-api-jdo-4.2.4.jar:/opt/dolphinscheduler/libs/zeppelin-common-0.10.1.jar:/opt/dolphinscheduler/libs/zt-zip-1.15.jar:/opt/dolphinscheduler/libs/animal-sniffer-annotations-1.19.jar:/opt/dolphinscheduler/libs/google-http-client-jackson2-1.42.3.jar:/opt/dolphinscheduler/libs/netty-buffer-4.1.53.Final.jar:/opt/dolphinscheduler/libs/jsr305-3.0.0.jar:/opt/dolphinscheduler/libs/netty-transport-classes-epoll-4.1.79.Final.jar:/opt/dolphinscheduler/libs/spring-boot-actuator-autoconfigure-2.7.3.jar:/opt/dolphinscheduler/libs/simpleclient-0.15.0.jar:/opt/dolphinscheduler/libs/aliyun-sdk-oss-3.15.1.jar:/opt/dolphinscheduler/libs/json-utils-2.17.282.jar:/opt/dolphinscheduler/libs/tephra-api-0.6.0.jar:/opt/dolphinscheduler/libs/spring-jdbc-5.3.22.jar:/opt/dolphinscheduler/libs/grpc-xds-1.41.0.jar:/opt/dolphinscheduler/libs/logback-classic-1.2.11.jar:/opt/dolphinscheduler/libs/google-cloud-storage-2.18.0.jar:/opt/dolphinscheduler/libs/micrometer-registry-prometheus-1.9.3.jar:/opt/dolphinscheduler/libs/grpc-core-1.41.0.jar:/opt/dolphinscheduler/libs/aws-java-sdk-kms-1.12.300.jar:/opt/dolphinscheduler/libs/kubernetes-model-rbac-5.10.2.jar:/opt/dolphinscheduler/libs/ini4j-0.5.4.jar:/opt/dolphinscheduler/libs/spring-boot-starter-web-2.7.3.jar:/opt/dolphinscheduler/libs/spring-cloud-commons-3.1.3.jar:/opt/dolphinscheduler/libs/netty-codec-http-4.1.53.Final.jar:/opt/dolphinscheduler/libs/jetty-client-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/jetcd-common-0.5.11.jar:/opt/dolphinscheduler/libs/metrics-spi-2.17.282.jar:/opt/dolphinscheduler/libs/utils-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-emr-3.2.1.jar:/opt/dolphinscheduler/libs/protocol-core-2.17.282.jar:/opt/dolphinscheduler/libs/micrometer-core-1.9.3.jar:/opt/dolphinscheduler/libs/netty-transport-native-unix-common-4.1.53.Final.jar:/opt/dolphinscheduler/libs/zjsonpatch-0.4.11.jar:/opt/dolphinscheduler/libs/annotations-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-sql-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-common-3.2.1.jar:/opt/dolphinscheduler/libs/apache-client-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-oceanbase-3.2.1.jar:/opt/dolphinscheduler/libs/jdo-api-3.0.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-datax-3.2.1.jar:/opt/dolphinscheduler/libs/postgresql-42.4.1.jar:/opt/dolphinscheduler/libs/jetty-util-ajax-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/gax-2.23.0.jar:/opt/dolphinscheduler/libs/grpc-stub-1.41.0.jar:/opt/dolphinscheduler/libs/libfb303-0.9.3.jar:/opt/dolphinscheduler/libs/spring-core-5.3.22.jar:/opt/dolphinscheduler/libs/jaxb-api-2.3.1.jar:/opt/dolphinscheduler/libs/hive-service-rpc-2.3.9.jar:/opt/dolphinscheduler/libs/kubernetes-model-flowcontrol-5.10.2.jar:/opt/dolphinscheduler/libs/commons-logging-1.1.1.jar:/opt/dolphinscheduler/libs/mybatis-spring-2.0.7.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-oracle-3.2.1.jar:/opt/dolphinscheduler/libs/opencensus-proto-0.2.0.jar:/opt/dolphinscheduler/libs/grpc-protobuf-1.41.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-clickhouse-3.2.1.jar:/opt/dolphinscheduler/libs/spring-cloud-starter-bootstrap-3.1.3.jar:/opt/dolphinscheduler/libs/kubernetes-model-admissionregistration-5.10.2.jar:/opt/dolphinscheduler/libs/grpc-google-cloud-storage-v2-2.18.0-alpha.jar:/opt/dolphinscheduler/libs/esdk-obs-java-bundle-3.23.3.jar:/opt/dolphinscheduler/libs/dnsjava-2.1.7.jar:/opt/dolphinscheduler/libs/asm-9.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-spark-3.2.1.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/dolphinscheduler/libs/re2j-1.6.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-vertica-3.2.1.jar:/opt/dolphinscheduler/libs/json-smart-2.4.8.jar:/opt/dolphinscheduler/libs/reactor-netty-http-1.0.22.jar:/opt/dolphinscheduler/libs/google-api-services-storage-v1-rev20220705-2.0.0.jar:/opt/dolphinscheduler/libs/kubernetes-client-6.0.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-kyuubi-3.2.1.jar:/opt/dolphinscheduler/libs/auto-value-annotations-1.10.1.jar:/opt/dolphinscheduler/libs/commons-cli-1.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-data-quality-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-chunjun-3.2.1.jar:/opt/dolphinscheduler/libs/kerb-server-1.0.1.jar:/opt/dolphinscheduler/libs/content-type-2.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-remoteshell-3.2.1.jar:/opt/dolphinscheduler/libs/opencensus-contrib-http-util-0.31.1.jar:/opt/dolphinscheduler/libs/druid-1.2.20.jar:/opt/dolphinscheduler/libs/javolution-5.5.1.jar:/opt/dolphinscheduler/libs/protobuf-java-3.17.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-db2-3.2.1.jar:/opt/dolphinscheduler/libs/aws-java-sdk-core-1.12.300.jar:/opt/dolphinscheduler/libs/spring-boot-starter-logging-2.7.3.jar:/opt/dolphinscheduler/libs/kerby-pkix-1.0.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-procedure-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-etcd-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-sqoop-3.2.1.jar:/opt/dolphinscheduler/libs/zookeeper-jute-3.8.0.jar:/opt/dolphinscheduler/libs/netty-resolver-dns-native-macos-4.1.53.Final-osx-x86_64.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-sagemaker-3.2.1.jar:/opt/dolphinscheduler/libs/spring-boot-configuration-processor-2.6.1.jar:/opt/dolphinscheduler/libs/hive-common-2.3.9.jar:/opt/dolphinscheduler/libs/kerb-common-1.0.1.jar:/opt/dolphinscheduler/libs/stax-api-1.0.1.jar:/opt/dolphinscheduler/libs/kubernetes-model-storageclass-5.10.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-s3-3.2.1.jar:/opt/dolphinscheduler/libs/spring-boot-starter-jetty-2.7.3.jar:/opt/dolphinscheduler/libs/okio-2.8.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-dms-3.2.1.jar:/opt/dolphinscheduler/libs/simpleclient_common-0.15.0.jar:/opt/dolphinscheduler/libs/sdk-core-2.17.282.jar:/opt/dolphinscheduler/libs/javax.activation-api-1.2.0.jar:/opt/dolphinscheduler/libs/netty-handler-proxy-4.1.53.Final.jar:/opt/dolphinscheduler/libs/kerby-config-1.0.1.jar:/opt/dolphinscheduler/libs/netty-tcnative-classes-2.0.53.Final.jar:/opt/dolphinscheduler/libs/jackson-jaxrs-base-2.13.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-trino-3.2.1.jar:/opt/dolphinscheduler/libs/presto-jdbc-0.238.1.jar:/opt/dolphinscheduler/libs/websocket-api-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-flink-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-api-3.2.1.jar:/opt/dolphinscheduler/libs/kubernetes-model-metrics-5.10.2.jar:/opt/dolphinscheduler/libs/datasync-2.17.282.jar:/opt/dolphinscheduler/libs/hadoop-yarn-api-3.2.4.jar:/opt/dolphinscheduler/libs/google-http-client-apache-v2-1.42.3.jar:/opt/dolphinscheduler/libs/hadoop-annotations-3.2.4.jar:/opt/dolphinscheduler/libs/google-oauth-client-1.34.1.jar:/opt/dolphinscheduler/libs/sshd-common-2.8.0.jar:/opt/dolphinscheduler/libs/LatencyUtils-2.0.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-spark-3.2.1.jar:/opt/dolphinscheduler/libs/slf4j-api-1.7.36.jar:/opt/dolphinscheduler/libs/snowflake-jdbc-3.13.29.jar:/opt/dolphinscheduler/libs/jackson-datatype-jsr310-2.13.3.jar:/opt/dolphinscheduler/libs/trino-jdbc-402.jar:/opt/dolphinscheduler/libs/logging-interceptor-4.9.3.jar:/opt/dolphinscheduler/libs/simpleclient_tracer_otel_agent-0.15.0.jar:/opt/dolphinscheduler/libs/janino-3.0.16.jar:/opt/dolphinscheduler/libs/jackson-dataformat-yaml-2.13.3.jar:/opt/dolphinscheduler/libs/ion-java-1.0.2.jar:/opt/dolphinscheduler/libs/commons-dbcp-1.4.jar:/opt/dolphinscheduler/libs/jsp-api-2.1.jar:/opt/dolphinscheduler/libs/google-http-client-gson-1.42.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-hivecli-3.2.1.jar:/opt/dolphinscheduler/libs/spring-boot-actuator-2.7.3.jar:/opt/dolphinscheduler/libs/google-cloud-core-http-2.10.0.jar:/opt/dolphinscheduler/libs/DmJdbcDriver18-8.1.2.79.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-java-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-dameng-3.2.1.jar:/opt/dolphinscheduler/libs/sshd-sftp-2.8.0.jar:/opt/dolphinscheduler/libs/kerb-crypto-1.0.1.jar:/opt/dolphinscheduler/libs/conscrypt-openjdk-uber-2.5.2.jar:/opt/dolphinscheduler/libs/httpcore-4.4.15.jar:/opt/dolphinscheduler/libs/lz4-java-1.4.0.jar:/opt/dolphinscheduler/libs/guava-retrying-2.0.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-flink-stream-3.2.1.jar:/opt/dolphinscheduler/libs/spring-tx-5.3.22.jar:/opt/dolphinscheduler/libs/grpc-auth-1.41.0.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/dolphinscheduler/libs/databend-jdbc-0.0.7.jar:/opt/dolphinscheduler/libs/bcprov-jdk15on-1.69.jar:/opt/dolphinscheduler/libs/commons-lang-2.6.jar:/opt/dolphinscheduler/libs/kubernetes-model-policy-5.10.2.jar:/opt/dolphinscheduler/libs/commons-io-2.11.0.jar:/opt/dolphinscheduler/libs/grpc-grpclb-1.41.0.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/dolphinscheduler/libs/opentracing-api-0.33.0.jar:/opt/dolphinscheduler/libs/sshd-scp-2.8.0.jar:/opt/dolphinscheduler/libs/reload4j-1.2.18.3.jar:/opt/dolphinscheduler/libs/netty-tcnative-2.0.48.Final.jar:/opt/dolphinscheduler/libs/jdom2-2.0.6.1.jar:/opt/dolphinscheduler/libs/spring-boot-starter-json-2.7.3.jar:/opt/dolphinscheduler/libs/netty-transport-native-epoll-4.1.53.Final.jar:/opt/dolphinscheduler/libs/google-cloud-core-2.10.0.jar:/opt/dolphinscheduler/libs/javax.jdo-3.2.0-m3.jar:/opt/dolphinscheduler/libs/gax-httpjson-0.108.0.jar:/opt/dolphinscheduler/libs/mybatis-plus-core-3.5.2.jar:/opt/dolphinscheduler/libs/auto-service-annotations-1.0.1.jar:/opt/dolphinscheduler/libs/nimbus-jose-jwt-9.22.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-zeppelin-3.2.1.jar:/opt/dolphinscheduler/libs/commons-compress-1.21.jar:/opt/dolphinscheduler/libs/hadoop-hdfs-client-3.2.4.jar:/opt/dolphinscheduler/libs/msal4j-persistence-extension-1.1.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-pytorch-3.2.1.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/dolphinscheduler/libs/jetty-servlets-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/woodstox-core-6.4.0.jar:/opt/dolphinscheduler/libs/spring-cloud-kubernetes-commons-2.1.3.jar:/opt/dolphinscheduler/libs/grpc-protobuf-lite-1.41.0.jar:/opt/dolphinscheduler/libs/jetty-webapp-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/eventstream-1.0.1.jar:/opt/dolphinscheduler/libs/commons-compiler-3.1.7.jar:/opt/dolphinscheduler/libs/netty-transport-4.1.53.Final.jar:/opt/dolphinscheduler/libs/spring-cloud-context-3.1.3.jar:/opt/dolphinscheduler/libs/aws-java-sdk-dms-1.12.300.jar:/opt/dolphinscheduler/libs/hive-storage-api-2.4.0.jar:/opt/dolphinscheduler/libs/client-java-spring-integration-13.0.2.jar:/opt/dolphinscheduler/libs/spring-boot-starter-actuator-2.7.3.jar:/opt/dolphinscheduler/libs/third-party-jackson-core-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-hdfs-3.2.1.jar:/opt/dolphinscheduler/libs/netty-codec-4.1.53.Final.jar:/opt/dolphinscheduler/libs/google-http-client-appengine-1.42.3.jar:/opt/dolphinscheduler/libs/commons-configuration2-2.1.1.jar:/opt/dolphinscheduler/libs/datanucleus-rdbms-4.1.19.jar:/opt/dolphinscheduler/libs/swagger-annotations-1.6.2.jar:/opt/dolphinscheduler/libs/simpleclient_tracer_otel-0.15.0.jar:/opt/dolphinscheduler/libs/kotlin-stdlib-common-1.6.21.jar:/opt/dolphinscheduler/libs/aws-core-2.17.282.jar:/opt/dolphinscheduler/libs/kerb-core-1.0.1.jar:/opt/dolphinscheduler/libs/httpasyncclient-4.1.5.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-worker-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-spi-3.2.1.jar:/opt/dolphinscheduler/libs/okhttp-2.7.5.jar:/opt/dolphinscheduler/libs/google-api-client-2.2.0.jar:/opt/dolphinscheduler/libs/kotlin-stdlib-1.6.21.jar:/opt/dolphinscheduler/libs/jakarta.websocket-api-1.1.2.jar:/opt/dolphinscheduler/libs/libthrift-0.9.3.jar:/opt/dolphinscheduler/libs/spring-boot-starter-aop-2.7.3.jar:/opt/dolphinscheduler/libs/netty-common-4.1.53.Final.jar:/opt/dolphinscheduler/libs/log4j-1.2-api-2.17.2.jar:/opt/dolphinscheduler/libs/jackson-databind-2.13.4.jar:/opt/dolphinscheduler/libs/jackson-dataformat-xml-2.13.3.jar:/opt/dolphinscheduler/libs/kubernetes-model-events-5.10.2.jar:/opt/dolphinscheduler/libs/gson-2.9.1.jar:/opt/dolphinscheduler/libs/proto-google-iam-v1-1.9.0.jar:/opt/dolphinscheduler/libs/netty-codec-socks-4.1.53.Final.jar:/opt/dolphinscheduler/libs/zjsonpatch-0.3.0.jar:/opt/dolphinscheduler/libs/hadoop-mapreduce-client-common-3.2.4.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-api-3.2.1.jar:/opt/dolphinscheduler/libs/azure-storage-blob-12.21.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-zeppelin-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-api-3.2.1.jar:/opt/dolphinscheduler/libs/aws-java-sdk-s3-1.12.300.jar:/opt/dolphinscheduler/libs/stax2-api-4.2.1.jar:/opt/dolphinscheduler/libs/jakarta.annotation-api-1.3.5.jar:/opt/dolphinscheduler/libs/kubernetes-model-discovery-5.10.2.jar:/opt/dolphinscheduler/libs/jna-5.10.0.jar:/opt/dolphinscheduler/libs/spring-jcl-5.3.22.jar
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:08.647 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:08.647 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.io.tmpdir=/tmp
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:08.647 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.compiler=<NA>
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:08.647 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.name=Linux
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:08.647 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.arch=amd64
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:08.648 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.version=6.5.0-28-generic
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:08.648 +0800 o.a.z.ZooKeeper:[98] - Client environment:user.name=root
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:08.648 +0800 o.a.z.ZooKeeper:[98] - Client environment:user.home=/root
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:08.648 +0800 o.a.z.ZooKeeper:[98] - Client environment:user.dir=/opt/dolphinscheduler
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:08.649 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.memory.free=3210MB
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:08.649 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.memory.max=3840MB
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:08.649 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.memory.total=3840MB
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:08.662 +0800 o.a.z.ZooKeeper:[637] - Initiating client connection, connectString=dolphinscheduler-zookeeper:2181 sessionTimeout=30000 watcher=org.apache.curator.ConnectionState@1de08775
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:08.676 +0800 o.a.z.c.X509Util:[77] - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:08.699 +0800 o.a.z.ClientCnxnSocket:[239] - jute.maxbuffer value is 1048575 Bytes
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:08.743 +0800 o.a.z.ClientCnxn:[1732] - zookeeper.request.timeout value is 0. feature enabled=false
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:08.803 +0800 o.a.c.f.i.CuratorFrameworkImpl:[386] - Default schema
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:08.882 +0800 o.a.z.ClientCnxn:[1171] - Opening socket connection to server dolphinscheduler-zookeeper/172.18.0.4:2181.
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:08.909 +0800 o.a.z.ClientCnxn:[1173] - SASL config status: Will not attempt to authenticate using SASL (unknown error)
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:09.002 +0800 o.a.z.ClientCnxn:[1005] - Socket connection established, initiating session, client: /172.18.1.1:41724, server: dolphinscheduler-zookeeper/172.18.0.4:2181
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:09.036 +0800 o.a.z.ClientCnxn:[1444] - Session establishment complete on server dolphinscheduler-zookeeper/172.18.0.4:2181, session id = 0x10000027d500001, negotiated timeout = 30000
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:09.044 +0800 o.a.c.f.s.ConnectionStateManager:[252] - State change: CONNECTED
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:09.910 +0800 o.a.c.f.i.EnsembleTracker:[201] - New config event received: {}
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:09.917 +0800 o.a.c.f.i.EnsembleTracker:[201] - New config event received: {}
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:10.862 +0800 o.a.d.s.w.r.WorkerRpcServer:[41] - WorkerRpcServer starting...
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.124 +0800 o.a.d.e.b.NettyRemotingServer:[112] - WorkerRpcServer bind success at port: 1234
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.125 +0800 o.a.d.s.w.r.WorkerRpcServer:[43] - WorkerRpcServer started...
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.298 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: JAVA - JavaTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.327 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: JAVA - JavaTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.338 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: JUPYTER - JupyterTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.341 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: JUPYTER - JupyterTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.366 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SPARK - SparkTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.381 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SPARK - SparkTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.382 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: FLINK_STREAM - FlinkStreamTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.404 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: FLINK_STREAM - FlinkStreamTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.405 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: PYTHON - PythonTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.465 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: PYTHON - PythonTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.466 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DATASYNC - DatasyncTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.483 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DATASYNC - DatasyncTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.483 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DATA_FACTORY - DatafactoryTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.498 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DATA_FACTORY - DatafactoryTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.499 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: CHUNJUN - ChunJunTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.508 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: CHUNJUN - ChunJunTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.508 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: REMOTESHELL - RemoteShellTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.531 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: REMOTESHELL - RemoteShellTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.532 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: PIGEON - PigeonTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.543 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: PIGEON - PigeonTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.543 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SHELL - ShellTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.544 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SHELL - ShellTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.545 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: PROCEDURE - ProcedureTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.547 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: PROCEDURE - ProcedureTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.548 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: MR - MapReduceTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.581 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: MR - MapReduceTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.586 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SQOOP - SqoopTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.589 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SQOOP - SqoopTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.590 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: PYTORCH - PytorchTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.595 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: PYTORCH - PytorchTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.596 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: K8S - K8sTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.640 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: K8S - K8sTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.640 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SEATUNNEL - SeatunnelTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.656 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SEATUNNEL - SeatunnelTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.657 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SAGEMAKER - SagemakerTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.660 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SAGEMAKER - SagemakerTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.660 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: HTTP - HttpTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.682 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: HTTP - HttpTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.696 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: EMR - EmrTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.718 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: EMR - EmrTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.720 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DMS - DmsTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.727 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DMS - DmsTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.728 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DATA_QUALITY - DataQualityTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.733 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DATA_QUALITY - DataQualityTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.733 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: KUBEFLOW - KubeflowTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.741 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: KUBEFLOW - KubeflowTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.742 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SQL - SqlTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.760 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SQL - SqlTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.761 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DVC - DvcTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.782 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DVC - DvcTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.783 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DATAX - DataxTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.808 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DATAX - DataxTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:12.815 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: ZEPPELIN - ZeppelinTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:13.161 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: ZEPPELIN - ZeppelinTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:13.162 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DINKY - DinkyTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:13.195 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DINKY - DinkyTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:13.196 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: MLFLOW - MlflowTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:13.199 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: MLFLOW - MlflowTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:13.200 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: OPENMLDB - OpenmldbTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:13.221 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: OPENMLDB - OpenmldbTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:13.222 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: LINKIS - LinkisTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:13.223 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: LINKIS - LinkisTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:13.224 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: FLINK - FlinkTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:13.225 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: FLINK - FlinkTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:13.225 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: HIVECLI - HiveCliTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:13.227 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: HIVECLI - HiveCliTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:13.410 +0800 o.a.d.c.u.JSONUtils:[72] - init timezone: sun.util.calendar.ZoneInfo[id="Asia/Shanghai",offset=28800000,dstSavings=0,useDaylight=false,transitions=31,lastRule=null]
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:13.907 +0800 o.a.d.s.w.r.WorkerRegistryClient:[104] - Worker node: 172.18.1.1:1234 registry to ZK /nodes/worker/172.18.1.1:1234 successfully
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:14.981 +0800 o.a.d.c.m.BaseHeartBeatTask:[48] - Starting WorkerHeartBeatTask...
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:14.984 +0800 o.a.d.c.m.BaseHeartBeatTask:[50] - Started WorkerHeartBeatTask, heartBeatInterval: 10000...
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:14.998 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.070460704607046 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:14.985 +0800 o.a.d.s.w.r.WorkerRegistryClient:[114] - Worker node: 172.18.1.1:1234 registry finished
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:15.027 +0800 o.a.d.s.w.m.MessageRetryRunner:[69] - Message retry runner staring
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:15.680 +0800 o.a.d.s.w.m.MessageRetryRunner:[72] - Injected message sender: TaskInstanceExecutionFinishEventSender
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:15.688 +0800 o.a.d.s.w.m.MessageRetryRunner:[72] - Injected message sender: TaskInstanceExecutionInfoUpdateEventSender
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:15.689 +0800 o.a.d.s.w.m.MessageRetryRunner:[72] - Injected message sender: TaskInstanceExecutionRunningEventSender
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:15.691 +0800 o.a.d.s.w.m.MessageRetryRunner:[75] - Message retry runner started
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:16.087 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.4206349206349207 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:17.148 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.6486486486486487 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:18.165 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.4335260115606936 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:19.475 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.822429906542056 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:20.501 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.183168316831683 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:21.504 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0744186046511628 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:22.658 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.5714285714285714 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:23.925 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.7053571428571428 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:25.179 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.2878048780487805 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:28.604 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.4526515151515151 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:29.755 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.2818181818181817 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:31.029 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.6938309570403502 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:01:32.179 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.100038940809969 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:14:51.096 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.2891566265060241 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:14:52.135 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.2542372881355932 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:14:53.140 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.526358789129687 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:14:54.148 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.393939393939394 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:14:55.188 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.3776595744680853 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:14:55.744 +0800 o.s.b.a.e.w.EndpointLinksResolver:[58] - Exposing 3 endpoint(s) beneath base path '/actuator'
[WI-0][TI-0] - [INFO] 2024-05-04 03:14:56.114 +0800 o.e.j.s.h.C.application:[2368] - Initializing Spring DispatcherServlet 'dispatcherServlet'
[WI-0][TI-0] - [INFO] 2024-05-04 03:14:56.115 +0800 o.s.w.s.DispatcherServlet:[525] - Initializing Servlet 'dispatcherServlet'
[WI-0][TI-0] - [INFO] 2024-05-04 03:14:56.142 +0800 o.s.w.s.DispatcherServlet:[547] - Completed initialization in 19 ms
[WI-0][TI-0] - [INFO] 2024-05-04 03:14:56.178 +0800 o.e.j.s.AbstractConnector:[333] - Started ServerConnector@1f45db49{HTTP/1.1, (http/1.1)}{0.0.0.0:1235}
[WI-0][TI-0] - [INFO] 2024-05-04 03:14:56.199 +0800 o.s.b.w.e.j.JettyWebServer:[172] - Jetty started on port(s) 1235 (http/1.1) with context path '/'
[WI-0][TI-0] - [INFO] 2024-05-04 03:14:56.216 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.7912087912087913 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:14:56.275 +0800 o.a.d.s.w.WorkerServer:[61] - Started WorkerServer in 92.976 seconds (JVM running for 105.711)
[WI-0][TI-0] - [INFO] 2024-05-04 03:14:57.259 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8638971405247284 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:14:58.363 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7887815351583467 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:14:59.367 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8735402204981625 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:15:00.369 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8294573643410853 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:15:01.376 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9473684210526315 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:15:02.393 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8612716763005781 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:15:03.395 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8951612903225807 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:15:04.457 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9746835443037973 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:15:05.475 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9040590405904059 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:15:08.494 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8784722222222222 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:15:11.632 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8973214285714286 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:15:13.740 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0657276995305165 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:15:14.774 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9865470852017937 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:15:15.776 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7876106194690264 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:15:16.781 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7945205479452055 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:15:17.789 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8661087866108785 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [WARN] 2024-05-04 03:16:10.901 +0800 o.s.c.k.c.p.AbstractKubernetesProfileEnvironmentPostProcessor:[258] - Not running inside kubernetes. Skipping 'kubernetes' profile activation.
[WI-0][TI-0] - [WARN] 2024-05-04 03:16:13.339 +0800 o.s.c.k.c.p.AbstractKubernetesProfileEnvironmentPostProcessor:[258] - Not running inside kubernetes. Skipping 'kubernetes' profile activation.
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:13.341 +0800 o.a.d.s.w.WorkerServer:[634] - No active profile set, falling back to 1 default profile: "default"
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:20.404 +0800 o.s.c.c.s.GenericScope:[283] - BeanFactory id=7e7bf7c6-2cb3-34d2-a0d5-a56161c67d0e
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:23.601 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'org.springframework.cloud.commons.config.CommonsConfigAutoConfiguration' of type [org.springframework.cloud.commons.config.CommonsConfigAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:23.605 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'org.springframework.cloud.client.loadbalancer.LoadBalancerDefaultMappingsProviderAutoConfiguration' of type [org.springframework.cloud.client.loadbalancer.LoadBalancerDefaultMappingsProviderAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:23.608 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'loadBalancerClientsDefaultsMappingsProvider' of type [org.springframework.cloud.client.loadbalancer.LoadBalancerDefaultMappingsProviderAutoConfiguration$$Lambda$392/302905744] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:23.626 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'defaultsBindHandlerAdvisor' of type [org.springframework.cloud.commons.config.DefaultsBindHandlerAdvisor] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:23.651 +0800 o.a.d.c.u.NetUtils:[312] - Get all NetworkInterfaces: [name:eth0 (eth0), name:lo (lo)]
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:23.705 +0800 o.a.d.s.w.c.WorkerConfig:[98] - 
****************************Worker Configuration**************************************
  listen-port -> 1234
  exec-threads -> 100
  max-heartbeat-interval -> PT10S
  host-weight -> 100
  tenantConfig -> TenantConfig(autoCreateTenantEnabled=true, distributedTenantEnabled=false, defaultTenantEnabled=false)
  server-load-protection -> WorkerServerLoadProtection(enabled=true, maxCpuUsagePercentageThresholds=0.7, maxJVMMemoryUsagePercentageThresholds=0.7, maxSystemMemoryUsagePercentageThresholds=0.7, maxDiskUsagePercentageThresholds=0.7)
  registry-disconnect-strategy -> ConnectStrategyProperties(strategy=WAITING, maxWaitingTime=PT1M40S)
  task-execute-threads-full-policy: REJECT
  address -> 172.18.1.1:1234
  registry-path: /nodes/worker/172.18.1.1:1234
****************************Worker Configuration**************************************
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:23.706 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'workerConfig' of type [org.apache.dolphinscheduler.server.worker.config.WorkerConfig$$EnhancerBySpringCGLIB$$da954724] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:24.073 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'io.kubernetes.client.spring.extended.manifests.config.KubernetesManifestsAutoConfiguration' of type [io.kubernetes.client.spring.extended.manifests.config.KubernetesManifestsAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:24.094 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'kubernetes.manifests-io.kubernetes.client.spring.extended.manifests.config.KubernetesManifestsProperties' of type [io.kubernetes.client.spring.extended.manifests.config.KubernetesManifestsProperties] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:24.273 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'io.kubernetes.client.spring.extended.controller.config.KubernetesInformerAutoConfiguration' of type [io.kubernetes.client.spring.extended.controller.config.KubernetesInformerAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:24.329 +0800 o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:[376] - Bean 'defaultApiClient' of type [io.kubernetes.client.openapi.ApiClient] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:24.955 +0800 o.e.j.u.log:[170] - Logging initialized @22561ms to org.eclipse.jetty.util.log.Slf4jLog
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:26.236 +0800 o.s.b.w.e.j.JettyServletWebServerFactory:[166] - Server initialized with port: 1235
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:26.250 +0800 o.e.j.s.Server:[375] - jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 1.8.0_402-b06
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:26.714 +0800 o.e.j.s.h.C.application:[2368] - Initializing Spring embedded WebApplicationContext
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:26.731 +0800 o.s.b.w.s.c.ServletWebServerApplicationContext:[292] - Root WebApplicationContext: initialization completed in 13339 ms
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:30.087 +0800 o.e.j.s.session:[334] - DefaultSessionIdManager workerName=node0
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:30.183 +0800 o.e.j.s.session:[339] - No SessionScavenger set, using defaults
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:30.200 +0800 o.e.j.s.session:[132] - node0 Scavenging every 600000ms
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:30.221 +0800 o.e.j.s.h.ContextHandler:[921] - Started o.s.b.w.e.j.JettyEmbeddedWebAppContext@52433946{application,/,[file:///tmp/jetty-docbase.1235.1989215528566119064/],AVAILABLE}
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:30.224 +0800 o.e.j.s.Server:[415] - Started @27829ms
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:30.957 +0800 o.a.c.f.i.CuratorFrameworkImpl:[338] - Starting
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:30.975 +0800 o.a.z.ZooKeeper:[98] - Client environment:zookeeper.version=3.8.0-5a02a05eddb59aee6ac762f7ea82e92a68eb9c0f, built on 2022-02-25 08:49 UTC
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:30.975 +0800 o.a.z.ZooKeeper:[98] - Client environment:host.name=addf0afec0d4
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:30.977 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.version=1.8.0_402
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:30.977 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.vendor=Temurin
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:30.978 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.home=/opt/java/openjdk
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:30.978 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.class.path=/opt/dolphinscheduler/conf:/opt/dolphinscheduler/libs/kerb-client-1.0.1.jar:/opt/dolphinscheduler/libs/spring-cloud-starter-kubernetes-client-config-2.1.3.jar:/opt/dolphinscheduler/libs/oauth2-oidc-sdk-9.35.jar:/opt/dolphinscheduler/libs/jsqlparser-4.4.jar:/opt/dolphinscheduler/libs/reactive-streams-1.0.4.jar:/opt/dolphinscheduler/libs/kubernetes-model-extensions-5.10.2.jar:/opt/dolphinscheduler/libs/zookeeper-3.8.0.jar:/opt/dolphinscheduler/libs/kubernetes-model-apps-5.10.2.jar:/opt/dolphinscheduler/libs/grpc-api-1.41.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-pigeon-3.2.1.jar:/opt/dolphinscheduler/libs/client-java-api-13.0.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-obs-3.2.1.jar:/opt/dolphinscheduler/libs/spring-web-5.3.22.jar:/opt/dolphinscheduler/libs/aws-java-sdk-emr-1.12.300.jar:/opt/dolphinscheduler/libs/spring-context-5.3.22.jar:/opt/dolphinscheduler/libs/gapic-google-cloud-storage-v2-2.18.0-alpha.jar:/opt/dolphinscheduler/libs/spring-cloud-kubernetes-client-config-2.1.3.jar:/opt/dolphinscheduler/libs/jetty-io-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/annotations-13.0.jar:/opt/dolphinscheduler/libs/jpam-1.1.jar:/opt/dolphinscheduler/libs/joda-time-2.10.13.jar:/opt/dolphinscheduler/libs/spring-cloud-kubernetes-client-autoconfig-2.1.3.jar:/opt/dolphinscheduler/libs/kerb-identity-1.0.1.jar:/opt/dolphinscheduler/libs/vertica-jdbc-12.0.4-0.jar:/opt/dolphinscheduler/libs/grpc-googleapis-1.52.1.jar:/opt/dolphinscheduler/libs/aliyun-java-sdk-kms-2.11.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-dvc-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-hana-3.2.1.jar:/opt/dolphinscheduler/libs/kubernetes-httpclient-okhttp-6.0.0.jar:/opt/dolphinscheduler/libs/jline-2.12.jar:/opt/dolphinscheduler/libs/sshd-core-2.8.0.jar:/opt/dolphinscheduler/libs/jna-platform-5.10.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-sqlserver-3.2.1.jar:/opt/dolphinscheduler/libs/commons-beanutils-1.9.4.jar:/opt/dolphinscheduler/libs/grpc-services-1.41.0.jar:/opt/dolphinscheduler/libs/jmespath-java-1.12.300.jar:/opt/dolphinscheduler/libs/curator-client-5.3.0.jar:/opt/dolphinscheduler/libs/proto-google-common-protos-2.0.1.jar:/opt/dolphinscheduler/libs/mybatis-3.5.10.jar:/opt/dolphinscheduler/libs/jackson-annotations-2.13.4.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-all-3.2.1.jar:/opt/dolphinscheduler/libs/jakarta.xml.bind-api-2.3.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-jupyter-3.2.1.jar:/opt/dolphinscheduler/libs/mybatis-plus-extension-3.5.2.jar:/opt/dolphinscheduler/libs/j2objc-annotations-1.3.jar:/opt/dolphinscheduler/libs/Java-WebSocket-1.5.1.jar:/opt/dolphinscheduler/libs/azure-storage-common-12.20.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-sagemaker-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-databend-3.2.1.jar:/opt/dolphinscheduler/libs/google-auth-library-oauth2-http-1.15.0.jar:/opt/dolphinscheduler/libs/jetty-security-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-python-3.2.1.jar:/opt/dolphinscheduler/libs/snappy-java-1.1.10.1.jar:/opt/dolphinscheduler/libs/kubernetes-model-batch-5.10.2.jar:/opt/dolphinscheduler/libs/netty-transport-native-epoll-4.1.53.Final-linux-x86_64.jar:/opt/dolphinscheduler/libs/jackson-core-asl-1.9.13.jar:/opt/dolphinscheduler/libs/gson-fire-1.8.5.jar:/opt/dolphinscheduler/libs/clickhouse-jdbc-0.4.6.jar:/opt/dolphinscheduler/libs/grpc-netty-shaded-1.41.0.jar:/opt/dolphinscheduler/libs/caffeine-2.9.3.jar:/opt/dolphinscheduler/libs/aliyun-java-sdk-core-4.5.10.jar:/opt/dolphinscheduler/libs/jackson-module-jaxb-annotations-2.13.3.jar:/opt/dolphinscheduler/libs/jetty-http-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/jersey-servlet-1.19.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-dinky-3.2.1.jar:/opt/dolphinscheduler/libs/simpleclient_tracer_common-0.15.0.jar:/opt/dolphinscheduler/libs/netty-handler-4.1.53.Final.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-datafactory-3.2.1.jar:/opt/dolphinscheduler/libs/json-path-2.7.0.jar:/opt/dolphinscheduler/libs/mybatis-plus-annotation-3.5.2.jar:/opt/dolphinscheduler/libs/azure-resourcemanager-datafactory-1.0.0-beta.19.jar:/opt/dolphinscheduler/libs/commons-codec-1.11.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-master-3.2.1.jar:/opt/dolphinscheduler/libs/spring-aop-5.3.22.jar:/opt/dolphinscheduler/libs/kubernetes-model-core-5.10.2.jar:/opt/dolphinscheduler/libs/kubernetes-model-networking-5.10.2.jar:/opt/dolphinscheduler/libs/kotlin-stdlib-jdk7-1.6.21.jar:/opt/dolphinscheduler/libs/kerby-xdr-1.0.1.jar:/opt/dolphinscheduler/libs/aliyun-java-sdk-ram-3.1.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-k8s-3.2.1.jar:/opt/dolphinscheduler/libs/guava-31.1-jre.jar:/opt/dolphinscheduler/libs/netty-codec-dns-4.1.53.Final.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-kubeflow-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-azure-sql-3.2.1.jar:/opt/dolphinscheduler/libs/HikariCP-4.0.3.jar:/opt/dolphinscheduler/libs/hive-metastore-2.3.9.jar:/opt/dolphinscheduler/libs/msal4j-1.13.3.jar:/opt/dolphinscheduler/libs/jackson-core-2.13.4.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-hive-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-mr-3.2.1.jar:/opt/dolphinscheduler/libs/kubernetes-model-node-5.10.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-common-3.2.1.jar:/opt/dolphinscheduler/libs/jose4j-0.7.8.jar:/opt/dolphinscheduler/libs/jul-to-slf4j-1.7.36.jar:/opt/dolphinscheduler/libs/azure-identity-1.7.1.jar:/opt/dolphinscheduler/libs/spring-boot-autoconfigure-2.7.3.jar:/opt/dolphinscheduler/libs/commons-math3-3.1.1.jar:/opt/dolphinscheduler/libs/jackson-jaxrs-json-provider-2.13.3.jar:/opt/dolphinscheduler/libs/perfmark-api-0.23.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-doris-3.2.1.jar:/opt/dolphinscheduler/libs/httpclient-4.5.13.jar:/opt/dolphinscheduler/libs/hive-service-2.3.9.jar:/opt/dolphinscheduler/libs/kerby-util-1.0.1.jar:/opt/dolphinscheduler/libs/commons-collections-3.2.2.jar:/opt/dolphinscheduler/libs/hadoop-yarn-client-3.2.4.jar:/opt/dolphinscheduler/libs/commons-text-1.8.jar:/opt/dolphinscheduler/libs/dolphinscheduler-worker-3.2.1.jar:/opt/dolphinscheduler/libs/hadoop-yarn-common-3.2.4.jar:/opt/dolphinscheduler/libs/zeppelin-client-0.10.1.jar:/opt/dolphinscheduler/libs/azure-core-management-1.10.1.jar:/opt/dolphinscheduler/libs/hadoop-mapreduce-client-jobclient-3.2.4.jar:/opt/dolphinscheduler/libs/jta-1.1.jar:/opt/dolphinscheduler/libs/annotations-4.1.1.4.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-alert-3.2.1.jar:/opt/dolphinscheduler/libs/curator-framework-5.3.0.jar:/opt/dolphinscheduler/libs/hive-jdbc-2.3.9.jar:/opt/dolphinscheduler/libs/kyuubi-hive-jdbc-shaded-1.7.0.jar:/opt/dolphinscheduler/libs/client-java-proto-13.0.2.jar:/opt/dolphinscheduler/libs/checker-qual-3.19.0.jar:/opt/dolphinscheduler/libs/jetty-servlet-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/google-cloud-core-grpc-2.10.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-shell-3.2.1.jar:/opt/dolphinscheduler/libs/spring-security-rsa-1.0.10.RELEASE.jar:/opt/dolphinscheduler/libs/avro-1.7.7.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-postgresql-3.2.1.jar:/opt/dolphinscheduler/libs/netty-nio-client-2.17.282.jar:/opt/dolphinscheduler/libs/spring-webmvc-5.3.22.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-mysql-3.2.1.jar:/opt/dolphinscheduler/libs/opentracing-util-0.33.0.jar:/opt/dolphinscheduler/libs/auto-value-1.10.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-all-3.2.1.jar:/opt/dolphinscheduler/libs/jetcd-core-0.5.11.jar:/opt/dolphinscheduler/libs/grpc-context-1.41.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-datasync-3.2.1.jar:/opt/dolphinscheduler/libs/jackson-dataformat-cbor-2.13.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-gcs-3.2.1.jar:/opt/dolphinscheduler/libs/client-java-extended-13.0.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-redshift-3.2.1.jar:/opt/dolphinscheduler/libs/opencsv-2.3.jar:/opt/dolphinscheduler/libs/jcip-annotations-1.0-1.jar:/opt/dolphinscheduler/libs/accessors-smart-2.4.8.jar:/opt/dolphinscheduler/libs/hadoop-client-3.2.4.jar:/opt/dolphinscheduler/libs/commons-collections4-4.3.jar:/opt/dolphinscheduler/libs/metrics-core-4.2.11.jar:/opt/dolphinscheduler/libs/netty-resolver-4.1.53.Final.jar:/opt/dolphinscheduler/libs/parquet-hadoop-bundle-1.8.1.jar:/opt/dolphinscheduler/libs/bucket4j-core-6.2.0.jar:/opt/dolphinscheduler/libs/spring-cloud-starter-3.1.3.jar:/opt/dolphinscheduler/libs/mssql-jdbc-11.2.1.jre8.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/dolphinscheduler/libs/kubernetes-model-coordination-5.10.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-linkis-3.2.1.jar:/opt/dolphinscheduler/libs/commons-net-3.6.jar:/opt/dolphinscheduler/libs/netty-transport-native-kqueue-4.1.53.Final-osx-x86_64.jar:/opt/dolphinscheduler/libs/dolphinscheduler-meter-3.2.1.jar:/opt/dolphinscheduler/libs/kubernetes-client-api-6.0.0.jar:/opt/dolphinscheduler/libs/kubernetes-model-certificates-5.10.2.jar:/opt/dolphinscheduler/libs/opencensus-api-0.31.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-mlflow-3.2.1.jar:/opt/dolphinscheduler/libs/kotlin-stdlib-jdk8-1.6.21.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-jdbc-3.2.1.jar:/opt/dolphinscheduler/libs/audience-annotations-0.12.0.jar:/opt/dolphinscheduler/libs/simpleclient_httpserver-0.15.0.jar:/opt/dolphinscheduler/libs/jetty-xml-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/kerby-asn1-1.0.1.jar:/opt/dolphinscheduler/libs/lang-tag-1.6.jar:/opt/dolphinscheduler/libs/api-common-2.6.0.jar:/opt/dolphinscheduler/libs/HdrHistogram-2.1.12.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-abs-3.2.1.jar:/opt/dolphinscheduler/libs/failsafe-2.4.4.jar:/opt/dolphinscheduler/libs/hbase-noop-htrace-4.1.1.jar:/opt/dolphinscheduler/libs/jamon-runtime-2.3.1.jar:/opt/dolphinscheduler/libs/jetty-util-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/threetenbp-1.6.5.jar:/opt/dolphinscheduler/libs/jakarta.activation-api-1.2.2.jar:/opt/dolphinscheduler/libs/kubernetes-model-common-5.10.2.jar:/opt/dolphinscheduler/libs/okhttp-4.9.3.jar:/opt/dolphinscheduler/libs/profiles-2.17.282.jar:/opt/dolphinscheduler/libs/hadoop-auth-3.2.4.jar:/opt/dolphinscheduler/libs/grpc-netty-1.41.0.jar:/opt/dolphinscheduler/libs/httpcore-nio-4.4.15.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-seatunnel-3.2.1.jar:/opt/dolphinscheduler/libs/aws-java-sdk-sagemaker-1.12.300.jar:/opt/dolphinscheduler/libs/oshi-core-6.1.1.jar:/opt/dolphinscheduler/libs/bonecp-0.8.0.RELEASE.jar:/opt/dolphinscheduler/libs/jackson-module-parameter-names-2.13.3.jar:/opt/dolphinscheduler/libs/bcutil-jdk15on-1.69.jar:/opt/dolphinscheduler/libs/aws-json-protocol-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-base-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-all-3.2.1.jar:/opt/dolphinscheduler/libs/derby-10.14.2.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-snowflake-3.2.1.jar:/opt/dolphinscheduler/libs/netty-codec-http2-4.1.53.Final.jar:/opt/dolphinscheduler/libs/jetty-continuation-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/jackson-mapper-asl-1.9.13.jar:/opt/dolphinscheduler/libs/datanucleus-core-4.1.17.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/dolphinscheduler/libs/failureaccess-1.0.1.jar:/opt/dolphinscheduler/libs/error_prone_annotations-2.5.1.jar:/opt/dolphinscheduler/libs/kerb-admin-1.0.1.jar:/opt/dolphinscheduler/libs/token-provider-1.0.1.jar:/opt/dolphinscheduler/libs/reactor-netty-core-1.0.22.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-all-3.2.1.jar:/opt/dolphinscheduler/libs/jakarta.servlet-api-4.0.4.jar:/opt/dolphinscheduler/libs/http-client-spi-2.17.282.jar:/opt/dolphinscheduler/libs/regions-2.17.282.jar:/opt/dolphinscheduler/libs/logback-core-1.2.11.jar:/opt/dolphinscheduler/libs/json-1.8.jar:/opt/dolphinscheduler/libs/gax-grpc-2.23.0.jar:/opt/dolphinscheduler/libs/google-http-client-1.42.3.jar:/opt/dolphinscheduler/libs/hive-serde-2.3.9.jar:/opt/dolphinscheduler/libs/jettison-1.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-http-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-zookeeper-3.2.1.jar:/opt/dolphinscheduler/libs/spring-security-crypto-5.7.3.jar:/opt/dolphinscheduler/libs/auth-2.17.282.jar:/opt/dolphinscheduler/libs/proto-google-cloud-storage-v2-2.18.0-alpha.jar:/opt/dolphinscheduler/libs/jetty-server-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/javax.annotation-api-1.3.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-starrocks-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-dataquality-3.2.1.jar:/opt/dolphinscheduler/libs/jcl-over-slf4j-1.7.36.jar:/opt/dolphinscheduler/libs/commons-lang3-3.12.0.jar:/opt/dolphinscheduler/libs/tomcat-embed-el-9.0.65.jar:/opt/dolphinscheduler/libs/opentracing-noop-0.33.0.jar:/opt/dolphinscheduler/libs/client-java-13.0.2.jar:/opt/dolphinscheduler/libs/spring-beans-5.3.22.jar:/opt/dolphinscheduler/libs/snakeyaml-1.33.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-ssh-3.2.1.jar:/opt/dolphinscheduler/libs/commons-pool-1.6.jar:/opt/dolphinscheduler/libs/javax.servlet-api-3.1.0.jar:/opt/dolphinscheduler/libs/hadoop-common-3.2.4.jar:/opt/dolphinscheduler/libs/kubernetes-model-apiextensions-5.10.2.jar:/opt/dolphinscheduler/libs/spring-boot-2.7.3.jar:/opt/dolphinscheduler/libs/bcprov-ext-jdk15on-1.69.jar:/opt/dolphinscheduler/libs/spring-boot-starter-2.7.3.jar:/opt/dolphinscheduler/libs/paranamer-2.3.jar:/opt/dolphinscheduler/libs/httpmime-4.5.13.jar:/opt/dolphinscheduler/libs/reactor-core-3.4.22.jar:/opt/dolphinscheduler/libs/azure-core-1.36.0.jar:/opt/dolphinscheduler/libs/bcpkix-jdk15on-1.69.jar:/opt/dolphinscheduler/libs/kubernetes-model-scheduling-5.10.2.jar:/opt/dolphinscheduler/libs/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/dolphinscheduler/libs/websocket-client-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/unirest-java-3.7.04-standalone.jar:/opt/dolphinscheduler/libs/hadoop-mapreduce-client-core-3.2.4.jar:/opt/dolphinscheduler/libs/google-auth-library-credentials-1.15.0.jar:/opt/dolphinscheduler/libs/azure-storage-internal-avro-12.6.0.jar:/opt/dolphinscheduler/libs/jackson-datatype-jdk8-2.13.3.jar:/opt/dolphinscheduler/libs/spring-expression-5.3.22.jar:/opt/dolphinscheduler/libs/aspectjweaver-1.9.7.jar:/opt/dolphinscheduler/libs/websocket-common-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/client-java-api-fluent-13.0.2.jar:/opt/dolphinscheduler/libs/mybatis-plus-3.5.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-athena-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-oss-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-openmldb-3.2.1.jar:/opt/dolphinscheduler/libs/curator-recipes-5.3.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-api-3.2.1.jar:/opt/dolphinscheduler/libs/netty-all-4.1.53.Final.jar:/opt/dolphinscheduler/libs/netty-resolver-dns-4.1.53.Final.jar:/opt/dolphinscheduler/libs/kubernetes-model-autoscaling-5.10.2.jar:/opt/dolphinscheduler/libs/kerb-util-1.0.1.jar:/opt/dolphinscheduler/libs/grpc-alts-1.41.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-presto-3.2.1.jar:/opt/dolphinscheduler/libs/kerb-simplekdc-1.0.1.jar:/opt/dolphinscheduler/libs/protobuf-java-util-3.17.2.jar:/opt/dolphinscheduler/libs/azure-core-http-netty-1.13.0.jar:/opt/dolphinscheduler/libs/transaction-api-1.1.jar:/opt/dolphinscheduler/libs/datanucleus-api-jdo-4.2.4.jar:/opt/dolphinscheduler/libs/zeppelin-common-0.10.1.jar:/opt/dolphinscheduler/libs/zt-zip-1.15.jar:/opt/dolphinscheduler/libs/animal-sniffer-annotations-1.19.jar:/opt/dolphinscheduler/libs/google-http-client-jackson2-1.42.3.jar:/opt/dolphinscheduler/libs/netty-buffer-4.1.53.Final.jar:/opt/dolphinscheduler/libs/jsr305-3.0.0.jar:/opt/dolphinscheduler/libs/netty-transport-classes-epoll-4.1.79.Final.jar:/opt/dolphinscheduler/libs/spring-boot-actuator-autoconfigure-2.7.3.jar:/opt/dolphinscheduler/libs/simpleclient-0.15.0.jar:/opt/dolphinscheduler/libs/aliyun-sdk-oss-3.15.1.jar:/opt/dolphinscheduler/libs/json-utils-2.17.282.jar:/opt/dolphinscheduler/libs/tephra-api-0.6.0.jar:/opt/dolphinscheduler/libs/spring-jdbc-5.3.22.jar:/opt/dolphinscheduler/libs/grpc-xds-1.41.0.jar:/opt/dolphinscheduler/libs/logback-classic-1.2.11.jar:/opt/dolphinscheduler/libs/google-cloud-storage-2.18.0.jar:/opt/dolphinscheduler/libs/micrometer-registry-prometheus-1.9.3.jar:/opt/dolphinscheduler/libs/grpc-core-1.41.0.jar:/opt/dolphinscheduler/libs/aws-java-sdk-kms-1.12.300.jar:/opt/dolphinscheduler/libs/kubernetes-model-rbac-5.10.2.jar:/opt/dolphinscheduler/libs/ini4j-0.5.4.jar:/opt/dolphinscheduler/libs/spring-boot-starter-web-2.7.3.jar:/opt/dolphinscheduler/libs/spring-cloud-commons-3.1.3.jar:/opt/dolphinscheduler/libs/netty-codec-http-4.1.53.Final.jar:/opt/dolphinscheduler/libs/jetty-client-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/jetcd-common-0.5.11.jar:/opt/dolphinscheduler/libs/metrics-spi-2.17.282.jar:/opt/dolphinscheduler/libs/utils-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-emr-3.2.1.jar:/opt/dolphinscheduler/libs/protocol-core-2.17.282.jar:/opt/dolphinscheduler/libs/micrometer-core-1.9.3.jar:/opt/dolphinscheduler/libs/netty-transport-native-unix-common-4.1.53.Final.jar:/opt/dolphinscheduler/libs/zjsonpatch-0.4.11.jar:/opt/dolphinscheduler/libs/annotations-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-sql-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-common-3.2.1.jar:/opt/dolphinscheduler/libs/apache-client-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-oceanbase-3.2.1.jar:/opt/dolphinscheduler/libs/jdo-api-3.0.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-datax-3.2.1.jar:/opt/dolphinscheduler/libs/postgresql-42.4.1.jar:/opt/dolphinscheduler/libs/jetty-util-ajax-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/gax-2.23.0.jar:/opt/dolphinscheduler/libs/grpc-stub-1.41.0.jar:/opt/dolphinscheduler/libs/libfb303-0.9.3.jar:/opt/dolphinscheduler/libs/spring-core-5.3.22.jar:/opt/dolphinscheduler/libs/jaxb-api-2.3.1.jar:/opt/dolphinscheduler/libs/hive-service-rpc-2.3.9.jar:/opt/dolphinscheduler/libs/kubernetes-model-flowcontrol-5.10.2.jar:/opt/dolphinscheduler/libs/commons-logging-1.1.1.jar:/opt/dolphinscheduler/libs/mybatis-spring-2.0.7.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-oracle-3.2.1.jar:/opt/dolphinscheduler/libs/opencensus-proto-0.2.0.jar:/opt/dolphinscheduler/libs/grpc-protobuf-1.41.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-clickhouse-3.2.1.jar:/opt/dolphinscheduler/libs/spring-cloud-starter-bootstrap-3.1.3.jar:/opt/dolphinscheduler/libs/kubernetes-model-admissionregistration-5.10.2.jar:/opt/dolphinscheduler/libs/grpc-google-cloud-storage-v2-2.18.0-alpha.jar:/opt/dolphinscheduler/libs/esdk-obs-java-bundle-3.23.3.jar:/opt/dolphinscheduler/libs/dnsjava-2.1.7.jar:/opt/dolphinscheduler/libs/asm-9.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-spark-3.2.1.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/dolphinscheduler/libs/re2j-1.6.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-vertica-3.2.1.jar:/opt/dolphinscheduler/libs/json-smart-2.4.8.jar:/opt/dolphinscheduler/libs/reactor-netty-http-1.0.22.jar:/opt/dolphinscheduler/libs/google-api-services-storage-v1-rev20220705-2.0.0.jar:/opt/dolphinscheduler/libs/kubernetes-client-6.0.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-kyuubi-3.2.1.jar:/opt/dolphinscheduler/libs/auto-value-annotations-1.10.1.jar:/opt/dolphinscheduler/libs/commons-cli-1.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-data-quality-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-chunjun-3.2.1.jar:/opt/dolphinscheduler/libs/kerb-server-1.0.1.jar:/opt/dolphinscheduler/libs/content-type-2.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-remoteshell-3.2.1.jar:/opt/dolphinscheduler/libs/opencensus-contrib-http-util-0.31.1.jar:/opt/dolphinscheduler/libs/druid-1.2.20.jar:/opt/dolphinscheduler/libs/javolution-5.5.1.jar:/opt/dolphinscheduler/libs/protobuf-java-3.17.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-db2-3.2.1.jar:/opt/dolphinscheduler/libs/aws-java-sdk-core-1.12.300.jar:/opt/dolphinscheduler/libs/spring-boot-starter-logging-2.7.3.jar:/opt/dolphinscheduler/libs/kerby-pkix-1.0.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-procedure-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-etcd-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-sqoop-3.2.1.jar:/opt/dolphinscheduler/libs/zookeeper-jute-3.8.0.jar:/opt/dolphinscheduler/libs/netty-resolver-dns-native-macos-4.1.53.Final-osx-x86_64.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-sagemaker-3.2.1.jar:/opt/dolphinscheduler/libs/spring-boot-configuration-processor-2.6.1.jar:/opt/dolphinscheduler/libs/hive-common-2.3.9.jar:/opt/dolphinscheduler/libs/kerb-common-1.0.1.jar:/opt/dolphinscheduler/libs/stax-api-1.0.1.jar:/opt/dolphinscheduler/libs/kubernetes-model-storageclass-5.10.2.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-s3-3.2.1.jar:/opt/dolphinscheduler/libs/spring-boot-starter-jetty-2.7.3.jar:/opt/dolphinscheduler/libs/okio-2.8.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-dms-3.2.1.jar:/opt/dolphinscheduler/libs/simpleclient_common-0.15.0.jar:/opt/dolphinscheduler/libs/sdk-core-2.17.282.jar:/opt/dolphinscheduler/libs/javax.activation-api-1.2.0.jar:/opt/dolphinscheduler/libs/netty-handler-proxy-4.1.53.Final.jar:/opt/dolphinscheduler/libs/kerby-config-1.0.1.jar:/opt/dolphinscheduler/libs/netty-tcnative-classes-2.0.53.Final.jar:/opt/dolphinscheduler/libs/jackson-jaxrs-base-2.13.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-trino-3.2.1.jar:/opt/dolphinscheduler/libs/presto-jdbc-0.238.1.jar:/opt/dolphinscheduler/libs/websocket-api-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-flink-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-api-3.2.1.jar:/opt/dolphinscheduler/libs/kubernetes-model-metrics-5.10.2.jar:/opt/dolphinscheduler/libs/datasync-2.17.282.jar:/opt/dolphinscheduler/libs/hadoop-yarn-api-3.2.4.jar:/opt/dolphinscheduler/libs/google-http-client-apache-v2-1.42.3.jar:/opt/dolphinscheduler/libs/hadoop-annotations-3.2.4.jar:/opt/dolphinscheduler/libs/google-oauth-client-1.34.1.jar:/opt/dolphinscheduler/libs/sshd-common-2.8.0.jar:/opt/dolphinscheduler/libs/LatencyUtils-2.0.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-spark-3.2.1.jar:/opt/dolphinscheduler/libs/slf4j-api-1.7.36.jar:/opt/dolphinscheduler/libs/snowflake-jdbc-3.13.29.jar:/opt/dolphinscheduler/libs/jackson-datatype-jsr310-2.13.3.jar:/opt/dolphinscheduler/libs/trino-jdbc-402.jar:/opt/dolphinscheduler/libs/logging-interceptor-4.9.3.jar:/opt/dolphinscheduler/libs/simpleclient_tracer_otel_agent-0.15.0.jar:/opt/dolphinscheduler/libs/janino-3.0.16.jar:/opt/dolphinscheduler/libs/jackson-dataformat-yaml-2.13.3.jar:/opt/dolphinscheduler/libs/ion-java-1.0.2.jar:/opt/dolphinscheduler/libs/commons-dbcp-1.4.jar:/opt/dolphinscheduler/libs/jsp-api-2.1.jar:/opt/dolphinscheduler/libs/google-http-client-gson-1.42.3.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-hivecli-3.2.1.jar:/opt/dolphinscheduler/libs/spring-boot-actuator-2.7.3.jar:/opt/dolphinscheduler/libs/google-cloud-core-http-2.10.0.jar:/opt/dolphinscheduler/libs/DmJdbcDriver18-8.1.2.79.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-java-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-dameng-3.2.1.jar:/opt/dolphinscheduler/libs/sshd-sftp-2.8.0.jar:/opt/dolphinscheduler/libs/kerb-crypto-1.0.1.jar:/opt/dolphinscheduler/libs/conscrypt-openjdk-uber-2.5.2.jar:/opt/dolphinscheduler/libs/httpcore-4.4.15.jar:/opt/dolphinscheduler/libs/lz4-java-1.4.0.jar:/opt/dolphinscheduler/libs/guava-retrying-2.0.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-flink-stream-3.2.1.jar:/opt/dolphinscheduler/libs/spring-tx-5.3.22.jar:/opt/dolphinscheduler/libs/grpc-auth-1.41.0.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/dolphinscheduler/libs/databend-jdbc-0.0.7.jar:/opt/dolphinscheduler/libs/bcprov-jdk15on-1.69.jar:/opt/dolphinscheduler/libs/commons-lang-2.6.jar:/opt/dolphinscheduler/libs/kubernetes-model-policy-5.10.2.jar:/opt/dolphinscheduler/libs/commons-io-2.11.0.jar:/opt/dolphinscheduler/libs/grpc-grpclb-1.41.0.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/dolphinscheduler/libs/opentracing-api-0.33.0.jar:/opt/dolphinscheduler/libs/sshd-scp-2.8.0.jar:/opt/dolphinscheduler/libs/reload4j-1.2.18.3.jar:/opt/dolphinscheduler/libs/netty-tcnative-2.0.48.Final.jar:/opt/dolphinscheduler/libs/jdom2-2.0.6.1.jar:/opt/dolphinscheduler/libs/spring-boot-starter-json-2.7.3.jar:/opt/dolphinscheduler/libs/netty-transport-native-epoll-4.1.53.Final.jar:/opt/dolphinscheduler/libs/google-cloud-core-2.10.0.jar:/opt/dolphinscheduler/libs/javax.jdo-3.2.0-m3.jar:/opt/dolphinscheduler/libs/gax-httpjson-0.108.0.jar:/opt/dolphinscheduler/libs/mybatis-plus-core-3.5.2.jar:/opt/dolphinscheduler/libs/auto-service-annotations-1.0.1.jar:/opt/dolphinscheduler/libs/nimbus-jose-jwt-9.22.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-zeppelin-3.2.1.jar:/opt/dolphinscheduler/libs/commons-compress-1.21.jar:/opt/dolphinscheduler/libs/hadoop-hdfs-client-3.2.4.jar:/opt/dolphinscheduler/libs/msal4j-persistence-extension-1.1.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-pytorch-3.2.1.jar:/opt/dolphinscheduler/libs/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/dolphinscheduler/libs/jetty-servlets-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/woodstox-core-6.4.0.jar:/opt/dolphinscheduler/libs/spring-cloud-kubernetes-commons-2.1.3.jar:/opt/dolphinscheduler/libs/grpc-protobuf-lite-1.41.0.jar:/opt/dolphinscheduler/libs/jetty-webapp-9.4.48.v20220622.jar:/opt/dolphinscheduler/libs/eventstream-1.0.1.jar:/opt/dolphinscheduler/libs/commons-compiler-3.1.7.jar:/opt/dolphinscheduler/libs/netty-transport-4.1.53.Final.jar:/opt/dolphinscheduler/libs/spring-cloud-context-3.1.3.jar:/opt/dolphinscheduler/libs/aws-java-sdk-dms-1.12.300.jar:/opt/dolphinscheduler/libs/hive-storage-api-2.4.0.jar:/opt/dolphinscheduler/libs/client-java-spring-integration-13.0.2.jar:/opt/dolphinscheduler/libs/spring-boot-starter-actuator-2.7.3.jar:/opt/dolphinscheduler/libs/third-party-jackson-core-2.17.282.jar:/opt/dolphinscheduler/libs/dolphinscheduler-storage-hdfs-3.2.1.jar:/opt/dolphinscheduler/libs/netty-codec-4.1.53.Final.jar:/opt/dolphinscheduler/libs/google-http-client-appengine-1.42.3.jar:/opt/dolphinscheduler/libs/commons-configuration2-2.1.1.jar:/opt/dolphinscheduler/libs/datanucleus-rdbms-4.1.19.jar:/opt/dolphinscheduler/libs/swagger-annotations-1.6.2.jar:/opt/dolphinscheduler/libs/simpleclient_tracer_otel-0.15.0.jar:/opt/dolphinscheduler/libs/kotlin-stdlib-common-1.6.21.jar:/opt/dolphinscheduler/libs/aws-core-2.17.282.jar:/opt/dolphinscheduler/libs/kerb-core-1.0.1.jar:/opt/dolphinscheduler/libs/httpasyncclient-4.1.5.jar:/opt/dolphinscheduler/libs/dolphinscheduler-extract-worker-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-spi-3.2.1.jar:/opt/dolphinscheduler/libs/okhttp-2.7.5.jar:/opt/dolphinscheduler/libs/google-api-client-2.2.0.jar:/opt/dolphinscheduler/libs/kotlin-stdlib-1.6.21.jar:/opt/dolphinscheduler/libs/jakarta.websocket-api-1.1.2.jar:/opt/dolphinscheduler/libs/libthrift-0.9.3.jar:/opt/dolphinscheduler/libs/spring-boot-starter-aop-2.7.3.jar:/opt/dolphinscheduler/libs/netty-common-4.1.53.Final.jar:/opt/dolphinscheduler/libs/log4j-1.2-api-2.17.2.jar:/opt/dolphinscheduler/libs/jackson-databind-2.13.4.jar:/opt/dolphinscheduler/libs/jackson-dataformat-xml-2.13.3.jar:/opt/dolphinscheduler/libs/kubernetes-model-events-5.10.2.jar:/opt/dolphinscheduler/libs/gson-2.9.1.jar:/opt/dolphinscheduler/libs/proto-google-iam-v1-1.9.0.jar:/opt/dolphinscheduler/libs/netty-codec-socks-4.1.53.Final.jar:/opt/dolphinscheduler/libs/zjsonpatch-0.3.0.jar:/opt/dolphinscheduler/libs/hadoop-mapreduce-client-common-3.2.4.jar:/opt/dolphinscheduler/libs/dolphinscheduler-registry-api-3.2.1.jar:/opt/dolphinscheduler/libs/azure-storage-blob-12.21.0.jar:/opt/dolphinscheduler/libs/dolphinscheduler-task-zeppelin-3.2.1.jar:/opt/dolphinscheduler/libs/dolphinscheduler-datasource-api-3.2.1.jar:/opt/dolphinscheduler/libs/aws-java-sdk-s3-1.12.300.jar:/opt/dolphinscheduler/libs/stax2-api-4.2.1.jar:/opt/dolphinscheduler/libs/jakarta.annotation-api-1.3.5.jar:/opt/dolphinscheduler/libs/kubernetes-model-discovery-5.10.2.jar:/opt/dolphinscheduler/libs/jna-5.10.0.jar:/opt/dolphinscheduler/libs/spring-jcl-5.3.22.jar
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:30.979 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:30.979 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.io.tmpdir=/tmp
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:30.979 +0800 o.a.z.ZooKeeper:[98] - Client environment:java.compiler=<NA>
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:30.980 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.name=Linux
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:30.980 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.arch=amd64
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:30.980 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.version=6.5.0-28-generic
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:30.980 +0800 o.a.z.ZooKeeper:[98] - Client environment:user.name=root
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:30.980 +0800 o.a.z.ZooKeeper:[98] - Client environment:user.home=/root
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:30.981 +0800 o.a.z.ZooKeeper:[98] - Client environment:user.dir=/opt/dolphinscheduler
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:31.054 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.memory.free=3228MB
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:31.054 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.memory.max=3840MB
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:31.055 +0800 o.a.z.ZooKeeper:[98] - Client environment:os.memory.total=3840MB
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:31.063 +0800 o.a.z.ZooKeeper:[637] - Initiating client connection, connectString=dolphinscheduler-zookeeper:2181 sessionTimeout=30000 watcher=org.apache.curator.ConnectionState@6996bbc4
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:31.068 +0800 o.a.z.c.X509Util:[77] - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:31.082 +0800 o.a.z.ClientCnxnSocket:[239] - jute.maxbuffer value is 1048575 Bytes
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:31.142 +0800 o.a.z.ClientCnxn:[1732] - zookeeper.request.timeout value is 0. feature enabled=false
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:31.269 +0800 o.a.c.f.i.CuratorFrameworkImpl:[386] - Default schema
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:31.327 +0800 o.a.z.ClientCnxn:[1171] - Opening socket connection to server dolphinscheduler-zookeeper/172.18.0.4:2181.
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:31.514 +0800 o.a.z.ClientCnxn:[1173] - SASL config status: Will not attempt to authenticate using SASL (unknown error)
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:31.521 +0800 o.a.z.ClientCnxn:[1005] - Socket connection established, initiating session, client: /172.18.1.1:51746, server: dolphinscheduler-zookeeper/172.18.0.4:2181
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:31.622 +0800 o.a.z.ClientCnxn:[1444] - Session establishment complete on server dolphinscheduler-zookeeper/172.18.0.4:2181, session id = 0x1000005169a0000, negotiated timeout = 30000
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:31.695 +0800 o.a.c.f.s.ConnectionStateManager:[252] - State change: CONNECTED
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:31.867 +0800 o.a.c.f.i.EnsembleTracker:[201] - New config event received: {}
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:31.876 +0800 o.a.c.f.i.EnsembleTracker:[201] - New config event received: {}
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:32.599 +0800 o.a.d.s.w.r.WorkerRpcServer:[41] - WorkerRpcServer starting...
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.175 +0800 o.a.d.e.b.NettyRemotingServer:[112] - WorkerRpcServer bind success at port: 1234
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.176 +0800 o.a.d.s.w.r.WorkerRpcServer:[43] - WorkerRpcServer started...
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.353 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: JAVA - JavaTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.357 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: JAVA - JavaTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.366 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: JUPYTER - JupyterTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.369 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: JUPYTER - JupyterTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.370 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SPARK - SparkTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.373 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SPARK - SparkTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.395 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: FLINK_STREAM - FlinkStreamTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.402 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: FLINK_STREAM - FlinkStreamTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.403 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: PYTHON - PythonTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.403 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: PYTHON - PythonTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.404 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DATASYNC - DatasyncTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.405 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DATASYNC - DatasyncTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.405 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DATA_FACTORY - DatafactoryTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.407 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DATA_FACTORY - DatafactoryTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.407 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: CHUNJUN - ChunJunTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.409 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: CHUNJUN - ChunJunTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.416 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: REMOTESHELL - RemoteShellTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.420 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: REMOTESHELL - RemoteShellTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.420 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: PIGEON - PigeonTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.423 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: PIGEON - PigeonTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.425 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SHELL - ShellTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.427 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SHELL - ShellTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.430 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: PROCEDURE - ProcedureTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.433 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: PROCEDURE - ProcedureTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.440 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: MR - MapReduceTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.441 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: MR - MapReduceTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.442 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SQOOP - SqoopTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.443 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SQOOP - SqoopTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.446 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: PYTORCH - PytorchTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.449 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: PYTORCH - PytorchTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.529 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: K8S - K8sTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.536 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: K8S - K8sTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.537 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SEATUNNEL - SeatunnelTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.539 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SEATUNNEL - SeatunnelTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.540 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SAGEMAKER - SagemakerTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.543 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SAGEMAKER - SagemakerTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.546 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: HTTP - HttpTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.549 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: HTTP - HttpTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.551 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: EMR - EmrTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.556 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: EMR - EmrTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.557 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DMS - DmsTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.560 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DMS - DmsTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.562 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DATA_QUALITY - DataQualityTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.564 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DATA_QUALITY - DataQualityTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.566 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: KUBEFLOW - KubeflowTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.568 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: KUBEFLOW - KubeflowTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.570 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: SQL - SqlTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.572 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: SQL - SqlTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.573 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DVC - DvcTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.575 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DVC - DvcTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.576 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DATAX - DataxTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.579 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DATAX - DataxTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.580 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: ZEPPELIN - ZeppelinTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.583 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: ZEPPELIN - ZeppelinTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.584 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: DINKY - DinkyTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.586 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: DINKY - DinkyTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.587 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: MLFLOW - MlflowTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.589 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: MLFLOW - MlflowTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.590 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: OPENMLDB - OpenmldbTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.592 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: OPENMLDB - OpenmldbTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.592 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: LINKIS - LinkisTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.594 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: LINKIS - LinkisTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.595 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: FLINK - FlinkTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.597 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: FLINK - FlinkTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.597 +0800 o.a.d.p.t.a.TaskPluginManager:[63] - Registering task plugin: HIVECLI - HiveCliTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.600 +0800 o.a.d.p.t.a.TaskPluginManager:[68] - Registered task plugin: HIVECLI - HiveCliTaskChannelFactory
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.689 +0800 o.a.d.c.u.JSONUtils:[72] - init timezone: sun.util.calendar.ZoneInfo[id="Asia/Shanghai",offset=28800000,dstSavings=0,useDaylight=false,transitions=31,lastRule=null]
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:33.854 +0800 o.a.d.s.w.r.WorkerRegistryClient:[104] - Worker node: 172.18.1.1:1234 registry to ZK /nodes/worker/172.18.1.1:1234 successfully
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:34.964 +0800 o.a.d.c.m.BaseHeartBeatTask:[48] - Starting WorkerHeartBeatTask...
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:34.969 +0800 o.a.d.c.m.BaseHeartBeatTask:[50] - Started WorkerHeartBeatTask, heartBeatInterval: 10000...
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:34.975 +0800 o.a.d.s.w.r.WorkerRegistryClient:[114] - Worker node: 172.18.1.1:1234 registry finished
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:34.984 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0875 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:34.989 +0800 o.a.d.s.w.m.MessageRetryRunner:[69] - Message retry runner staring
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:35.031 +0800 o.a.d.s.w.m.MessageRetryRunner:[72] - Injected message sender: TaskInstanceExecutionFinishEventSender
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:35.032 +0800 o.a.d.s.w.m.MessageRetryRunner:[72] - Injected message sender: TaskInstanceExecutionInfoUpdateEventSender
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:35.058 +0800 o.a.d.s.w.m.MessageRetryRunner:[72] - Injected message sender: TaskInstanceExecutionRunningEventSender
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:35.061 +0800 o.a.d.s.w.m.MessageRetryRunner:[75] - Message retry runner started
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:36.102 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.5084745762711864 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:37.108 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.329479768786127 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:38.493 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.4112903225806452 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:39.852 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.3953488372093021 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:41.045 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.629310344827586 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:42.058 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.4347826086956523 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:43.263 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.4624277456647399 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:44.275 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.4328358208955225 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:45.349 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.650916016687829 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:46.515 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.447674418604651 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:46.965 +0800 o.s.b.a.e.w.EndpointLinksResolver:[58] - Exposing 3 endpoint(s) beneath base path '/actuator'
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:47.255 +0800 o.e.j.s.h.C.application:[2368] - Initializing Spring DispatcherServlet 'dispatcherServlet'
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:47.255 +0800 o.s.w.s.DispatcherServlet:[525] - Initializing Servlet 'dispatcherServlet'
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:47.257 +0800 o.s.w.s.DispatcherServlet:[547] - Completed initialization in 1 ms
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:47.357 +0800 o.e.j.s.AbstractConnector:[333] - Started ServerConnector@acb5508{HTTP/1.1, (http/1.1)}{0.0.0.0:1235}
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:47.359 +0800 o.s.b.w.e.j.JettyWebServer:[172] - Jetty started on port(s) 1235 (http/1.1) with context path '/'
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:47.528 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.3916666666666666 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:47.709 +0800 o.a.d.s.w.WorkerServer:[61] - Started WorkerServer in 42.739 seconds (JVM running for 45.314)
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:48.532 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0070422535211268 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:49.534 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9763313609467456 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:50.535 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8048410938654841 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:52.572 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7819148936170213 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:53.592 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9418604651162791 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:54.602 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8986175115207373 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:56.627 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8844621513944223 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:57.638 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8177581120943953 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:16:58.665 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7615528372409107 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:17:03.726 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.725609756097561 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:24:11.212 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8106666666666666 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:24:12.227 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8957746478873239 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:24:13.229 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8387978142076502 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:24:14.234 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7571428571428571 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:24:15.235 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.89196675900277 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:24:16.237 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.858695652173913 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:24:21.258 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7112299465240641 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:39:21.696 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=2839, taskName=search for intake article JSON files, firstSubmitTime=1714765161322, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=30, appIds=null, processInstanceId=886, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""},{"prop":"is_exists","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${GNEWS_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set does_file_exist to 1 \nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\n    is_exists=0\nelse\n    is_exists=1\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"\necho \"#{setValue(is_exists=${is_exists})}\"","resourceList":[]}, environmentConfig=null, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake article JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240504'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='2839'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524226'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240504033921'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='886'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240503'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:21.728 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake article JSON files to wait queue success
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:21.736 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:21.742 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:21.743 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:21.743 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:21.744 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714765161744
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:21.745 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 886_2839
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:21.752 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 2839,
  "taskName" : "search for intake article JSON files",
  "firstSubmitTime" : 1714765161322,
  "startTime" : 1714765161744,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240504/13475238524230/30/886/2839.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 30,
  "processInstanceId" : 886,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"},{\"prop\":\"is_exists\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${GNEWS_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set does_file_exist to 1 \\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\n    is_exists=0\\nelse\\n    is_exists=1\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\\necho \\\"#{setValue(is_exists=${is_exists})}\\\"\",\"resourceList\":[]}",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake article JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "2839"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524226"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504033921"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "886"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240503"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    }
  },
  "taskAppId" : "886_2839",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:21.762 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:21.763 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:21.763 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:21.851 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:21.872 +0800 o.a.d.c.u.OSUtils:[231] - create linux os user: default
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:21.873 +0800 o.a.d.c.u.OSUtils:[233] - execute cmd: sudo useradd -g root
 default
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:21.988 +0800 o.a.d.c.u.OSUtils:[190] - create user default success
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:21.989 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:21.995 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2839 check successfully
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:21.996 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:22.004 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:22.015 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:22.018 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:22.022 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  }, {
    "prop" : "is_exists",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${GNEWS_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set does_file_exist to 1 \nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\n    is_exists=0\nelse\n    is_exists=1\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"\necho \"#{setValue(is_exists=${is_exists})}\"",
  "resourceList" : [ ]
}
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:22.022 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:22.022 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:22.025 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:22.026 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:22.026 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:22.035 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:22.036 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:22.036 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/google_news/in -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set does_file_exist to 1 
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
    is_exists=0
else
    is_exists=1
fi

echo "#{setValue(raw_file_dir=${raw_file_dir})}"
echo "#{setValue(is_exists=${is_exists})}"
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:22.036 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:22.038 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2839/886_2839.sh
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:22.043 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 356
[WI-0][TI-2839] - [INFO] 2024-05-04 03:39:22.307 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=2839, success=true)
[WI-0][TI-2839] - [INFO] 2024-05-04 03:39:22.319 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=2839)
[WI-0][TI-0] - [INFO] 2024-05-04 03:39:23.043 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	#{setValue(raw_file_dir=/local_storage/google_news/in/830-20240503110322.json)}
	#{setValue(is_exists=1)}
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:23.045 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2839, processId:356 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:23.048 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:23.048 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:23.049 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:23.058 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:23.065 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:23.065 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:23.066 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2839
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:23.081 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2839
[WI-886][TI-2839] - [INFO] 2024-05-04 03:39:23.085 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-2839] - [INFO] 2024-05-04 03:39:23.298 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=2839, success=true)
[WI-0][TI-0] - [INFO] 2024-05-04 03:39:24.440 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=2841, taskName=move to processing, firstSubmitTime=1714765164426, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=30, appIds=null, processInstanceId=886, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the google news processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${GNEWS_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${GNEW_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"${setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to processing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240504'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='2841'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524227'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240504033924'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='886'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/google_news/in/830-20240503110322.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240503'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}, is_exists=Property{prop='is_exists', direct=IN, type=VARCHAR, value='1'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/in/830-20240503110322.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.441 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to processing to wait queue success
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.441 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.443 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.444 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.444 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.444 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714765164444
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.444 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 886_2841
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.445 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 2841,
  "taskName" : "move to processing",
  "firstSubmitTime" : 1714765164426,
  "startTime" : 1714765164444,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240504/13475238524230/30/886/2841.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 30,
  "processInstanceId" : 886,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the google news processing folder\\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\\nfilename=$(basename \\\"${raw_file_dir}\\\")\\nif ! grep -q \\\"${GNEWS_HOME}/processing\\\" <<< \\\"${raw_file_dir}\\\"; then\\n    mv ${raw_file_dir} ${GNEW_HOME}/processing\\nelse\\n    echo JSON is already in processing\\nfi\\n\\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\\necho \\\"${setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to processing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "2841"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524227"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504033924"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "886"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news/in/830-20240503110322.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240503"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    },
    "is_exists" : {
      "prop" : "is_exists",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1"
    }
  },
  "taskAppId" : "886_2841",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/google_news/in/830-20240503110322.json\"},{\"prop\":\"is_exists\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"1\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.445 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.445 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.445 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.449 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.450 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.451 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2841 check successfully
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.451 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.451 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.452 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.452 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.452 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the google news processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${GNEWS_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${GNEW_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"${setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\"",
  "resourceList" : [ ]
}
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.452 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.453 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/in/830-20240503110322.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}] successfully
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.454 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.454 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.454 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.455 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.455 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.455 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the google news processing folder
# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error
filename=$(basename "/local_storage/google_news/in/830-20240503110322.json")
if ! grep -q "/local_storage/google_news/processing" <<< "/local_storage/google_news/in/830-20240503110322.json"; then
    mv /local_storage/google_news/in/830-20240503110322.json ${GNEW_HOME}/processing
else
    echo JSON is already in processing
fi

# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing
echo "${setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}"
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.455 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.455 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2841/886_2841.sh
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:24.459 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 373
[WI-0][TI-2841] - [INFO] 2024-05-04 03:39:25.307 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=2841, success=true)
[WI-0][TI-2841] - [INFO] 2024-05-04 03:39:25.318 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=2841)
[WI-0][TI-0] - [INFO] 2024-05-04 03:39:25.460 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	mv: cannot create regular file '/processing': Permission denied
	/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2841/886_2841.sh: line 15: ${setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}: bad substitution
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:25.464 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2841, processId:373 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:25.465 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:25.466 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:25.466 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:25.468 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:25.473 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:25.473 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:25.474 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2841
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:25.475 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2841
[WI-886][TI-2841] - [INFO] 2024-05-04 03:39:25.475 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-2841] - [INFO] 2024-05-04 03:39:26.298 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=2841, success=true)
[WI-0][TI-0] - [INFO] 2024-05-04 03:39:26.349 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=2842, taskName=move to processing, firstSubmitTime=1714765166322, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=30, appIds=null, processInstanceId=886, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the google news processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${GNEWS_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${GNEW_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"${setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to processing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240504'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='2842'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524227'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240504033926'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='886'}, raw_file_dir=Property{prop='raw_file_dir', direct=OUT, type=VARCHAR, value='/local_storage/google_news/processing/${filename}'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240503'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}, is_exists=Property{prop='is_exists', direct=IN, type=VARCHAR, value='1'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"},{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":"${GNEWS_HOME}/processing/${filename}"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.351 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to processing to wait queue success
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.351 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.353 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.353 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.354 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.354 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714765166354
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.355 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 886_2842
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.355 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 2842,
  "taskName" : "move to processing",
  "firstSubmitTime" : 1714765166322,
  "startTime" : 1714765166354,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240504/13475238524230/30/886/2842.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 30,
  "processInstanceId" : 886,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the google news processing folder\\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\\nfilename=$(basename \\\"${raw_file_dir}\\\")\\nif ! grep -q \\\"${GNEWS_HOME}/processing\\\" <<< \\\"${raw_file_dir}\\\"; then\\n    mv ${raw_file_dir} ${GNEW_HOME}/processing\\nelse\\n    echo JSON is already in processing\\nfi\\n\\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\\necho \\\"${setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to processing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "2842"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524227"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504033926"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "886"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "OUT",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news/processing/${filename}"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240503"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    },
    "is_exists" : {
      "prop" : "is_exists",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1"
    }
  },
  "taskAppId" : "886_2842",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"is_exists\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"1\"},{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"${GNEWS_HOME}/processing/${filename}\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.356 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.356 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.358 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.363 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.364 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.365 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2842 check successfully
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.365 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.366 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.367 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.368 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.368 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the google news processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${GNEWS_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${GNEW_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"${setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\"",
  "resourceList" : [ ]
}
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.369 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.370 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"},{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":"${GNEWS_HOME}/processing/${filename}"}] successfully
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.371 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.371 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.372 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.373 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.373 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.374 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the google news processing folder
# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error
filename=$(basename "/local_storage/google_news/processing/${filename}")
if ! grep -q "/local_storage/google_news/processing" <<< "/local_storage/google_news/processing/${filename}"; then
    mv /local_storage/google_news/processing/${filename} ${GNEW_HOME}/processing
else
    echo JSON is already in processing
fi

# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing
echo "${setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}"
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.374 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.375 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2842/886_2842.sh
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:26.384 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 389
[WI-0][TI-0] - [INFO] 2024-05-04 03:39:26.384 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-2842] - [INFO] 2024-05-04 03:39:27.307 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=2842, success=true)
[WI-0][TI-2842] - [INFO] 2024-05-04 03:39:27.318 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=2842)
[WI-0][TI-0] - [INFO] 2024-05-04 03:39:27.391 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	JSON is already in processing
	/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2842/886_2842.sh: line 15: ${setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}: bad substitution
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:27.393 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2842, processId:389 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:27.393 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:27.393 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:27.394 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:27.397 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:27.404 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:27.404 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:27.405 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2842
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:27.405 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2842
[WI-886][TI-2842] - [INFO] 2024-05-04 03:39:27.406 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-2842] - [INFO] 2024-05-04 03:39:28.306 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=2842, success=true)
[WI-0][TI-0] - [INFO] 2024-05-04 03:39:28.389 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=2843, taskName=move to processing, firstSubmitTime=1714765168377, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=30, appIds=null, processInstanceId=886, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the google news processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${GNEWS_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${GNEW_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"${setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to processing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240504'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='2843'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524227'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240504033928'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='886'}, raw_file_dir=Property{prop='raw_file_dir', direct=OUT, type=VARCHAR, value='/local_storage/google_news/processing/${filename}'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240503'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}, is_exists=Property{prop='is_exists', direct=IN, type=VARCHAR, value='1'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"},{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":"${GNEWS_HOME}/processing/${filename}"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.390 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to processing to wait queue success
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.390 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.391 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.391 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.391 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.392 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714765168392
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.392 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 886_2843
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.392 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 2843,
  "taskName" : "move to processing",
  "firstSubmitTime" : 1714765168377,
  "startTime" : 1714765168392,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240504/13475238524230/30/886/2843.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 30,
  "processInstanceId" : 886,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the google news processing folder\\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\\nfilename=$(basename \\\"${raw_file_dir}\\\")\\nif ! grep -q \\\"${GNEWS_HOME}/processing\\\" <<< \\\"${raw_file_dir}\\\"; then\\n    mv ${raw_file_dir} ${GNEW_HOME}/processing\\nelse\\n    echo JSON is already in processing\\nfi\\n\\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\\necho \\\"${setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to processing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "2843"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524227"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504033928"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "886"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "OUT",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news/processing/${filename}"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240503"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    },
    "is_exists" : {
      "prop" : "is_exists",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1"
    }
  },
  "taskAppId" : "886_2843",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"is_exists\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"1\"},{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"${GNEWS_HOME}/processing/${filename}\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.393 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.394 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.394 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.398 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.399 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.399 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2843 check successfully
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.399 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.400 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.400 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.401 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the google news processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${GNEWS_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${GNEW_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"${setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\"",
  "resourceList" : [ ]
}
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"},{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":"${GNEWS_HOME}/processing/${filename}"}] successfully
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.402 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.402 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.402 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.402 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.402 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.403 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the google news processing folder
# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error
filename=$(basename "/local_storage/google_news/processing/${filename}")
if ! grep -q "/local_storage/google_news/processing" <<< "/local_storage/google_news/processing/${filename}"; then
    mv /local_storage/google_news/processing/${filename} ${GNEW_HOME}/processing
else
    echo JSON is already in processing
fi

# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing
echo "${setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}"
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.403 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.403 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2843/886_2843.sh
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:28.406 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 401
[WI-0][TI-2843] - [INFO] 2024-05-04 03:39:29.323 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=2843, success=true)
[WI-0][TI-2843] - [INFO] 2024-05-04 03:39:29.333 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=2843)
[WI-0][TI-0] - [INFO] 2024-05-04 03:39:29.409 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	JSON is already in processing
	/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2843/886_2843.sh: line 15: ${setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}: bad substitution
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:29.413 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2843, processId:401 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:29.415 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:29.415 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:29.415 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:29.416 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:29.424 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:29.424 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:29.424 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2843
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:29.425 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2843
[WI-886][TI-2843] - [INFO] 2024-05-04 03:39:29.425 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-2843] - [INFO] 2024-05-04 03:39:30.327 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=2843, success=true)
[WI-0][TI-0] - [INFO] 2024-05-04 03:39:30.412 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=2844, taskName=move to processing, firstSubmitTime=1714765170400, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=30, appIds=null, processInstanceId=886, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the google news processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${GNEWS_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${GNEW_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"${setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to processing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240504'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='2844'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524227'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240504033930'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='886'}, raw_file_dir=Property{prop='raw_file_dir', direct=OUT, type=VARCHAR, value='/local_storage/google_news/processing/${filename}'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240503'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}, is_exists=Property{prop='is_exists', direct=IN, type=VARCHAR, value='1'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"},{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":"${GNEWS_HOME}/processing/${filename}"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.413 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to processing to wait queue success
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.414 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.416 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.416 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.416 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.416 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714765170416
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.416 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 886_2844
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.417 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 2844,
  "taskName" : "move to processing",
  "firstSubmitTime" : 1714765170400,
  "startTime" : 1714765170416,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240504/13475238524230/30/886/2844.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 30,
  "processInstanceId" : 886,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the google news processing folder\\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\\nfilename=$(basename \\\"${raw_file_dir}\\\")\\nif ! grep -q \\\"${GNEWS_HOME}/processing\\\" <<< \\\"${raw_file_dir}\\\"; then\\n    mv ${raw_file_dir} ${GNEW_HOME}/processing\\nelse\\n    echo JSON is already in processing\\nfi\\n\\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\\necho \\\"${setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to processing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "2844"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524227"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504033930"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "886"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "OUT",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news/processing/${filename}"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240503"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    },
    "is_exists" : {
      "prop" : "is_exists",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1"
    }
  },
  "taskAppId" : "886_2844",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"is_exists\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"1\"},{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"${GNEWS_HOME}/processing/${filename}\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.420 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.420 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.420 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.425 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.426 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.427 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2844 check successfully
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.427 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.427 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.428 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.428 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.429 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the google news processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${GNEWS_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${GNEW_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"${setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\"",
  "resourceList" : [ ]
}
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.429 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.429 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"},{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":"${GNEWS_HOME}/processing/${filename}"}] successfully
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.429 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.430 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.430 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.431 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.431 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.431 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the google news processing folder
# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error
filename=$(basename "/local_storage/google_news/processing/${filename}")
if ! grep -q "/local_storage/google_news/processing" <<< "/local_storage/google_news/processing/${filename}"; then
    mv /local_storage/google_news/processing/${filename} ${GNEW_HOME}/processing
else
    echo JSON is already in processing
fi

# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing
echo "${setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}"
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.431 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.433 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2844/886_2844.sh
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:30.437 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 413
[WI-0][TI-2844] - [INFO] 2024-05-04 03:39:31.336 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=2844, success=true)
[WI-0][TI-2844] - [INFO] 2024-05-04 03:39:31.343 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=2844)
[WI-0][TI-0] - [INFO] 2024-05-04 03:39:31.439 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	JSON is already in processing
	/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2844/886_2844.sh: line 15: ${setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}: bad substitution
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:31.452 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2844, processId:413 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:31.453 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:31.454 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:31.454 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:31.455 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:31.458 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:31.459 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:31.459 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2844
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:31.459 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_30/886/2844
[WI-886][TI-2844] - [INFO] 2024-05-04 03:39:31.460 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-2844] - [INFO] 2024-05-04 03:39:32.333 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=2844, success=true)
[WI-0][TI-0] - [INFO] 2024-05-04 03:43:25.067 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8236914600550964 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:45:09.466 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7464788732394366 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:45:10.494 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8086253369272237 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:46:12.674 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.739010989010989 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:46:14.706 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9232876712328768 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:46:20.551 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=2845, taskName=search for intake article JSON files, firstSubmitTime=1714765580536, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=31, appIds=null, processInstanceId=886, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""},{"prop":"is_exists","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${GNEWS_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set does_file_exist to 1 \nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\n    is_exists=0\nelse\n    is_exists=1\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"\necho \"#{setValue(is_exists=${is_exists})}\"","resourceList":[]}, environmentConfig=null, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake article JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240504'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='2845'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524226'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240504034620'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='886'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/google_news/in/830-20240503110322.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240503'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}, is_exists=Property{prop='is_exists', direct=IN, type=VARCHAR, value='1'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/in/830-20240503110322.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.552 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake article JSON files to wait queue success
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.553 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.555 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.555 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.556 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.556 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714765580556
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.557 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 886_2845
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.558 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 2845,
  "taskName" : "search for intake article JSON files",
  "firstSubmitTime" : 1714765580536,
  "startTime" : 1714765580556,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240504/13475238524230/31/886/2845.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 31,
  "processInstanceId" : 886,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"},{\"prop\":\"is_exists\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${GNEWS_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set does_file_exist to 1 \\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\n    is_exists=0\\nelse\\n    is_exists=1\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\\necho \\\"#{setValue(is_exists=${is_exists})}\\\"\",\"resourceList\":[]}",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake article JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "2845"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524226"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504034620"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "886"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news/in/830-20240503110322.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240503"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    },
    "is_exists" : {
      "prop" : "is_exists",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1"
    }
  },
  "taskAppId" : "886_2845",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/google_news/in/830-20240503110322.json\"},{\"prop\":\"is_exists\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"1\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.560 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.560 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.560 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.568 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.568 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.570 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2845 check successfully
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.570 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.571 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.572 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.572 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.572 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  }, {
    "prop" : "is_exists",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${GNEWS_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set does_file_exist to 1 \nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\n    is_exists=0\nelse\n    is_exists=1\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"\necho \"#{setValue(is_exists=${is_exists})}\"",
  "resourceList" : [ ]
}
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.573 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.573 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/in/830-20240503110322.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}] successfully
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.573 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.573 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.573 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.574 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.574 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.574 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/google_news/in -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set does_file_exist to 1 
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
    is_exists=0
else
    is_exists=1
fi

echo "#{setValue(raw_file_dir=/local_storage/google_news/in/830-20240503110322.json)}"
echo "#{setValue(is_exists=1)}"
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.574 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.574 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2845/886_2845.sh
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:20.578 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 510
[WI-0][TI-2845] - [INFO] 2024-05-04 03:46:21.044 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=2845, success=true)
[WI-0][TI-2845] - [INFO] 2024-05-04 03:46:21.055 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=2845)
[WI-0][TI-0] - [INFO] 2024-05-04 03:46:21.581 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	#{setValue(raw_file_dir=/local_storage/google_news/in/830-20240503110322.json)}
	#{setValue(is_exists=1)}
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:21.581 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2845, processId:510 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:21.582 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:21.582 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:21.582 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:21.583 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:21.587 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:21.587 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:21.587 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2845
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:21.592 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2845
[WI-886][TI-2845] - [INFO] 2024-05-04 03:46:21.592 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-2845] - [INFO] 2024-05-04 03:46:22.043 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=2845, success=true)
[WI-0][TI-0] - [INFO] 2024-05-04 03:46:23.089 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=2847, taskName=move to processing, firstSubmitTime=1714765583083, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=31, appIds=null, processInstanceId=886, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the google news processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${GNEWS_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${GNEW_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to processing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240504'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='2847'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524227'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240504034623'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='886'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/google_news/in/830-20240503110322.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240503'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}, is_exists=Property{prop='is_exists', direct=IN, type=VARCHAR, value='1'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/in/830-20240503110322.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.090 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to processing to wait queue success
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.090 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.092 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.092 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.092 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.092 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714765583092
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.092 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 886_2847
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.093 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 2847,
  "taskName" : "move to processing",
  "firstSubmitTime" : 1714765583083,
  "startTime" : 1714765583092,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240504/13475238524230/31/886/2847.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 31,
  "processInstanceId" : 886,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the google news processing folder\\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\\nfilename=$(basename \\\"${raw_file_dir}\\\")\\nif ! grep -q \\\"${GNEWS_HOME}/processing\\\" <<< \\\"${raw_file_dir}\\\"; then\\n    mv ${raw_file_dir} ${GNEW_HOME}/processing\\nelse\\n    echo JSON is already in processing\\nfi\\n\\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\\necho \\\"#{setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to processing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "2847"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524227"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504034623"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "886"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news/in/830-20240503110322.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240503"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    },
    "is_exists" : {
      "prop" : "is_exists",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1"
    }
  },
  "taskAppId" : "886_2847",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/google_news/in/830-20240503110322.json\"},{\"prop\":\"is_exists\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"1\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.093 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.093 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.094 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.099 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.100 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.100 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2847 check successfully
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.100 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.101 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.101 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.101 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.101 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the google news processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${GNEWS_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${GNEW_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\"",
  "resourceList" : [ ]
}
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.102 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.102 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/in/830-20240503110322.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}] successfully
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.102 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.102 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.103 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.103 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.103 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.103 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the google news processing folder
# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error
filename=$(basename "/local_storage/google_news/in/830-20240503110322.json")
if ! grep -q "/local_storage/google_news/processing" <<< "/local_storage/google_news/in/830-20240503110322.json"; then
    mv /local_storage/google_news/in/830-20240503110322.json ${GNEW_HOME}/processing
else
    echo JSON is already in processing
fi

# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing
echo "#{setValue(raw_file_dir=/local_storage/google_news/processing/${filename})}"
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.104 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.104 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2847/886_2847.sh
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:23.108 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 524
[WI-0][TI-2847] - [INFO] 2024-05-04 03:46:24.049 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=2847, success=true)
[WI-0][TI-2847] - [INFO] 2024-05-04 03:46:24.056 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=2847)
[WI-0][TI-0] - [INFO] 2024-05-04 03:46:24.110 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	mv: cannot create regular file '/processing': Permission denied
	#{setValue(raw_file_dir=/local_storage/google_news/processing/830-20240503110322.json)}
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:24.113 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2847, processId:524 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:24.115 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:24.115 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:24.115 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:24.116 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:24.120 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:24.120 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:24.121 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2847
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:24.122 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2847
[WI-886][TI-2847] - [INFO] 2024-05-04 03:46:24.122 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-2847] - [INFO] 2024-05-04 03:46:25.047 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=2847, success=true)
[WI-0][TI-0] - [INFO] 2024-05-04 03:46:25.116 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=2848, taskName=preprocessing and kafka, firstSubmitTime=1714765585106, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=31, appIds=null, processInstanceId=886, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name gnews_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_gnews_preprocessing.py\n","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='preprocessing and kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240504'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='2848'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524229'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240504034625'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='886'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/google_news/processing/830-20240503110322.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240503'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}, is_exists=Property{prop='is_exists', direct=IN, type=VARCHAR, value='1'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/830-20240503110322.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.117 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: preprocessing and kafka to wait queue success
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.117 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.118 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.118 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.119 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.119 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714765585119
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.119 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 886_2848
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.121 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 2848,
  "taskName" : "preprocessing and kafka",
  "firstSubmitTime" : 1714765585106,
  "startTime" : 1714765585119,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240504/13475238524230/31/886/2848.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 31,
  "processInstanceId" : 886,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name gnews_preprocessing \\\\\\n    --files ${raw_file_dir} \\\\\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\\\\n    --conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n    spark_gnews_preprocessing.py\\n\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocessing and kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "2848"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524229"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504034625"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "886"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news/processing/830-20240503110322.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240503"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    },
    "is_exists" : {
      "prop" : "is_exists",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1"
    }
  },
  "taskAppId" : "886_2848",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/google_news/processing/830-20240503110322.json\"},{\"prop\":\"is_exists\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"1\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.122 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.123 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.123 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.126 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.127 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.127 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2848 check successfully
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.127 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.160 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py, resourceRelativePath=spark_gnews_preprocessing.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2848/spark_gnews_preprocessing.py)})
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.161 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.162 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.164 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name gnews_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_gnews_preprocessing.py\n",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py",
    "res" : null
  } ]
}
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.165 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.166 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/830-20240503110322.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}] successfully
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.166 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.167 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.167 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.168 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.169 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.169 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master local \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name gnews_preprocessing \
    --files /local_storage/google_news/processing/830-20240503110322.json \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
    --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
    spark_gnews_preprocessing.py

[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.170 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.170 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2848/886_2848.sh
[WI-886][TI-2848] - [INFO] 2024-05-04 03:46:25.179 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 537
[WI-0][TI-2848] - [INFO] 2024-05-04 03:46:26.072 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=2848, success=true)
[WI-0][TI-2848] - [INFO] 2024-05-04 03:46:26.105 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=2848)
[WI-0][TI-0] - [INFO] 2024-05-04 03:46:26.182 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-05-04 03:46:26.808 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7208672086720868 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:46:30.196 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-3234c5f8-cde8-4bac-83cb-a5756798e094;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-04 03:46:35.201 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
[WI-0][TI-0] - [INFO] 2024-05-04 03:46:36.209 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
[WI-0][TI-0] - [INFO] 2024-05-04 03:46:37.211 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
[WI-0][TI-0] - [INFO] 2024-05-04 03:46:39.214 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.slf4j#slf4j-api;2.0.7 in central
[WI-0][TI-0] - [INFO] 2024-05-04 03:46:42.219 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[WI-0][TI-0] - [INFO] 2024-05-04 03:46:45.232 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found commons-logging#commons-logging;1.1.3 in central
[WI-0][TI-0] - [INFO] 2024-05-04 03:46:46.233 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found com.google.code.findbugs#jsr305;3.0.0 in central
[WI-0][TI-0] - [INFO] 2024-05-04 03:46:49.241 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.commons#commons-pool2;2.11.1 in central
	downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.jar ...
[WI-0][TI-0] - [INFO] 2024-05-04 03:46:50.242 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1!spark-sql-kafka-0-10_2.12.jar (842ms)
	downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.1/spark-token-provider-kafka-0-10_2.12-3.5.1.jar ...
		[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1!spark-token-provider-kafka-0-10_2.12.jar (403ms)
	downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...
[WI-0][TI-0] - [INFO] 2024-05-04 03:46:52.245 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar (2357ms)
[WI-0][TI-0] - [INFO] 2024-05-04 03:46:53.253 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...
		[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (391ms)
	downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...
		[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (426ms)
	downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:00.266 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (7134ms)
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:01.267 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...
		[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (510ms)
	downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.3/snappy-java-1.1.10.3.jar ...
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:02.267 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.3!snappy-java.jar(bundle) (728ms)
	downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...
		[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (411ms)
	downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:06.273 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (4102ms)
	downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:07.274 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   11  |   11  |   0   ||   11  |   11  |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-3234c5f8-cde8-4bac-83cb-a5756798e094
		confs: [default]
		11 artifacts copied, 0 already retrieved (56767kB/348ms)
	24/05/04 03:47:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:09.276 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:47:09 INFO SparkContext: Running Spark version 3.5.1
	24/05/04 03:47:09 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/05/04 03:47:09 INFO SparkContext: Java version 1.8.0_402
	24/05/04 03:47:09 INFO ResourceUtils: ==============================================================
	24/05/04 03:47:09 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/05/04 03:47:09 INFO ResourceUtils: ==============================================================
	24/05/04 03:47:09 INFO SparkContext: Submitted application: gnews_preprocessing
	24/05/04 03:47:09 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/05/04 03:47:09 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
	24/05/04 03:47:09 INFO ResourceProfileManager: Added ResourceProfile id: 0
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:10.277 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:47:09 INFO SecurityManager: Changing view acls to: default
	24/05/04 03:47:09 INFO SecurityManager: Changing modify acls to: default
	24/05/04 03:47:09 INFO SecurityManager: Changing view acls groups to: 
	24/05/04 03:47:09 INFO SecurityManager: Changing modify acls groups to: 
	24/05/04 03:47:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
	24/05/04 03:47:10 INFO Utils: Successfully started service 'sparkDriver' on port 33787.
	24/05/04 03:47:10 INFO SparkEnv: Registering MapOutputTracker
	24/05/04 03:47:10 INFO SparkEnv: Registering BlockManagerMaster
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:11.283 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:47:10 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/05/04 03:47:10 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/05/04 03:47:10 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/05/04 03:47:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b90e426d-b6af-4e1d-a133-4d099d7a2633
	24/05/04 03:47:10 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
	24/05/04 03:47:10 INFO SparkEnv: Registering OutputCommitCoordinator
	24/05/04 03:47:11 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:11.978 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7244318181818181 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:12.284 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:47:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/05/04 03:47:11 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://addf0afec0d4:33787/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714765628994
	24/05/04 03:47:11 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://addf0afec0d4:33787/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714765628994
	24/05/04 03:47:11 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://addf0afec0d4:33787/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714765628994
	24/05/04 03:47:11 INFO SparkContext: Added JAR file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://addf0afec0d4:33787/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714765628994
	24/05/04 03:47:11 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://addf0afec0d4:33787/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714765628994
	24/05/04 03:47:11 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://addf0afec0d4:33787/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714765628994
	24/05/04 03:47:11 INFO SparkContext: Added JAR file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at spark://addf0afec0d4:33787/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714765628994
	24/05/04 03:47:11 INFO SparkContext: Added JAR file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://addf0afec0d4:33787/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714765628994
	24/05/04 03:47:11 INFO SparkContext: Added JAR file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://addf0afec0d4:33787/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714765628994
	24/05/04 03:47:11 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://addf0afec0d4:33787/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714765628994
	24/05/04 03:47:11 INFO SparkContext: Added JAR file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at spark://addf0afec0d4:33787/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714765628994
	24/05/04 03:47:11 ERROR SparkContext: Error initializing SparkContext.
	java.io.FileNotFoundException: File file:/local_storage/google_news/processing/830-20240503110322.json does not exist
		at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
		at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
		at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
		at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
		at org.apache.spark.SparkContext.addFile(SparkContext.scala:1755)
		at org.apache.spark.SparkContext.$anonfun$new$16(SparkContext.scala:533)
		at org.apache.spark.SparkContext.$anonfun$new$16$adapted(SparkContext.scala:533)
		at scala.collection.immutable.List.foreach(List.scala:431)
		at org.apache.spark.SparkContext.<init>(SparkContext.scala:533)
		at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
		at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
		at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
		at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
		at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:238)
		at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
		at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	24/05/04 03:47:11 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/05/04 03:47:11 INFO SparkUI: Stopped Spark web UI at http://addf0afec0d4:4040
	24/05/04 03:47:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/05/04 03:47:11 INFO MemoryStore: MemoryStore cleared
	24/05/04 03:47:11 INFO BlockManager: BlockManager stopped
	24/05/04 03:47:11 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/05/04 03:47:11 WARN MetricsSystem: Stopping a MetricsSystem that is not running
	24/05/04 03:47:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/05/04 03:47:11 INFO SparkContext: Successfully stopped SparkContext
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2848/spark_gnews_preprocessing.py", line 12, in <module>
	    sc = pyspark.SparkContext(appName="gnews_preprocessing")
	         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/context.py", line 203, in __init__
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/context.py", line 296, in _do_init
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/context.py", line 421, in _initialize_context
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1587, in __call__
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
	py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
	: java.io.FileNotFoundException: File file:/local_storage/google_news/processing/830-20240503110322.json does not exist
		at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
		at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
		at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
		at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
		at org.apache.spark.SparkContext.addFile(SparkContext.scala:1755)
		at org.apache.spark.SparkContext.$anonfun$new$16(SparkContext.scala:533)
		at org.apache.spark.SparkContext.$anonfun$new$16$adapted(SparkContext.scala:533)
		at scala.collection.immutable.List.foreach(List.scala:431)
		at org.apache.spark.SparkContext.<init>(SparkContext.scala:533)
		at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
		at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
		at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
		at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
		at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:238)
		at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
		at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	
	24/05/04 03:47:11 INFO ShutdownHookManager: Shutdown hook called
	24/05/04 03:47:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-dae17d46-33a6-4d03-8660-c2348c2f7b1e
	24/05/04 03:47:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-c763f07e-4564-4d25-b3e7-c9d6dd6e1c49
[WI-886][TI-2848] - [INFO] 2024-05-04 03:47:12.287 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2848, processId:537 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-886][TI-2848] - [INFO] 2024-05-04 03:47:12.288 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2848] - [INFO] 2024-05-04 03:47:12.289 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-886][TI-2848] - [INFO] 2024-05-04 03:47:12.289 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2848] - [INFO] 2024-05-04 03:47:12.290 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-886][TI-2848] - [INFO] 2024-05-04 03:47:12.296 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-886][TI-2848] - [INFO] 2024-05-04 03:47:12.298 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-886][TI-2848] - [INFO] 2024-05-04 03:47:12.298 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2848
[WI-886][TI-2848] - [INFO] 2024-05-04 03:47:12.300 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2848
[WI-886][TI-2848] - [INFO] 2024-05-04 03:47:12.304 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-2848] - [INFO] 2024-05-04 03:47:13.146 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=2848, success=true)
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:13.247 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=2849, taskName=preprocessing and kafka, firstSubmitTime=1714765633232, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=31, appIds=null, processInstanceId=886, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name gnews_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_gnews_preprocessing.py\n","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='preprocessing and kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240504'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='2849'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524229'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240504034713'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='886'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/google_news/processing/830-20240503110322.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240503'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}, is_exists=Property{prop='is_exists', direct=IN, type=VARCHAR, value='1'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/830-20240503110322.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.248 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: preprocessing and kafka to wait queue success
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.248 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.251 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.251 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.251 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.251 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714765633251
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.252 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 886_2849
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.252 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 2849,
  "taskName" : "preprocessing and kafka",
  "firstSubmitTime" : 1714765633232,
  "startTime" : 1714765633251,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240504/13475238524230/31/886/2849.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 31,
  "processInstanceId" : 886,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name gnews_preprocessing \\\\\\n    --files ${raw_file_dir} \\\\\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\\\\n    --conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n    spark_gnews_preprocessing.py\\n\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocessing and kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "2849"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524229"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504034713"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "886"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news/processing/830-20240503110322.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240503"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    },
    "is_exists" : {
      "prop" : "is_exists",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1"
    }
  },
  "taskAppId" : "886_2849",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/google_news/processing/830-20240503110322.json\"},{\"prop\":\"is_exists\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"1\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.253 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.255 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.255 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.265 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.267 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.268 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2849 check successfully
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.268 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.377 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py, resourceRelativePath=spark_gnews_preprocessing.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2849/spark_gnews_preprocessing.py)})
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.380 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.381 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.382 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name gnews_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_gnews_preprocessing.py\n",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py",
    "res" : null
  } ]
}
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.382 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.382 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/830-20240503110322.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}] successfully
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.383 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.390 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.390 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.393 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.394 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.395 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master local \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name gnews_preprocessing \
    --files /local_storage/google_news/processing/830-20240503110322.json \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
    --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
    spark_gnews_preprocessing.py

[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.395 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.396 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2849/886_2849.sh
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:13.409 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 641
[WI-0][TI-2849] - [INFO] 2024-05-04 03:47:14.144 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=2849, success=true)
[WI-0][TI-2849] - [INFO] 2024-05-04 03:47:14.156 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=2849)
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:14.409 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:16.418 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-df6fb9df-49bc-4575-b3ef-054b5d5e2037;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:17.027 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7745454545454546 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:17.607 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 587ms :: artifacts dl 20ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-df6fb9df-49bc-4575-b3ef-054b5d5e2037
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/13ms)
	24/05/04 03:47:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:18.611 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:47:17 INFO SparkContext: Running Spark version 3.5.1
	24/05/04 03:47:17 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/05/04 03:47:17 INFO SparkContext: Java version 1.8.0_402
	24/05/04 03:47:17 INFO ResourceUtils: ==============================================================
	24/05/04 03:47:17 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/05/04 03:47:17 INFO ResourceUtils: ==============================================================
	24/05/04 03:47:17 INFO SparkContext: Submitted application: gnews_preprocessing
	24/05/04 03:47:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/05/04 03:47:17 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
	24/05/04 03:47:17 INFO ResourceProfileManager: Added ResourceProfile id: 0
	24/05/04 03:47:18 INFO SecurityManager: Changing view acls to: default
	24/05/04 03:47:18 INFO SecurityManager: Changing modify acls to: default
	24/05/04 03:47:18 INFO SecurityManager: Changing view acls groups to: 
	24/05/04 03:47:18 INFO SecurityManager: Changing modify acls groups to: 
	24/05/04 03:47:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
	24/05/04 03:47:18 INFO Utils: Successfully started service 'sparkDriver' on port 44293.
	24/05/04 03:47:18 INFO SparkEnv: Registering MapOutputTracker
	24/05/04 03:47:18 INFO SparkEnv: Registering BlockManagerMaster
	24/05/04 03:47:18 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/05/04 03:47:18 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/05/04 03:47:18 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:19.612 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:47:18 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c1d4a164-ccbc-43f8-9eef-2a276b90aa10
	24/05/04 03:47:18 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
	24/05/04 03:47:18 INFO SparkEnv: Registering OutputCommitCoordinator
	24/05/04 03:47:18 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/05/04 03:47:18 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/05/04 03:47:19 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://addf0afec0d4:44293/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714765637762
	24/05/04 03:47:19 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://addf0afec0d4:44293/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714765637762
	24/05/04 03:47:19 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://addf0afec0d4:44293/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714765637762
	24/05/04 03:47:19 INFO SparkContext: Added JAR file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://addf0afec0d4:44293/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714765637762
	24/05/04 03:47:19 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://addf0afec0d4:44293/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714765637762
	24/05/04 03:47:19 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://addf0afec0d4:44293/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714765637762
	24/05/04 03:47:19 INFO SparkContext: Added JAR file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at spark://addf0afec0d4:44293/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714765637762
	24/05/04 03:47:19 INFO SparkContext: Added JAR file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://addf0afec0d4:44293/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714765637762
	24/05/04 03:47:19 INFO SparkContext: Added JAR file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://addf0afec0d4:44293/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714765637762
	24/05/04 03:47:19 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://addf0afec0d4:44293/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714765637762
	24/05/04 03:47:19 INFO SparkContext: Added JAR file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at spark://addf0afec0d4:44293/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714765637762
	24/05/04 03:47:19 ERROR SparkContext: Error initializing SparkContext.
	java.io.FileNotFoundException: File file:/local_storage/google_news/processing/830-20240503110322.json does not exist
		at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
		at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
		at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
		at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
		at org.apache.spark.SparkContext.addFile(SparkContext.scala:1755)
		at org.apache.spark.SparkContext.$anonfun$new$16(SparkContext.scala:533)
		at org.apache.spark.SparkContext.$anonfun$new$16$adapted(SparkContext.scala:533)
		at scala.collection.immutable.List.foreach(List.scala:431)
		at org.apache.spark.SparkContext.<init>(SparkContext.scala:533)
		at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
		at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
		at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
		at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
		at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:238)
		at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
		at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	24/05/04 03:47:19 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/05/04 03:47:19 INFO SparkUI: Stopped Spark web UI at http://addf0afec0d4:4040
	24/05/04 03:47:19 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/05/04 03:47:19 INFO MemoryStore: MemoryStore cleared
	24/05/04 03:47:19 INFO BlockManager: BlockManager stopped
	24/05/04 03:47:19 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/05/04 03:47:19 WARN MetricsSystem: Stopping a MetricsSystem that is not running
	24/05/04 03:47:19 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/05/04 03:47:19 INFO SparkContext: Successfully stopped SparkContext
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2849/spark_gnews_preprocessing.py", line 12, in <module>
	    sc = pyspark.SparkContext(appName="gnews_preprocessing")
	         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/context.py", line 203, in __init__
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/context.py", line 296, in _do_init
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/context.py", line 421, in _initialize_context
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1587, in __call__
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
	py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
	: java.io.FileNotFoundException: File file:/local_storage/google_news/processing/830-20240503110322.json does not exist
		at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
		at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
		at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
		at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
		at org.apache.spark.SparkContext.addFile(SparkContext.scala:1755)
		at org.apache.spark.SparkContext.$anonfun$new$16(SparkContext.scala:533)
		at org.apache.spark.SparkContext.$anonfun$new$16$adapted(SparkContext.scala:533)
		at scala.collection.immutable.List.foreach(List.scala:431)
		at org.apache.spark.SparkContext.<init>(SparkContext.scala:533)
		at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
		at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
		at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
		at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
		at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:238)
		at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
		at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	
	24/05/04 03:47:19 INFO ShutdownHookManager: Shutdown hook called
	24/05/04 03:47:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-e8dd28fd-278c-47ca-8844-e3bf0d15efe3
	24/05/04 03:47:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-8986ec66-e99d-4cfd-833a-e0029f7ee175
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:19.615 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2849, processId:641 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:19.615 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:19.615 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:19.615 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:19.617 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:19.621 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:19.621 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:19.622 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2849
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:19.622 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2849
[WI-886][TI-2849] - [INFO] 2024-05-04 03:47:19.623 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-2849] - [INFO] 2024-05-04 03:47:20.161 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=2849, success=true)
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:20.204 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=2850, taskName=preprocessing and kafka, firstSubmitTime=1714765640187, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=31, appIds=null, processInstanceId=886, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name gnews_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_gnews_preprocessing.py\n","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='preprocessing and kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240504'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='2850'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524229'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240504034720'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='886'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/google_news/processing/830-20240503110322.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240503'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}, is_exists=Property{prop='is_exists', direct=IN, type=VARCHAR, value='1'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/830-20240503110322.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.215 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: preprocessing and kafka to wait queue success
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.218 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.219 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.220 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.220 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.220 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714765640220
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.221 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 886_2850
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.221 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 2850,
  "taskName" : "preprocessing and kafka",
  "firstSubmitTime" : 1714765640187,
  "startTime" : 1714765640220,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240504/13475238524230/31/886/2850.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 31,
  "processInstanceId" : 886,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name gnews_preprocessing \\\\\\n    --files ${raw_file_dir} \\\\\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\\\\n    --conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n    spark_gnews_preprocessing.py\\n\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocessing and kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "2850"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524229"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504034720"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "886"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news/processing/830-20240503110322.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240503"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    },
    "is_exists" : {
      "prop" : "is_exists",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1"
    }
  },
  "taskAppId" : "886_2850",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/google_news/processing/830-20240503110322.json\"},{\"prop\":\"is_exists\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"1\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.222 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.223 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.223 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.229 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.230 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.231 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2850 check successfully
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.231 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.233 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py, resourceRelativePath=spark_gnews_preprocessing.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2850/spark_gnews_preprocessing.py)})
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.234 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.235 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.236 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name gnews_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_gnews_preprocessing.py\n",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py",
    "res" : null
  } ]
}
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.237 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.238 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/830-20240503110322.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}] successfully
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.239 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.239 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.240 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.240 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.241 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.241 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master local \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name gnews_preprocessing \
    --files /local_storage/google_news/processing/830-20240503110322.json \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
    --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
    spark_gnews_preprocessing.py

[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.241 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.241 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2850/886_2850.sh
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:20.248 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 727
[WI-0][TI-2850] - [INFO] 2024-05-04 03:47:21.169 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=2850, success=true)
[WI-0][TI-2850] - [INFO] 2024-05-04 03:47:21.196 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=2850)
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:21.250 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:23.256 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-bd409617-1155-40a2-af84-4cb61bfe0bc0;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:24.087 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7241379310344828 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:24.260 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 590ms :: artifacts dl 24ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-bd409617-1155-40a2-af84-4cb61bfe0bc0
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/12ms)
	24/05/04 03:47:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:25.262 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:47:24 INFO SparkContext: Running Spark version 3.5.1
	24/05/04 03:47:24 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/05/04 03:47:24 INFO SparkContext: Java version 1.8.0_402
	24/05/04 03:47:24 INFO ResourceUtils: ==============================================================
	24/05/04 03:47:24 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/05/04 03:47:24 INFO ResourceUtils: ==============================================================
	24/05/04 03:47:24 INFO SparkContext: Submitted application: gnews_preprocessing
	24/05/04 03:47:25 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/05/04 03:47:25 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
	24/05/04 03:47:25 INFO ResourceProfileManager: Added ResourceProfile id: 0
	24/05/04 03:47:25 INFO SecurityManager: Changing view acls to: default
	24/05/04 03:47:25 INFO SecurityManager: Changing modify acls to: default
	24/05/04 03:47:25 INFO SecurityManager: Changing view acls groups to: 
	24/05/04 03:47:25 INFO SecurityManager: Changing modify acls groups to: 
	24/05/04 03:47:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:26.264 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:47:25 INFO Utils: Successfully started service 'sparkDriver' on port 36125.
	24/05/04 03:47:25 INFO SparkEnv: Registering MapOutputTracker
	24/05/04 03:47:25 INFO SparkEnv: Registering BlockManagerMaster
	24/05/04 03:47:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/05/04 03:47:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/05/04 03:47:25 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/05/04 03:47:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8151c79c-9fd6-4438-8cb9-890f12e2c208
	24/05/04 03:47:25 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
	24/05/04 03:47:25 INFO SparkEnv: Registering OutputCommitCoordinator
	24/05/04 03:47:25 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/05/04 03:47:26 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/05/04 03:47:26 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://addf0afec0d4:36125/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714765644924
	24/05/04 03:47:26 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://addf0afec0d4:36125/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714765644924
	24/05/04 03:47:26 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://addf0afec0d4:36125/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714765644924
	24/05/04 03:47:26 INFO SparkContext: Added JAR file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://addf0afec0d4:36125/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714765644924
	24/05/04 03:47:26 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://addf0afec0d4:36125/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714765644924
	24/05/04 03:47:26 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://addf0afec0d4:36125/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714765644924
	24/05/04 03:47:26 INFO SparkContext: Added JAR file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at spark://addf0afec0d4:36125/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714765644924
	24/05/04 03:47:26 INFO SparkContext: Added JAR file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://addf0afec0d4:36125/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714765644924
	24/05/04 03:47:26 INFO SparkContext: Added JAR file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://addf0afec0d4:36125/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714765644924
	24/05/04 03:47:26 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://addf0afec0d4:36125/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714765644924
	24/05/04 03:47:26 INFO SparkContext: Added JAR file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at spark://addf0afec0d4:36125/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714765644924
	24/05/04 03:47:26 ERROR SparkContext: Error initializing SparkContext.
	java.io.FileNotFoundException: File file:/local_storage/google_news/processing/830-20240503110322.json does not exist
		at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
		at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
		at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
		at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
		at org.apache.spark.SparkContext.addFile(SparkContext.scala:1755)
		at org.apache.spark.SparkContext.$anonfun$new$16(SparkContext.scala:533)
		at org.apache.spark.SparkContext.$anonfun$new$16$adapted(SparkContext.scala:533)
		at scala.collection.immutable.List.foreach(List.scala:431)
		at org.apache.spark.SparkContext.<init>(SparkContext.scala:533)
		at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
		at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
		at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
		at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
		at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:238)
		at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
		at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	24/05/04 03:47:26 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/05/04 03:47:26 INFO SparkUI: Stopped Spark web UI at http://addf0afec0d4:4040
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:27.270 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:47:26 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/05/04 03:47:26 INFO MemoryStore: MemoryStore cleared
	24/05/04 03:47:26 INFO BlockManager: BlockManager stopped
	24/05/04 03:47:26 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/05/04 03:47:26 WARN MetricsSystem: Stopping a MetricsSystem that is not running
	24/05/04 03:47:26 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/05/04 03:47:26 INFO SparkContext: Successfully stopped SparkContext
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2850/spark_gnews_preprocessing.py", line 12, in <module>
	    sc = pyspark.SparkContext(appName="gnews_preprocessing")
	         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/context.py", line 203, in __init__
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/context.py", line 296, in _do_init
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/context.py", line 421, in _initialize_context
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1587, in __call__
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
	py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
	: java.io.FileNotFoundException: File file:/local_storage/google_news/processing/830-20240503110322.json does not exist
		at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
		at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
		at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
		at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
		at org.apache.spark.SparkContext.addFile(SparkContext.scala:1755)
		at org.apache.spark.SparkContext.$anonfun$new$16(SparkContext.scala:533)
		at org.apache.spark.SparkContext.$anonfun$new$16$adapted(SparkContext.scala:533)
		at scala.collection.immutable.List.foreach(List.scala:431)
		at org.apache.spark.SparkContext.<init>(SparkContext.scala:533)
		at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
		at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
		at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
		at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
		at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:238)
		at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
		at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	
	24/05/04 03:47:26 INFO ShutdownHookManager: Shutdown hook called
	24/05/04 03:47:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-170676b3-ba05-4582-a5dc-814046e07616
	24/05/04 03:47:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-a68ae406-c101-4475-b141-0c0000d06254
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:27.272 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2850, processId:727 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:27.272 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:27.273 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:27.273 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:27.275 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:27.288 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:27.288 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:27.289 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2850
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:27.290 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2850
[WI-886][TI-2850] - [INFO] 2024-05-04 03:47:27.291 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-2850] - [INFO] 2024-05-04 03:47:28.190 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=2850, success=true)
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:28.310 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=2851, taskName=preprocessing and kafka, firstSubmitTime=1714765648299, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=31, appIds=null, processInstanceId=886, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name gnews_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_gnews_preprocessing.py\n","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='preprocessing and kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240504'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='2851'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524229'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240504034728'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='886'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/google_news/processing/830-20240503110322.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240503'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}, is_exists=Property{prop='is_exists', direct=IN, type=VARCHAR, value='1'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/830-20240503110322.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.312 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: preprocessing and kafka to wait queue success
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.313 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.316 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.316 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.317 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.317 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714765648317
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.317 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 886_2851
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.317 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 2851,
  "taskName" : "preprocessing and kafka",
  "firstSubmitTime" : 1714765648299,
  "startTime" : 1714765648317,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240504/13475238524230/31/886/2851.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 31,
  "processInstanceId" : 886,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name gnews_preprocessing \\\\\\n    --files ${raw_file_dir} \\\\\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\\\\n    --conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n    spark_gnews_preprocessing.py\\n\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocessing and kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "2851"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524229"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504034728"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "886"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news/processing/830-20240503110322.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240503"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    },
    "is_exists" : {
      "prop" : "is_exists",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1"
    }
  },
  "taskAppId" : "886_2851",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/google_news/processing/830-20240503110322.json\"},{\"prop\":\"is_exists\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"1\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.318 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.318 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.319 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.328 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.330 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.331 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2851 check successfully
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.332 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.334 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py, resourceRelativePath=spark_gnews_preprocessing.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2851/spark_gnews_preprocessing.py)})
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.336 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.336 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.337 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name gnews_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_gnews_preprocessing.py\n",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py",
    "res" : null
  } ]
}
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.337 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.337 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/830-20240503110322.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}] successfully
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.337 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.337 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.340 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.342 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.342 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.342 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master local \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name gnews_preprocessing \
    --files /local_storage/google_news/processing/830-20240503110322.json \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
    --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
    spark_gnews_preprocessing.py

[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.343 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.343 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2851/886_2851.sh
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:28.351 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 813
[WI-0][TI-2851] - [INFO] 2024-05-04 03:47:29.202 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=2851, success=true)
[WI-0][TI-2851] - [INFO] 2024-05-04 03:47:29.209 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=2851)
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:29.351 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:30.121 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7412790697674418 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:31.355 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-405547f5-346b-42e2-b979-93a8099d5462;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:32.153 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7592592592592593 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:32.356 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 720ms :: artifacts dl 18ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-405547f5-346b-42e2-b979-93a8099d5462
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/11ms)
	24/05/04 03:47:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:33.362 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:47:33 INFO SparkContext: Running Spark version 3.5.1
	24/05/04 03:47:33 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/05/04 03:47:33 INFO SparkContext: Java version 1.8.0_402
	24/05/04 03:47:33 INFO ResourceUtils: ==============================================================
	24/05/04 03:47:33 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/05/04 03:47:33 INFO ResourceUtils: ==============================================================
	24/05/04 03:47:33 INFO SparkContext: Submitted application: gnews_preprocessing
	24/05/04 03:47:33 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/05/04 03:47:33 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
	24/05/04 03:47:33 INFO ResourceProfileManager: Added ResourceProfile id: 0
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:34.364 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:47:33 INFO SecurityManager: Changing view acls to: default
	24/05/04 03:47:33 INFO SecurityManager: Changing modify acls to: default
	24/05/04 03:47:33 INFO SecurityManager: Changing view acls groups to: 
	24/05/04 03:47:33 INFO SecurityManager: Changing modify acls groups to: 
	24/05/04 03:47:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
	24/05/04 03:47:33 INFO Utils: Successfully started service 'sparkDriver' on port 43559.
	24/05/04 03:47:33 INFO SparkEnv: Registering MapOutputTracker
	24/05/04 03:47:33 INFO SparkEnv: Registering BlockManagerMaster
	24/05/04 03:47:33 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/05/04 03:47:33 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/05/04 03:47:33 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/05/04 03:47:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-bb40b3d3-6bf7-433c-abb1-592967142b06
	24/05/04 03:47:33 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
	24/05/04 03:47:33 INFO SparkEnv: Registering OutputCommitCoordinator
	24/05/04 03:47:34 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/05/04 03:47:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/05/04 03:47:34 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://addf0afec0d4:43559/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714765653045
	24/05/04 03:47:34 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://addf0afec0d4:43559/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714765653045
	24/05/04 03:47:34 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://addf0afec0d4:43559/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714765653045
	24/05/04 03:47:34 INFO SparkContext: Added JAR file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://addf0afec0d4:43559/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714765653045
	24/05/04 03:47:34 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://addf0afec0d4:43559/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714765653045
	24/05/04 03:47:34 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://addf0afec0d4:43559/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714765653045
	24/05/04 03:47:34 INFO SparkContext: Added JAR file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at spark://addf0afec0d4:43559/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714765653045
	24/05/04 03:47:34 INFO SparkContext: Added JAR file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://addf0afec0d4:43559/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714765653045
	24/05/04 03:47:34 INFO SparkContext: Added JAR file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://addf0afec0d4:43559/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714765653045
	24/05/04 03:47:34 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://addf0afec0d4:43559/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714765653045
	24/05/04 03:47:34 INFO SparkContext: Added JAR file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at spark://addf0afec0d4:43559/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714765653045
	24/05/04 03:47:34 ERROR SparkContext: Error initializing SparkContext.
	java.io.FileNotFoundException: File file:/local_storage/google_news/processing/830-20240503110322.json does not exist
		at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
		at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
		at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
		at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
		at org.apache.spark.SparkContext.addFile(SparkContext.scala:1755)
		at org.apache.spark.SparkContext.$anonfun$new$16(SparkContext.scala:533)
		at org.apache.spark.SparkContext.$anonfun$new$16$adapted(SparkContext.scala:533)
		at scala.collection.immutable.List.foreach(List.scala:431)
		at org.apache.spark.SparkContext.<init>(SparkContext.scala:533)
		at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
		at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
		at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
		at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
		at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:238)
		at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
		at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	24/05/04 03:47:34 INFO SparkContext: SparkContext is stopping with exitCode 0.
[WI-0][TI-0] - [INFO] 2024-05-04 03:47:35.366 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:47:34 INFO SparkUI: Stopped Spark web UI at http://addf0afec0d4:4040
	24/05/04 03:47:34 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/05/04 03:47:34 INFO MemoryStore: MemoryStore cleared
	24/05/04 03:47:34 INFO BlockManager: BlockManager stopped
	24/05/04 03:47:34 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/05/04 03:47:34 WARN MetricsSystem: Stopping a MetricsSystem that is not running
	24/05/04 03:47:34 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/05/04 03:47:34 INFO SparkContext: Successfully stopped SparkContext
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2851/spark_gnews_preprocessing.py", line 12, in <module>
	    sc = pyspark.SparkContext(appName="gnews_preprocessing")
	         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/context.py", line 203, in __init__
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/context.py", line 296, in _do_init
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/context.py", line 421, in _initialize_context
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1587, in __call__
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
	py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
	: java.io.FileNotFoundException: File file:/local_storage/google_news/processing/830-20240503110322.json does not exist
		at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
		at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
		at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
		at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
		at org.apache.spark.SparkContext.addFile(SparkContext.scala:1755)
		at org.apache.spark.SparkContext.$anonfun$new$16(SparkContext.scala:533)
		at org.apache.spark.SparkContext.$anonfun$new$16$adapted(SparkContext.scala:533)
		at scala.collection.immutable.List.foreach(List.scala:431)
		at org.apache.spark.SparkContext.<init>(SparkContext.scala:533)
		at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
		at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
		at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
		at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
		at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:238)
		at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
		at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	
	24/05/04 03:47:34 INFO ShutdownHookManager: Shutdown hook called
	24/05/04 03:47:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-5825d904-ebcc-4cbf-b8ba-90db9bdea6c0
	24/05/04 03:47:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-b0dbe403-a4a2-4051-b98e-914f7fb9f99c
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:35.368 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2851, processId:813 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:35.369 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:35.369 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:35.369 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:35.370 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:35.374 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:35.374 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:35.374 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2851
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:35.375 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_31/886/2851
[WI-886][TI-2851] - [INFO] 2024-05-04 03:47:35.375 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-2851] - [INFO] 2024-05-04 03:47:36.202 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=2851, success=true)
[WI-0][TI-0] - [INFO] 2024-05-04 03:51:14.343 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7867036011080333 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:56:51.737 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7034883720930232 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:56:52.745 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.863013698630137 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:56:53.747 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8554216867469879 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:56:54.749 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.718232044198895 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:56:57.767 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7615176151761518 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:08.865 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=2852, taskName=search for intake article JSON files, firstSubmitTime=1714766228851, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=32, appIds=null, processInstanceId=887, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""},{"prop":"is_exists","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${GNEWS_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set does_file_exist to 1 \nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\n    is_exists=0\nelse\n    is_exists=1\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"\necho \"#{setValue(is_exists=${is_exists})}\"","resourceList":[]}, environmentConfig=null, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake article JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240504'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='2852'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524226'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240504035708'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='887'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240503'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.867 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake article JSON files to wait queue success
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.869 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.880 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.880 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.880 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.880 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714766228880
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.881 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 887_2852
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.882 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 2852,
  "taskName" : "search for intake article JSON files",
  "firstSubmitTime" : 1714766228851,
  "startTime" : 1714766228880,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240504/13475238524230/32/887/2852.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 32,
  "processInstanceId" : 887,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"},{\"prop\":\"is_exists\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${GNEWS_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set does_file_exist to 1 \\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\n    is_exists=0\\nelse\\n    is_exists=1\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\\necho \\\"#{setValue(is_exists=${is_exists})}\\\"\",\"resourceList\":[]}",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake article JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "2852"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524226"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504035708"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "887"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240503"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    }
  },
  "taskAppId" : "887_2852",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.882 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.883 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.883 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.897 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.898 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.900 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2852 check successfully
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.900 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.902 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.903 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.903 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.903 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  }, {
    "prop" : "is_exists",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${GNEWS_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set does_file_exist to 1 \nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\n    is_exists=0\nelse\n    is_exists=1\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"\necho \"#{setValue(is_exists=${is_exists})}\"",
  "resourceList" : [ ]
}
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.904 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.905 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.905 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.905 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.905 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.906 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.906 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.906 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/google_news/in -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set does_file_exist to 1 
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
    is_exists=0
else
    is_exists=1
fi

echo "#{setValue(raw_file_dir=${raw_file_dir})}"
echo "#{setValue(is_exists=${is_exists})}"
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.907 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.907 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2852/887_2852.sh
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:08.913 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 1015
[WI-0][TI-2852] - [INFO] 2024-05-04 03:57:09.185 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=2852, success=true)
[WI-0][TI-2852] - [INFO] 2024-05-04 03:57:09.192 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=2852)
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:09.915 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	#{setValue(raw_file_dir=/local_storage/google_news/in/830-20240503110322.json)}
	#{setValue(is_exists=1)}
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:09.917 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2852, processId:1015 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:09.918 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:09.920 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:09.920 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:09.921 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:09.924 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:09.924 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:09.925 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2852
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:09.925 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2852
[WI-887][TI-2852] - [INFO] 2024-05-04 03:57:09.925 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-2852] - [INFO] 2024-05-04 03:57:10.179 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=2852, success=true)
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:11.317 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=2854, taskName=move to processing, firstSubmitTime=1714766231293, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=32, appIds=null, processInstanceId=887, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the google news processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${GNEWS_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${GNEWS_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to processing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240504'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='2854'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524227'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240504035711'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='887'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/google_news/in/830-20240503110322.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240503'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}, is_exists=Property{prop='is_exists', direct=IN, type=VARCHAR, value='1'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/in/830-20240503110322.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.324 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.326 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.326 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.326 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.326 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714766231326
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.327 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 887_2854
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.324 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to processing to wait queue success
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.331 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 2854,
  "taskName" : "move to processing",
  "firstSubmitTime" : 1714766231293,
  "startTime" : 1714766231326,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240504/13475238524230/32/887/2854.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 32,
  "processInstanceId" : 887,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the google news processing folder\\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\\nfilename=$(basename \\\"${raw_file_dir}\\\")\\nif ! grep -q \\\"${GNEWS_HOME}/processing\\\" <<< \\\"${raw_file_dir}\\\"; then\\n    mv ${raw_file_dir} ${GNEWS_HOME}/processing\\nelse\\n    echo JSON is already in processing\\nfi\\n\\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\\necho \\\"#{setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to processing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "2854"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524227"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504035711"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "887"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news/in/830-20240503110322.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240503"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    },
    "is_exists" : {
      "prop" : "is_exists",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1"
    }
  },
  "taskAppId" : "887_2854",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/google_news/in/830-20240503110322.json\"},{\"prop\":\"is_exists\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"1\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.332 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.332 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.332 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.356 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.360 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.362 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2854 check successfully
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.363 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.363 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.364 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.365 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.366 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the google news processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${GNEWS_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${GNEWS_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\"",
  "resourceList" : [ ]
}
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.366 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.367 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/in/830-20240503110322.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}] successfully
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.367 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.367 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.367 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.368 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.368 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.368 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the google news processing folder
# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error
filename=$(basename "/local_storage/google_news/in/830-20240503110322.json")
if ! grep -q "/local_storage/google_news/processing" <<< "/local_storage/google_news/in/830-20240503110322.json"; then
    mv /local_storage/google_news/in/830-20240503110322.json /local_storage/google_news/processing
else
    echo JSON is already in processing
fi

# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing
echo "#{setValue(raw_file_dir=/local_storage/google_news/processing/${filename})}"
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.368 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.368 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2854/887_2854.sh
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:11.380 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 1029
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:11.382 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-2854] - [INFO] 2024-05-04 03:57:12.199 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=2854, success=true)
[WI-0][TI-2854] - [INFO] 2024-05-04 03:57:12.205 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=2854)
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:12.424 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2854, processId:1029 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:12.426 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:12.427 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:12.427 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:12.427 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:12.434 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:12.434 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:12.435 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2854
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:12.435 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2854
[WI-887][TI-2854] - [INFO] 2024-05-04 03:57:12.435 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-2854] - [INFO] 2024-05-04 03:57:13.193 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=2854, success=true)
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:13.233 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=2855, taskName=preprocessing and kafka, firstSubmitTime=1714766233224, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=32, appIds=null, processInstanceId=887, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name gnews_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_gnews_preprocessing.py\n","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='preprocessing and kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240504'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='2855'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524229'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240504035713'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='887'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/google_news/processing/830-20240503110322.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240503'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}, is_exists=Property{prop='is_exists', direct=IN, type=VARCHAR, value='1'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/830-20240503110322.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.235 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: preprocessing and kafka to wait queue success
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.235 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.236 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.237 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.237 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.237 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714766233237
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.237 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 887_2855
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.237 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 2855,
  "taskName" : "preprocessing and kafka",
  "firstSubmitTime" : 1714766233224,
  "startTime" : 1714766233237,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240504/13475238524230/32/887/2855.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 32,
  "processInstanceId" : 887,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name gnews_preprocessing \\\\\\n    --files ${raw_file_dir} \\\\\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\\\\n    --conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n    spark_gnews_preprocessing.py\\n\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocessing and kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "2855"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524229"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504035713"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "887"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news/processing/830-20240503110322.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240503"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    },
    "is_exists" : {
      "prop" : "is_exists",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1"
    }
  },
  "taskAppId" : "887_2855",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/google_news/processing/830-20240503110322.json\"},{\"prop\":\"is_exists\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"1\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.238 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.238 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.238 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.241 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.242 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.243 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2855 check successfully
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.243 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.245 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py, resourceRelativePath=spark_gnews_preprocessing.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2855/spark_gnews_preprocessing.py)})
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.250 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.258 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.259 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name gnews_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_gnews_preprocessing.py\n",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py",
    "res" : null
  } ]
}
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.259 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.260 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/830-20240503110322.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}] successfully
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.260 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.260 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.261 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.262 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.262 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.262 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master local \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name gnews_preprocessing \
    --files /local_storage/google_news/processing/830-20240503110322.json \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
    --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
    spark_gnews_preprocessing.py

[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.263 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.263 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2855/887_2855.sh
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:13.267 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 1048
[WI-0][TI-2855] - [INFO] 2024-05-04 03:57:14.196 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=2855, success=true)
[WI-0][TI-2855] - [INFO] 2024-05-04 03:57:14.217 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=2855)
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:14.268 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:15.270 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-cbb185ce-868b-44dc-b6f1-16516352010c;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:16.273 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 462ms :: artifacts dl 10ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-cbb185ce-868b-44dc-b6f1-16516352010c
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/7ms)
	24/05/04 03:57:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:17.275 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:57:16 INFO SparkContext: Running Spark version 3.5.1
	24/05/04 03:57:16 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/05/04 03:57:16 INFO SparkContext: Java version 1.8.0_402
	24/05/04 03:57:16 INFO ResourceUtils: ==============================================================
	24/05/04 03:57:16 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/05/04 03:57:16 INFO ResourceUtils: ==============================================================
	24/05/04 03:57:16 INFO SparkContext: Submitted application: gnews_preprocessing
	24/05/04 03:57:16 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/05/04 03:57:16 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
	24/05/04 03:57:16 INFO ResourceProfileManager: Added ResourceProfile id: 0
	24/05/04 03:57:16 INFO SecurityManager: Changing view acls to: default
	24/05/04 03:57:16 INFO SecurityManager: Changing modify acls to: default
	24/05/04 03:57:16 INFO SecurityManager: Changing view acls groups to: 
	24/05/04 03:57:16 INFO SecurityManager: Changing modify acls groups to: 
	24/05/04 03:57:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
	24/05/04 03:57:16 INFO Utils: Successfully started service 'sparkDriver' on port 42971.
	24/05/04 03:57:16 INFO SparkEnv: Registering MapOutputTracker
	24/05/04 03:57:17 INFO SparkEnv: Registering BlockManagerMaster
	24/05/04 03:57:17 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/05/04 03:57:17 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/05/04 03:57:17 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/05/04 03:57:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-dd0b65e4-3f43-4ea9-9784-6c3b910a0e2c
	24/05/04 03:57:17 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
	24/05/04 03:57:17 INFO SparkEnv: Registering OutputCommitCoordinator
	24/05/04 03:57:17 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/05/04 03:57:17 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:18.277 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:57:17 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://addf0afec0d4:42971/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://addf0afec0d4:42971/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://addf0afec0d4:42971/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO SparkContext: Added JAR file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://addf0afec0d4:42971/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://addf0afec0d4:42971/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://addf0afec0d4:42971/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO SparkContext: Added JAR file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at spark://addf0afec0d4:42971/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO SparkContext: Added JAR file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://addf0afec0d4:42971/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO SparkContext: Added JAR file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://addf0afec0d4:42971/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://addf0afec0d4:42971/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO SparkContext: Added JAR file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at spark://addf0afec0d4:42971/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO SparkContext: Added file file:///local_storage/google_news/processing/830-20240503110322.json at file:///local_storage/google_news/processing/830-20240503110322.json with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: Copying /local_storage/google_news/processing/830-20240503110322.json to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/830-20240503110322.json
	24/05/04 03:57:17 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/05/04 03:57:17 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/05/04 03:57:17 INFO SparkContext: Added file file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: Copying /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.apache.kafka_kafka-clients-3.4.1.jar
	24/05/04 03:57:17 INFO SparkContext: Added file file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: Copying /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/com.google.code.findbugs_jsr305-3.0.0.jar
	24/05/04 03:57:17 INFO SparkContext: Added file file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: Copying /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.apache.commons_commons-pool2-2.11.1.jar
	24/05/04 03:57:17 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/05/04 03:57:17 INFO SparkContext: Added file file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: Copying /tmp/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.lz4_lz4-java-1.8.0.jar
	24/05/04 03:57:17 INFO SparkContext: Added file file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: Copying /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/05/04 03:57:17 INFO SparkContext: Added file file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: Copying /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.slf4j_slf4j-api-2.0.7.jar
	24/05/04 03:57:17 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/05/04 03:57:17 INFO SparkContext: Added file file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: Copying /tmp/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/commons-logging_commons-logging-1.1.3.jar
	24/05/04 03:57:17 INFO Executor: Starting executor ID driver on host addf0afec0d4
	24/05/04 03:57:17 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/05/04 03:57:17 INFO Executor: Java version 1.8.0_402
	24/05/04 03:57:17 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/05/04 03:57:17 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@53aa1333 for default.
	24/05/04 03:57:17 INFO Executor: Fetching file:///local_storage/google_news/processing/830-20240503110322.json with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: /local_storage/google_news/processing/830-20240503110322.json has been previously copied to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/830-20240503110322.json
	24/05/04 03:57:17 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/05/04 03:57:17 INFO Executor: Fetching file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.apache.commons_commons-pool2-2.11.1.jar
	24/05/04 03:57:17 INFO Executor: Fetching file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.apache.kafka_kafka-clients-3.4.1.jar
	24/05/04 03:57:17 INFO Executor: Fetching file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: /tmp/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.lz4_lz4-java-1.8.0.jar
	24/05/04 03:57:17 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/05/04 03:57:17 INFO Executor: Fetching file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/05/04 03:57:17 INFO Executor: Fetching file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.slf4j_slf4j-api-2.0.7.jar
	24/05/04 03:57:17 INFO Executor: Fetching file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/com.google.code.findbugs_jsr305-3.0.0.jar
	24/05/04 03:57:17 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/05/04 03:57:17 INFO Executor: Fetching file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: /tmp/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/commons-logging_commons-logging-1.1.3.jar
	24/05/04 03:57:17 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/05/04 03:57:17 INFO Executor: Fetching spark://addf0afec0d4:42971/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO TransportClientFactory: Successfully created connection to addf0afec0d4/172.18.1.1:42971 after 34 ms (0 ms spent in bootstraps)
	24/05/04 03:57:17 INFO Utils: Fetching spark://addf0afec0d4:42971/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/fetchFileTemp2531578112075449112.tmp
	24/05/04 03:57:17 INFO Utils: /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/fetchFileTemp2531578112075449112.tmp has been previously copied to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.apache.commons_commons-pool2-2.11.1.jar
	24/05/04 03:57:17 INFO Executor: Adding file:/tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
	24/05/04 03:57:17 INFO Executor: Fetching spark://addf0afec0d4:42971/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: Fetching spark://addf0afec0d4:42971/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/fetchFileTemp2550873636547932399.tmp
	24/05/04 03:57:17 INFO Utils: /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/fetchFileTemp2550873636547932399.tmp has been previously copied to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/05/04 03:57:17 INFO Executor: Adding file:/tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
	24/05/04 03:57:17 INFO Executor: Fetching spark://addf0afec0d4:42971/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: Fetching spark://addf0afec0d4:42971/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/fetchFileTemp9113170255206010767.tmp
	24/05/04 03:57:17 INFO Utils: /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/fetchFileTemp9113170255206010767.tmp has been previously copied to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.slf4j_slf4j-api-2.0.7.jar
	24/05/04 03:57:17 INFO Executor: Adding file:/tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.slf4j_slf4j-api-2.0.7.jar to class loader default
	24/05/04 03:57:17 INFO Executor: Fetching spark://addf0afec0d4:42971/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: Fetching spark://addf0afec0d4:42971/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/fetchFileTemp2371438482925289160.tmp
	24/05/04 03:57:17 INFO Utils: /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/fetchFileTemp2371438482925289160.tmp has been previously copied to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/05/04 03:57:17 INFO Executor: Adding file:/tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
	24/05/04 03:57:17 INFO Executor: Fetching spark://addf0afec0d4:42971/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: Fetching spark://addf0afec0d4:42971/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/fetchFileTemp4095872042551450257.tmp
	24/05/04 03:57:17 INFO Utils: /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/fetchFileTemp4095872042551450257.tmp has been previously copied to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/commons-logging_commons-logging-1.1.3.jar
	24/05/04 03:57:17 INFO Executor: Adding file:/tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/commons-logging_commons-logging-1.1.3.jar to class loader default
	24/05/04 03:57:17 INFO Executor: Fetching spark://addf0afec0d4:42971/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714766236408
	24/05/04 03:57:17 INFO Utils: Fetching spark://addf0afec0d4:42971/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/fetchFileTemp8512302108993212119.tmp
	24/05/04 03:57:18 INFO Utils: /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/fetchFileTemp8512302108993212119.tmp has been previously copied to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/05/04 03:57:18 INFO Executor: Adding file:/tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
	24/05/04 03:57:18 INFO Executor: Fetching spark://addf0afec0d4:42971/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714766236408
	24/05/04 03:57:18 INFO Utils: Fetching spark://addf0afec0d4:42971/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/fetchFileTemp708489760425307676.tmp
	24/05/04 03:57:18 INFO Utils: /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/fetchFileTemp708489760425307676.tmp has been previously copied to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.lz4_lz4-java-1.8.0.jar
	24/05/04 03:57:18 INFO Executor: Adding file:/tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.lz4_lz4-java-1.8.0.jar to class loader default
	24/05/04 03:57:18 INFO Executor: Fetching spark://addf0afec0d4:42971/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714766236408
	24/05/04 03:57:18 INFO Utils: Fetching spark://addf0afec0d4:42971/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/fetchFileTemp3612971656896117807.tmp
	24/05/04 03:57:18 INFO Utils: /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/fetchFileTemp3612971656896117807.tmp has been previously copied to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.apache.kafka_kafka-clients-3.4.1.jar
	24/05/04 03:57:18 INFO Executor: Adding file:/tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
	24/05/04 03:57:18 INFO Executor: Fetching spark://addf0afec0d4:42971/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714766236408
	24/05/04 03:57:18 INFO Utils: Fetching spark://addf0afec0d4:42971/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/fetchFileTemp7745140558778298942.tmp
	24/05/04 03:57:18 INFO Utils: /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/fetchFileTemp7745140558778298942.tmp has been previously copied to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/05/04 03:57:18 INFO Executor: Adding file:/tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
	24/05/04 03:57:18 INFO Executor: Fetching spark://addf0afec0d4:42971/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714766236408
	24/05/04 03:57:18 INFO Utils: Fetching spark://addf0afec0d4:42971/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/fetchFileTemp7663527471021261379.tmp
	24/05/04 03:57:18 INFO Utils: /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/fetchFileTemp7663527471021261379.tmp has been previously copied to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/com.google.code.findbugs_jsr305-3.0.0.jar
	24/05/04 03:57:18 INFO Executor: Adding file:/tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
	24/05/04 03:57:18 INFO Executor: Fetching spark://addf0afec0d4:42971/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766236408
	24/05/04 03:57:18 INFO Utils: Fetching spark://addf0afec0d4:42971/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/fetchFileTemp631589710120940367.tmp
	24/05/04 03:57:18 INFO Utils: /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/fetchFileTemp631589710120940367.tmp has been previously copied to /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:19.281 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:57:18 INFO Executor: Adding file:/tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/userFiles-fcff876b-953b-4d1d-a8d8-587c731592c6/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
	24/05/04 03:57:18 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37471.
	24/05/04 03:57:18 INFO NettyBlockTransferService: Server created on addf0afec0d4:37471
	24/05/04 03:57:18 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/05/04 03:57:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, addf0afec0d4, 37471, None)
	24/05/04 03:57:18 INFO BlockManagerMasterEndpoint: Registering block manager addf0afec0d4:37471 with 366.3 MiB RAM, BlockManagerId(driver, addf0afec0d4, 37471, None)
	24/05/04 03:57:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, addf0afec0d4, 37471, None)
	24/05/04 03:57:18 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, addf0afec0d4, 37471, None)
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:20.284 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:57:19 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 345.5 KiB, free 366.0 MiB)
	24/05/04 03:57:19 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 365.9 MiB)
	24/05/04 03:57:19 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on addf0afec0d4:37471 (size: 32.7 KiB, free: 366.3 MiB)
	24/05/04 03:57:19 INFO SparkContext: Created broadcast 0 from wholeTextFiles at NativeMethodAccessorImpl.java:0
	24/05/04 03:57:19 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/05/04 03:57:19 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2855/spark-warehouse'.
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:24.287 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:57:24 INFO CodeGenerator: Code generated in 267.456829 ms
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:25.290 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2855/spark_gnews_preprocessing.py", line 39, in <module>
	    raw_df = spark.read.json(raw_df)
	             ^^^^^^^^^^^^^^^^^^^^^^^
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 440, in json
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
	py4j.protocol.Py4JJavaError: An error occurred while calling o39.json.
	: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2855/830-20240503110322.json
		at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:340)
		at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:279)
		at org.apache.spark.input.WholeTextFileInputFormat.setMinPartitions(WholeTextFileInputFormat.scala:52)
		at org.apache.spark.rdd.WholeTextFileRDD.getPartitions(WholeTextFileRDD.scala:54)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:57)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.sql.execution.SQLExecutionRDD.getPartitions(SQLExecutionRDD.scala:44)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.infer(JsonInferSchema.scala:120)
		at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$5(JsonDataSource.scala:109)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:109)
		at org.apache.spark.sql.DataFrameReader.$anonfun$json$4(DataFrameReader.scala:416)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:416)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:391)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:377)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:282)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: java.io.IOException: Input path does not exist: file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2855/830-20240503110322.json
		at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:313)
		... 56 more
	
	24/05/04 03:57:24 INFO SparkContext: Invoking stop() from shutdown hook
	24/05/04 03:57:24 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/05/04 03:57:24 INFO SparkUI: Stopped Spark web UI at http://addf0afec0d4:4040
	24/05/04 03:57:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/05/04 03:57:24 INFO MemoryStore: MemoryStore cleared
	24/05/04 03:57:24 INFO BlockManager: BlockManager stopped
	24/05/04 03:57:24 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/05/04 03:57:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/05/04 03:57:24 INFO SparkContext: Successfully stopped SparkContext
	24/05/04 03:57:24 INFO ShutdownHookManager: Shutdown hook called
	24/05/04 03:57:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-9cfc39a8-b90f-4bb0-8b60-b010f583ff41
	24/05/04 03:57:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10
	24/05/04 03:57:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-14224247-f28e-45bc-903d-5358357a0e10/pyspark-5351dfc8-6c7d-4a0e-87ea-8b056c3e5ca4
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:25.294 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2855, processId:1048 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:25.295 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:25.295 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:25.295 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:25.296 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:25.300 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:25.300 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:25.300 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2855
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:25.301 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2855
[WI-887][TI-2855] - [INFO] 2024-05-04 03:57:25.302 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-2855] - [INFO] 2024-05-04 03:57:26.233 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=2855, success=true)
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:26.289 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=2856, taskName=preprocessing and kafka, firstSubmitTime=1714766246275, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=32, appIds=null, processInstanceId=887, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name gnews_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_gnews_preprocessing.py\n","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='preprocessing and kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240504'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='2856'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524229'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240504035726'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='887'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/google_news/processing/830-20240503110322.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240503'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}, is_exists=Property{prop='is_exists', direct=IN, type=VARCHAR, value='1'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/830-20240503110322.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.290 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: preprocessing and kafka to wait queue success
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.290 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.291 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.292 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.292 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.292 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714766246292
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.292 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 887_2856
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.292 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 2856,
  "taskName" : "preprocessing and kafka",
  "firstSubmitTime" : 1714766246275,
  "startTime" : 1714766246292,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240504/13475238524230/32/887/2856.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 32,
  "processInstanceId" : 887,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name gnews_preprocessing \\\\\\n    --files ${raw_file_dir} \\\\\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\\\\n    --conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n    spark_gnews_preprocessing.py\\n\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocessing and kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "2856"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524229"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504035726"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "887"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news/processing/830-20240503110322.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240503"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    },
    "is_exists" : {
      "prop" : "is_exists",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1"
    }
  },
  "taskAppId" : "887_2856",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/google_news/processing/830-20240503110322.json\"},{\"prop\":\"is_exists\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"1\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.293 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.293 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.293 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.297 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.297 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.298 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2856 check successfully
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.298 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.301 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py, resourceRelativePath=spark_gnews_preprocessing.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2856/spark_gnews_preprocessing.py)})
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.302 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.302 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.303 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name gnews_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_gnews_preprocessing.py\n",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py",
    "res" : null
  } ]
}
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.303 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.304 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/830-20240503110322.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}] successfully
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.304 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.304 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.304 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.305 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.305 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.305 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master local \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name gnews_preprocessing \
    --files /local_storage/google_news/processing/830-20240503110322.json \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
    --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
    spark_gnews_preprocessing.py

[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.305 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.305 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2856/887_2856.sh
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:26.312 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 1224
[WI-0][TI-2856] - [INFO] 2024-05-04 03:57:27.240 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=2856, success=true)
[WI-0][TI-2856] - [INFO] 2024-05-04 03:57:27.246 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=2856)
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:27.314 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:28.327 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-8e49daf7-83c7-4740-939e-68bcf9304705;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:29.328 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 351ms :: artifacts dl 11ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-8e49daf7-83c7-4740-939e-68bcf9304705
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/8ms)
	24/05/04 03:57:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:30.334 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:57:29 INFO SparkContext: Running Spark version 3.5.1
	24/05/04 03:57:29 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/05/04 03:57:29 INFO SparkContext: Java version 1.8.0_402
	24/05/04 03:57:29 INFO ResourceUtils: ==============================================================
	24/05/04 03:57:29 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/05/04 03:57:29 INFO ResourceUtils: ==============================================================
	24/05/04 03:57:29 INFO SparkContext: Submitted application: gnews_preprocessing
	24/05/04 03:57:29 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/05/04 03:57:29 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
	24/05/04 03:57:29 INFO ResourceProfileManager: Added ResourceProfile id: 0
	24/05/04 03:57:29 INFO SecurityManager: Changing view acls to: default
	24/05/04 03:57:29 INFO SecurityManager: Changing modify acls to: default
	24/05/04 03:57:29 INFO SecurityManager: Changing view acls groups to: 
	24/05/04 03:57:29 INFO SecurityManager: Changing modify acls groups to: 
	24/05/04 03:57:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
	24/05/04 03:57:29 INFO Utils: Successfully started service 'sparkDriver' on port 44319.
	24/05/04 03:57:29 INFO SparkEnv: Registering MapOutputTracker
	24/05/04 03:57:29 INFO SparkEnv: Registering BlockManagerMaster
	24/05/04 03:57:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/05/04 03:57:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/05/04 03:57:30 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/05/04 03:57:30 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d945fb3e-2654-4c7c-8196-1aef2a71468f
	24/05/04 03:57:30 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
	24/05/04 03:57:30 INFO SparkEnv: Registering OutputCommitCoordinator
	24/05/04 03:57:30 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/05/04 03:57:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:31.336 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:57:30 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://addf0afec0d4:44319/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://addf0afec0d4:44319/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://addf0afec0d4:44319/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO SparkContext: Added JAR file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://addf0afec0d4:44319/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://addf0afec0d4:44319/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://addf0afec0d4:44319/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO SparkContext: Added JAR file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at spark://addf0afec0d4:44319/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO SparkContext: Added JAR file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://addf0afec0d4:44319/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO SparkContext: Added JAR file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://addf0afec0d4:44319/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://addf0afec0d4:44319/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO SparkContext: Added JAR file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at spark://addf0afec0d4:44319/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO SparkContext: Added file file:///local_storage/google_news/processing/830-20240503110322.json at file:///local_storage/google_news/processing/830-20240503110322.json with timestamp 1714766249444
	24/05/04 03:57:30 INFO Utils: Copying /local_storage/google_news/processing/830-20240503110322.json to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/830-20240503110322.json
	24/05/04 03:57:30 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/05/04 03:57:30 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/05/04 03:57:30 INFO SparkContext: Added file file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO Utils: Copying /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.apache.kafka_kafka-clients-3.4.1.jar
	24/05/04 03:57:30 INFO SparkContext: Added file file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO Utils: Copying /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/com.google.code.findbugs_jsr305-3.0.0.jar
	24/05/04 03:57:30 INFO SparkContext: Added file file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO Utils: Copying /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.apache.commons_commons-pool2-2.11.1.jar
	24/05/04 03:57:30 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/05/04 03:57:30 INFO SparkContext: Added file file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO Utils: Copying /tmp/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.lz4_lz4-java-1.8.0.jar
	24/05/04 03:57:30 INFO SparkContext: Added file file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO Utils: Copying /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/05/04 03:57:30 INFO SparkContext: Added file file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO Utils: Copying /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.slf4j_slf4j-api-2.0.7.jar
	24/05/04 03:57:30 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/05/04 03:57:30 INFO SparkContext: Added file file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO Utils: Copying /tmp/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/commons-logging_commons-logging-1.1.3.jar
	24/05/04 03:57:30 INFO Executor: Starting executor ID driver on host addf0afec0d4
	24/05/04 03:57:30 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/05/04 03:57:30 INFO Executor: Java version 1.8.0_402
	24/05/04 03:57:30 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/05/04 03:57:30 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@66261a59 for default.
	24/05/04 03:57:30 INFO Executor: Fetching file:///local_storage/google_news/processing/830-20240503110322.json with timestamp 1714766249444
	24/05/04 03:57:30 INFO Utils: /local_storage/google_news/processing/830-20240503110322.json has been previously copied to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/830-20240503110322.json
	24/05/04 03:57:30 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/05/04 03:57:30 INFO Executor: Fetching file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO Utils: /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.apache.commons_commons-pool2-2.11.1.jar
	24/05/04 03:57:30 INFO Executor: Fetching file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO Utils: /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.apache.kafka_kafka-clients-3.4.1.jar
	24/05/04 03:57:30 INFO Executor: Fetching file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO Utils: /tmp/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.lz4_lz4-java-1.8.0.jar
	24/05/04 03:57:30 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO Utils: /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/05/04 03:57:30 INFO Executor: Fetching file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO Utils: /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/05/04 03:57:30 INFO Executor: Fetching file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO Utils: /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.slf4j_slf4j-api-2.0.7.jar
	24/05/04 03:57:30 INFO Executor: Fetching file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO Utils: /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/com.google.code.findbugs_jsr305-3.0.0.jar
	24/05/04 03:57:30 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO Utils: /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/05/04 03:57:30 INFO Executor: Fetching file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO Utils: /tmp/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/commons-logging_commons-logging-1.1.3.jar
	24/05/04 03:57:30 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/05/04 03:57:30 INFO Executor: Fetching spark://addf0afec0d4:44319/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO TransportClientFactory: Successfully created connection to addf0afec0d4/172.18.1.1:44319 after 36 ms (0 ms spent in bootstraps)
	24/05/04 03:57:30 INFO Utils: Fetching spark://addf0afec0d4:44319/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/fetchFileTemp2670371962916670066.tmp
	24/05/04 03:57:30 INFO Utils: /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/fetchFileTemp2670371962916670066.tmp has been previously copied to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/commons-logging_commons-logging-1.1.3.jar
	24/05/04 03:57:30 INFO Executor: Adding file:/tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/commons-logging_commons-logging-1.1.3.jar to class loader default
	24/05/04 03:57:30 INFO Executor: Fetching spark://addf0afec0d4:44319/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO Utils: Fetching spark://addf0afec0d4:44319/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/fetchFileTemp8923887540586624177.tmp
	24/05/04 03:57:30 INFO Utils: /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/fetchFileTemp8923887540586624177.tmp has been previously copied to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.slf4j_slf4j-api-2.0.7.jar
	24/05/04 03:57:30 INFO Executor: Adding file:/tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.slf4j_slf4j-api-2.0.7.jar to class loader default
	24/05/04 03:57:30 INFO Executor: Fetching spark://addf0afec0d4:44319/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714766249444
	24/05/04 03:57:30 INFO Utils: Fetching spark://addf0afec0d4:44319/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/fetchFileTemp4426466065717333643.tmp
	24/05/04 03:57:31 INFO Utils: /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/fetchFileTemp4426466065717333643.tmp has been previously copied to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/05/04 03:57:31 INFO Executor: Adding file:/tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
	24/05/04 03:57:31 INFO Executor: Fetching spark://addf0afec0d4:44319/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766249444
	24/05/04 03:57:31 INFO Utils: Fetching spark://addf0afec0d4:44319/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/fetchFileTemp6613653740212353998.tmp
	24/05/04 03:57:31 INFO Utils: /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/fetchFileTemp6613653740212353998.tmp has been previously copied to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/05/04 03:57:31 INFO Executor: Adding file:/tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
	24/05/04 03:57:31 INFO Executor: Fetching spark://addf0afec0d4:44319/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714766249444
	24/05/04 03:57:31 INFO Utils: Fetching spark://addf0afec0d4:44319/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/fetchFileTemp6037689789367444803.tmp
	24/05/04 03:57:31 INFO Utils: /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/fetchFileTemp6037689789367444803.tmp has been previously copied to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/05/04 03:57:31 INFO Executor: Adding file:/tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
	24/05/04 03:57:31 INFO Executor: Fetching spark://addf0afec0d4:44319/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714766249444
	24/05/04 03:57:31 INFO Utils: Fetching spark://addf0afec0d4:44319/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/fetchFileTemp4200437220042178889.tmp
	24/05/04 03:57:31 INFO Utils: /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/fetchFileTemp4200437220042178889.tmp has been previously copied to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.apache.kafka_kafka-clients-3.4.1.jar
	24/05/04 03:57:31 INFO Executor: Adding file:/tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
	24/05/04 03:57:31 INFO Executor: Fetching spark://addf0afec0d4:44319/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714766249444
	24/05/04 03:57:31 INFO Utils: Fetching spark://addf0afec0d4:44319/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/fetchFileTemp4845855519535287291.tmp
	24/05/04 03:57:31 INFO Utils: /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/fetchFileTemp4845855519535287291.tmp has been previously copied to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/05/04 03:57:31 INFO Executor: Adding file:/tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
	24/05/04 03:57:31 INFO Executor: Fetching spark://addf0afec0d4:44319/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714766249444
	24/05/04 03:57:31 INFO Utils: Fetching spark://addf0afec0d4:44319/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/fetchFileTemp651844080505067723.tmp
	24/05/04 03:57:31 INFO Utils: /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/fetchFileTemp651844080505067723.tmp has been previously copied to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/com.google.code.findbugs_jsr305-3.0.0.jar
	24/05/04 03:57:31 INFO Executor: Adding file:/tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
	24/05/04 03:57:31 INFO Executor: Fetching spark://addf0afec0d4:44319/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714766249444
	24/05/04 03:57:31 INFO Utils: Fetching spark://addf0afec0d4:44319/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/fetchFileTemp3028528838711937348.tmp
	24/05/04 03:57:31 INFO Utils: /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/fetchFileTemp3028528838711937348.tmp has been previously copied to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.lz4_lz4-java-1.8.0.jar
	24/05/04 03:57:31 INFO Executor: Adding file:/tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.lz4_lz4-java-1.8.0.jar to class loader default
	24/05/04 03:57:31 INFO Executor: Fetching spark://addf0afec0d4:44319/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714766249444
	24/05/04 03:57:31 INFO Utils: Fetching spark://addf0afec0d4:44319/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/fetchFileTemp4080041352160963475.tmp
	24/05/04 03:57:31 INFO Utils: /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/fetchFileTemp4080041352160963475.tmp has been previously copied to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.apache.commons_commons-pool2-2.11.1.jar
	24/05/04 03:57:31 INFO Executor: Adding file:/tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
	24/05/04 03:57:31 INFO Executor: Fetching spark://addf0afec0d4:44319/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766249444
	24/05/04 03:57:31 INFO Utils: Fetching spark://addf0afec0d4:44319/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/fetchFileTemp6704852174346338750.tmp
	24/05/04 03:57:31 INFO Utils: /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/fetchFileTemp6704852174346338750.tmp has been previously copied to /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/05/04 03:57:31 INFO Executor: Adding file:/tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/userFiles-0f9b0422-8431-4ea6-9b9e-2bbad1385bbf/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
	24/05/04 03:57:31 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46633.
	24/05/04 03:57:31 INFO NettyBlockTransferService: Server created on addf0afec0d4:46633
	24/05/04 03:57:31 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/05/04 03:57:31 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, addf0afec0d4, 46633, None)
	24/05/04 03:57:31 INFO BlockManagerMasterEndpoint: Registering block manager addf0afec0d4:46633 with 366.3 MiB RAM, BlockManagerId(driver, addf0afec0d4, 46633, None)
	24/05/04 03:57:31 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, addf0afec0d4, 46633, None)
	24/05/04 03:57:31 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, addf0afec0d4, 46633, None)
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:32.339 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:57:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 345.5 KiB, free 366.0 MiB)
	24/05/04 03:57:32 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 365.9 MiB)
	24/05/04 03:57:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on addf0afec0d4:46633 (size: 32.7 KiB, free: 366.3 MiB)
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:33.343 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:57:32 INFO SparkContext: Created broadcast 0 from wholeTextFiles at NativeMethodAccessorImpl.java:0
	24/05/04 03:57:32 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/05/04 03:57:32 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2856/spark-warehouse'.
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:36.362 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:57:35 INFO CodeGenerator: Code generated in 245.595906 ms
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2856/spark_gnews_preprocessing.py", line 39, in <module>
	    raw_df = spark.read.json(raw_df)
	             ^^^^^^^^^^^^^^^^^^^^^^^
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 440, in json
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
	py4j.protocol.Py4JJavaError: An error occurred while calling o39.json.
	: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2856/830-20240503110322.json
		at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:340)
		at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:279)
		at org.apache.spark.input.WholeTextFileInputFormat.setMinPartitions(WholeTextFileInputFormat.scala:52)
		at org.apache.spark.rdd.WholeTextFileRDD.getPartitions(WholeTextFileRDD.scala:54)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:57)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.sql.execution.SQLExecutionRDD.getPartitions(SQLExecutionRDD.scala:44)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.infer(JsonInferSchema.scala:120)
		at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$5(JsonDataSource.scala:109)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:109)
		at org.apache.spark.sql.DataFrameReader.$anonfun$json$4(DataFrameReader.scala:416)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:416)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:391)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:377)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:282)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: java.io.IOException: Input path does not exist: file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2856/830-20240503110322.json
		at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:313)
		... 56 more
	
	24/05/04 03:57:35 INFO SparkContext: Invoking stop() from shutdown hook
	24/05/04 03:57:35 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/05/04 03:57:35 INFO SparkUI: Stopped Spark web UI at http://addf0afec0d4:4040
	24/05/04 03:57:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/05/04 03:57:35 INFO MemoryStore: MemoryStore cleared
	24/05/04 03:57:35 INFO BlockManager: BlockManager stopped
	24/05/04 03:57:35 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/05/04 03:57:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/05/04 03:57:35 INFO SparkContext: Successfully stopped SparkContext
	24/05/04 03:57:35 INFO ShutdownHookManager: Shutdown hook called
	24/05/04 03:57:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-afc0bf9b-5029-4520-8cc5-ccbac5600117
	24/05/04 03:57:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2
	24/05/04 03:57:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-55676c86-4563-4ac8-b518-2c9900140cc2/pyspark-6fc34ef9-1eac-46fc-ae6f-80ef35dc961f
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:36.366 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2856, processId:1224 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:36.367 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:36.367 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:36.367 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:36.368 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:36.372 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:36.372 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:36.372 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2856
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:36.373 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2856
[WI-887][TI-2856] - [INFO] 2024-05-04 03:57:36.373 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-2856] - [INFO] 2024-05-04 03:57:37.316 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=2856, success=true)
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:37.437 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=2857, taskName=preprocessing and kafka, firstSubmitTime=1714766257421, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=32, appIds=null, processInstanceId=887, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name gnews_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_gnews_preprocessing.py\n","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='preprocessing and kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240504'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='2857'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524229'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240504035737'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='887'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/google_news/processing/830-20240503110322.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240503'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}, is_exists=Property{prop='is_exists', direct=IN, type=VARCHAR, value='1'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/830-20240503110322.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.438 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: preprocessing and kafka to wait queue success
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.439 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.441 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.441 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.441 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.442 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714766257442
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.442 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 887_2857
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.446 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 2857,
  "taskName" : "preprocessing and kafka",
  "firstSubmitTime" : 1714766257421,
  "startTime" : 1714766257442,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240504/13475238524230/32/887/2857.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 32,
  "processInstanceId" : 887,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name gnews_preprocessing \\\\\\n    --files ${raw_file_dir} \\\\\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\\\\n    --conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n    spark_gnews_preprocessing.py\\n\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocessing and kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "2857"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524229"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504035737"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "887"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news/processing/830-20240503110322.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240503"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    },
    "is_exists" : {
      "prop" : "is_exists",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1"
    }
  },
  "taskAppId" : "887_2857",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/google_news/processing/830-20240503110322.json\"},{\"prop\":\"is_exists\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"1\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.447 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.447 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.447 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.450 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.451 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.451 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2857 check successfully
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.451 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.459 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py, resourceRelativePath=spark_gnews_preprocessing.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2857/spark_gnews_preprocessing.py)})
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.460 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.460 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.461 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name gnews_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_gnews_preprocessing.py\n",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py",
    "res" : null
  } ]
}
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.461 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.462 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/830-20240503110322.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}] successfully
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.462 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.462 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.462 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.462 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.463 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.463 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master local \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name gnews_preprocessing \
    --files /local_storage/google_news/processing/830-20240503110322.json \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
    --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
    spark_gnews_preprocessing.py

[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.463 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.463 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2857/887_2857.sh
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:37.465 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 1399
[WI-0][TI-2857] - [INFO] 2024-05-04 03:57:38.324 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=2857, success=true)
[WI-0][TI-2857] - [INFO] 2024-05-04 03:57:38.338 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=2857)
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:38.466 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:39.469 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-4fbffcc8-3fb9-4591-abc8-20447fe64837;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:40.510 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 394ms :: artifacts dl 9ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-4fbffcc8-3fb9-4591-abc8-20447fe64837
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/6ms)
	24/05/04 03:57:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:41.512 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:57:40 INFO SparkContext: Running Spark version 3.5.1
	24/05/04 03:57:40 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/05/04 03:57:40 INFO SparkContext: Java version 1.8.0_402
	24/05/04 03:57:40 INFO ResourceUtils: ==============================================================
	24/05/04 03:57:40 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/05/04 03:57:40 INFO ResourceUtils: ==============================================================
	24/05/04 03:57:40 INFO SparkContext: Submitted application: gnews_preprocessing
	24/05/04 03:57:40 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/05/04 03:57:40 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
	24/05/04 03:57:40 INFO ResourceProfileManager: Added ResourceProfile id: 0
	24/05/04 03:57:40 INFO SecurityManager: Changing view acls to: default
	24/05/04 03:57:40 INFO SecurityManager: Changing modify acls to: default
	24/05/04 03:57:40 INFO SecurityManager: Changing view acls groups to: 
	24/05/04 03:57:40 INFO SecurityManager: Changing modify acls groups to: 
	24/05/04 03:57:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
	24/05/04 03:57:41 INFO Utils: Successfully started service 'sparkDriver' on port 46167.
	24/05/04 03:57:41 INFO SparkEnv: Registering MapOutputTracker
	24/05/04 03:57:41 INFO SparkEnv: Registering BlockManagerMaster
	24/05/04 03:57:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/05/04 03:57:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/05/04 03:57:41 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/05/04 03:57:41 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-77b6421a-06e6-4497-81e5-edbd089d32c7
	24/05/04 03:57:41 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
	24/05/04 03:57:41 INFO SparkEnv: Registering OutputCommitCoordinator
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:42.513 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:57:41 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/05/04 03:57:41 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/05/04 03:57:41 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://addf0afec0d4:46167/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766260685
	24/05/04 03:57:41 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://addf0afec0d4:46167/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766260685
	24/05/04 03:57:41 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://addf0afec0d4:46167/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714766260685
	24/05/04 03:57:41 INFO SparkContext: Added JAR file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://addf0afec0d4:46167/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714766260685
	24/05/04 03:57:41 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://addf0afec0d4:46167/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714766260685
	24/05/04 03:57:41 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://addf0afec0d4:46167/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714766260685
	24/05/04 03:57:41 INFO SparkContext: Added JAR file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at spark://addf0afec0d4:46167/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714766260685
	24/05/04 03:57:41 INFO SparkContext: Added JAR file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://addf0afec0d4:46167/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714766260685
	24/05/04 03:57:41 INFO SparkContext: Added JAR file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://addf0afec0d4:46167/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714766260685
	24/05/04 03:57:41 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://addf0afec0d4:46167/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714766260685
	24/05/04 03:57:41 INFO SparkContext: Added JAR file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at spark://addf0afec0d4:46167/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714766260685
	24/05/04 03:57:41 INFO SparkContext: Added file file:///local_storage/google_news/processing/830-20240503110322.json at file:///local_storage/google_news/processing/830-20240503110322.json with timestamp 1714766260685
	24/05/04 03:57:41 INFO Utils: Copying /local_storage/google_news/processing/830-20240503110322.json to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/830-20240503110322.json
	24/05/04 03:57:41 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766260685
	24/05/04 03:57:41 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/05/04 03:57:41 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766260685
	24/05/04 03:57:41 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/05/04 03:57:41 INFO SparkContext: Added file file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714766260685
	24/05/04 03:57:41 INFO Utils: Copying /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.apache.kafka_kafka-clients-3.4.1.jar
	24/05/04 03:57:41 INFO SparkContext: Added file file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714766260685
	24/05/04 03:57:41 INFO Utils: Copying /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/com.google.code.findbugs_jsr305-3.0.0.jar
	24/05/04 03:57:41 INFO SparkContext: Added file file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714766260685
	24/05/04 03:57:41 INFO Utils: Copying /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.apache.commons_commons-pool2-2.11.1.jar
	24/05/04 03:57:41 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714766260685
	24/05/04 03:57:41 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/05/04 03:57:41 INFO SparkContext: Added file file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714766260685
	24/05/04 03:57:41 INFO Utils: Copying /tmp/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.lz4_lz4-java-1.8.0.jar
	24/05/04 03:57:41 INFO SparkContext: Added file file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714766260685
	24/05/04 03:57:41 INFO Utils: Copying /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/05/04 03:57:41 INFO SparkContext: Added file file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714766260685
	24/05/04 03:57:41 INFO Utils: Copying /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.slf4j_slf4j-api-2.0.7.jar
	24/05/04 03:57:41 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714766260685
	24/05/04 03:57:41 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/05/04 03:57:41 INFO SparkContext: Added file file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714766260685
	24/05/04 03:57:41 INFO Utils: Copying /tmp/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/commons-logging_commons-logging-1.1.3.jar
	24/05/04 03:57:41 INFO Executor: Starting executor ID driver on host addf0afec0d4
	24/05/04 03:57:41 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/05/04 03:57:41 INFO Executor: Java version 1.8.0_402
	24/05/04 03:57:41 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/05/04 03:57:41 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1c7091bb for default.
	24/05/04 03:57:41 INFO Executor: Fetching file:///local_storage/google_news/processing/830-20240503110322.json with timestamp 1714766260685
	24/05/04 03:57:42 INFO Utils: /local_storage/google_news/processing/830-20240503110322.json has been previously copied to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/830-20240503110322.json
	24/05/04 03:57:42 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714766260685
	24/05/04 03:57:42 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/05/04 03:57:42 INFO Executor: Fetching file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714766260685
	24/05/04 03:57:42 INFO Utils: /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.apache.commons_commons-pool2-2.11.1.jar
	24/05/04 03:57:42 INFO Executor: Fetching file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714766260685
	24/05/04 03:57:42 INFO Utils: /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.apache.kafka_kafka-clients-3.4.1.jar
	24/05/04 03:57:42 INFO Executor: Fetching file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714766260685
	24/05/04 03:57:42 INFO Utils: /tmp/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.lz4_lz4-java-1.8.0.jar
	24/05/04 03:57:42 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766260685
	24/05/04 03:57:42 INFO Utils: /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/05/04 03:57:42 INFO Executor: Fetching file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714766260685
	24/05/04 03:57:42 INFO Utils: /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/05/04 03:57:42 INFO Executor: Fetching file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714766260685
	24/05/04 03:57:42 INFO Utils: /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.slf4j_slf4j-api-2.0.7.jar
	24/05/04 03:57:42 INFO Executor: Fetching file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714766260685
	24/05/04 03:57:42 INFO Utils: /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/com.google.code.findbugs_jsr305-3.0.0.jar
	24/05/04 03:57:42 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766260685
	24/05/04 03:57:42 INFO Utils: /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/05/04 03:57:42 INFO Executor: Fetching file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714766260685
	24/05/04 03:57:42 INFO Utils: /tmp/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/commons-logging_commons-logging-1.1.3.jar
	24/05/04 03:57:42 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714766260685
	24/05/04 03:57:42 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/05/04 03:57:42 INFO Executor: Fetching spark://addf0afec0d4:46167/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714766260685
	24/05/04 03:57:42 INFO TransportClientFactory: Successfully created connection to addf0afec0d4/172.18.1.1:46167 after 31 ms (0 ms spent in bootstraps)
	24/05/04 03:57:42 INFO Utils: Fetching spark://addf0afec0d4:46167/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/fetchFileTemp7872629199709048649.tmp
	24/05/04 03:57:42 INFO Utils: /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/fetchFileTemp7872629199709048649.tmp has been previously copied to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.apache.commons_commons-pool2-2.11.1.jar
	24/05/04 03:57:42 INFO Executor: Adding file:/tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
	24/05/04 03:57:42 INFO Executor: Fetching spark://addf0afec0d4:46167/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714766260685
	24/05/04 03:57:42 INFO Utils: Fetching spark://addf0afec0d4:46167/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/fetchFileTemp7526861590980841003.tmp
	24/05/04 03:57:42 INFO Utils: /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/fetchFileTemp7526861590980841003.tmp has been previously copied to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/05/04 03:57:42 INFO Executor: Adding file:/tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
	24/05/04 03:57:42 INFO Executor: Fetching spark://addf0afec0d4:46167/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714766260685
	24/05/04 03:57:42 INFO Utils: Fetching spark://addf0afec0d4:46167/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/fetchFileTemp2044430468619830738.tmp
	24/05/04 03:57:42 INFO Utils: /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/fetchFileTemp2044430468619830738.tmp has been previously copied to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/commons-logging_commons-logging-1.1.3.jar
	24/05/04 03:57:42 INFO Executor: Adding file:/tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/commons-logging_commons-logging-1.1.3.jar to class loader default
	24/05/04 03:57:42 INFO Executor: Fetching spark://addf0afec0d4:46167/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766260685
	24/05/04 03:57:42 INFO Utils: Fetching spark://addf0afec0d4:46167/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/fetchFileTemp1333040073739429243.tmp
	24/05/04 03:57:42 INFO Utils: /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/fetchFileTemp1333040073739429243.tmp has been previously copied to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/05/04 03:57:42 INFO Executor: Adding file:/tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
	24/05/04 03:57:42 INFO Executor: Fetching spark://addf0afec0d4:46167/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714766260685
	24/05/04 03:57:42 INFO Utils: Fetching spark://addf0afec0d4:46167/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/fetchFileTemp6140640277896403089.tmp
	24/05/04 03:57:42 INFO Utils: /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/fetchFileTemp6140640277896403089.tmp has been previously copied to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:43.518 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:57:42 INFO Executor: Adding file:/tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
	24/05/04 03:57:42 INFO Executor: Fetching spark://addf0afec0d4:46167/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766260685
	24/05/04 03:57:42 INFO Utils: Fetching spark://addf0afec0d4:46167/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/fetchFileTemp5357514547347416069.tmp
	24/05/04 03:57:42 INFO Utils: /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/fetchFileTemp5357514547347416069.tmp has been previously copied to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/05/04 03:57:42 INFO Executor: Adding file:/tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
	24/05/04 03:57:42 INFO Executor: Fetching spark://addf0afec0d4:46167/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714766260685
	24/05/04 03:57:42 INFO Utils: Fetching spark://addf0afec0d4:46167/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/fetchFileTemp2579363523868899851.tmp
	24/05/04 03:57:42 INFO Utils: /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/fetchFileTemp2579363523868899851.tmp has been previously copied to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.slf4j_slf4j-api-2.0.7.jar
	24/05/04 03:57:42 INFO Executor: Adding file:/tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.slf4j_slf4j-api-2.0.7.jar to class loader default
	24/05/04 03:57:42 INFO Executor: Fetching spark://addf0afec0d4:46167/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714766260685
	24/05/04 03:57:42 INFO Utils: Fetching spark://addf0afec0d4:46167/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/fetchFileTemp120913240980482087.tmp
	24/05/04 03:57:42 INFO Utils: /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/fetchFileTemp120913240980482087.tmp has been previously copied to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.lz4_lz4-java-1.8.0.jar
	24/05/04 03:57:42 INFO Executor: Adding file:/tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.lz4_lz4-java-1.8.0.jar to class loader default
	24/05/04 03:57:42 INFO Executor: Fetching spark://addf0afec0d4:46167/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714766260685
	24/05/04 03:57:42 INFO Utils: Fetching spark://addf0afec0d4:46167/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/fetchFileTemp2650732733327903610.tmp
	24/05/04 03:57:42 INFO Utils: /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/fetchFileTemp2650732733327903610.tmp has been previously copied to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/com.google.code.findbugs_jsr305-3.0.0.jar
	24/05/04 03:57:42 INFO Executor: Adding file:/tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
	24/05/04 03:57:42 INFO Executor: Fetching spark://addf0afec0d4:46167/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714766260685
	24/05/04 03:57:42 INFO Utils: Fetching spark://addf0afec0d4:46167/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/fetchFileTemp2219891073745426685.tmp
	24/05/04 03:57:42 INFO Utils: /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/fetchFileTemp2219891073745426685.tmp has been previously copied to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/05/04 03:57:42 INFO Executor: Adding file:/tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
	24/05/04 03:57:42 INFO Executor: Fetching spark://addf0afec0d4:46167/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714766260685
	24/05/04 03:57:42 INFO Utils: Fetching spark://addf0afec0d4:46167/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/fetchFileTemp869427599223123015.tmp
	24/05/04 03:57:42 INFO Utils: /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/fetchFileTemp869427599223123015.tmp has been previously copied to /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.apache.kafka_kafka-clients-3.4.1.jar
	24/05/04 03:57:42 INFO Executor: Adding file:/tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/userFiles-e449f149-22e2-473d-80dc-91df5d631b17/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
	24/05/04 03:57:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41603.
	24/05/04 03:57:42 INFO NettyBlockTransferService: Server created on addf0afec0d4:41603
	24/05/04 03:57:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/05/04 03:57:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, addf0afec0d4, 41603, None)
	24/05/04 03:57:42 INFO BlockManagerMasterEndpoint: Registering block manager addf0afec0d4:41603 with 366.3 MiB RAM, BlockManagerId(driver, addf0afec0d4, 41603, None)
	24/05/04 03:57:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, addf0afec0d4, 41603, None)
	24/05/04 03:57:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, addf0afec0d4, 41603, None)
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:44.519 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:57:43 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 345.5 KiB, free 366.0 MiB)
	24/05/04 03:57:43 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 365.9 MiB)
	24/05/04 03:57:43 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on addf0afec0d4:41603 (size: 32.7 KiB, free: 366.3 MiB)
	24/05/04 03:57:43 INFO SparkContext: Created broadcast 0 from wholeTextFiles at NativeMethodAccessorImpl.java:0
	24/05/04 03:57:43 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/05/04 03:57:43 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2857/spark-warehouse'.
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:47.542 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:57:47 INFO CodeGenerator: Code generated in 250.483885 ms
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2857/spark_gnews_preprocessing.py", line 39, in <module>
	    raw_df = spark.read.json(raw_df)
	             ^^^^^^^^^^^^^^^^^^^^^^^
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 440, in json
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
	py4j.protocol.Py4JJavaError: An error occurred while calling o39.json.
	: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2857/830-20240503110322.json
		at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:340)
		at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:279)
		at org.apache.spark.input.WholeTextFileInputFormat.setMinPartitions(WholeTextFileInputFormat.scala:52)
		at org.apache.spark.rdd.WholeTextFileRDD.getPartitions(WholeTextFileRDD.scala:54)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:57)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.sql.execution.SQLExecutionRDD.getPartitions(SQLExecutionRDD.scala:44)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.infer(JsonInferSchema.scala:120)
		at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$5(JsonDataSource.scala:109)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:109)
		at org.apache.spark.sql.DataFrameReader.$anonfun$json$4(DataFrameReader.scala:416)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:416)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:391)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:377)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:282)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: java.io.IOException: Input path does not exist: file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2857/830-20240503110322.json
		at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:313)
		... 56 more
	
	24/05/04 03:57:47 INFO SparkContext: Invoking stop() from shutdown hook
	24/05/04 03:57:47 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/05/04 03:57:47 INFO SparkUI: Stopped Spark web UI at http://addf0afec0d4:4040
	24/05/04 03:57:47 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/05/04 03:57:47 INFO MemoryStore: MemoryStore cleared
	24/05/04 03:57:47 INFO BlockManager: BlockManager stopped
	24/05/04 03:57:47 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/05/04 03:57:47 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/05/04 03:57:47 INFO SparkContext: Successfully stopped SparkContext
	24/05/04 03:57:47 INFO ShutdownHookManager: Shutdown hook called
	24/05/04 03:57:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-9f868b4e-b78d-42d1-b392-8136e48af2e7
	24/05/04 03:57:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b/pyspark-0980a182-a0b2-4e61-ba83-c0cc0be97312
	24/05/04 03:57:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-34b7f66a-71ce-43a9-a73f-e32e166e100b
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:47.544 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2857, processId:1399 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:47.545 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:47.545 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:47.545 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:47.545 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:47.552 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:47.552 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:47.552 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2857
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:47.553 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2857
[WI-887][TI-2857] - [INFO] 2024-05-04 03:57:47.553 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-2857] - [INFO] 2024-05-04 03:57:48.537 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=2857, success=true)
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:48.570 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=2858, taskName=preprocessing and kafka, firstSubmitTime=1714766268557, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=32, appIds=null, processInstanceId=887, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name gnews_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_gnews_preprocessing.py\n","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='preprocessing and kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240504'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='2858'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524229'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240504035748'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='887'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/google_news/processing/830-20240503110322.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240503'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}, is_exists=Property{prop='is_exists', direct=IN, type=VARCHAR, value='1'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/830-20240503110322.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.572 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: preprocessing and kafka to wait queue success
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.572 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.574 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.574 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.575 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.575 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714766268575
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.575 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 887_2858
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.576 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 2858,
  "taskName" : "preprocessing and kafka",
  "firstSubmitTime" : 1714766268557,
  "startTime" : 1714766268575,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240504/13475238524230/32/887/2858.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 32,
  "processInstanceId" : 887,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name gnews_preprocessing \\\\\\n    --files ${raw_file_dir} \\\\\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\\\\n    --conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n    spark_gnews_preprocessing.py\\n\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocessing and kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "2858"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524229"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504035748"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "887"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news/processing/830-20240503110322.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240503"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    },
    "is_exists" : {
      "prop" : "is_exists",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1"
    }
  },
  "taskAppId" : "887_2858",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/google_news/processing/830-20240503110322.json\"},{\"prop\":\"is_exists\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"1\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.577 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.577 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.577 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.584 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.585 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.586 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2858 check successfully
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.586 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.588 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py, resourceRelativePath=spark_gnews_preprocessing.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2858/spark_gnews_preprocessing.py)})
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.589 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.589 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.590 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name gnews_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_gnews_preprocessing.py\n",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py",
    "res" : null
  } ]
}
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.591 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.591 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/830-20240503110322.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}] successfully
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.592 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.592 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.592 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.593 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.593 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.593 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master local \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name gnews_preprocessing \
    --files /local_storage/google_news/processing/830-20240503110322.json \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
    --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
    spark_gnews_preprocessing.py

[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.594 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.595 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2858/887_2858.sh
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:48.598 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 1580
[WI-0][TI-2858] - [INFO] 2024-05-04 03:57:49.540 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=2858, success=true)
[WI-0][TI-2858] - [INFO] 2024-05-04 03:57:49.547 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=2858)
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:49.598 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:50.601 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-0eeb7c55-4004-4af9-926f-6b23a84e7af6;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 357ms :: artifacts dl 14ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-0eeb7c55-4004-4af9-926f-6b23a84e7af6
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/9ms)
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:51.602 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:57:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	24/05/04 03:57:51 INFO SparkContext: Running Spark version 3.5.1
	24/05/04 03:57:51 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/05/04 03:57:51 INFO SparkContext: Java version 1.8.0_402
	24/05/04 03:57:51 INFO ResourceUtils: ==============================================================
	24/05/04 03:57:51 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/05/04 03:57:51 INFO ResourceUtils: ==============================================================
	24/05/04 03:57:51 INFO SparkContext: Submitted application: gnews_preprocessing
	24/05/04 03:57:51 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/05/04 03:57:51 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
	24/05/04 03:57:51 INFO ResourceProfileManager: Added ResourceProfile id: 0
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:52.604 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:57:51 INFO SecurityManager: Changing view acls to: default
	24/05/04 03:57:51 INFO SecurityManager: Changing modify acls to: default
	24/05/04 03:57:51 INFO SecurityManager: Changing view acls groups to: 
	24/05/04 03:57:51 INFO SecurityManager: Changing modify acls groups to: 
	24/05/04 03:57:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
	24/05/04 03:57:51 INFO Utils: Successfully started service 'sparkDriver' on port 45527.
	24/05/04 03:57:51 INFO SparkEnv: Registering MapOutputTracker
	24/05/04 03:57:51 INFO SparkEnv: Registering BlockManagerMaster
	24/05/04 03:57:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/05/04 03:57:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/05/04 03:57:52 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/05/04 03:57:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f6ada0f1-d7a3-4e73-976a-dbbf1ef41a15
	24/05/04 03:57:52 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
	24/05/04 03:57:52 INFO SparkEnv: Registering OutputCommitCoordinator
	24/05/04 03:57:52 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/05/04 03:57:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/05/04 03:57:52 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://addf0afec0d4:45527/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://addf0afec0d4:45527/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://addf0afec0d4:45527/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO SparkContext: Added JAR file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://addf0afec0d4:45527/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://addf0afec0d4:45527/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://addf0afec0d4:45527/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO SparkContext: Added JAR file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at spark://addf0afec0d4:45527/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO SparkContext: Added JAR file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://addf0afec0d4:45527/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO SparkContext: Added JAR file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://addf0afec0d4:45527/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://addf0afec0d4:45527/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO SparkContext: Added JAR file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at spark://addf0afec0d4:45527/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO SparkContext: Added file file:///local_storage/google_news/processing/830-20240503110322.json at file:///local_storage/google_news/processing/830-20240503110322.json with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: Copying /local_storage/google_news/processing/830-20240503110322.json to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/830-20240503110322.json
	24/05/04 03:57:52 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/05/04 03:57:52 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/05/04 03:57:52 INFO SparkContext: Added file file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: Copying /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.apache.kafka_kafka-clients-3.4.1.jar
	24/05/04 03:57:52 INFO SparkContext: Added file file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: Copying /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/com.google.code.findbugs_jsr305-3.0.0.jar
	24/05/04 03:57:52 INFO SparkContext: Added file file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: Copying /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.apache.commons_commons-pool2-2.11.1.jar
	24/05/04 03:57:52 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/05/04 03:57:52 INFO SparkContext: Added file file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: Copying /tmp/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.lz4_lz4-java-1.8.0.jar
	24/05/04 03:57:52 INFO SparkContext: Added file file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: Copying /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/05/04 03:57:52 INFO SparkContext: Added file file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: Copying /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.slf4j_slf4j-api-2.0.7.jar
	24/05/04 03:57:52 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/05/04 03:57:52 INFO SparkContext: Added file file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: Copying /tmp/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/commons-logging_commons-logging-1.1.3.jar
	24/05/04 03:57:52 INFO Executor: Starting executor ID driver on host addf0afec0d4
	24/05/04 03:57:52 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/05/04 03:57:52 INFO Executor: Java version 1.8.0_402
	24/05/04 03:57:52 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/05/04 03:57:52 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@66261a59 for default.
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:53.606 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:57:52 INFO Executor: Fetching file:///local_storage/google_news/processing/830-20240503110322.json with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: /local_storage/google_news/processing/830-20240503110322.json has been previously copied to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/830-20240503110322.json
	24/05/04 03:57:52 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/05/04 03:57:52 INFO Executor: Fetching file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.apache.commons_commons-pool2-2.11.1.jar
	24/05/04 03:57:52 INFO Executor: Fetching file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.apache.kafka_kafka-clients-3.4.1.jar
	24/05/04 03:57:52 INFO Executor: Fetching file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: /tmp/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.lz4_lz4-java-1.8.0.jar
	24/05/04 03:57:52 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/05/04 03:57:52 INFO Executor: Fetching file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/05/04 03:57:52 INFO Executor: Fetching file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.slf4j_slf4j-api-2.0.7.jar
	24/05/04 03:57:52 INFO Executor: Fetching file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/com.google.code.findbugs_jsr305-3.0.0.jar
	24/05/04 03:57:52 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/05/04 03:57:52 INFO Executor: Fetching file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: /tmp/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/commons-logging_commons-logging-1.1.3.jar
	24/05/04 03:57:52 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/05/04 03:57:52 INFO Executor: Fetching spark://addf0afec0d4:45527/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO TransportClientFactory: Successfully created connection to addf0afec0d4/172.18.1.1:45527 after 35 ms (0 ms spent in bootstraps)
	24/05/04 03:57:52 INFO Utils: Fetching spark://addf0afec0d4:45527/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/fetchFileTemp7599280655372928961.tmp
	24/05/04 03:57:52 INFO Utils: /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/fetchFileTemp7599280655372928961.tmp has been previously copied to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/05/04 03:57:52 INFO Executor: Adding file:/tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
	24/05/04 03:57:52 INFO Executor: Fetching spark://addf0afec0d4:45527/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: Fetching spark://addf0afec0d4:45527/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/fetchFileTemp651866399893954613.tmp
	24/05/04 03:57:52 INFO Utils: /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/fetchFileTemp651866399893954613.tmp has been previously copied to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.lz4_lz4-java-1.8.0.jar
	24/05/04 03:57:52 INFO Executor: Adding file:/tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.lz4_lz4-java-1.8.0.jar to class loader default
	24/05/04 03:57:52 INFO Executor: Fetching spark://addf0afec0d4:45527/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: Fetching spark://addf0afec0d4:45527/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/fetchFileTemp7391364433630275254.tmp
	24/05/04 03:57:52 INFO Utils: /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/fetchFileTemp7391364433630275254.tmp has been previously copied to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/com.google.code.findbugs_jsr305-3.0.0.jar
	24/05/04 03:57:52 INFO Executor: Adding file:/tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
	24/05/04 03:57:52 INFO Executor: Fetching spark://addf0afec0d4:45527/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: Fetching spark://addf0afec0d4:45527/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/fetchFileTemp8254693061564080592.tmp
	24/05/04 03:57:52 INFO Utils: /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/fetchFileTemp8254693061564080592.tmp has been previously copied to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/05/04 03:57:52 INFO Executor: Adding file:/tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
	24/05/04 03:57:52 INFO Executor: Fetching spark://addf0afec0d4:45527/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: Fetching spark://addf0afec0d4:45527/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/fetchFileTemp5955605849286848288.tmp
	24/05/04 03:57:52 INFO Utils: /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/fetchFileTemp5955605849286848288.tmp has been previously copied to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/05/04 03:57:52 INFO Executor: Adding file:/tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
	24/05/04 03:57:52 INFO Executor: Fetching spark://addf0afec0d4:45527/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714766271516
	24/05/04 03:57:52 INFO Utils: Fetching spark://addf0afec0d4:45527/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/fetchFileTemp6946809396845198055.tmp
	24/05/04 03:57:53 INFO Utils: /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/fetchFileTemp6946809396845198055.tmp has been previously copied to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.apache.kafka_kafka-clients-3.4.1.jar
	24/05/04 03:57:53 INFO Executor: Adding file:/tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
	24/05/04 03:57:53 INFO Executor: Fetching spark://addf0afec0d4:45527/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714766271516
	24/05/04 03:57:53 INFO Utils: Fetching spark://addf0afec0d4:45527/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/fetchFileTemp8866304526913514828.tmp
	24/05/04 03:57:53 INFO Utils: /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/fetchFileTemp8866304526913514828.tmp has been previously copied to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/05/04 03:57:53 INFO Executor: Adding file:/tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
	24/05/04 03:57:53 INFO Executor: Fetching spark://addf0afec0d4:45527/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714766271516
	24/05/04 03:57:53 INFO Utils: Fetching spark://addf0afec0d4:45527/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/fetchFileTemp6932774916849752572.tmp
	24/05/04 03:57:53 INFO Utils: /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/fetchFileTemp6932774916849752572.tmp has been previously copied to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/05/04 03:57:53 INFO Executor: Adding file:/tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
	24/05/04 03:57:53 INFO Executor: Fetching spark://addf0afec0d4:45527/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714766271516
	24/05/04 03:57:53 INFO Utils: Fetching spark://addf0afec0d4:45527/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/fetchFileTemp8768390807154064969.tmp
	24/05/04 03:57:53 INFO Utils: /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/fetchFileTemp8768390807154064969.tmp has been previously copied to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.slf4j_slf4j-api-2.0.7.jar
	24/05/04 03:57:53 INFO Executor: Adding file:/tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.slf4j_slf4j-api-2.0.7.jar to class loader default
	24/05/04 03:57:53 INFO Executor: Fetching spark://addf0afec0d4:45527/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714766271516
	24/05/04 03:57:53 INFO Utils: Fetching spark://addf0afec0d4:45527/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/fetchFileTemp5068581497218897037.tmp
	24/05/04 03:57:53 INFO Utils: /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/fetchFileTemp5068581497218897037.tmp has been previously copied to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/commons-logging_commons-logging-1.1.3.jar
	24/05/04 03:57:53 INFO Executor: Adding file:/tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/commons-logging_commons-logging-1.1.3.jar to class loader default
	24/05/04 03:57:53 INFO Executor: Fetching spark://addf0afec0d4:45527/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714766271516
	24/05/04 03:57:53 INFO Utils: Fetching spark://addf0afec0d4:45527/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/fetchFileTemp4980244278260174679.tmp
	24/05/04 03:57:53 INFO Utils: /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/fetchFileTemp4980244278260174679.tmp has been previously copied to /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.apache.commons_commons-pool2-2.11.1.jar
	24/05/04 03:57:53 INFO Executor: Adding file:/tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/userFiles-5f8fe9c4-1432-4d76-a6b1-2479720295ad/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
	24/05/04 03:57:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36481.
	24/05/04 03:57:53 INFO NettyBlockTransferService: Server created on addf0afec0d4:36481
	24/05/04 03:57:53 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/05/04 03:57:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, addf0afec0d4, 36481, None)
	24/05/04 03:57:53 INFO BlockManagerMasterEndpoint: Registering block manager addf0afec0d4:36481 with 366.3 MiB RAM, BlockManagerId(driver, addf0afec0d4, 36481, None)
	24/05/04 03:57:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, addf0afec0d4, 36481, None)
	24/05/04 03:57:53 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, addf0afec0d4, 36481, None)
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:54.610 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:57:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 345.5 KiB, free 366.0 MiB)
	24/05/04 03:57:54 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 365.9 MiB)
	24/05/04 03:57:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on addf0afec0d4:36481 (size: 32.7 KiB, free: 366.3 MiB)
	24/05/04 03:57:54 INFO SparkContext: Created broadcast 0 from wholeTextFiles at NativeMethodAccessorImpl.java:0
	24/05/04 03:57:54 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/05/04 03:57:54 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2858/spark-warehouse'.
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:57.615 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/04 03:57:57 INFO CodeGenerator: Code generated in 334.339845 ms
[WI-0][TI-0] - [INFO] 2024-05-04 03:57:58.617 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Traceback (most recent call last):
	  File "/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2858/spark_gnews_preprocessing.py", line 39, in <module>
	    raw_df = spark.read.json(raw_df)
	             ^^^^^^^^^^^^^^^^^^^^^^^
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 440, in json
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
	  File "/opt/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
	py4j.protocol.Py4JJavaError: An error occurred while calling o39.json.
	: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2858/830-20240503110322.json
		at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:340)
		at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:279)
		at org.apache.spark.input.WholeTextFileInputFormat.setMinPartitions(WholeTextFileInputFormat.scala:52)
		at org.apache.spark.rdd.WholeTextFileRDD.getPartitions(WholeTextFileRDD.scala:54)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:57)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.sql.execution.SQLExecutionRDD.getPartitions(SQLExecutionRDD.scala:44)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
		at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)
		at org.apache.spark.sql.catalyst.json.JsonInferSchema.infer(JsonInferSchema.scala:120)
		at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$5(JsonDataSource.scala:109)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:109)
		at org.apache.spark.sql.DataFrameReader.$anonfun$json$4(DataFrameReader.scala:416)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:416)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:391)
		at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:377)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:282)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: java.io.IOException: Input path does not exist: file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2858/830-20240503110322.json
		at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:313)
		... 56 more
	
	24/05/04 03:57:57 INFO SparkContext: Invoking stop() from shutdown hook
	24/05/04 03:57:57 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/05/04 03:57:57 INFO SparkUI: Stopped Spark web UI at http://addf0afec0d4:4040
	24/05/04 03:57:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/05/04 03:57:57 INFO MemoryStore: MemoryStore cleared
	24/05/04 03:57:57 INFO BlockManager: BlockManager stopped
	24/05/04 03:57:57 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/05/04 03:57:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/05/04 03:57:57 INFO SparkContext: Successfully stopped SparkContext
	24/05/04 03:57:57 INFO ShutdownHookManager: Shutdown hook called
	24/05/04 03:57:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-72ebb1a3-7c03-47ab-9fd9-eeb9087981ad
	24/05/04 03:57:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed/pyspark-39036cfe-7475-4c3b-8a4f-f4b60f986b7e
	24/05/04 03:57:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-5b9fdc23-3004-4175-8f93-1fee44483fed
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:58.620 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2858, processId:1580 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:58.621 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:58.621 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:58.621 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:58.623 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:58.627 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:58.627 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:58.627 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2858
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:58.628 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_32/887/2858
[WI-887][TI-2858] - [INFO] 2024-05-04 03:57:58.628 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-2858] - [INFO] 2024-05-04 03:57:59.572 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=2858, success=true)
