[WI-0][TI-0] - [INFO] 2024-05-07 08:02:45.740 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9102902374670185 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:02:46.820 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9120603015075376 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:02:47.821 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7101063829787233 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:02:48.822 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8802228412256268 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [WARN] 2024-05-07 08:03:09.896 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[147] - [id: 0x9b5d4ccc, L:/172.18.1.1:1234 - R:/172.18.0.13:48102] is not writable, over high water level : 65536
[WI-0][TI-0] - [WARN] 2024-05-07 08:03:09.908 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[154] - [id: 0x9b5d4ccc, L:/172.18.1.1:1234 - R:/172.18.0.13:48102] is writable, to low water : 32768
[WI-0][TI-0] - [INFO] 2024-05-07 08:03:10.354 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7026239067055394 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:17:45.859 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8614130434782609 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:17:49.945 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8342245989304813 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:17:51.019 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8260869565217391 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:17:52.020 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7100271002710027 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:17:57.117 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3944, taskName=[null check] filtered data persistence, firstSubmitTime=1715041076906, startTime=0, taskType=DATA_QUALITY, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13505298546272, processDefineVersion=21, appIds=null, processInstanceId=1051, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13001194483488, taskParams={"localParams":[],"resourceList":[],"ruleId":6,"ruleInputParameter":{"check_type":"0","comparison_type":1,"comparison_name":"0","failure_strategy":"0","operator":"0","src_connector_type":1,"src_datasource_id":6,"src_database":"persistence","src_field":"content","src_table":"filtered_data_persistence","threshold":"0"},"sparkParameters":{"deployMode":"local","driverCores":1,"driverMemory":"512M","executorCores":1,"executorMemory":"1G","numExecutors":1,"others":"--conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n--packages org.postgresql:postgresql:42.7.3","yarnQueue":""}}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='[null check] filtered data persistence'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13001194483488'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='1051'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240507'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240506'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3944'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='qualti_test'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13505264408800'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13505298546272'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240507081756'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=org.apache.dolphinscheduler.plugin.task.api.DataQualityTaskExecutionContext@f4f8fc2, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.142 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: [null check] filtered data persistence to wait queue success
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.161 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.170 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.171 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.172 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.174 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1715041077174
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.175 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 1051_3944
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.198 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3944,
  "taskName" : "[null check] filtered data persistence",
  "firstSubmitTime" : 1715041076906,
  "startTime" : 1715041077174,
  "taskType" : "DATA_QUALITY",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240507/13505298546272/21/1051/3944.log",
  "processId" : 0,
  "processDefineCode" : 13505298546272,
  "processDefineVersion" : 21,
  "processInstanceId" : 1051,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13001194483488,
  "taskParams" : "{\"localParams\":[],\"resourceList\":[],\"ruleId\":6,\"ruleInputParameter\":{\"check_type\":\"0\",\"comparison_type\":1,\"comparison_name\":\"0\",\"failure_strategy\":\"0\",\"operator\":\"0\",\"src_connector_type\":1,\"src_datasource_id\":6,\"src_database\":\"persistence\",\"src_field\":\"content\",\"src_table\":\"filtered_data_persistence\",\"threshold\":\"0\"},\"sparkParameters\":{\"deployMode\":\"local\",\"driverCores\":1,\"driverMemory\":\"512M\",\"executorCores\":1,\"executorMemory\":\"1G\",\"numExecutors\":1,\"others\":\"--conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n--packages org.postgresql:postgresql:42.7.3\",\"yarnQueue\":\"\"}}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "[null check] filtered data persistence"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13001194483488"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1051"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240507"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240506"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3944"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "qualti_test"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13505264408800"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13505298546272"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240507081756"
    }
  },
  "taskAppId" : "1051_3944",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "dataQualityTaskExecutionContext" : {
    "ruleId" : 6,
    "ruleName" : "$t(uniqueness_check)",
    "ruleType" : 0,
    "ruleInputEntryList" : "[{\"id\":1,\"field\":\"src_connector_type\",\"type\":\"select\",\"title\":\"$t(src_connector_type)\",\"data\":\"\",\"options\":\"[{\\\"label\\\":\\\"HIVE\\\",\\\"value\\\":\\\"HIVE\\\"},{\\\"label\\\":\\\"JDBC\\\",\\\"value\\\":\\\"JDBC\\\"}]\",\"placeholder\":\"please select source connector type\",\"optionSourceType\":2,\"dataType\":2,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":1,\"createTime\":null,\"updateTime\":null},{\"id\":2,\"field\":\"src_datasource_id\",\"type\":\"select\",\"title\":\"$t(src_datasource_id)\",\"data\":\"\",\"options\":null,\"placeholder\":\"please select source datasource id\",\"optionSourceType\":1,\"dataType\":2,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":2,\"createTime\":null,\"updateTime\":null},{\"id\":30,\"field\":\"src_database\",\"type\":\"select\",\"title\":\"$t(src_database)\",\"data\":null,\"options\":null,\"placeholder\":\"Please select source database\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":true,\"valuesMap\":null,\"index\":2,\"createTime\":null,\"updateTime\":null},{\"id\":3,\"field\":\"src_table\",\"type\":\"select\",\"title\":\"$t(src_table)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter source table name\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":true,\"valuesMap\":null,\"index\":3,\"createTime\":null,\"updateTime\":null},{\"id\":4,\"field\":\"src_filter\",\"type\":\"input\",\"title\":\"$t(src_filter)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter filter expression\",\"optionSourceType\":0,\"dataType\":3,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":4,\"createTime\":null,\"updateTime\":null},{\"id\":5,\"field\":\"src_field\",\"type\":\"select\",\"title\":\"$t(src_field)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter column, only single column is supported\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":null,\"index\":5,\"createTime\":null,\"updateTime\":null},{\"id\":6,\"field\":\"statistics_name\",\"type\":\"input\",\"title\":\"$t(statistics_name)\",\"data\":\"duplicate_count.duplicates\",\"options\":null,\"placeholder\":\"Please enter statistics name, the alias in statistics execute sql\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":1,\"isShow\":false,\"canEdit\":false,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":\"{\\\"statistics_name\\\":\\\"duplicate_count.duplicates\\\"}\",\"index\":6,\"createTime\":null,\"updateTime\":null},{\"id\":7,\"field\":\"check_type\",\"type\":\"select\",\"title\":\"$t(check_type)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"Expected - Actual\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"Actual - Expected\\\",\\\"value\\\":\\\"1\\\"},{\\\"label\\\":\\\"Actual / Expected\\\",\\\"value\\\":\\\"2\\\"},{\\\"label\\\":\\\"(Expected - Actual) / Expected\\\",\\\"value\\\":\\\"3\\\"}]\",\"placeholder\":\"please select check type\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":7,\"createTime\":null,\"updateTime\":null},{\"id\":8,\"field\":\"operator\",\"type\":\"select\",\"title\":\"$t(operator)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"=\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"<\\\",\\\"value\\\":\\\"1\\\"},{\\\"label\\\":\\\"<=\\\",\\\"value\\\":\\\"2\\\"},{\\\"label\\\":\\\">\\\",\\\"value\\\":\\\"3\\\"},{\\\"label\\\":\\\">=\\\",\\\"value\\\":\\\"4\\\"},{\\\"label\\\":\\\"!=\\\",\\\"value\\\":\\\"5\\\"}]\",\"placeholder\":\"please select operator\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":8,\"createTime\":null,\"updateTime\":null},{\"id\":9,\"field\":\"threshold\",\"type\":\"input\",\"title\":\"$t(threshold)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter threshold, number is needed\",\"optionSourceType\":0,\"dataType\":2,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":null,\"index\":9,\"createTime\":null,\"updateTime\":null},{\"id\":10,\"field\":\"failure_strategy\",\"type\":\"select\",\"title\":\"$t(failure_strategy)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"Alert\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"Block\\\",\\\"value\\\":\\\"1\\\"}]\",\"placeholder\":\"please select failure strategy\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":10,\"createTime\":null,\"updateTime\":null},{\"id\":17,\"field\":\"comparison_name\",\"type\":\"input\",\"title\":\"$t(comparison_name)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter comparison name, the alias in comparison execute sql\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":false,\"canEdit\":false,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":\"\",\"index\":11,\"createTime\":null,\"updateTime\":null},{\"id\":19,\"field\":\"comparison_type\",\"type\":\"select\",\"title\":\"$t(comparison_type)\",\"data\":\"\",\"options\":null,\"placeholder\":\"Please enter comparison title\",\"optionSourceType\":3,\"dataType\":0,\"inputType\":2,\"isShow\":true,\"canEdit\":false,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":12,\"createTime\":null,\"updateTime\":null}]",
    "executeSqlList" : "[{\"id\":6,\"index\":1,\"sql\":\"SELECT ${src_field} FROM ${src_table} group by ${src_field} having count(*) > 1\",\"tableAlias\":\"duplicate_items\",\"type\":0,\"createTime\":\"2021-03-03 11:31:24\",\"updateTime\":\"2021-03-03 11:31:24\",\"errorOutputSql\":true},{\"id\":7,\"index\":1,\"sql\":\"SELECT COUNT(*) AS duplicates FROM duplicate_items\",\"tableAlias\":\"duplicate_count\",\"type\":1,\"createTime\":\"2021-03-03 11:31:24\",\"updateTime\":\"2021-03-03 11:31:24\",\"errorOutputSql\":false}]",
    "comparisonNeedStatisticsValueTable" : false,
    "compareWithFixedValue" : true,
    "hdfsPath" : "hdfs://mycluster:8020/user/default/data_quality_error_data",
    "sourceConnectorType" : "JDBC",
    "sourceType" : 1,
    "sourceConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"persistence\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence\",\"driverClassName\":\"org.postgresql.Driver\",\"validationQuery\":\"select version()\",\"other\":{\"password\":\"root\"}}",
    "targetConnectorType" : null,
    "targetType" : 0,
    "targetConnectionParams" : null,
    "writerConnectorType" : "JDBC",
    "writerType" : 1,
    "writerTable" : "t_ds_dq_execute_result",
    "writerConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"dolphinscheduler\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\"}",
    "statisticsValueConnectorType" : "JDBC",
    "statisticsValueType" : 1,
    "statisticsValueTable" : "t_ds_dq_task_statistics_value",
    "statisticsValueWriterConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"dolphinscheduler\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\"}"
  },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.215 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.216 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.217 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.353 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.428 +0800 o.a.d.c.u.OSUtils:[231] - create linux os user: default
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.430 +0800 o.a.d.c.u.OSUtils:[233] - execute cmd: sudo useradd -g root
 default
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.562 +0800 o.a.d.c.u.OSUtils:[190] - create user default success
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.563 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.569 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1051/3944 check successfully
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.571 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.dq.DataQualityTaskChannel successfully
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.591 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.603 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.607 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: DATA_QUALITY create successfully
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.611 +0800 o.a.d.p.t.d.DataQualityTask:[89] - Initialize data quality task params {
  "localParams" : [ ],
  "varPool" : null,
  "ruleId" : 6,
  "ruleInputParameter" : {
    "check_type" : "0",
    "comparison_type" : "1",
    "comparison_name" : "0",
    "failure_strategy" : "0",
    "operator" : "0",
    "src_connector_type" : "1",
    "src_datasource_id" : "6",
    "src_database" : "persistence",
    "src_field" : "content",
    "src_table" : "filtered_data_persistence",
    "threshold" : "0"
  },
  "sparkParameters" : {
    "localParams" : null,
    "varPool" : null,
    "mainJar" : null,
    "mainClass" : null,
    "deployMode" : "local",
    "mainArgs" : null,
    "driverCores" : 1,
    "driverMemory" : "512M",
    "numExecutors" : 1,
    "executorCores" : 1,
    "executorMemory" : "1G",
    "appName" : null,
    "yarnQueue" : "",
    "others" : "--conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n--packages org.postgresql:postgresql:42.7.3",
    "programType" : null,
    "resourceList" : [ ]
  }
}
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.654 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: HANA
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.655 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: HANA
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.658 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: SQLSERVER
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.659 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: SQLSERVER
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.661 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: SAGEMAKER
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.662 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: SAGEMAKER
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.664 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: DATABEND
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.665 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: DATABEND
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.696 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: AZURESQL
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.697 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: AZURESQL
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.701 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: HIVE
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.701 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: HIVE
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.704 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: DORIS
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.705 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: DORIS
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.708 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: POSTGRESQL
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.708 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: POSTGRESQL
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.711 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: MYSQL
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.713 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: MYSQL
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.715 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: REDSHIFT
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.716 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: REDSHIFT
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.718 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: SNOWFLAKE
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.718 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: SNOWFLAKE
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.721 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: STARROCKS
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.722 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: STARROCKS
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.724 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: SSH
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.725 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: SSH
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.727 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: ATHENA
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.728 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: ATHENA
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.731 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: PRESTO
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.732 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: PRESTO
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.735 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: OCEANBASE
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.735 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: OCEANBASE
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.752 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: ORACLE
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.753 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: ORACLE
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.755 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: CLICKHOUSE
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.756 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: CLICKHOUSE
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.758 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: VERTICA
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.759 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: VERTICA
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.762 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: KYUUBI
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.764 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: KYUUBI
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.767 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: DB2
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.768 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: DB2
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.770 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: TRINO
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.771 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: TRINO
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.774 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: SPARK
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.774 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: SPARK
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.777 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: DAMENG
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.777 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: DAMENG
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.779 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[45] - start register processor: ZEPPELIN
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.780 +0800 o.a.d.p.d.a.p.DataSourceProcessorManager:[51] - done register processor: ZEPPELIN
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.796 +0800 o.a.d.p.t.d.DataQualityTask:[119] - data quality configuration: {
  "name" : "$t(uniqueness_check)",
  "env" : {
    "type" : "batch",
    "config" : null
  },
  "readers" : [ {
    "type" : "JDBC",
    "config" : {
      "database" : "persistence",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "output_table" : "persistence_filtered_data_persistence",
      "table" : "filtered_data_persistence",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence?password=root"
    }
  } ],
  "transformers" : [ {
    "type" : "sql",
    "config" : {
      "index" : 1,
      "output_table" : "duplicate_items",
      "sql" : "SELECT content FROM persistence_filtered_data_persistence group by content having count(*) > 1"
    }
  }, {
    "type" : "sql",
    "config" : {
      "index" : 2,
      "output_table" : "duplicate_count",
      "sql" : "SELECT COUNT(*) AS duplicates FROM duplicate_items"
    }
  } ],
  "writers" : [ {
    "type" : "JDBC",
    "config" : {
      "database" : "dolphinscheduler",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "table" : "t_ds_dq_execute_result",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler",
      "sql" : "select 0 as rule_type,'$t(uniqueness_check)' as rule_name,0 as process_definition_id,1051 as process_instance_id,3944 as task_instance_id,duplicate_count.duplicates AS statistics_value,0 AS comparison_value,1 AS comparison_type,0 as check_type,0 as threshold,0 as operator,0 as failure_strategy,'hdfs://mycluster:8020/user/default/data_quality_error_data/0_1051_[null check] filtered data persistence' as error_output_path,'2024-05-07 08:17:57' as create_time,'2024-05-07 08:17:57' as update_time from duplicate_count "
    }
  }, {
    "type" : "JDBC",
    "config" : {
      "database" : "dolphinscheduler",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "table" : "t_ds_dq_task_statistics_value",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler",
      "sql" : "select 0 as process_definition_id,3944 as task_instance_id,6 as rule_id,'KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG=' as unique_code,'duplicate_count.duplicates'AS statistics_name,duplicate_count.duplicates AS statistics_value,'2024-05-07 08:17:57' as data_time,'2024-05-07 08:17:57' as create_time,'2024-05-07 08:17:57' as update_time from duplicate_count"
    }
  }, {
    "type" : "hdfs_file",
    "config" : {
      "path" : "hdfs://mycluster:8020/user/default/data_quality_error_data/0_1051_[null check] filtered data persistence",
      "input_table" : "duplicate_items"
    }
  } ]
}
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.846 +0800 o.a.d.p.d.a.u.CommonUtils:[136] - Trying to get data quality jar in path
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.855 +0800 o.a.d.p.d.a.u.CommonUtils:[147] - data quality jar path is empty, will try to auto discover it from build-in rules.
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.858 +0800 o.a.d.p.d.a.u.CommonUtils:[187] - Try to get data quality jar from path /opt/dolphinscheduler/conf/../libs
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.863 +0800 o.a.d.p.d.a.u.CommonUtils:[182] - get default data quality jar name: /opt/dolphinscheduler/conf/../libs/dolphinscheduler-data-quality-3.2.1.jar
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.865 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.866 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.870 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.871 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.873 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.895 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.902 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.903 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${SPARK_HOME}/bin/spark-submit --master local --driver-cores 1 --driver-memory 512M --num-executors 1 --executor-cores 1 --executor-memory 1G --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
--packages org.postgresql:postgresql:42.7.3 /opt/dolphinscheduler/conf/../libs/dolphinscheduler-data-quality-3.2.1.jar "{\"name\":\"$t(uniqueness_check)\",\"env\":{\"type\":\"batch\",\"config\":null},\"readers\":[{\"type\":\"JDBC\",\"config\":{\"database\":\"persistence\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"output_table\":\"persistence_filtered_data_persistence\",\"table\":\"filtered_data_persistence\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence?password=root\"} }],\"transformers\":[{\"type\":\"sql\",\"config\":{\"index\":1,\"output_table\":\"duplicate_items\",\"sql\":\"SELECT content FROM persistence_filtered_data_persistence group by content having count(*) > 1\"} },{\"type\":\"sql\",\"config\":{\"index\":2,\"output_table\":\"duplicate_count\",\"sql\":\"SELECT COUNT(*) AS duplicates FROM duplicate_items\"} }],\"writers\":[{\"type\":\"JDBC\",\"config\":{\"database\":\"dolphinscheduler\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"table\":\"t_ds_dq_execute_result\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\",\"sql\":\"select 0 as rule_type,'$t(uniqueness_check)' as rule_name,0 as process_definition_id,1051 as process_instance_id,3944 as task_instance_id,duplicate_count.duplicates AS statistics_value,0 AS comparison_value,1 AS comparison_type,0 as check_type,0 as threshold,0 as operator,0 as failure_strategy,'hdfs://mycluster:8020/user/default/data_quality_error_data/0_1051_[null check] filtered data persistence' as error_output_path,'2024-05-07 08:17:57' as create_time,'2024-05-07 08:17:57' as update_time from duplicate_count \"} },{\"type\":\"JDBC\",\"config\":{\"database\":\"dolphinscheduler\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"table\":\"t_ds_dq_task_statistics_value\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\",\"sql\":\"select 0 as process_definition_id,3944 as task_instance_id,6 as rule_id,'KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG=' as unique_code,'duplicate_count.duplicates'AS statistics_name,duplicate_count.duplicates AS statistics_value,'2024-05-07 08:17:57' as data_time,'2024-05-07 08:17:57' as create_time,'2024-05-07 08:17:57' as update_time from duplicate_count\"} },{\"type\":\"hdfs_file\",\"config\":{\"path\":\"hdfs://mycluster:8020/user/default/data_quality_error_data/0_1051_[null check] filtered data persistence\",\"input_table\":\"duplicate_items\"} }]}"
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.903 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-0][TI-3944] - [INFO] 2024-05-07 08:17:57.906 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3944, success=true)
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.911 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1051/3944/1051_3944.sh
[WI-0][TI-0] - [INFO] 2024-05-07 08:17:57.952 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:17:57.957 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 316
[WI-0][TI-0] - [INFO] 2024-05-07 08:17:58.047 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 1.0733137829912023 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3944] - [INFO] 2024-05-07 08:17:58.822 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3944)
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:01.106 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8333333333333333 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:03.995 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.postgresql#postgresql added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-a9ba599f-613a-4d79-97a1-e4bccde33846;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:05.997 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.postgresql#postgresql;42.7.3 in central
		found org.checkerframework#checker-qual;3.42.0 in central
	downloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.7.3/postgresql-42.7.3.jar ...
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:06.998 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.postgresql#postgresql;42.7.3!postgresql.jar (1241ms)
	downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.42.0/checker-qual-3.42.0.jar ...
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:07.999 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		[SUCCESSFUL ] org.checkerframework#checker-qual;3.42.0!checker-qual.jar (370ms)
	:: resolution report :: resolve 2239ms :: artifacts dl 1620ms
		:: modules in use:
		org.checkerframework#checker-qual;3.42.0 from central in [default]
		org.postgresql#postgresql;42.7.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   2   |   2   |   2   |   0   ||   2   |   2   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-a9ba599f-613a-4d79-97a1-e4bccde33846
		confs: [default]
		2 artifacts copied, 0 already retrieved (1289kB/11ms)
	24/05/07 08:18:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:09.002 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:18:08 INFO SparkContext: Running Spark version 3.5.1
	24/05/07 08:18:08 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/05/07 08:18:08 INFO SparkContext: Java version 1.8.0_402
	24/05/07 08:18:08 INFO ResourceUtils: ==============================================================
	24/05/07 08:18:08 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/05/07 08:18:08 INFO ResourceUtils: ==============================================================
	24/05/07 08:18:08 INFO SparkContext: Submitted application: (uniqueness_check)
	24/05/07 08:18:08 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/05/07 08:18:08 INFO ResourceProfile: Limiting resource is cpu
	24/05/07 08:18:08 INFO ResourceProfileManager: Added ResourceProfile id: 0
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:10.005 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:18:09 INFO SecurityManager: Changing view acls to: default
	24/05/07 08:18:09 INFO SecurityManager: Changing modify acls to: default
	24/05/07 08:18:09 INFO SecurityManager: Changing view acls groups to: 
	24/05/07 08:18:09 INFO SecurityManager: Changing modify acls groups to: 
	24/05/07 08:18:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
	24/05/07 08:18:09 INFO Utils: Successfully started service 'sparkDriver' on port 37389.
	24/05/07 08:18:09 INFO SparkEnv: Registering MapOutputTracker
	24/05/07 08:18:09 INFO SparkEnv: Registering BlockManagerMaster
	24/05/07 08:18:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/05/07 08:18:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/05/07 08:18:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:11.009 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:18:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b48f2276-c557-430c-9a1f-636f69c4860a
	24/05/07 08:18:10 INFO MemoryStore: MemoryStore started with capacity 93.3 MiB
	24/05/07 08:18:10 INFO SparkEnv: Registering OutputCommitCoordinator
	24/05/07 08:18:10 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:11.170 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8400000000000001 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:12.010 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:18:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/05/07 08:18:11 INFO SparkContext: Added JAR file:///tmp/jars/org.postgresql_postgresql-42.7.3.jar at spark://941f01f0fea3:37389/jars/org.postgresql_postgresql-42.7.3.jar with timestamp 1715041088817
	24/05/07 08:18:11 INFO SparkContext: Added JAR file:///tmp/jars/org.checkerframework_checker-qual-3.42.0.jar at spark://941f01f0fea3:37389/jars/org.checkerframework_checker-qual-3.42.0.jar with timestamp 1715041088817
	24/05/07 08:18:11 INFO SparkContext: Added JAR file:/opt/dolphinscheduler/libs/dolphinscheduler-data-quality-3.2.1.jar at spark://941f01f0fea3:37389/jars/dolphinscheduler-data-quality-3.2.1.jar with timestamp 1715041088817
	24/05/07 08:18:11 INFO Executor: Starting executor ID driver on host 941f01f0fea3
	24/05/07 08:18:11 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/05/07 08:18:11 INFO Executor: Java version 1.8.0_402
	24/05/07 08:18:11 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/05/07 08:18:11 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@358ab600 for default.
	24/05/07 08:18:11 INFO Executor: Fetching spark://941f01f0fea3:37389/jars/dolphinscheduler-data-quality-3.2.1.jar with timestamp 1715041088817
	24/05/07 08:18:11 INFO TransportClientFactory: Successfully created connection to 941f01f0fea3/172.18.1.1:37389 after 65 ms (0 ms spent in bootstraps)
	24/05/07 08:18:11 INFO Utils: Fetching spark://941f01f0fea3:37389/jars/dolphinscheduler-data-quality-3.2.1.jar to /tmp/spark-f8348a7c-96bd-4240-8e01-f23ad443f97d/userFiles-60f6bf80-fad6-4288-95f6-f48c92f0bfa7/fetchFileTemp6384322544842811541.tmp
	24/05/07 08:18:11 INFO Executor: Adding file:/tmp/spark-f8348a7c-96bd-4240-8e01-f23ad443f97d/userFiles-60f6bf80-fad6-4288-95f6-f48c92f0bfa7/dolphinscheduler-data-quality-3.2.1.jar to class loader default
	24/05/07 08:18:11 INFO Executor: Fetching spark://941f01f0fea3:37389/jars/org.postgresql_postgresql-42.7.3.jar with timestamp 1715041088817
	24/05/07 08:18:11 INFO Utils: Fetching spark://941f01f0fea3:37389/jars/org.postgresql_postgresql-42.7.3.jar to /tmp/spark-f8348a7c-96bd-4240-8e01-f23ad443f97d/userFiles-60f6bf80-fad6-4288-95f6-f48c92f0bfa7/fetchFileTemp3459151903062515078.tmp
	24/05/07 08:18:11 INFO Executor: Adding file:/tmp/spark-f8348a7c-96bd-4240-8e01-f23ad443f97d/userFiles-60f6bf80-fad6-4288-95f6-f48c92f0bfa7/org.postgresql_postgresql-42.7.3.jar to class loader default
	24/05/07 08:18:11 INFO Executor: Fetching spark://941f01f0fea3:37389/jars/org.checkerframework_checker-qual-3.42.0.jar with timestamp 1715041088817
	24/05/07 08:18:11 INFO Utils: Fetching spark://941f01f0fea3:37389/jars/org.checkerframework_checker-qual-3.42.0.jar to /tmp/spark-f8348a7c-96bd-4240-8e01-f23ad443f97d/userFiles-60f6bf80-fad6-4288-95f6-f48c92f0bfa7/fetchFileTemp2887495807118151967.tmp
	24/05/07 08:18:11 INFO Executor: Adding file:/tmp/spark-f8348a7c-96bd-4240-8e01-f23ad443f97d/userFiles-60f6bf80-fad6-4288-95f6-f48c92f0bfa7/org.checkerframework_checker-qual-3.42.0.jar to class loader default
	24/05/07 08:18:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46707.
	24/05/07 08:18:11 INFO NettyBlockTransferService: Server created on 941f01f0fea3:46707
	24/05/07 08:18:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/05/07 08:18:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 941f01f0fea3, 46707, None)
	24/05/07 08:18:11 INFO BlockManagerMasterEndpoint: Registering block manager 941f01f0fea3:46707 with 93.3 MiB RAM, BlockManagerId(driver, 941f01f0fea3, 46707, None)
	24/05/07 08:18:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 941f01f0fea3, 46707, None)
	24/05/07 08:18:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 941f01f0fea3, 46707, None)
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:12.241 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8316326530612245 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:13.015 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:18:12 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/05/07 08:18:12 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1051/3944/spark-warehouse'.
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:13.243 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7005813953488371 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:16.261 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8647058823529411 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:18.342 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7395209580838322 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:20.385 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9178470254957507 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:21.063 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:18:20 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:21.415 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7756232686980609 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:22.423 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9455040871934606 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:23.066 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:18:22 INFO CodeGenerator: Code generated in 982.067114 ms
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:23.426 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.827893175074184 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:24.070 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:18:23 INFO DAGScheduler: Registering RDD 2 (save at JdbcWriter.java:87) as input to shuffle 0
	24/05/07 08:18:23 INFO DAGScheduler: Got map stage job 0 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:18:23 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (save at JdbcWriter.java:87)
	24/05/07 08:18:23 INFO DAGScheduler: Parents of final stage: List()
	24/05/07 08:18:23 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:18:23 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:18:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 34.8 KiB, free 93.3 MiB)
	24/05/07 08:18:23 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 93.2 MiB)
	24/05/07 08:18:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 941f01f0fea3:46707 (size: 16.4 KiB, free: 93.3 MiB)
	24/05/07 08:18:23 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:18:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:18:23 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
	24/05/07 08:18:23 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (941f01f0fea3, executor driver, partition 0, PROCESS_LOCAL, 7878 bytes) 
	24/05/07 08:18:23 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:24.430 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.806853582554517 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:25.074 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:18:24 INFO CodeGenerator: Code generated in 151.722064 ms
	24/05/07 08:18:24 INFO CodeGenerator: Code generated in 22.893594 ms
	24/05/07 08:18:24 INFO CodeGenerator: Code generated in 24.496076 ms
	24/05/07 08:18:24 INFO CodeGenerator: Code generated in 38.090267 ms
	24/05/07 08:18:24 INFO CodeGenerator: Code generated in 20.508457 ms
	24/05/07 08:18:24 INFO JDBCRDD: closed connection
	24/05/07 08:18:24 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2491 bytes result sent to driver
	24/05/07 08:18:24 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1075 ms on 941f01f0fea3 (executor driver) (1/1)
	24/05/07 08:18:24 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
	24/05/07 08:18:24 INFO DAGScheduler: ShuffleMapStage 0 (save at JdbcWriter.java:87) finished in 1.379 s
	24/05/07 08:18:24 INFO DAGScheduler: looking for newly runnable stages
	24/05/07 08:18:24 INFO DAGScheduler: running: Set()
	24/05/07 08:18:24 INFO DAGScheduler: waiting: Set()
	24/05/07 08:18:24 INFO DAGScheduler: failed: Set()
	24/05/07 08:18:24 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
	24/05/07 08:18:24 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
	24/05/07 08:18:24 INFO CodeGenerator: Code generated in 46.247691 ms
	24/05/07 08:18:25 INFO DAGScheduler: Registering RDD 5 (save at JdbcWriter.java:87) as input to shuffle 1
	24/05/07 08:18:25 INFO DAGScheduler: Got map stage job 1 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:18:25 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (save at JdbcWriter.java:87)
	24/05/07 08:18:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
	24/05/07 08:18:25 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:18:25 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[5] at save at JdbcWriter.java:87), which has no missing parents
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:25.434 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.855907780979827 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:26.077 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:18:25 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 40.5 KiB, free 93.2 MiB)
	24/05/07 08:18:25 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 93.2 MiB)
	24/05/07 08:18:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 941f01f0fea3:46707 (size: 19.1 KiB, free: 93.3 MiB)
	24/05/07 08:18:25 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:18:25 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[5] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:18:25 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
	24/05/07 08:18:25 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (941f01f0fea3, executor driver, partition 0, NODE_LOCAL, 8032 bytes) 
	24/05/07 08:18:25 INFO Executor: Running task 0.0 in stage 2.0 (TID 1)
	24/05/07 08:18:25 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 941f01f0fea3:46707 in memory (size: 16.4 KiB, free: 93.3 MiB)
	24/05/07 08:18:25 INFO ShuffleBlockFetcherIterator: Getting 1 (2.9 KiB) non-empty blocks including 1 (2.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
	24/05/07 08:18:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 49 ms
	24/05/07 08:18:25 INFO CodeGenerator: Code generated in 13.005044 ms
	24/05/07 08:18:25 INFO Executor: Finished task 0.0 in stage 2.0 (TID 1). 5420 bytes result sent to driver
	24/05/07 08:18:25 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 272 ms on 941f01f0fea3 (executor driver) (1/1)
	24/05/07 08:18:25 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
	24/05/07 08:18:25 INFO DAGScheduler: ShuffleMapStage 2 (save at JdbcWriter.java:87) finished in 0.447 s
	24/05/07 08:18:25 INFO DAGScheduler: looking for newly runnable stages
	24/05/07 08:18:25 INFO DAGScheduler: running: Set()
	24/05/07 08:18:25 INFO DAGScheduler: waiting: Set()
	24/05/07 08:18:25 INFO DAGScheduler: failed: Set()
	24/05/07 08:18:25 INFO CodeGenerator: Code generated in 15.008802 ms
	24/05/07 08:18:25 INFO SparkContext: Starting job: save at JdbcWriter.java:87
	24/05/07 08:18:25 INFO DAGScheduler: Got job 2 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:18:25 INFO DAGScheduler: Final stage: ResultStage 5 (save at JdbcWriter.java:87)
	24/05/07 08:18:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
	24/05/07 08:18:25 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:18:25 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[10] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:18:25 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 51.4 KiB, free 93.2 MiB)
	24/05/07 08:18:25 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.1 KiB, free 93.2 MiB)
	24/05/07 08:18:25 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 941f01f0fea3:46707 (size: 23.1 KiB, free: 93.3 MiB)
	24/05/07 08:18:25 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:18:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[10] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:18:25 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
	24/05/07 08:18:25 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 2) (941f01f0fea3, executor driver, partition 0, NODE_LOCAL, 8043 bytes) 
	24/05/07 08:18:25 INFO Executor: Running task 0.0 in stage 5.0 (TID 2)
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:26.452 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8988439306358382 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:27.083 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:18:26 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
	24/05/07 08:18:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
	24/05/07 08:18:26 INFO CodeGenerator: Code generated in 9.539223 ms
	24/05/07 08:18:26 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 941f01f0fea3:46707 in memory (size: 19.1 KiB, free: 93.3 MiB)
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:27.454 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7863247863247863 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:18:29.107 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:18:28 INFO CodeGenerator: Code generated in 13.788187 ms
	24/05/07 08:18:28 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 2)
	java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1051'::int4),('3944'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1051_[null check] filtered data persistence'),('2024-05-07 08:17:57'),('2024-05-07 08:17:57')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	24/05/07 08:18:28 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 2) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1051'::int4),('3944'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1051_[null check] filtered data persistence'),('2024-05-07 08:17:57'),('2024-05-07 08:17:57')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	24/05/07 08:18:28 ERROR TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job
	24/05/07 08:18:28 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
	24/05/07 08:18:28 INFO TaskSchedulerImpl: Cancelling stage 5
	24/05/07 08:18:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 2) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1051'::int4),('3944'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1051_[null check] filtered data persistence'),('2024-05-07 08:17:57'),('2024-05-07 08:17:57')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	Driver stacktrace:
	24/05/07 08:18:28 INFO DAGScheduler: ResultStage 5 (save at JdbcWriter.java:87) failed in 2.561 s due to Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 2) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1051'::int4),('3944'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1051_[null check] filtered data persistence'),('2024-05-07 08:17:57'),('2024-05-07 08:17:57')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	Driver stacktrace:
	24/05/07 08:18:28 INFO DAGScheduler: Job 2 failed: save at JdbcWriter.java:87, took 2.637686 s
	Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 2) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1051'::int4),('3944'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1051_[null check] filtered data persistence'),('2024-05-07 08:17:57'),('2024-05-07 08:17:57')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	Driver stacktrace:
		at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
		at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
		at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
		at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
		at scala.Option.foreach(Option.scala:407)
		at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
		at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)
		at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
		at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
		at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
		at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)
		at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3514)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4309)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
		at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4307)
		at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3514)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
		at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
		at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
		at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
		at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
		at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
		at org.apache.dolphinscheduler.data.quality.flow.batch.writer.JdbcWriter.write(JdbcWriter.java:87)
		at org.apache.dolphinscheduler.data.quality.execution.SparkBatchExecution.executeWriter(SparkBatchExecution.java:132)
		at org.apache.dolphinscheduler.data.quality.execution.SparkBatchExecution.execute(SparkBatchExecution.java:58)
		at org.apache.dolphinscheduler.data.quality.context.DataQualityContext.execute(DataQualityContext.java:62)
		at org.apache.dolphinscheduler.data.quality.DataQualityApplication.main(DataQualityApplication.java:78)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
		at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)
		at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
		at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
		at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
		at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
		at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
		at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
	Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1051'::int4),('3944'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1051_[null check] filtered data persistence'),('2024-05-07 08:17:57'),('2024-05-07 08:17:57')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	24/05/07 08:18:28 INFO SparkContext: Invoking stop() from shutdown hook
	24/05/07 08:18:28 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/05/07 08:18:28 INFO SparkUI: Stopped Spark web UI at http://941f01f0fea3:4040
	24/05/07 08:18:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/05/07 08:18:28 INFO MemoryStore: MemoryStore cleared
	24/05/07 08:18:28 INFO BlockManager: BlockManager stopped
	24/05/07 08:18:28 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/05/07 08:18:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/05/07 08:18:28 INFO SparkContext: Successfully stopped SparkContext
	24/05/07 08:18:28 INFO ShutdownHookManager: Shutdown hook called
	24/05/07 08:18:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-5d5aefac-ecc0-4421-83c1-8c7818ee04a7
	24/05/07 08:18:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-f8348a7c-96bd-4240-8e01-f23ad443f97d
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:18:29.120 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1051/3944, processId:316 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:18:29.121 +0800 o.a.d.p.t.a.u.LogUtils:[79] - Start finding appId in /opt/dolphinscheduler/logs/20240507/13505298546272/21/1051/3944.log, fetch way: log 
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:18:29.125 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:18:29.125 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:18:29.126 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:18:29.127 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:18:29.141 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:18:29.142 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:18:29.143 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1051/3944
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:18:29.174 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1051/3944
[WI-1051][TI-3944] - [INFO] 2024-05-07 08:18:29.178 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [WARN] 2024-05-07 08:18:37.986 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[147] - [id: 0x9b5d4ccc, L:/172.18.1.1:1234 - R:/172.18.0.13:48102] is not writable, over high water level : 65536
[WI-0][TI-0] - [WARN] 2024-05-07 08:18:37.986 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[154] - [id: 0x9b5d4ccc, L:/172.18.1.1:1234 - R:/172.18.0.13:48102] is writable, to low water : 32768
[WI-0][TI-3944] - [INFO] 2024-05-07 08:19:06.500 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3944, processInstanceId=1051, status=6, startTime=1715041077174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1051/3944.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1051/3944, endTime=1715041109126, processId=316, appIds=, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3944] - [INFO] 2024-05-07 08:19:06.509 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3944, processInstanceId=1051, status=6, startTime=1715041077174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1051/3944.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1051/3944, endTime=1715041109126, processId=316, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715041146500)
[WI-0][TI-0] - [INFO] 2024-05-07 08:19:28.737 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8202247191011235 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:23:05.457 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7361111111111112 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:23:14.619 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[55] - Receive TaskInstanceKillRequest: TaskInstanceKillRequest(taskInstanceId=3944)
[WI-0][TI-3944] - [ERROR] 2024-05-07 08:23:14.620 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[62] - Cannot find WorkerTaskExecutor for taskInstance: 3944
[WI-0][TI-0] - [INFO] 2024-05-07 08:23:15.508 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7999999999999999 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3944] - [INFO] 2024-05-07 08:24:07.191 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3944, processInstanceId=1051, status=6, startTime=1715041077174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1051/3944.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1051/3944, endTime=1715041109126, processId=316, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715041146500)
[WI-0][TI-3944] - [INFO] 2024-05-07 08:24:07.198 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3944, processInstanceId=1051, status=6, startTime=1715041077174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1051/3944.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1051/3944, endTime=1715041109126, processId=316, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715041447191)
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:35.711 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7272727272727273 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:38.803 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3945, taskName=[null check] filtered data persistence, firstSubmitTime=1715041538768, startTime=0, taskType=DATA_QUALITY, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13505298546272, processDefineVersion=21, appIds=null, processInstanceId=1052, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13001194483488, taskParams={"localParams":[],"resourceList":[],"ruleId":6,"ruleInputParameter":{"check_type":"0","comparison_type":1,"comparison_name":"0","failure_strategy":"0","operator":"0","src_connector_type":1,"src_datasource_id":6,"src_database":"persistence","src_field":"content","src_table":"filtered_data_persistence","threshold":"0"},"sparkParameters":{"deployMode":"local","driverCores":1,"driverMemory":"512M","executorCores":1,"executorMemory":"1G","numExecutors":1,"others":"--conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n--packages org.postgresql:postgresql:42.7.3","yarnQueue":""}}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='[null check] filtered data persistence'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13001194483488'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='1052'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240507'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240506'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3945'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='qualti_test'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13505264408800'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13505298546272'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240507082538'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=org.apache.dolphinscheduler.plugin.task.api.DataQualityTaskExecutionContext@26513d9f, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.804 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: [null check] filtered data persistence to wait queue success
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.804 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.811 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.811 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.811 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.811 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1715041538811
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.812 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 1052_3945
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.813 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3945,
  "taskName" : "[null check] filtered data persistence",
  "firstSubmitTime" : 1715041538768,
  "startTime" : 1715041538811,
  "taskType" : "DATA_QUALITY",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240507/13505298546272/21/1052/3945.log",
  "processId" : 0,
  "processDefineCode" : 13505298546272,
  "processDefineVersion" : 21,
  "processInstanceId" : 1052,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13001194483488,
  "taskParams" : "{\"localParams\":[],\"resourceList\":[],\"ruleId\":6,\"ruleInputParameter\":{\"check_type\":\"0\",\"comparison_type\":1,\"comparison_name\":\"0\",\"failure_strategy\":\"0\",\"operator\":\"0\",\"src_connector_type\":1,\"src_datasource_id\":6,\"src_database\":\"persistence\",\"src_field\":\"content\",\"src_table\":\"filtered_data_persistence\",\"threshold\":\"0\"},\"sparkParameters\":{\"deployMode\":\"local\",\"driverCores\":1,\"driverMemory\":\"512M\",\"executorCores\":1,\"executorMemory\":\"1G\",\"numExecutors\":1,\"others\":\"--conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n--packages org.postgresql:postgresql:42.7.3\",\"yarnQueue\":\"\"}}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "[null check] filtered data persistence"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13001194483488"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1052"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240507"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240506"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3945"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "qualti_test"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13505264408800"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13505298546272"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240507082538"
    }
  },
  "taskAppId" : "1052_3945",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "dataQualityTaskExecutionContext" : {
    "ruleId" : 6,
    "ruleName" : "$t(uniqueness_check)",
    "ruleType" : 0,
    "ruleInputEntryList" : "[{\"id\":1,\"field\":\"src_connector_type\",\"type\":\"select\",\"title\":\"$t(src_connector_type)\",\"data\":\"\",\"options\":\"[{\\\"label\\\":\\\"HIVE\\\",\\\"value\\\":\\\"HIVE\\\"},{\\\"label\\\":\\\"JDBC\\\",\\\"value\\\":\\\"JDBC\\\"}]\",\"placeholder\":\"please select source connector type\",\"optionSourceType\":2,\"dataType\":2,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":1,\"createTime\":null,\"updateTime\":null},{\"id\":2,\"field\":\"src_datasource_id\",\"type\":\"select\",\"title\":\"$t(src_datasource_id)\",\"data\":\"\",\"options\":null,\"placeholder\":\"please select source datasource id\",\"optionSourceType\":1,\"dataType\":2,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":2,\"createTime\":null,\"updateTime\":null},{\"id\":30,\"field\":\"src_database\",\"type\":\"select\",\"title\":\"$t(src_database)\",\"data\":null,\"options\":null,\"placeholder\":\"Please select source database\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":true,\"valuesMap\":null,\"index\":2,\"createTime\":null,\"updateTime\":null},{\"id\":3,\"field\":\"src_table\",\"type\":\"select\",\"title\":\"$t(src_table)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter source table name\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":true,\"valuesMap\":null,\"index\":3,\"createTime\":null,\"updateTime\":null},{\"id\":4,\"field\":\"src_filter\",\"type\":\"input\",\"title\":\"$t(src_filter)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter filter expression\",\"optionSourceType\":0,\"dataType\":3,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":4,\"createTime\":null,\"updateTime\":null},{\"id\":5,\"field\":\"src_field\",\"type\":\"select\",\"title\":\"$t(src_field)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter column, only single column is supported\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":null,\"index\":5,\"createTime\":null,\"updateTime\":null},{\"id\":6,\"field\":\"statistics_name\",\"type\":\"input\",\"title\":\"$t(statistics_name)\",\"data\":\"duplicate_count.duplicates\",\"options\":null,\"placeholder\":\"Please enter statistics name, the alias in statistics execute sql\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":1,\"isShow\":false,\"canEdit\":false,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":\"{\\\"statistics_name\\\":\\\"duplicate_count.duplicates\\\"}\",\"index\":6,\"createTime\":null,\"updateTime\":null},{\"id\":7,\"field\":\"check_type\",\"type\":\"select\",\"title\":\"$t(check_type)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"Expected - Actual\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"Actual - Expected\\\",\\\"value\\\":\\\"1\\\"},{\\\"label\\\":\\\"Actual / Expected\\\",\\\"value\\\":\\\"2\\\"},{\\\"label\\\":\\\"(Expected - Actual) / Expected\\\",\\\"value\\\":\\\"3\\\"}]\",\"placeholder\":\"please select check type\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":7,\"createTime\":null,\"updateTime\":null},{\"id\":8,\"field\":\"operator\",\"type\":\"select\",\"title\":\"$t(operator)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"=\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"<\\\",\\\"value\\\":\\\"1\\\"},{\\\"label\\\":\\\"<=\\\",\\\"value\\\":\\\"2\\\"},{\\\"label\\\":\\\">\\\",\\\"value\\\":\\\"3\\\"},{\\\"label\\\":\\\">=\\\",\\\"value\\\":\\\"4\\\"},{\\\"label\\\":\\\"!=\\\",\\\"value\\\":\\\"5\\\"}]\",\"placeholder\":\"please select operator\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":8,\"createTime\":null,\"updateTime\":null},{\"id\":9,\"field\":\"threshold\",\"type\":\"input\",\"title\":\"$t(threshold)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter threshold, number is needed\",\"optionSourceType\":0,\"dataType\":2,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":null,\"index\":9,\"createTime\":null,\"updateTime\":null},{\"id\":10,\"field\":\"failure_strategy\",\"type\":\"select\",\"title\":\"$t(failure_strategy)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"Alert\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"Block\\\",\\\"value\\\":\\\"1\\\"}]\",\"placeholder\":\"please select failure strategy\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":10,\"createTime\":null,\"updateTime\":null},{\"id\":17,\"field\":\"comparison_name\",\"type\":\"input\",\"title\":\"$t(comparison_name)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter comparison name, the alias in comparison execute sql\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":false,\"canEdit\":false,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":\"\",\"index\":11,\"createTime\":null,\"updateTime\":null},{\"id\":19,\"field\":\"comparison_type\",\"type\":\"select\",\"title\":\"$t(comparison_type)\",\"data\":\"\",\"options\":null,\"placeholder\":\"Please enter comparison title\",\"optionSourceType\":3,\"dataType\":0,\"inputType\":2,\"isShow\":true,\"canEdit\":false,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":12,\"createTime\":null,\"updateTime\":null}]",
    "executeSqlList" : "[{\"id\":6,\"index\":1,\"sql\":\"SELECT ${src_field} FROM ${src_table} group by ${src_field} having count(*) > 1\",\"tableAlias\":\"duplicate_items\",\"type\":0,\"createTime\":\"2021-03-03 11:31:24\",\"updateTime\":\"2021-03-03 11:31:24\",\"errorOutputSql\":true},{\"id\":7,\"index\":1,\"sql\":\"SELECT COUNT(*) AS duplicates FROM duplicate_items\",\"tableAlias\":\"duplicate_count\",\"type\":1,\"createTime\":\"2021-03-03 11:31:24\",\"updateTime\":\"2021-03-03 11:31:24\",\"errorOutputSql\":false}]",
    "comparisonNeedStatisticsValueTable" : false,
    "compareWithFixedValue" : true,
    "hdfsPath" : "hdfs://mycluster:8020/user/default/data_quality_error_data",
    "sourceConnectorType" : "JDBC",
    "sourceType" : 1,
    "sourceConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"persistence\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence\",\"driverClassName\":\"org.postgresql.Driver\",\"validationQuery\":\"select version()\",\"other\":{\"password\":\"root\"}}",
    "targetConnectorType" : null,
    "targetType" : 0,
    "targetConnectionParams" : null,
    "writerConnectorType" : "JDBC",
    "writerType" : 1,
    "writerTable" : "t_ds_dq_execute_result",
    "writerConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"dolphinscheduler\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\"}",
    "statisticsValueConnectorType" : "JDBC",
    "statisticsValueType" : 1,
    "statisticsValueTable" : "t_ds_dq_task_statistics_value",
    "statisticsValueWriterConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"dolphinscheduler\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\"}"
  },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.814 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.820 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.820 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.824 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.825 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.826 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1052/3945 check successfully
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.826 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.dq.DataQualityTaskChannel successfully
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.826 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.826 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.841 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: DATA_QUALITY create successfully
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.842 +0800 o.a.d.p.t.d.DataQualityTask:[89] - Initialize data quality task params {
  "localParams" : [ ],
  "varPool" : null,
  "ruleId" : 6,
  "ruleInputParameter" : {
    "check_type" : "0",
    "comparison_type" : "1",
    "comparison_name" : "0",
    "failure_strategy" : "0",
    "operator" : "0",
    "src_connector_type" : "1",
    "src_datasource_id" : "6",
    "src_database" : "persistence",
    "src_field" : "content",
    "src_table" : "filtered_data_persistence",
    "threshold" : "0"
  },
  "sparkParameters" : {
    "localParams" : null,
    "varPool" : null,
    "mainJar" : null,
    "mainClass" : null,
    "deployMode" : "local",
    "mainArgs" : null,
    "driverCores" : 1,
    "driverMemory" : "512M",
    "numExecutors" : 1,
    "executorCores" : 1,
    "executorMemory" : "1G",
    "appName" : null,
    "yarnQueue" : "",
    "others" : "--conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n--packages org.postgresql:postgresql:42.7.3",
    "programType" : null,
    "resourceList" : [ ]
  }
}
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.861 +0800 o.a.d.p.t.d.DataQualityTask:[119] - data quality configuration: {
  "name" : "$t(uniqueness_check)",
  "env" : {
    "type" : "batch",
    "config" : null
  },
  "readers" : [ {
    "type" : "JDBC",
    "config" : {
      "database" : "persistence",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "output_table" : "persistence_filtered_data_persistence",
      "table" : "filtered_data_persistence",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence?password=root"
    }
  } ],
  "transformers" : [ {
    "type" : "sql",
    "config" : {
      "index" : 1,
      "output_table" : "duplicate_items",
      "sql" : "SELECT content FROM persistence_filtered_data_persistence group by content having count(*) > 1"
    }
  }, {
    "type" : "sql",
    "config" : {
      "index" : 2,
      "output_table" : "duplicate_count",
      "sql" : "SELECT COUNT(*) AS duplicates FROM duplicate_items"
    }
  } ],
  "writers" : [ {
    "type" : "JDBC",
    "config" : {
      "database" : "dolphinscheduler",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "table" : "t_ds_dq_execute_result",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler",
      "sql" : "select 0 as rule_type,'$t(uniqueness_check)' as rule_name,0 as process_definition_id,1052 as process_instance_id,3945 as task_instance_id,duplicate_count.duplicates AS statistics_value,0 AS comparison_value,1 AS comparison_type,0 as check_type,0 as threshold,0 as operator,0 as failure_strategy,'hdfs://mycluster:8020/user/default/data_quality_error_data/0_1052_[null check] filtered data persistence' as error_output_path,'2024-05-07 08:25:38' as create_time,'2024-05-07 08:25:38' as update_time from duplicate_count "
    }
  }, {
    "type" : "JDBC",
    "config" : {
      "database" : "dolphinscheduler",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "table" : "t_ds_dq_task_statistics_value",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler",
      "sql" : "select 0 as process_definition_id,3945 as task_instance_id,6 as rule_id,'KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG=' as unique_code,'duplicate_count.duplicates'AS statistics_name,duplicate_count.duplicates AS statistics_value,'2024-05-07 08:25:38' as data_time,'2024-05-07 08:25:38' as create_time,'2024-05-07 08:25:38' as update_time from duplicate_count"
    }
  }, {
    "type" : "hdfs_file",
    "config" : {
      "path" : "hdfs://mycluster:8020/user/default/data_quality_error_data/0_1052_[null check] filtered data persistence",
      "input_table" : "duplicate_items"
    }
  } ]
}
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.862 +0800 o.a.d.p.d.a.u.CommonUtils:[136] - Trying to get data quality jar in path
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.864 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.865 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.865 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.867 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.867 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.870 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.871 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.871 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${SPARK_HOME}/bin/spark-submit --master local --driver-cores 1 --driver-memory 512M --num-executors 1 --executor-cores 1 --executor-memory 1G --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
--packages org.postgresql:postgresql:42.7.3 /opt/dolphinscheduler/conf/../libs/dolphinscheduler-data-quality-3.2.1.jar "{\"name\":\"$t(uniqueness_check)\",\"env\":{\"type\":\"batch\",\"config\":null},\"readers\":[{\"type\":\"JDBC\",\"config\":{\"database\":\"persistence\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"output_table\":\"persistence_filtered_data_persistence\",\"table\":\"filtered_data_persistence\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence?password=root\"} }],\"transformers\":[{\"type\":\"sql\",\"config\":{\"index\":1,\"output_table\":\"duplicate_items\",\"sql\":\"SELECT content FROM persistence_filtered_data_persistence group by content having count(*) > 1\"} },{\"type\":\"sql\",\"config\":{\"index\":2,\"output_table\":\"duplicate_count\",\"sql\":\"SELECT COUNT(*) AS duplicates FROM duplicate_items\"} }],\"writers\":[{\"type\":\"JDBC\",\"config\":{\"database\":\"dolphinscheduler\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"table\":\"t_ds_dq_execute_result\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\",\"sql\":\"select 0 as rule_type,'$t(uniqueness_check)' as rule_name,0 as process_definition_id,1052 as process_instance_id,3945 as task_instance_id,duplicate_count.duplicates AS statistics_value,0 AS comparison_value,1 AS comparison_type,0 as check_type,0 as threshold,0 as operator,0 as failure_strategy,'hdfs://mycluster:8020/user/default/data_quality_error_data/0_1052_[null check] filtered data persistence' as error_output_path,'2024-05-07 08:25:38' as create_time,'2024-05-07 08:25:38' as update_time from duplicate_count \"} },{\"type\":\"JDBC\",\"config\":{\"database\":\"dolphinscheduler\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"table\":\"t_ds_dq_task_statistics_value\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\",\"sql\":\"select 0 as process_definition_id,3945 as task_instance_id,6 as rule_id,'KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG=' as unique_code,'duplicate_count.duplicates'AS statistics_name,duplicate_count.duplicates AS statistics_value,'2024-05-07 08:25:38' as data_time,'2024-05-07 08:25:38' as create_time,'2024-05-07 08:25:38' as update_time from duplicate_count\"} },{\"type\":\"hdfs_file\",\"config\":{\"path\":\"hdfs://mycluster:8020/user/default/data_quality_error_data/0_1052_[null check] filtered data persistence\",\"input_table\":\"duplicate_items\"} }]}"
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.871 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.871 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1052/3945/1052_3945.sh
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:25:38.890 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 534
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:38.891 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-3945] - [INFO] 2024-05-07 08:25:39.637 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3945, processInstanceId=1052, startTime=1715041538811, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1052/3945.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3945] - [INFO] 2024-05-07 08:25:39.643 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3945, processInstanceId=1052, startTime=1715041538811, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1052/3945.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1715041539636)
[WI-0][TI-3945] - [INFO] 2024-05-07 08:25:39.643 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3945, processInstanceId=1052, startTime=1715041538811, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3945] - [INFO] 2024-05-07 08:25:39.655 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3945, processInstanceId=1052, startTime=1715041538811, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1715041539636)
[WI-0][TI-3945] - [INFO] 2024-05-07 08:25:39.692 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3945, success=true)
[WI-0][TI-3945] - [INFO] 2024-05-07 08:25:39.707 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3945)
[WI-0][TI-3945] - [INFO] 2024-05-07 08:25:39.722 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3945, success=true)
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:39.764 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9197707736389685 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3945] - [INFO] 2024-05-07 08:25:39.771 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3945)
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:41.954 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7361563517915309 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:41.959 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.postgresql#postgresql added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-616ea03c-f956-4554-9d79-e377b3bbb35c;1.0
		confs: [default]
		found org.postgresql#postgresql;42.7.3 in central
		found org.checkerframework#checker-qual;3.42.0 in central
	:: resolution report :: resolve 292ms :: artifacts dl 22ms
		:: modules in use:
		org.checkerframework#checker-qual;3.42.0 from central in [default]
		org.postgresql#postgresql;42.7.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-616ea03c-f956-4554-9d79-e377b3bbb35c
		confs: [default]
		0 artifacts copied, 2 already retrieved (0kB/6ms)
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:42.973 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:25:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	24/05/07 08:25:42 INFO SparkContext: Running Spark version 3.5.1
	24/05/07 08:25:42 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/05/07 08:25:42 INFO SparkContext: Java version 1.8.0_402
	24/05/07 08:25:42 INFO ResourceUtils: ==============================================================
	24/05/07 08:25:42 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/05/07 08:25:42 INFO ResourceUtils: ==============================================================
	24/05/07 08:25:42 INFO SparkContext: Submitted application: (uniqueness_check)
	24/05/07 08:25:42 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/05/07 08:25:42 INFO ResourceProfile: Limiting resource is cpu
	24/05/07 08:25:42 INFO ResourceProfileManager: Added ResourceProfile id: 0
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:42.993 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8006865938156258 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:43.981 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:25:43 INFO SecurityManager: Changing view acls to: default
	24/05/07 08:25:43 INFO SecurityManager: Changing modify acls to: default
	24/05/07 08:25:43 INFO SecurityManager: Changing view acls groups to: 
	24/05/07 08:25:43 INFO SecurityManager: Changing modify acls groups to: 
	24/05/07 08:25:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
	24/05/07 08:25:43 INFO Utils: Successfully started service 'sparkDriver' on port 41111.
	24/05/07 08:25:43 INFO SparkEnv: Registering MapOutputTracker
	24/05/07 08:25:43 INFO SparkEnv: Registering BlockManagerMaster
	24/05/07 08:25:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/05/07 08:25:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/05/07 08:25:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/05/07 08:25:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-32545f78-ed5b-40f7-89be-315d88b72846
	24/05/07 08:25:43 INFO MemoryStore: MemoryStore started with capacity 93.3 MiB
	24/05/07 08:25:43 INFO SparkEnv: Registering OutputCommitCoordinator
	24/05/07 08:25:43 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:43.994 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8868689720065867 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:44.984 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:25:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/05/07 08:25:44 INFO SparkContext: Added JAR file:///tmp/jars/org.postgresql_postgresql-42.7.3.jar at spark://941f01f0fea3:41111/jars/org.postgresql_postgresql-42.7.3.jar with timestamp 1715041542841
	24/05/07 08:25:44 INFO SparkContext: Added JAR file:///tmp/jars/org.checkerframework_checker-qual-3.42.0.jar at spark://941f01f0fea3:41111/jars/org.checkerframework_checker-qual-3.42.0.jar with timestamp 1715041542841
	24/05/07 08:25:44 INFO SparkContext: Added JAR file:/opt/dolphinscheduler/libs/dolphinscheduler-data-quality-3.2.1.jar at spark://941f01f0fea3:41111/jars/dolphinscheduler-data-quality-3.2.1.jar with timestamp 1715041542841
	24/05/07 08:25:44 INFO Executor: Starting executor ID driver on host 941f01f0fea3
	24/05/07 08:25:44 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/05/07 08:25:44 INFO Executor: Java version 1.8.0_402
	24/05/07 08:25:44 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/05/07 08:25:44 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@698fee9a for default.
	24/05/07 08:25:44 INFO Executor: Fetching spark://941f01f0fea3:41111/jars/org.postgresql_postgresql-42.7.3.jar with timestamp 1715041542841
	24/05/07 08:25:44 INFO TransportClientFactory: Successfully created connection to 941f01f0fea3/172.18.1.1:41111 after 49 ms (0 ms spent in bootstraps)
	24/05/07 08:25:44 INFO Utils: Fetching spark://941f01f0fea3:41111/jars/org.postgresql_postgresql-42.7.3.jar to /tmp/spark-b60c0790-255d-48b1-b56c-8b667a09414a/userFiles-18e72e40-a01f-4a80-9d35-f15b5f100195/fetchFileTemp1984511458320816602.tmp
	24/05/07 08:25:44 INFO Executor: Adding file:/tmp/spark-b60c0790-255d-48b1-b56c-8b667a09414a/userFiles-18e72e40-a01f-4a80-9d35-f15b5f100195/org.postgresql_postgresql-42.7.3.jar to class loader default
	24/05/07 08:25:44 INFO Executor: Fetching spark://941f01f0fea3:41111/jars/org.checkerframework_checker-qual-3.42.0.jar with timestamp 1715041542841
	24/05/07 08:25:44 INFO Utils: Fetching spark://941f01f0fea3:41111/jars/org.checkerframework_checker-qual-3.42.0.jar to /tmp/spark-b60c0790-255d-48b1-b56c-8b667a09414a/userFiles-18e72e40-a01f-4a80-9d35-f15b5f100195/fetchFileTemp4680026862024966890.tmp
	24/05/07 08:25:44 INFO Executor: Adding file:/tmp/spark-b60c0790-255d-48b1-b56c-8b667a09414a/userFiles-18e72e40-a01f-4a80-9d35-f15b5f100195/org.checkerframework_checker-qual-3.42.0.jar to class loader default
	24/05/07 08:25:44 INFO Executor: Fetching spark://941f01f0fea3:41111/jars/dolphinscheduler-data-quality-3.2.1.jar with timestamp 1715041542841
	24/05/07 08:25:44 INFO Utils: Fetching spark://941f01f0fea3:41111/jars/dolphinscheduler-data-quality-3.2.1.jar to /tmp/spark-b60c0790-255d-48b1-b56c-8b667a09414a/userFiles-18e72e40-a01f-4a80-9d35-f15b5f100195/fetchFileTemp9055877756268834004.tmp
	24/05/07 08:25:44 INFO Executor: Adding file:/tmp/spark-b60c0790-255d-48b1-b56c-8b667a09414a/userFiles-18e72e40-a01f-4a80-9d35-f15b5f100195/dolphinscheduler-data-quality-3.2.1.jar to class loader default
	24/05/07 08:25:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43799.
	24/05/07 08:25:44 INFO NettyBlockTransferService: Server created on 941f01f0fea3:43799
	24/05/07 08:25:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/05/07 08:25:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 941f01f0fea3, 43799, None)
	24/05/07 08:25:44 INFO BlockManagerMasterEndpoint: Registering block manager 941f01f0fea3:43799 with 93.3 MiB RAM, BlockManagerId(driver, 941f01f0fea3, 43799, None)
	24/05/07 08:25:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 941f01f0fea3, 43799, None)
	24/05/07 08:25:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 941f01f0fea3, 43799, None)
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:44.999 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7966573816155988 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:45.988 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:25:45 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/05/07 08:25:45 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1052/3945/spark-warehouse'.
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:46.001 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7409470752089136 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:47.082 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3946, taskName=[null check] filtered data persistence, firstSubmitTime=1715041547027, startTime=0, taskType=DATA_QUALITY, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13505298546272, processDefineVersion=21, appIds=null, processInstanceId=1053, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13001194483488, taskParams={"localParams":[],"resourceList":[],"ruleId":6,"ruleInputParameter":{"check_type":"0","comparison_type":1,"comparison_name":"0","failure_strategy":"0","operator":"0","src_connector_type":1,"src_datasource_id":6,"src_database":"persistence","src_field":"content","src_table":"filtered_data_persistence","threshold":"0"},"sparkParameters":{"deployMode":"local","driverCores":1,"driverMemory":"512M","executorCores":1,"executorMemory":"1G","numExecutors":1,"others":"--conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n--packages org.postgresql:postgresql:42.7.3","yarnQueue":""}}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='[null check] filtered data persistence'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13001194483488'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='1053'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240507'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240506'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3946'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='qualti_test'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13505264408800'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13505298546272'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240507082547'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=org.apache.dolphinscheduler.plugin.task.api.DataQualityTaskExecutionContext@3e87cecd, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.083 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: [null check] filtered data persistence to wait queue success
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.083 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.085 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.085 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.085 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.086 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1715041547086
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.086 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 1053_3946
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.086 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3946,
  "taskName" : "[null check] filtered data persistence",
  "firstSubmitTime" : 1715041547027,
  "startTime" : 1715041547086,
  "taskType" : "DATA_QUALITY",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240507/13505298546272/21/1053/3946.log",
  "processId" : 0,
  "processDefineCode" : 13505298546272,
  "processDefineVersion" : 21,
  "processInstanceId" : 1053,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13001194483488,
  "taskParams" : "{\"localParams\":[],\"resourceList\":[],\"ruleId\":6,\"ruleInputParameter\":{\"check_type\":\"0\",\"comparison_type\":1,\"comparison_name\":\"0\",\"failure_strategy\":\"0\",\"operator\":\"0\",\"src_connector_type\":1,\"src_datasource_id\":6,\"src_database\":\"persistence\",\"src_field\":\"content\",\"src_table\":\"filtered_data_persistence\",\"threshold\":\"0\"},\"sparkParameters\":{\"deployMode\":\"local\",\"driverCores\":1,\"driverMemory\":\"512M\",\"executorCores\":1,\"executorMemory\":\"1G\",\"numExecutors\":1,\"others\":\"--conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n--packages org.postgresql:postgresql:42.7.3\",\"yarnQueue\":\"\"}}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "[null check] filtered data persistence"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13001194483488"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1053"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240507"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240506"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3946"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "qualti_test"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13505264408800"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13505298546272"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240507082547"
    }
  },
  "taskAppId" : "1053_3946",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "dataQualityTaskExecutionContext" : {
    "ruleId" : 6,
    "ruleName" : "$t(uniqueness_check)",
    "ruleType" : 0,
    "ruleInputEntryList" : "[{\"id\":1,\"field\":\"src_connector_type\",\"type\":\"select\",\"title\":\"$t(src_connector_type)\",\"data\":\"\",\"options\":\"[{\\\"label\\\":\\\"HIVE\\\",\\\"value\\\":\\\"HIVE\\\"},{\\\"label\\\":\\\"JDBC\\\",\\\"value\\\":\\\"JDBC\\\"}]\",\"placeholder\":\"please select source connector type\",\"optionSourceType\":2,\"dataType\":2,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":1,\"createTime\":null,\"updateTime\":null},{\"id\":2,\"field\":\"src_datasource_id\",\"type\":\"select\",\"title\":\"$t(src_datasource_id)\",\"data\":\"\",\"options\":null,\"placeholder\":\"please select source datasource id\",\"optionSourceType\":1,\"dataType\":2,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":2,\"createTime\":null,\"updateTime\":null},{\"id\":30,\"field\":\"src_database\",\"type\":\"select\",\"title\":\"$t(src_database)\",\"data\":null,\"options\":null,\"placeholder\":\"Please select source database\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":true,\"valuesMap\":null,\"index\":2,\"createTime\":null,\"updateTime\":null},{\"id\":3,\"field\":\"src_table\",\"type\":\"select\",\"title\":\"$t(src_table)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter source table name\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":true,\"valuesMap\":null,\"index\":3,\"createTime\":null,\"updateTime\":null},{\"id\":4,\"field\":\"src_filter\",\"type\":\"input\",\"title\":\"$t(src_filter)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter filter expression\",\"optionSourceType\":0,\"dataType\":3,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":4,\"createTime\":null,\"updateTime\":null},{\"id\":5,\"field\":\"src_field\",\"type\":\"select\",\"title\":\"$t(src_field)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter column, only single column is supported\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":null,\"index\":5,\"createTime\":null,\"updateTime\":null},{\"id\":6,\"field\":\"statistics_name\",\"type\":\"input\",\"title\":\"$t(statistics_name)\",\"data\":\"duplicate_count.duplicates\",\"options\":null,\"placeholder\":\"Please enter statistics name, the alias in statistics execute sql\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":1,\"isShow\":false,\"canEdit\":false,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":\"{\\\"statistics_name\\\":\\\"duplicate_count.duplicates\\\"}\",\"index\":6,\"createTime\":null,\"updateTime\":null},{\"id\":7,\"field\":\"check_type\",\"type\":\"select\",\"title\":\"$t(check_type)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"Expected - Actual\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"Actual - Expected\\\",\\\"value\\\":\\\"1\\\"},{\\\"label\\\":\\\"Actual / Expected\\\",\\\"value\\\":\\\"2\\\"},{\\\"label\\\":\\\"(Expected - Actual) / Expected\\\",\\\"value\\\":\\\"3\\\"}]\",\"placeholder\":\"please select check type\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":7,\"createTime\":null,\"updateTime\":null},{\"id\":8,\"field\":\"operator\",\"type\":\"select\",\"title\":\"$t(operator)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"=\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"<\\\",\\\"value\\\":\\\"1\\\"},{\\\"label\\\":\\\"<=\\\",\\\"value\\\":\\\"2\\\"},{\\\"label\\\":\\\">\\\",\\\"value\\\":\\\"3\\\"},{\\\"label\\\":\\\">=\\\",\\\"value\\\":\\\"4\\\"},{\\\"label\\\":\\\"!=\\\",\\\"value\\\":\\\"5\\\"}]\",\"placeholder\":\"please select operator\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":8,\"createTime\":null,\"updateTime\":null},{\"id\":9,\"field\":\"threshold\",\"type\":\"input\",\"title\":\"$t(threshold)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter threshold, number is needed\",\"optionSourceType\":0,\"dataType\":2,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":null,\"index\":9,\"createTime\":null,\"updateTime\":null},{\"id\":10,\"field\":\"failure_strategy\",\"type\":\"select\",\"title\":\"$t(failure_strategy)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"Alert\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"Block\\\",\\\"value\\\":\\\"1\\\"}]\",\"placeholder\":\"please select failure strategy\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":10,\"createTime\":null,\"updateTime\":null},{\"id\":17,\"field\":\"comparison_name\",\"type\":\"input\",\"title\":\"$t(comparison_name)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter comparison name, the alias in comparison execute sql\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":false,\"canEdit\":false,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":\"\",\"index\":11,\"createTime\":null,\"updateTime\":null},{\"id\":19,\"field\":\"comparison_type\",\"type\":\"select\",\"title\":\"$t(comparison_type)\",\"data\":\"\",\"options\":null,\"placeholder\":\"Please enter comparison title\",\"optionSourceType\":3,\"dataType\":0,\"inputType\":2,\"isShow\":true,\"canEdit\":false,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":12,\"createTime\":null,\"updateTime\":null}]",
    "executeSqlList" : "[{\"id\":6,\"index\":1,\"sql\":\"SELECT ${src_field} FROM ${src_table} group by ${src_field} having count(*) > 1\",\"tableAlias\":\"duplicate_items\",\"type\":0,\"createTime\":\"2021-03-03 11:31:24\",\"updateTime\":\"2021-03-03 11:31:24\",\"errorOutputSql\":true},{\"id\":7,\"index\":1,\"sql\":\"SELECT COUNT(*) AS duplicates FROM duplicate_items\",\"tableAlias\":\"duplicate_count\",\"type\":1,\"createTime\":\"2021-03-03 11:31:24\",\"updateTime\":\"2021-03-03 11:31:24\",\"errorOutputSql\":false}]",
    "comparisonNeedStatisticsValueTable" : false,
    "compareWithFixedValue" : true,
    "hdfsPath" : "hdfs://mycluster:8020/user/default/data_quality_error_data",
    "sourceConnectorType" : "JDBC",
    "sourceType" : 1,
    "sourceConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"persistence\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence\",\"driverClassName\":\"org.postgresql.Driver\",\"validationQuery\":\"select version()\",\"other\":{\"password\":\"root\"}}",
    "targetConnectorType" : null,
    "targetType" : 0,
    "targetConnectionParams" : null,
    "writerConnectorType" : "JDBC",
    "writerType" : 1,
    "writerTable" : "t_ds_dq_execute_result",
    "writerConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"dolphinscheduler\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\"}",
    "statisticsValueConnectorType" : "JDBC",
    "statisticsValueType" : 1,
    "statisticsValueTable" : "t_ds_dq_task_statistics_value",
    "statisticsValueWriterConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"dolphinscheduler\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\"}"
  },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.088 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.092 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.093 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.106 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.106 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.108 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1053/3946 check successfully
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.110 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.dq.DataQualityTaskChannel successfully
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.111 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.111 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.111 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: DATA_QUALITY create successfully
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.112 +0800 o.a.d.p.t.d.DataQualityTask:[89] - Initialize data quality task params {
  "localParams" : [ ],
  "varPool" : null,
  "ruleId" : 6,
  "ruleInputParameter" : {
    "check_type" : "0",
    "comparison_type" : "1",
    "comparison_name" : "0",
    "failure_strategy" : "0",
    "operator" : "0",
    "src_connector_type" : "1",
    "src_datasource_id" : "6",
    "src_database" : "persistence",
    "src_field" : "content",
    "src_table" : "filtered_data_persistence",
    "threshold" : "0"
  },
  "sparkParameters" : {
    "localParams" : null,
    "varPool" : null,
    "mainJar" : null,
    "mainClass" : null,
    "deployMode" : "local",
    "mainArgs" : null,
    "driverCores" : 1,
    "driverMemory" : "512M",
    "numExecutors" : 1,
    "executorCores" : 1,
    "executorMemory" : "1G",
    "appName" : null,
    "yarnQueue" : "",
    "others" : "--conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n--packages org.postgresql:postgresql:42.7.3",
    "programType" : null,
    "resourceList" : [ ]
  }
}
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.125 +0800 o.a.d.p.t.d.DataQualityTask:[119] - data quality configuration: {
  "name" : "$t(uniqueness_check)",
  "env" : {
    "type" : "batch",
    "config" : null
  },
  "readers" : [ {
    "type" : "JDBC",
    "config" : {
      "database" : "persistence",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "output_table" : "persistence_filtered_data_persistence",
      "table" : "filtered_data_persistence",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence?password=root"
    }
  } ],
  "transformers" : [ {
    "type" : "sql",
    "config" : {
      "index" : 1,
      "output_table" : "duplicate_items",
      "sql" : "SELECT content FROM persistence_filtered_data_persistence group by content having count(*) > 1"
    }
  }, {
    "type" : "sql",
    "config" : {
      "index" : 2,
      "output_table" : "duplicate_count",
      "sql" : "SELECT COUNT(*) AS duplicates FROM duplicate_items"
    }
  } ],
  "writers" : [ {
    "type" : "JDBC",
    "config" : {
      "database" : "dolphinscheduler",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "table" : "t_ds_dq_execute_result",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler",
      "sql" : "select 0 as rule_type,'$t(uniqueness_check)' as rule_name,0 as process_definition_id,1053 as process_instance_id,3946 as task_instance_id,duplicate_count.duplicates AS statistics_value,0 AS comparison_value,1 AS comparison_type,0 as check_type,0 as threshold,0 as operator,0 as failure_strategy,'hdfs://mycluster:8020/user/default/data_quality_error_data/0_1053_[null check] filtered data persistence' as error_output_path,'2024-05-07 08:25:47' as create_time,'2024-05-07 08:25:47' as update_time from duplicate_count "
    }
  }, {
    "type" : "JDBC",
    "config" : {
      "database" : "dolphinscheduler",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "table" : "t_ds_dq_task_statistics_value",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler",
      "sql" : "select 0 as process_definition_id,3946 as task_instance_id,6 as rule_id,'KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG=' as unique_code,'duplicate_count.duplicates'AS statistics_name,duplicate_count.duplicates AS statistics_value,'2024-05-07 08:25:47' as data_time,'2024-05-07 08:25:47' as create_time,'2024-05-07 08:25:47' as update_time from duplicate_count"
    }
  }, {
    "type" : "hdfs_file",
    "config" : {
      "path" : "hdfs://mycluster:8020/user/default/data_quality_error_data/0_1053_[null check] filtered data persistence",
      "input_table" : "duplicate_items"
    }
  } ]
}
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.126 +0800 o.a.d.p.d.a.u.CommonUtils:[136] - Trying to get data quality jar in path
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.127 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.127 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.127 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.127 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.127 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.130 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.130 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.131 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${SPARK_HOME}/bin/spark-submit --master local --driver-cores 1 --driver-memory 512M --num-executors 1 --executor-cores 1 --executor-memory 1G --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
--packages org.postgresql:postgresql:42.7.3 /opt/dolphinscheduler/conf/../libs/dolphinscheduler-data-quality-3.2.1.jar "{\"name\":\"$t(uniqueness_check)\",\"env\":{\"type\":\"batch\",\"config\":null},\"readers\":[{\"type\":\"JDBC\",\"config\":{\"database\":\"persistence\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"output_table\":\"persistence_filtered_data_persistence\",\"table\":\"filtered_data_persistence\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence?password=root\"} }],\"transformers\":[{\"type\":\"sql\",\"config\":{\"index\":1,\"output_table\":\"duplicate_items\",\"sql\":\"SELECT content FROM persistence_filtered_data_persistence group by content having count(*) > 1\"} },{\"type\":\"sql\",\"config\":{\"index\":2,\"output_table\":\"duplicate_count\",\"sql\":\"SELECT COUNT(*) AS duplicates FROM duplicate_items\"} }],\"writers\":[{\"type\":\"JDBC\",\"config\":{\"database\":\"dolphinscheduler\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"table\":\"t_ds_dq_execute_result\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\",\"sql\":\"select 0 as rule_type,'$t(uniqueness_check)' as rule_name,0 as process_definition_id,1053 as process_instance_id,3946 as task_instance_id,duplicate_count.duplicates AS statistics_value,0 AS comparison_value,1 AS comparison_type,0 as check_type,0 as threshold,0 as operator,0 as failure_strategy,'hdfs://mycluster:8020/user/default/data_quality_error_data/0_1053_[null check] filtered data persistence' as error_output_path,'2024-05-07 08:25:47' as create_time,'2024-05-07 08:25:47' as update_time from duplicate_count \"} },{\"type\":\"JDBC\",\"config\":{\"database\":\"dolphinscheduler\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"table\":\"t_ds_dq_task_statistics_value\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\",\"sql\":\"select 0 as process_definition_id,3946 as task_instance_id,6 as rule_id,'KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG=' as unique_code,'duplicate_count.duplicates'AS statistics_name,duplicate_count.duplicates AS statistics_value,'2024-05-07 08:25:47' as data_time,'2024-05-07 08:25:47' as create_time,'2024-05-07 08:25:47' as update_time from duplicate_count\"} },{\"type\":\"hdfs_file\",\"config\":{\"path\":\"hdfs://mycluster:8020/user/default/data_quality_error_data/0_1053_[null check] filtered data persistence\",\"input_table\":\"duplicate_items\"} }]}"
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.131 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.131 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1053/3946/1053_3946.sh
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:25:47.139 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 628
[WI-0][TI-3946] - [INFO] 2024-05-07 08:25:47.689 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3946, processInstanceId=1053, startTime=1715041547086, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1053/3946.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3946] - [INFO] 2024-05-07 08:25:47.714 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3946, processInstanceId=1053, startTime=1715041547086, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1053/3946.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1715041547681)
[WI-0][TI-3946] - [INFO] 2024-05-07 08:25:47.714 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3946, processInstanceId=1053, startTime=1715041547086, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3946] - [INFO] 2024-05-07 08:25:47.724 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3946, processInstanceId=1053, startTime=1715041547086, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1715041547681)
[WI-0][TI-3946] - [INFO] 2024-05-07 08:25:47.751 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3946, success=true)
[WI-0][TI-3946] - [INFO] 2024-05-07 08:25:47.788 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3946)
[WI-0][TI-3946] - [INFO] 2024-05-07 08:25:47.812 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3946, success=true)
[WI-0][TI-3946] - [INFO] 2024-05-07 08:25:47.835 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3946)
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:48.089 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9942693409742119 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:48.141 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:49.143 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.956923076923077 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:50.152 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9432624113475176 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:51.158 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9439252336448598 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:52.165 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8741381425591952 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:53.037 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:25:52 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:53.176 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8771929824561404 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:53.162 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.postgresql#postgresql added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-7a531136-2a66-43ad-895c-c7bac366bb41;1.0
		confs: [default]
		found org.postgresql#postgresql;42.7.3 in central
		found org.checkerframework#checker-qual;3.42.0 in central
	:: resolution report :: resolve 278ms :: artifacts dl 22ms
		:: modules in use:
		org.checkerframework#checker-qual;3.42.0 from central in [default]
		org.postgresql#postgresql;42.7.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-7a531136-2a66-43ad-895c-c7bac366bb41
		confs: [default]
		0 artifacts copied, 2 already retrieved (0kB/36ms)
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:54.189 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:25:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:54.188 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9557522123893805 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:55.200 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.962059620596206 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:56.071 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:25:55 INFO CodeGenerator: Code generated in 1590.975664 ms
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:56.202 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:25:55 INFO SparkContext: Running Spark version 3.5.1
	24/05/07 08:25:55 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/05/07 08:25:55 INFO SparkContext: Java version 1.8.0_402
	24/05/07 08:25:55 INFO ResourceUtils: ==============================================================
	24/05/07 08:25:55 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/05/07 08:25:55 INFO ResourceUtils: ==============================================================
	24/05/07 08:25:55 INFO SparkContext: Submitted application: (uniqueness_check)
	24/05/07 08:25:55 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/05/07 08:25:55 INFO ResourceProfile: Limiting resource is cpu
	24/05/07 08:25:55 INFO ResourceProfileManager: Added ResourceProfile id: 0
	24/05/07 08:25:56 INFO SecurityManager: Changing view acls to: default
	24/05/07 08:25:56 INFO SecurityManager: Changing modify acls to: default
	24/05/07 08:25:56 INFO SecurityManager: Changing view acls groups to: 
	24/05/07 08:25:56 INFO SecurityManager: Changing modify acls groups to: 
	24/05/07 08:25:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:56.209 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9077490774907748 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:57.087 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:25:56 INFO DAGScheduler: Registering RDD 2 (save at JdbcWriter.java:87) as input to shuffle 0
	24/05/07 08:25:56 INFO DAGScheduler: Got map stage job 0 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:25:56 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (save at JdbcWriter.java:87)
	24/05/07 08:25:56 INFO DAGScheduler: Parents of final stage: List()
	24/05/07 08:25:56 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:25:56 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:25:56 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 34.8 KiB, free 93.3 MiB)
	24/05/07 08:25:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 93.3 MiB)
	24/05/07 08:25:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 941f01f0fea3:43799 (size: 16.4 KiB, free: 93.3 MiB)
	24/05/07 08:25:56 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:25:56 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:25:56 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
	24/05/07 08:25:56 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (941f01f0fea3, executor driver, partition 0, PROCESS_LOCAL, 7878 bytes) 
	24/05/07 08:25:56 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:57.214 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:25:56 INFO Utils: Successfully started service 'sparkDriver' on port 39051.
	24/05/07 08:25:56 INFO SparkEnv: Registering MapOutputTracker
	24/05/07 08:25:57 INFO SparkEnv: Registering BlockManagerMaster
	24/05/07 08:25:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/05/07 08:25:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/05/07 08:25:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:57.218 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.857615894039735 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:58.089 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:25:57 INFO CodeGenerator: Code generated in 38.250249 ms
	24/05/07 08:25:57 INFO CodeGenerator: Code generated in 14.535676 ms
	24/05/07 08:25:57 INFO CodeGenerator: Code generated in 11.615206 ms
	24/05/07 08:25:57 INFO CodeGenerator: Code generated in 6.914565 ms
	24/05/07 08:25:57 INFO CodeGenerator: Code generated in 12.308785 ms
	24/05/07 08:25:57 INFO JDBCRDD: closed connection
	24/05/07 08:25:57 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2534 bytes result sent to driver
	24/05/07 08:25:57 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 991 ms on 941f01f0fea3 (executor driver) (1/1)
	24/05/07 08:25:57 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
	24/05/07 08:25:57 INFO DAGScheduler: ShuffleMapStage 0 (save at JdbcWriter.java:87) finished in 1.336 s
	24/05/07 08:25:57 INFO DAGScheduler: looking for newly runnable stages
	24/05/07 08:25:57 INFO DAGScheduler: running: Set()
	24/05/07 08:25:57 INFO DAGScheduler: waiting: Set()
	24/05/07 08:25:57 INFO DAGScheduler: failed: Set()
	24/05/07 08:25:58 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:58.220 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:25:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7f306fd9-56e3-4929-92b8-30c74832cc2f
	24/05/07 08:25:57 INFO MemoryStore: MemoryStore started with capacity 93.3 MiB
	24/05/07 08:25:57 INFO SparkEnv: Registering OutputCommitCoordinator
	24/05/07 08:25:57 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/05/07 08:25:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
	24/05/07 08:25:57 INFO Utils: Successfully started service 'SparkUI' on port 4041.
	24/05/07 08:25:58 INFO SparkContext: Added JAR file:///tmp/jars/org.postgresql_postgresql-42.7.3.jar at spark://941f01f0fea3:39051/jars/org.postgresql_postgresql-42.7.3.jar with timestamp 1715041555750
	24/05/07 08:25:58 INFO SparkContext: Added JAR file:///tmp/jars/org.checkerframework_checker-qual-3.42.0.jar at spark://941f01f0fea3:39051/jars/org.checkerframework_checker-qual-3.42.0.jar with timestamp 1715041555750
	24/05/07 08:25:58 INFO SparkContext: Added JAR file:/opt/dolphinscheduler/libs/dolphinscheduler-data-quality-3.2.1.jar at spark://941f01f0fea3:39051/jars/dolphinscheduler-data-quality-3.2.1.jar with timestamp 1715041555750
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:58.231 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.920265780730897 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:59.092 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:25:58 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
	24/05/07 08:25:58 INFO CodeGenerator: Code generated in 60.853443 ms
	24/05/07 08:25:58 INFO DAGScheduler: Registering RDD 5 (save at JdbcWriter.java:87) as input to shuffle 1
	24/05/07 08:25:58 INFO DAGScheduler: Got map stage job 1 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:25:58 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (save at JdbcWriter.java:87)
	24/05/07 08:25:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
	24/05/07 08:25:58 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:25:58 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[5] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:25:58 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 40.5 KiB, free 93.2 MiB)
	24/05/07 08:25:58 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 93.2 MiB)
	24/05/07 08:25:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 941f01f0fea3:43799 (size: 19.1 KiB, free: 93.3 MiB)
	24/05/07 08:25:58 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:25:58 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[5] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:25:58 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
	24/05/07 08:25:58 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (941f01f0fea3, executor driver, partition 0, NODE_LOCAL, 8032 bytes) 
	24/05/07 08:25:58 INFO Executor: Running task 0.0 in stage 2.0 (TID 1)
	24/05/07 08:25:58 INFO ShuffleBlockFetcherIterator: Getting 1 (2.9 KiB) non-empty blocks including 1 (2.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
	24/05/07 08:25:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 25 ms
	24/05/07 08:25:58 INFO CodeGenerator: Code generated in 81.347181 ms
	24/05/07 08:25:58 INFO Executor: Finished task 0.0 in stage 2.0 (TID 1). 5420 bytes result sent to driver
	24/05/07 08:25:58 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 272 ms on 941f01f0fea3 (executor driver) (1/1)
	24/05/07 08:25:58 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
	24/05/07 08:25:58 INFO DAGScheduler: ShuffleMapStage 2 (save at JdbcWriter.java:87) finished in 0.329 s
	24/05/07 08:25:58 INFO DAGScheduler: looking for newly runnable stages
	24/05/07 08:25:58 INFO DAGScheduler: running: Set()
	24/05/07 08:25:58 INFO DAGScheduler: waiting: Set()
	24/05/07 08:25:58 INFO DAGScheduler: failed: Set()
	24/05/07 08:25:58 INFO CodeGenerator: Code generated in 21.849878 ms
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:59.237 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:25:58 INFO Executor: Starting executor ID driver on host 941f01f0fea3
	24/05/07 08:25:58 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/05/07 08:25:58 INFO Executor: Java version 1.8.0_402
	24/05/07 08:25:58 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/05/07 08:25:58 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@7fb29ca9 for default.
	24/05/07 08:25:58 INFO Executor: Fetching spark://941f01f0fea3:39051/jars/org.postgresql_postgresql-42.7.3.jar with timestamp 1715041555750
	24/05/07 08:25:58 INFO TransportClientFactory: Successfully created connection to 941f01f0fea3/172.18.1.1:39051 after 179 ms (0 ms spent in bootstraps)
	24/05/07 08:25:58 INFO Utils: Fetching spark://941f01f0fea3:39051/jars/org.postgresql_postgresql-42.7.3.jar to /tmp/spark-a37722a1-0bea-45c9-8cc1-40a9abffa1b9/userFiles-33fedafd-b8a0-48e0-a6d6-dd8bd176ddb3/fetchFileTemp7939221146906775262.tmp
	24/05/07 08:25:59 INFO Executor: Adding file:/tmp/spark-a37722a1-0bea-45c9-8cc1-40a9abffa1b9/userFiles-33fedafd-b8a0-48e0-a6d6-dd8bd176ddb3/org.postgresql_postgresql-42.7.3.jar to class loader default
	24/05/07 08:25:59 INFO Executor: Fetching spark://941f01f0fea3:39051/jars/org.checkerframework_checker-qual-3.42.0.jar with timestamp 1715041555750
	24/05/07 08:25:59 INFO Utils: Fetching spark://941f01f0fea3:39051/jars/org.checkerframework_checker-qual-3.42.0.jar to /tmp/spark-a37722a1-0bea-45c9-8cc1-40a9abffa1b9/userFiles-33fedafd-b8a0-48e0-a6d6-dd8bd176ddb3/fetchFileTemp2768793051556173615.tmp
	24/05/07 08:25:59 INFO Executor: Adding file:/tmp/spark-a37722a1-0bea-45c9-8cc1-40a9abffa1b9/userFiles-33fedafd-b8a0-48e0-a6d6-dd8bd176ddb3/org.checkerframework_checker-qual-3.42.0.jar to class loader default
	24/05/07 08:25:59 INFO Executor: Fetching spark://941f01f0fea3:39051/jars/dolphinscheduler-data-quality-3.2.1.jar with timestamp 1715041555750
	24/05/07 08:25:59 INFO Utils: Fetching spark://941f01f0fea3:39051/jars/dolphinscheduler-data-quality-3.2.1.jar to /tmp/spark-a37722a1-0bea-45c9-8cc1-40a9abffa1b9/userFiles-33fedafd-b8a0-48e0-a6d6-dd8bd176ddb3/fetchFileTemp8234151838708943089.tmp
	24/05/07 08:25:59 INFO Executor: Adding file:/tmp/spark-a37722a1-0bea-45c9-8cc1-40a9abffa1b9/userFiles-33fedafd-b8a0-48e0-a6d6-dd8bd176ddb3/dolphinscheduler-data-quality-3.2.1.jar to class loader default
	24/05/07 08:25:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40765.
	24/05/07 08:25:59 INFO NettyBlockTransferService: Server created on 941f01f0fea3:40765
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:59.256 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.940379403794038 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:25:59.624 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[55] - Receive TaskInstanceKillRequest: TaskInstanceKillRequest(taskInstanceId=3946)
[WI-0][TI-3946] - [INFO] 2024-05-07 08:25:59.722 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[134] - process id:628, cmd:sudo -u default kill -9 628 632 635 638 661 662 663 664 665 666 667 668 669 670 671 672 673 674 676 678 682 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 707 708 709 710 711 712 713 714 715 716 718 727 737 738 739 740 741
[WI-0][TI-3946] - [ERROR] 2024-05-07 08:25:59.845 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[139] - kill task error
org.apache.dolphinscheduler.common.shell.AbstractShell$ExitCodeException: kill: (628): Operation not permitted
kill: (662): No such process
kill: (663): No such process
kill: (664): No such process
kill: (665): No such process
kill: (666): No such process
kill: (667): No such process
kill: (668): No such process
kill: (669): No such process
kill: (670): No such process
kill: (671): No such process
kill: (672): No such process
kill: (673): No such process
kill: (674): No such process
kill: (676): No such process
kill: (678): No such process
kill: (682): No such process
kill: (684): No such process
kill: (685): No such process
kill: (686): No such process
kill: (687): No such process
kill: (688): No such process
kill: (689): No such process
kill: (690): No such process
kill: (691): No such process
kill: (693): No such process
kill: (694): No such process
kill: (695): No such process
kill: (696): No such process
kill: (698): No such process
kill: (699): No such process
kill: (700): No such process
kill: (701): No such process
kill: (702): No such process
kill: (703): No such process
kill: (704): No such process
kill: (708): No such process
kill: (709): No such process
kill: (711): No such process
kill: (712): No such process
kill: (713): No such process
kill: (714): No such process
kill: (715): No such process
kill: (716): No such process
kill: (727): No such process
kill: (739): No such process
kill: (740): No such process
kill: (741): No such process

	at org.apache.dolphinscheduler.common.shell.AbstractShell.runCommand(AbstractShell.java:205)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.run(AbstractShell.java:118)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execute(ShellExecutor.java:125)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:103)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:86)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeShell(OSUtils.java:345)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeCmd(OSUtils.java:334)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.killProcess(TaskInstanceKillOperationFunction.java:135)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.doKill(TaskInstanceKillOperationFunction.java:96)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.operate(TaskInstanceKillOperationFunction.java:69)
	at org.apache.dolphinscheduler.server.worker.rpc.TaskInstanceOperatorImpl.killTask(TaskInstanceOperatorImpl.java:49)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.dolphinscheduler.extract.base.server.ServerMethodInvokerImpl.invoke(ServerMethodInvokerImpl.java:41)
	at org.apache.dolphinscheduler.extract.base.server.JdkDynamicServerHandler.lambda$processReceived$0(JdkDynamicServerHandler.java:108)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[WI-0][TI-3946] - [INFO] 2024-05-07 08:25:59.845 +0800 o.a.d.p.t.a.u.ProcessUtils:[182] - Get appIds from worker 172.18.1.1:1234, taskLogPath: /opt/dolphinscheduler/logs/20240507/13505298546272/21/1053/3946.log
[WI-0][TI-3946] - [INFO] 2024-05-07 08:25:59.846 +0800 o.a.d.p.t.a.u.LogUtils:[79] - Start finding appId in /opt/dolphinscheduler/logs/20240507/13505298546272/21/1053/3946.log, fetch way: log 
[WI-0][TI-3946] - [INFO] 2024-05-07 08:25:59.846 +0800 o.a.d.p.t.a.u.ProcessUtils:[188] - The appId is empty
[WI-0][TI-3946] - [INFO] 2024-05-07 08:25:59.846 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[220] - Begin to kill process process, pid is : 628
[WI-0][TI-3946] - [INFO] 2024-05-07 08:25:59.846 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[225] - Success kill task: 1053_3946, pid: 628
[WI-0][TI-3946] - [INFO] 2024-05-07 08:25:59.846 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[119] - kill task by cancelApplication, taskInstanceId: 3946
[WI-0][TI-0] - [INFO] 2024-05-07 08:26:00.107 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:25:59 INFO SparkContext: Starting job: save at JdbcWriter.java:87
	24/05/07 08:25:59 INFO DAGScheduler: Got job 2 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:25:59 INFO DAGScheduler: Final stage: ResultStage 5 (save at JdbcWriter.java:87)
	24/05/07 08:25:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
	24/05/07 08:25:59 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:25:59 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 941f01f0fea3:43799 in memory (size: 19.1 KiB, free: 93.3 MiB)
	24/05/07 08:25:59 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[10] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:25:59 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 941f01f0fea3:43799 in memory (size: 16.4 KiB, free: 93.3 MiB)
	24/05/07 08:25:59 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 51.4 KiB, free 93.2 MiB)
	24/05/07 08:25:59 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.0 KiB, free 93.2 MiB)
	24/05/07 08:25:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 941f01f0fea3:43799 (size: 23.0 KiB, free: 93.3 MiB)
	24/05/07 08:25:59 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:25:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[10] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:25:59 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
	24/05/07 08:25:59 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 2) (941f01f0fea3, executor driver, partition 0, NODE_LOCAL, 8043 bytes) 
	24/05/07 08:25:59 INFO Executor: Running task 0.0 in stage 5.0 (TID 2)
	24/05/07 08:26:00 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
	24/05/07 08:26:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[WI-0][TI-0] - [INFO] 2024-05-07 08:26:00.324 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:25:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/05/07 08:25:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 941f01f0fea3, 40765, None)
	24/05/07 08:25:59 INFO BlockManagerMasterEndpoint: Registering block manager 941f01f0fea3:40765 with 93.3 MiB RAM, BlockManagerId(driver, 941f01f0fea3, 40765, None)
	24/05/07 08:25:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 941f01f0fea3, 40765, None)
	24/05/07 08:25:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 941f01f0fea3, 40765, None)
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:26:00.342 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has killed. execute path:/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1053/3946, processId:628 ,exitStatusCode:137 ,processWaitForStatus:true ,processExitValue:137
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:26:00.343 +0800 o.a.d.p.t.a.u.LogUtils:[79] - Start finding appId in /opt/dolphinscheduler/logs/20240507/13505298546272/21/1053/3946.log, fetch way: log 
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:26:00.343 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:26:00.343 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:26:00.343 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:26:00.348 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-0][TI-0] - [INFO] 2024-05-07 08:26:00.353 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8941176470588236 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:26:00.363 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: KILL to master : 172.18.1.1:1234
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:26:00.363 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:26:00.364 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1053/3946
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:26:00.366 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1053/3946
[WI-1053][TI-3946] - [INFO] 2024-05-07 08:26:00.366 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3946] - [INFO] 2024-05-07 08:26:00.940 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3946, processInstanceId=1053, status=9, startTime=1715041547086, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1053/3946.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1053/3946, endTime=1715041560344, processId=628, appIds=, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3946] - [INFO] 2024-05-07 08:26:00.947 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3946, processInstanceId=1053, status=9, startTime=1715041547086, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1053/3946.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1053/3946, endTime=1715041560344, processId=628, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715041560939)
[WI-0][TI-0] - [INFO] 2024-05-07 08:26:01.111 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:26:00 INFO CodeGenerator: Code generated in 53.84302 ms
[WI-0][TI-0] - [INFO] 2024-05-07 08:26:02.117 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:26:01 INFO CodeGenerator: Code generated in 17.976216 ms
	24/05/07 08:26:01 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 2)
	java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1052'::int4),('3945'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1052_[null check] filtered data persistence'),('2024-05-07 08:25:38'),('2024-05-07 08:25:38')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	24/05/07 08:26:01 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 2) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1052'::int4),('3945'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1052_[null check] filtered data persistence'),('2024-05-07 08:25:38'),('2024-05-07 08:25:38')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	24/05/07 08:26:01 ERROR TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job
	24/05/07 08:26:01 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
	24/05/07 08:26:01 INFO TaskSchedulerImpl: Cancelling stage 5
	24/05/07 08:26:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 2) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1052'::int4),('3945'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1052_[null check] filtered data persistence'),('2024-05-07 08:25:38'),('2024-05-07 08:25:38')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	Driver stacktrace:
	24/05/07 08:26:01 INFO DAGScheduler: ResultStage 5 (save at JdbcWriter.java:87) failed in 2.556 s due to Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 2) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1052'::int4),('3945'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1052_[null check] filtered data persistence'),('2024-05-07 08:25:38'),('2024-05-07 08:25:38')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	Driver stacktrace:
	24/05/07 08:26:01 INFO DAGScheduler: Job 2 failed: save at JdbcWriter.java:87, took 2.671417 s
	Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 2) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1052'::int4),('3945'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1052_[null check] filtered data persistence'),('2024-05-07 08:25:38'),('2024-05-07 08:25:38')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	Driver stacktrace:
		at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
		at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
		at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
		at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
		at scala.Option.foreach(Option.scala:407)
		at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
		at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)
		at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
		at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
		at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
		at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)
		at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3514)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4309)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
		at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4307)
		at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3514)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
		at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
		at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
		at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
		at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
		at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
		at org.apache.dolphinscheduler.data.quality.flow.batch.writer.JdbcWriter.write(JdbcWriter.java:87)
		at org.apache.dolphinscheduler.data.quality.execution.SparkBatchExecution.executeWriter(SparkBatchExecution.java:132)
		at org.apache.dolphinscheduler.data.quality.execution.SparkBatchExecution.execute(SparkBatchExecution.java:58)
		at org.apache.dolphinscheduler.data.quality.context.DataQualityContext.execute(DataQualityContext.java:62)
		at org.apache.dolphinscheduler.data.quality.DataQualityApplication.main(DataQualityApplication.java:78)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
		at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)
		at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
		at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
		at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
		at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
		at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
		at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
	Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1052'::int4),('3945'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1052_[null check] filtered data persistence'),('2024-05-07 08:25:38'),('2024-05-07 08:25:38')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	24/05/07 08:26:02 INFO SparkContext: Invoking stop() from shutdown hook
	24/05/07 08:26:02 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/05/07 08:26:02 INFO SparkUI: Stopped Spark web UI at http://941f01f0fea3:4040
	24/05/07 08:26:02 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[WI-0][TI-0] - [INFO] 2024-05-07 08:26:03.127 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:26:02 INFO MemoryStore: MemoryStore cleared
	24/05/07 08:26:02 INFO BlockManager: BlockManager stopped
	24/05/07 08:26:02 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/05/07 08:26:02 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/05/07 08:26:02 INFO SparkContext: Successfully stopped SparkContext
	24/05/07 08:26:02 INFO ShutdownHookManager: Shutdown hook called
	24/05/07 08:26:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-9cc046a7-7245-41ea-bc0b-6bcbfb56e35c
	24/05/07 08:26:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-b60c0790-255d-48b1-b56c-8b667a09414a
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:26:03.130 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1052/3945, processId:534 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:26:03.131 +0800 o.a.d.p.t.a.u.LogUtils:[79] - Start finding appId in /opt/dolphinscheduler/logs/20240507/13505298546272/21/1052/3945.log, fetch way: log 
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:26:03.132 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:26:03.133 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:26:03.133 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:26:03.134 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:26:03.150 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:26:03.150 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:26:03.150 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1052/3945
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:26:03.151 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1052/3945
[WI-1052][TI-3945] - [INFO] 2024-05-07 08:26:03.151 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3945] - [INFO] 2024-05-07 08:26:03.958 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3945, processInstanceId=1052, status=6, startTime=1715041538811, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1052/3945.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1052/3945, endTime=1715041563133, processId=534, appIds=, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3945] - [INFO] 2024-05-07 08:26:03.982 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3945, processInstanceId=1052, status=6, startTime=1715041538811, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1052/3945.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1052/3945, endTime=1715041563133, processId=534, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715041563955)
[WI-0][TI-0] - [WARN] 2024-05-07 08:26:06.455 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[147] - [id: 0x9b5d4ccc, L:/172.18.1.1:1234 - R:/172.18.0.13:48102] is not writable, over high water level : 65536
[WI-0][TI-0] - [WARN] 2024-05-07 08:26:06.456 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[154] - [id: 0x9b5d4ccc, L:/172.18.1.1:1234 - R:/172.18.0.13:48102] is writable, to low water : 32768
[WI-0][TI-3944] - [INFO] 2024-05-07 08:29:07.346 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3944, processInstanceId=1051, status=6, startTime=1715041077174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1051/3944.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1051/3944, endTime=1715041109126, processId=316, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715041447191)
[WI-0][TI-3944] - [INFO] 2024-05-07 08:29:07.349 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3944, processInstanceId=1051, status=6, startTime=1715041077174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1051/3944.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1051/3944, endTime=1715041109126, processId=316, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715041747346)
[WI-0][TI-3946] - [INFO] 2024-05-07 08:31:01.802 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3946, processInstanceId=1053, status=9, startTime=1715041547086, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1053/3946.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1053/3946, endTime=1715041560344, processId=628, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715041560939)
[WI-0][TI-3946] - [INFO] 2024-05-07 08:31:01.811 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3946, processInstanceId=1053, status=9, startTime=1715041547086, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1053/3946.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1053/3946, endTime=1715041560344, processId=628, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715041861802)
[WI-0][TI-3945] - [INFO] 2024-05-07 08:31:04.817 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3945, processInstanceId=1052, status=6, startTime=1715041538811, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1052/3945.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1052/3945, endTime=1715041563133, processId=534, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715041563955)
[WI-0][TI-3945] - [INFO] 2024-05-07 08:31:04.822 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3945, processInstanceId=1052, status=6, startTime=1715041538811, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1052/3945.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1052/3945, endTime=1715041563133, processId=534, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715041864816)
[WI-0][TI-0] - [INFO] 2024-05-07 08:31:44.259 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[55] - Receive TaskInstanceKillRequest: TaskInstanceKillRequest(taskInstanceId=3945)
[WI-0][TI-3945] - [ERROR] 2024-05-07 08:31:44.260 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[62] - Cannot find WorkerTaskExecutor for taskInstance: 3945
[WI-0][TI-0] - [INFO] 2024-05-07 08:31:54.067 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3947, taskName=[null check] filtered data persistence, firstSubmitTime=1715041914044, startTime=0, taskType=DATA_QUALITY, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13505298546272, processDefineVersion=21, appIds=null, processInstanceId=1054, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13001194483488, taskParams={"localParams":[],"resourceList":[],"ruleId":6,"ruleInputParameter":{"check_type":"0","comparison_type":1,"comparison_name":"0","failure_strategy":"0","operator":"0","src_connector_type":1,"src_datasource_id":6,"src_database":"persistence","src_field":"content","src_table":"filtered_data_persistence","threshold":"0"},"sparkParameters":{"deployMode":"local","driverCores":1,"driverMemory":"512M","executorCores":1,"executorMemory":"1G","numExecutors":1,"others":"--conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n--packages org.postgresql:postgresql:42.7.3","yarnQueue":""}}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='[null check] filtered data persistence'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13001194483488'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='1054'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240507'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240506'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3947'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='qualti_test'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13505264408800'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13505298546272'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240507083154'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=org.apache.dolphinscheduler.plugin.task.api.DataQualityTaskExecutionContext@2f3935fc, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.070 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: [null check] filtered data persistence to wait queue success
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.070 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.072 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.072 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.072 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.072 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1715041914072
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.073 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 1054_3947
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.073 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3947,
  "taskName" : "[null check] filtered data persistence",
  "firstSubmitTime" : 1715041914044,
  "startTime" : 1715041914072,
  "taskType" : "DATA_QUALITY",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240507/13505298546272/21/1054/3947.log",
  "processId" : 0,
  "processDefineCode" : 13505298546272,
  "processDefineVersion" : 21,
  "processInstanceId" : 1054,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13001194483488,
  "taskParams" : "{\"localParams\":[],\"resourceList\":[],\"ruleId\":6,\"ruleInputParameter\":{\"check_type\":\"0\",\"comparison_type\":1,\"comparison_name\":\"0\",\"failure_strategy\":\"0\",\"operator\":\"0\",\"src_connector_type\":1,\"src_datasource_id\":6,\"src_database\":\"persistence\",\"src_field\":\"content\",\"src_table\":\"filtered_data_persistence\",\"threshold\":\"0\"},\"sparkParameters\":{\"deployMode\":\"local\",\"driverCores\":1,\"driverMemory\":\"512M\",\"executorCores\":1,\"executorMemory\":\"1G\",\"numExecutors\":1,\"others\":\"--conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n--packages org.postgresql:postgresql:42.7.3\",\"yarnQueue\":\"\"}}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "[null check] filtered data persistence"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13001194483488"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1054"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240507"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240506"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3947"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "qualti_test"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13505264408800"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13505298546272"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240507083154"
    }
  },
  "taskAppId" : "1054_3947",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "dataQualityTaskExecutionContext" : {
    "ruleId" : 6,
    "ruleName" : "$t(uniqueness_check)",
    "ruleType" : 0,
    "ruleInputEntryList" : "[{\"id\":1,\"field\":\"src_connector_type\",\"type\":\"select\",\"title\":\"$t(src_connector_type)\",\"data\":\"\",\"options\":\"[{\\\"label\\\":\\\"HIVE\\\",\\\"value\\\":\\\"HIVE\\\"},{\\\"label\\\":\\\"JDBC\\\",\\\"value\\\":\\\"JDBC\\\"}]\",\"placeholder\":\"please select source connector type\",\"optionSourceType\":2,\"dataType\":2,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":1,\"createTime\":null,\"updateTime\":null},{\"id\":2,\"field\":\"src_datasource_id\",\"type\":\"select\",\"title\":\"$t(src_datasource_id)\",\"data\":\"\",\"options\":null,\"placeholder\":\"please select source datasource id\",\"optionSourceType\":1,\"dataType\":2,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":2,\"createTime\":null,\"updateTime\":null},{\"id\":30,\"field\":\"src_database\",\"type\":\"select\",\"title\":\"$t(src_database)\",\"data\":null,\"options\":null,\"placeholder\":\"Please select source database\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":true,\"valuesMap\":null,\"index\":2,\"createTime\":null,\"updateTime\":null},{\"id\":3,\"field\":\"src_table\",\"type\":\"select\",\"title\":\"$t(src_table)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter source table name\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":true,\"valuesMap\":null,\"index\":3,\"createTime\":null,\"updateTime\":null},{\"id\":4,\"field\":\"src_filter\",\"type\":\"input\",\"title\":\"$t(src_filter)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter filter expression\",\"optionSourceType\":0,\"dataType\":3,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":4,\"createTime\":null,\"updateTime\":null},{\"id\":5,\"field\":\"src_field\",\"type\":\"select\",\"title\":\"$t(src_field)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter column, only single column is supported\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":null,\"index\":5,\"createTime\":null,\"updateTime\":null},{\"id\":6,\"field\":\"statistics_name\",\"type\":\"input\",\"title\":\"$t(statistics_name)\",\"data\":\"duplicate_count.duplicates\",\"options\":null,\"placeholder\":\"Please enter statistics name, the alias in statistics execute sql\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":1,\"isShow\":false,\"canEdit\":false,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":\"{\\\"statistics_name\\\":\\\"duplicate_count.duplicates\\\"}\",\"index\":6,\"createTime\":null,\"updateTime\":null},{\"id\":7,\"field\":\"check_type\",\"type\":\"select\",\"title\":\"$t(check_type)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"Expected - Actual\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"Actual - Expected\\\",\\\"value\\\":\\\"1\\\"},{\\\"label\\\":\\\"Actual / Expected\\\",\\\"value\\\":\\\"2\\\"},{\\\"label\\\":\\\"(Expected - Actual) / Expected\\\",\\\"value\\\":\\\"3\\\"}]\",\"placeholder\":\"please select check type\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":7,\"createTime\":null,\"updateTime\":null},{\"id\":8,\"field\":\"operator\",\"type\":\"select\",\"title\":\"$t(operator)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"=\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"<\\\",\\\"value\\\":\\\"1\\\"},{\\\"label\\\":\\\"<=\\\",\\\"value\\\":\\\"2\\\"},{\\\"label\\\":\\\">\\\",\\\"value\\\":\\\"3\\\"},{\\\"label\\\":\\\">=\\\",\\\"value\\\":\\\"4\\\"},{\\\"label\\\":\\\"!=\\\",\\\"value\\\":\\\"5\\\"}]\",\"placeholder\":\"please select operator\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":8,\"createTime\":null,\"updateTime\":null},{\"id\":9,\"field\":\"threshold\",\"type\":\"input\",\"title\":\"$t(threshold)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter threshold, number is needed\",\"optionSourceType\":0,\"dataType\":2,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":null,\"index\":9,\"createTime\":null,\"updateTime\":null},{\"id\":10,\"field\":\"failure_strategy\",\"type\":\"select\",\"title\":\"$t(failure_strategy)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"Alert\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"Block\\\",\\\"value\\\":\\\"1\\\"}]\",\"placeholder\":\"please select failure strategy\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":10,\"createTime\":null,\"updateTime\":null},{\"id\":17,\"field\":\"comparison_name\",\"type\":\"input\",\"title\":\"$t(comparison_name)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter comparison name, the alias in comparison execute sql\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":false,\"canEdit\":false,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":\"\",\"index\":11,\"createTime\":null,\"updateTime\":null},{\"id\":19,\"field\":\"comparison_type\",\"type\":\"select\",\"title\":\"$t(comparison_type)\",\"data\":\"\",\"options\":null,\"placeholder\":\"Please enter comparison title\",\"optionSourceType\":3,\"dataType\":0,\"inputType\":2,\"isShow\":true,\"canEdit\":false,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":12,\"createTime\":null,\"updateTime\":null}]",
    "executeSqlList" : "[{\"id\":6,\"index\":1,\"sql\":\"SELECT ${src_field} FROM ${src_table} group by ${src_field} having count(*) > 1\",\"tableAlias\":\"duplicate_items\",\"type\":0,\"createTime\":\"2021-03-03 11:31:24\",\"updateTime\":\"2021-03-03 11:31:24\",\"errorOutputSql\":true},{\"id\":7,\"index\":1,\"sql\":\"SELECT COUNT(*) AS duplicates FROM duplicate_items\",\"tableAlias\":\"duplicate_count\",\"type\":1,\"createTime\":\"2021-03-03 11:31:24\",\"updateTime\":\"2021-03-03 11:31:24\",\"errorOutputSql\":false}]",
    "comparisonNeedStatisticsValueTable" : false,
    "compareWithFixedValue" : true,
    "hdfsPath" : "hdfs://mycluster:8020/user/default/data_quality_error_data",
    "sourceConnectorType" : "JDBC",
    "sourceType" : 1,
    "sourceConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"persistence\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence\",\"driverClassName\":\"org.postgresql.Driver\",\"validationQuery\":\"select version()\",\"other\":{\"password\":\"root\"}}",
    "targetConnectorType" : null,
    "targetType" : 0,
    "targetConnectionParams" : null,
    "writerConnectorType" : "JDBC",
    "writerType" : 1,
    "writerTable" : "t_ds_dq_execute_result",
    "writerConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"dolphinscheduler\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\"}",
    "statisticsValueConnectorType" : "JDBC",
    "statisticsValueType" : 1,
    "statisticsValueTable" : "t_ds_dq_task_statistics_value",
    "statisticsValueWriterConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"dolphinscheduler\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\"}"
  },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.074 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.074 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.074 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.086 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.087 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.087 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1054/3947 check successfully
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.087 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.dq.DataQualityTaskChannel successfully
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.087 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.091 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.091 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: DATA_QUALITY create successfully
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.092 +0800 o.a.d.p.t.d.DataQualityTask:[89] - Initialize data quality task params {
  "localParams" : [ ],
  "varPool" : null,
  "ruleId" : 6,
  "ruleInputParameter" : {
    "check_type" : "0",
    "comparison_type" : "1",
    "comparison_name" : "0",
    "failure_strategy" : "0",
    "operator" : "0",
    "src_connector_type" : "1",
    "src_datasource_id" : "6",
    "src_database" : "persistence",
    "src_field" : "content",
    "src_table" : "filtered_data_persistence",
    "threshold" : "0"
  },
  "sparkParameters" : {
    "localParams" : null,
    "varPool" : null,
    "mainJar" : null,
    "mainClass" : null,
    "deployMode" : "local",
    "mainArgs" : null,
    "driverCores" : 1,
    "driverMemory" : "512M",
    "numExecutors" : 1,
    "executorCores" : 1,
    "executorMemory" : "1G",
    "appName" : null,
    "yarnQueue" : "",
    "others" : "--conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n--packages org.postgresql:postgresql:42.7.3",
    "programType" : null,
    "resourceList" : [ ]
  }
}
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.116 +0800 o.a.d.p.t.d.DataQualityTask:[119] - data quality configuration: {
  "name" : "$t(uniqueness_check)",
  "env" : {
    "type" : "batch",
    "config" : null
  },
  "readers" : [ {
    "type" : "JDBC",
    "config" : {
      "database" : "persistence",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "output_table" : "persistence_filtered_data_persistence",
      "table" : "filtered_data_persistence",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence?password=root"
    }
  } ],
  "transformers" : [ {
    "type" : "sql",
    "config" : {
      "index" : 1,
      "output_table" : "duplicate_items",
      "sql" : "SELECT content FROM persistence_filtered_data_persistence group by content having count(*) > 1"
    }
  }, {
    "type" : "sql",
    "config" : {
      "index" : 2,
      "output_table" : "duplicate_count",
      "sql" : "SELECT COUNT(*) AS duplicates FROM duplicate_items"
    }
  } ],
  "writers" : [ {
    "type" : "JDBC",
    "config" : {
      "database" : "dolphinscheduler",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "table" : "t_ds_dq_execute_result",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler",
      "sql" : "select 0 as rule_type,'$t(uniqueness_check)' as rule_name,0 as process_definition_id,1054 as process_instance_id,3947 as task_instance_id,duplicate_count.duplicates AS statistics_value,0 AS comparison_value,1 AS comparison_type,0 as check_type,0 as threshold,0 as operator,0 as failure_strategy,'hdfs://mycluster:8020/user/default/data_quality_error_data/0_1054_[null check] filtered data persistence' as error_output_path,'2024-05-07 08:31:54' as create_time,'2024-05-07 08:31:54' as update_time from duplicate_count "
    }
  }, {
    "type" : "JDBC",
    "config" : {
      "database" : "dolphinscheduler",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "table" : "t_ds_dq_task_statistics_value",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler",
      "sql" : "select 0 as process_definition_id,3947 as task_instance_id,6 as rule_id,'KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG=' as unique_code,'duplicate_count.duplicates'AS statistics_name,duplicate_count.duplicates AS statistics_value,'2024-05-07 08:31:54' as data_time,'2024-05-07 08:31:54' as create_time,'2024-05-07 08:31:54' as update_time from duplicate_count"
    }
  }, {
    "type" : "hdfs_file",
    "config" : {
      "path" : "hdfs://mycluster:8020/user/default/data_quality_error_data/0_1054_[null check] filtered data persistence",
      "input_table" : "duplicate_items"
    }
  } ]
}
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.117 +0800 o.a.d.p.d.a.u.CommonUtils:[136] - Trying to get data quality jar in path
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.118 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.118 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.118 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.118 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.119 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.120 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.126 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.155 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${SPARK_HOME}/bin/spark-submit --master local --driver-cores 1 --driver-memory 512M --num-executors 1 --executor-cores 1 --executor-memory 1G --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
--packages org.postgresql:postgresql:42.7.3 /opt/dolphinscheduler/conf/../libs/dolphinscheduler-data-quality-3.2.1.jar "{\"name\":\"$t(uniqueness_check)\",\"env\":{\"type\":\"batch\",\"config\":null},\"readers\":[{\"type\":\"JDBC\",\"config\":{\"database\":\"persistence\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"output_table\":\"persistence_filtered_data_persistence\",\"table\":\"filtered_data_persistence\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence?password=root\"} }],\"transformers\":[{\"type\":\"sql\",\"config\":{\"index\":1,\"output_table\":\"duplicate_items\",\"sql\":\"SELECT content FROM persistence_filtered_data_persistence group by content having count(*) > 1\"} },{\"type\":\"sql\",\"config\":{\"index\":2,\"output_table\":\"duplicate_count\",\"sql\":\"SELECT COUNT(*) AS duplicates FROM duplicate_items\"} }],\"writers\":[{\"type\":\"JDBC\",\"config\":{\"database\":\"dolphinscheduler\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"table\":\"t_ds_dq_execute_result\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\",\"sql\":\"select 0 as rule_type,'$t(uniqueness_check)' as rule_name,0 as process_definition_id,1054 as process_instance_id,3947 as task_instance_id,duplicate_count.duplicates AS statistics_value,0 AS comparison_value,1 AS comparison_type,0 as check_type,0 as threshold,0 as operator,0 as failure_strategy,'hdfs://mycluster:8020/user/default/data_quality_error_data/0_1054_[null check] filtered data persistence' as error_output_path,'2024-05-07 08:31:54' as create_time,'2024-05-07 08:31:54' as update_time from duplicate_count \"} },{\"type\":\"JDBC\",\"config\":{\"database\":\"dolphinscheduler\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"table\":\"t_ds_dq_task_statistics_value\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\",\"sql\":\"select 0 as process_definition_id,3947 as task_instance_id,6 as rule_id,'KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG=' as unique_code,'duplicate_count.duplicates'AS statistics_name,duplicate_count.duplicates AS statistics_value,'2024-05-07 08:31:54' as data_time,'2024-05-07 08:31:54' as create_time,'2024-05-07 08:31:54' as update_time from duplicate_count\"} },{\"type\":\"hdfs_file\",\"config\":{\"path\":\"hdfs://mycluster:8020/user/default/data_quality_error_data/0_1054_[null check] filtered data persistence\",\"input_table\":\"duplicate_items\"} }]}"
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.172 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.176 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1054/3947/1054_3947.sh
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:31:54.200 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 835
[WI-0][TI-0] - [INFO] 2024-05-07 08:31:54.211 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-3947] - [INFO] 2024-05-07 08:31:54.962 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3947, processInstanceId=1054, startTime=1715041914072, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1054/3947.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3947] - [INFO] 2024-05-07 08:31:54.972 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3947, processInstanceId=1054, startTime=1715041914072, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1054/3947.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1715041914962)
[WI-0][TI-3947] - [INFO] 2024-05-07 08:31:54.974 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3947, processInstanceId=1054, startTime=1715041914072, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3947] - [INFO] 2024-05-07 08:31:54.977 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3947, processInstanceId=1054, startTime=1715041914072, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1715041914962)
[WI-0][TI-3947] - [INFO] 2024-05-07 08:31:54.984 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3947, success=true)
[WI-0][TI-3947] - [INFO] 2024-05-07 08:31:54.990 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3947)
[WI-0][TI-3947] - [INFO] 2024-05-07 08:31:54.997 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3947, success=true)
[WI-0][TI-3947] - [INFO] 2024-05-07 08:31:55.002 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3947)
[WI-0][TI-0] - [INFO] 2024-05-07 08:31:55.098 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8117283950617283 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:31:56.115 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.784741144414169 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:31:57.244 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.postgresql#postgresql added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-846fa007-4827-442c-a1eb-de31e6305cdc;1.0
		confs: [default]
		found org.postgresql#postgresql;42.7.3 in central
		found org.checkerframework#checker-qual;3.42.0 in central
	:: resolution report :: resolve 105ms :: artifacts dl 5ms
		:: modules in use:
		org.checkerframework#checker-qual;3.42.0 from central in [default]
		org.postgresql#postgresql;42.7.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-846fa007-4827-442c-a1eb-de31e6305cdc
		confs: [default]
		0 artifacts copied, 2 already retrieved (0kB/4ms)
	24/05/07 08:31:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-07 08:31:58.126 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7686567164179104 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:31:58.250 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:31:57 INFO SparkContext: Running Spark version 3.5.1
	24/05/07 08:31:57 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/05/07 08:31:57 INFO SparkContext: Java version 1.8.0_402
	24/05/07 08:31:57 INFO ResourceUtils: ==============================================================
	24/05/07 08:31:57 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/05/07 08:31:57 INFO ResourceUtils: ==============================================================
	24/05/07 08:31:57 INFO SparkContext: Submitted application: (uniqueness_check)
	24/05/07 08:31:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/05/07 08:31:57 INFO ResourceProfile: Limiting resource is cpu
	24/05/07 08:31:57 INFO ResourceProfileManager: Added ResourceProfile id: 0
	24/05/07 08:31:57 INFO SecurityManager: Changing view acls to: default
	24/05/07 08:31:57 INFO SecurityManager: Changing modify acls to: default
	24/05/07 08:31:57 INFO SecurityManager: Changing view acls groups to: 
	24/05/07 08:31:57 INFO SecurityManager: Changing modify acls groups to: 
	24/05/07 08:31:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
	24/05/07 08:31:58 INFO Utils: Successfully started service 'sparkDriver' on port 40377.
	24/05/07 08:31:58 INFO SparkEnv: Registering MapOutputTracker
	24/05/07 08:31:58 INFO SparkEnv: Registering BlockManagerMaster
	24/05/07 08:31:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/05/07 08:31:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[WI-0][TI-0] - [INFO] 2024-05-07 08:31:59.144 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7638888888888888 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:31:59.252 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:31:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/05/07 08:31:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d7fc85c1-37eb-46ef-900b-650c7ce296d2
	24/05/07 08:31:58 INFO MemoryStore: MemoryStore started with capacity 93.3 MiB
	24/05/07 08:31:58 INFO SparkEnv: Registering OutputCommitCoordinator
	24/05/07 08:31:58 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/05/07 08:31:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/05/07 08:31:58 INFO SparkContext: Added JAR file:///tmp/jars/org.postgresql_postgresql-42.7.3.jar at spark://941f01f0fea3:40377/jars/org.postgresql_postgresql-42.7.3.jar with timestamp 1715041917534
	24/05/07 08:31:58 INFO SparkContext: Added JAR file:///tmp/jars/org.checkerframework_checker-qual-3.42.0.jar at spark://941f01f0fea3:40377/jars/org.checkerframework_checker-qual-3.42.0.jar with timestamp 1715041917534
	24/05/07 08:31:58 INFO SparkContext: Added JAR file:/opt/dolphinscheduler/libs/dolphinscheduler-data-quality-3.2.1.jar at spark://941f01f0fea3:40377/jars/dolphinscheduler-data-quality-3.2.1.jar with timestamp 1715041917534
	24/05/07 08:31:59 INFO Executor: Starting executor ID driver on host 941f01f0fea3
	24/05/07 08:31:59 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/05/07 08:31:59 INFO Executor: Java version 1.8.0_402
	24/05/07 08:31:59 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/05/07 08:31:59 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@698fee9a for default.
	24/05/07 08:31:59 INFO Executor: Fetching spark://941f01f0fea3:40377/jars/org.postgresql_postgresql-42.7.3.jar with timestamp 1715041917534
	24/05/07 08:31:59 INFO TransportClientFactory: Successfully created connection to 941f01f0fea3/172.18.1.1:40377 after 65 ms (0 ms spent in bootstraps)
	24/05/07 08:31:59 INFO Utils: Fetching spark://941f01f0fea3:40377/jars/org.postgresql_postgresql-42.7.3.jar to /tmp/spark-218198b3-5e31-46fd-8f35-09905fbec001/userFiles-66b515a1-fe1b-451b-901d-a1928868967b/fetchFileTemp6444040045570067040.tmp
[WI-0][TI-0] - [INFO] 2024-05-07 08:32:00.179 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.782608695652174 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:32:00.257 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:31:59 INFO Executor: Adding file:/tmp/spark-218198b3-5e31-46fd-8f35-09905fbec001/userFiles-66b515a1-fe1b-451b-901d-a1928868967b/org.postgresql_postgresql-42.7.3.jar to class loader default
	24/05/07 08:31:59 INFO Executor: Fetching spark://941f01f0fea3:40377/jars/org.checkerframework_checker-qual-3.42.0.jar with timestamp 1715041917534
	24/05/07 08:31:59 INFO Utils: Fetching spark://941f01f0fea3:40377/jars/org.checkerframework_checker-qual-3.42.0.jar to /tmp/spark-218198b3-5e31-46fd-8f35-09905fbec001/userFiles-66b515a1-fe1b-451b-901d-a1928868967b/fetchFileTemp4822581081072325299.tmp
	24/05/07 08:31:59 INFO Executor: Adding file:/tmp/spark-218198b3-5e31-46fd-8f35-09905fbec001/userFiles-66b515a1-fe1b-451b-901d-a1928868967b/org.checkerframework_checker-qual-3.42.0.jar to class loader default
	24/05/07 08:31:59 INFO Executor: Fetching spark://941f01f0fea3:40377/jars/dolphinscheduler-data-quality-3.2.1.jar with timestamp 1715041917534
	24/05/07 08:31:59 INFO Utils: Fetching spark://941f01f0fea3:40377/jars/dolphinscheduler-data-quality-3.2.1.jar to /tmp/spark-218198b3-5e31-46fd-8f35-09905fbec001/userFiles-66b515a1-fe1b-451b-901d-a1928868967b/fetchFileTemp3270172366011287391.tmp
	24/05/07 08:31:59 INFO Executor: Adding file:/tmp/spark-218198b3-5e31-46fd-8f35-09905fbec001/userFiles-66b515a1-fe1b-451b-901d-a1928868967b/dolphinscheduler-data-quality-3.2.1.jar to class loader default
	24/05/07 08:31:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40763.
	24/05/07 08:31:59 INFO NettyBlockTransferService: Server created on 941f01f0fea3:40763
	24/05/07 08:31:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/05/07 08:31:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 941f01f0fea3, 40763, None)
	24/05/07 08:31:59 INFO BlockManagerMasterEndpoint: Registering block manager 941f01f0fea3:40763 with 93.3 MiB RAM, BlockManagerId(driver, 941f01f0fea3, 40763, None)
	24/05/07 08:31:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 941f01f0fea3, 40763, None)
	24/05/07 08:31:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 941f01f0fea3, 40763, None)
	24/05/07 08:31:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/05/07 08:31:59 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1054/3947/spark-warehouse'.
[WI-0][TI-0] - [INFO] 2024-05-07 08:32:05.266 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:32:04 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[WI-0][TI-0] - [INFO] 2024-05-07 08:32:06.283 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:32:05 INFO CodeGenerator: Code generated in 461.44812 ms
	24/05/07 08:32:06 INFO DAGScheduler: Registering RDD 2 (save at JdbcWriter.java:87) as input to shuffle 0
	24/05/07 08:32:06 INFO DAGScheduler: Got map stage job 0 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:32:06 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (save at JdbcWriter.java:87)
	24/05/07 08:32:06 INFO DAGScheduler: Parents of final stage: List()
	24/05/07 08:32:06 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:32:06 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at save at JdbcWriter.java:87), which has no missing parents
[WI-0][TI-0] - [INFO] 2024-05-07 08:32:07.292 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:32:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 34.8 KiB, free 93.3 MiB)
	24/05/07 08:32:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 93.3 MiB)
	24/05/07 08:32:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 941f01f0fea3:40763 (size: 16.4 KiB, free: 93.3 MiB)
	24/05/07 08:32:06 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:32:06 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:32:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
	24/05/07 08:32:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (941f01f0fea3, executor driver, partition 0, PROCESS_LOCAL, 7878 bytes) 
	24/05/07 08:32:06 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
	24/05/07 08:32:07 INFO CodeGenerator: Code generated in 147.854289 ms
	24/05/07 08:32:07 INFO CodeGenerator: Code generated in 24.557112 ms
	24/05/07 08:32:07 INFO CodeGenerator: Code generated in 13.680958 ms
	24/05/07 08:32:07 INFO CodeGenerator: Code generated in 18.718769 ms
	24/05/07 08:32:07 INFO CodeGenerator: Code generated in 9.210708 ms
	24/05/07 08:32:07 INFO JDBCRDD: closed connection
[WI-0][TI-0] - [INFO] 2024-05-07 08:32:08.236 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7988165680473374 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:32:08.297 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:32:07 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2534 bytes result sent to driver
	24/05/07 08:32:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 935 ms on 941f01f0fea3 (executor driver) (1/1)
	24/05/07 08:32:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
	24/05/07 08:32:07 INFO DAGScheduler: ShuffleMapStage 0 (save at JdbcWriter.java:87) finished in 1.183 s
	24/05/07 08:32:07 INFO DAGScheduler: looking for newly runnable stages
	24/05/07 08:32:07 INFO DAGScheduler: running: Set()
	24/05/07 08:32:07 INFO DAGScheduler: waiting: Set()
	24/05/07 08:32:07 INFO DAGScheduler: failed: Set()
	24/05/07 08:32:07 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
	24/05/07 08:32:07 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
	24/05/07 08:32:07 INFO CodeGenerator: Code generated in 17.366812 ms
	24/05/07 08:32:07 INFO DAGScheduler: Registering RDD 5 (save at JdbcWriter.java:87) as input to shuffle 1
	24/05/07 08:32:07 INFO DAGScheduler: Got map stage job 1 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:32:07 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (save at JdbcWriter.java:87)
	24/05/07 08:32:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
	24/05/07 08:32:07 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:32:07 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[5] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:32:07 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 40.5 KiB, free 93.2 MiB)
	24/05/07 08:32:07 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 93.2 MiB)
	24/05/07 08:32:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 941f01f0fea3:40763 (size: 19.1 KiB, free: 93.3 MiB)
	24/05/07 08:32:07 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:32:07 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[5] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:32:07 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
	24/05/07 08:32:07 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (941f01f0fea3, executor driver, partition 0, NODE_LOCAL, 8032 bytes) 
	24/05/07 08:32:07 INFO Executor: Running task 0.0 in stage 2.0 (TID 1)
	24/05/07 08:32:07 INFO ShuffleBlockFetcherIterator: Getting 1 (2.9 KiB) non-empty blocks including 1 (2.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
	24/05/07 08:32:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 23 ms
	24/05/07 08:32:07 INFO CodeGenerator: Code generated in 36.425944 ms
	24/05/07 08:32:07 INFO Executor: Finished task 0.0 in stage 2.0 (TID 1). 5420 bytes result sent to driver
	24/05/07 08:32:07 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 170 ms on 941f01f0fea3 (executor driver) (1/1)
	24/05/07 08:32:07 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
	24/05/07 08:32:07 INFO DAGScheduler: ShuffleMapStage 2 (save at JdbcWriter.java:87) finished in 0.206 s
	24/05/07 08:32:07 INFO DAGScheduler: looking for newly runnable stages
	24/05/07 08:32:07 INFO DAGScheduler: running: Set()
	24/05/07 08:32:07 INFO DAGScheduler: waiting: Set()
	24/05/07 08:32:07 INFO DAGScheduler: failed: Set()
	24/05/07 08:32:08 INFO CodeGenerator: Code generated in 15.00424 ms
	24/05/07 08:32:08 INFO SparkContext: Starting job: save at JdbcWriter.java:87
	24/05/07 08:32:08 INFO DAGScheduler: Got job 2 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:32:08 INFO DAGScheduler: Final stage: ResultStage 5 (save at JdbcWriter.java:87)
	24/05/07 08:32:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
	24/05/07 08:32:08 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:32:08 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[10] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:32:08 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 51.4 KiB, free 93.1 MiB)
[WI-0][TI-0] - [INFO] 2024-05-07 08:32:09.246 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.719298245614035 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:32:09.299 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:32:08 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.0 KiB, free 93.1 MiB)
	24/05/07 08:32:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 941f01f0fea3:40763 (size: 23.0 KiB, free: 93.2 MiB)
	24/05/07 08:32:08 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:32:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[10] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:32:08 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
	24/05/07 08:32:08 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 2) (941f01f0fea3, executor driver, partition 0, NODE_LOCAL, 8043 bytes) 
	24/05/07 08:32:08 INFO Executor: Running task 0.0 in stage 5.0 (TID 2)
	24/05/07 08:32:08 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 941f01f0fea3:40763 in memory (size: 16.4 KiB, free: 93.3 MiB)
	24/05/07 08:32:08 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 941f01f0fea3:40763 in memory (size: 19.1 KiB, free: 93.3 MiB)
	24/05/07 08:32:08 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
	24/05/07 08:32:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
	24/05/07 08:32:08 INFO CodeGenerator: Code generated in 55.817921 ms
[WI-0][TI-0] - [INFO] 2024-05-07 08:32:11.303 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:32:10 INFO CodeGenerator: Code generated in 13.077498 ms
	24/05/07 08:32:10 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 2)
	java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1054'::int4),('3947'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1054_[null check] filtered data persistence'),('2024-05-07 08:31:54'),('2024-05-07 08:31:54')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	24/05/07 08:32:10 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 2) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1054'::int4),('3947'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1054_[null check] filtered data persistence'),('2024-05-07 08:31:54'),('2024-05-07 08:31:54')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	24/05/07 08:32:10 ERROR TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job
	24/05/07 08:32:10 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
	24/05/07 08:32:10 INFO TaskSchedulerImpl: Cancelling stage 5
	24/05/07 08:32:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 2) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1054'::int4),('3947'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1054_[null check] filtered data persistence'),('2024-05-07 08:31:54'),('2024-05-07 08:31:54')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	Driver stacktrace:
	24/05/07 08:32:10 INFO DAGScheduler: ResultStage 5 (save at JdbcWriter.java:87) failed in 2.374 s due to Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 2) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1054'::int4),('3947'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1054_[null check] filtered data persistence'),('2024-05-07 08:31:54'),('2024-05-07 08:31:54')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	Driver stacktrace:
	24/05/07 08:32:10 INFO DAGScheduler: Job 2 failed: save at JdbcWriter.java:87, took 2.407427 s
	Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 2) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1054'::int4),('3947'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1054_[null check] filtered data persistence'),('2024-05-07 08:31:54'),('2024-05-07 08:31:54')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	Driver stacktrace:
		at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
		at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
		at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
		at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
		at scala.Option.foreach(Option.scala:407)
		at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
		at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)
		at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
		at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
		at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
		at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)
		at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3514)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4309)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
		at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4307)
		at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3514)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
		at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
		at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
		at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
		at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
		at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
		at org.apache.dolphinscheduler.data.quality.flow.batch.writer.JdbcWriter.write(JdbcWriter.java:87)
		at org.apache.dolphinscheduler.data.quality.execution.SparkBatchExecution.executeWriter(SparkBatchExecution.java:132)
		at org.apache.dolphinscheduler.data.quality.execution.SparkBatchExecution.execute(SparkBatchExecution.java:58)
		at org.apache.dolphinscheduler.data.quality.context.DataQualityContext.execute(DataQualityContext.java:62)
		at org.apache.dolphinscheduler.data.quality.DataQualityApplication.main(DataQualityApplication.java:78)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
		at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)
		at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
		at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
		at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
		at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
		at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
		at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
	Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1054'::int4),('3947'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1054_[null check] filtered data persistence'),('2024-05-07 08:31:54'),('2024-05-07 08:31:54')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	24/05/07 08:32:10 INFO SparkContext: Invoking stop() from shutdown hook
	24/05/07 08:32:10 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/05/07 08:32:10 INFO SparkUI: Stopped Spark web UI at http://941f01f0fea3:4040
	24/05/07 08:32:10 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/05/07 08:32:10 INFO MemoryStore: MemoryStore cleared
	24/05/07 08:32:10 INFO BlockManager: BlockManager stopped
	24/05/07 08:32:10 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/05/07 08:32:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/05/07 08:32:10 INFO SparkContext: Successfully stopped SparkContext
	24/05/07 08:32:10 INFO ShutdownHookManager: Shutdown hook called
	24/05/07 08:32:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-4b55f8af-3fa2-4b43-b671-745893782cd5
	24/05/07 08:32:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-218198b3-5e31-46fd-8f35-09905fbec001
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:32:11.311 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1054/3947, processId:835 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:32:11.311 +0800 o.a.d.p.t.a.u.LogUtils:[79] - Start finding appId in /opt/dolphinscheduler/logs/20240507/13505298546272/21/1054/3947.log, fetch way: log 
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:32:11.316 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:32:11.316 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:32:11.317 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:32:11.318 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:32:11.324 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:32:11.324 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:32:11.325 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1054/3947
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:32:11.326 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1054/3947
[WI-1054][TI-3947] - [INFO] 2024-05-07 08:32:11.326 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3947] - [INFO] 2024-05-07 08:32:12.023 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3947, processInstanceId=1054, status=6, startTime=1715041914072, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1054/3947.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1054/3947, endTime=1715041931317, processId=835, appIds=, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3947] - [INFO] 2024-05-07 08:32:12.029 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3947, processInstanceId=1054, status=6, startTime=1715041914072, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1054/3947.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1054/3947, endTime=1715041931317, processId=835, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715041932022)
[WI-0][TI-0] - [INFO] 2024-05-07 08:32:56.469 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7447447447447448 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [WARN] 2024-05-07 08:33:03.140 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[147] - [id: 0x9b5d4ccc, L:/172.18.1.1:1234 - R:/172.18.0.13:48102] is not writable, over high water level : 65536
[WI-0][TI-0] - [WARN] 2024-05-07 08:33:03.140 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[154] - [id: 0x9b5d4ccc, L:/172.18.1.1:1234 - R:/172.18.0.13:48102] is writable, to low water : 32768
[WI-0][TI-3944] - [INFO] 2024-05-07 08:34:07.497 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3944, processInstanceId=1051, status=6, startTime=1715041077174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1051/3944.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1051/3944, endTime=1715041109126, processId=316, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715041747346)
[WI-0][TI-3944] - [INFO] 2024-05-07 08:34:07.500 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3944, processInstanceId=1051, status=6, startTime=1715041077174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1051/3944.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1051/3944, endTime=1715041109126, processId=316, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042047497)
[WI-0][TI-0] - [INFO] 2024-05-07 08:34:26.647 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7114285714285714 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3946] - [INFO] 2024-05-07 08:36:02.761 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3946, processInstanceId=1053, status=9, startTime=1715041547086, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1053/3946.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1053/3946, endTime=1715041560344, processId=628, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715041861802)
[WI-0][TI-3946] - [INFO] 2024-05-07 08:36:02.770 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3946, processInstanceId=1053, status=9, startTime=1715041547086, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1053/3946.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1053/3946, endTime=1715041560344, processId=628, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042162761)
[WI-0][TI-3945] - [INFO] 2024-05-07 08:36:05.773 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3945, processInstanceId=1052, status=6, startTime=1715041538811, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1052/3945.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1052/3945, endTime=1715041563133, processId=534, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715041864816)
[WI-0][TI-3945] - [INFO] 2024-05-07 08:36:05.778 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3945, processInstanceId=1052, status=6, startTime=1715041538811, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1052/3945.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1052/3945, endTime=1715041563133, processId=534, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042165773)
[WI-0][TI-3947] - [INFO] 2024-05-07 08:37:12.976 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3947, processInstanceId=1054, status=6, startTime=1715041914072, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1054/3947.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1054/3947, endTime=1715041931317, processId=835, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715041932022)
[WI-0][TI-3947] - [INFO] 2024-05-07 08:37:12.992 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3947, processInstanceId=1054, status=6, startTime=1715041914072, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1054/3947.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1054/3947, endTime=1715041931317, processId=835, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042232975)
[WI-0][TI-0] - [INFO] 2024-05-07 08:38:41.033 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[55] - Receive TaskInstanceKillRequest: TaskInstanceKillRequest(taskInstanceId=3947)
[WI-0][TI-3947] - [ERROR] 2024-05-07 08:38:41.035 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[62] - Cannot find WorkerTaskExecutor for taskInstance: 3947
[WI-0][TI-0] - [INFO] 2024-05-07 08:38:43.675 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7206703910614525 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:38:44.682 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8205128205128205 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:38:45.683 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7223796033994334 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:38:54.792 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7427745664739884 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:38:56.876 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3948, taskName=[null check] filtered data persistence, firstSubmitTime=1715042336838, startTime=0, taskType=DATA_QUALITY, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13505298546272, processDefineVersion=21, appIds=null, processInstanceId=1055, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13001194483488, taskParams={"localParams":[],"resourceList":[],"ruleId":6,"ruleInputParameter":{"check_type":"0","comparison_type":1,"comparison_name":"0","failure_strategy":"0","operator":"0","src_connector_type":1,"src_datasource_id":6,"src_database":"persistence","src_field":"content","src_table":"filtered_data_persistence","threshold":"0"},"sparkParameters":{"deployMode":"local","driverCores":1,"driverMemory":"512M","executorCores":1,"executorMemory":"1G","numExecutors":1,"others":"--conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n--packages org.postgresql:postgresql:42.7.3","yarnQueue":""}}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='[null check] filtered data persistence'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13001194483488'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='1055'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240507'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240506'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3948'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='qualti_test'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13505264408800'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13505298546272'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240507083856'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=org.apache.dolphinscheduler.plugin.task.api.DataQualityTaskExecutionContext@7723b69e, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.887 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: [null check] filtered data persistence to wait queue success
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.894 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.909 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.909 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.909 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.910 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1715042336910
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.911 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 1055_3948
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.912 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3948,
  "taskName" : "[null check] filtered data persistence",
  "firstSubmitTime" : 1715042336838,
  "startTime" : 1715042336910,
  "taskType" : "DATA_QUALITY",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240507/13505298546272/21/1055/3948.log",
  "processId" : 0,
  "processDefineCode" : 13505298546272,
  "processDefineVersion" : 21,
  "processInstanceId" : 1055,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13001194483488,
  "taskParams" : "{\"localParams\":[],\"resourceList\":[],\"ruleId\":6,\"ruleInputParameter\":{\"check_type\":\"0\",\"comparison_type\":1,\"comparison_name\":\"0\",\"failure_strategy\":\"0\",\"operator\":\"0\",\"src_connector_type\":1,\"src_datasource_id\":6,\"src_database\":\"persistence\",\"src_field\":\"content\",\"src_table\":\"filtered_data_persistence\",\"threshold\":\"0\"},\"sparkParameters\":{\"deployMode\":\"local\",\"driverCores\":1,\"driverMemory\":\"512M\",\"executorCores\":1,\"executorMemory\":\"1G\",\"numExecutors\":1,\"others\":\"--conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n--packages org.postgresql:postgresql:42.7.3\",\"yarnQueue\":\"\"}}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "[null check] filtered data persistence"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13001194483488"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1055"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240507"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240506"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3948"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "qualti_test"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13505264408800"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13505298546272"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240507083856"
    }
  },
  "taskAppId" : "1055_3948",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "dataQualityTaskExecutionContext" : {
    "ruleId" : 6,
    "ruleName" : "$t(uniqueness_check)",
    "ruleType" : 0,
    "ruleInputEntryList" : "[{\"id\":1,\"field\":\"src_connector_type\",\"type\":\"select\",\"title\":\"$t(src_connector_type)\",\"data\":\"\",\"options\":\"[{\\\"label\\\":\\\"HIVE\\\",\\\"value\\\":\\\"HIVE\\\"},{\\\"label\\\":\\\"JDBC\\\",\\\"value\\\":\\\"JDBC\\\"}]\",\"placeholder\":\"please select source connector type\",\"optionSourceType\":2,\"dataType\":2,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":1,\"createTime\":null,\"updateTime\":null},{\"id\":2,\"field\":\"src_datasource_id\",\"type\":\"select\",\"title\":\"$t(src_datasource_id)\",\"data\":\"\",\"options\":null,\"placeholder\":\"please select source datasource id\",\"optionSourceType\":1,\"dataType\":2,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":2,\"createTime\":null,\"updateTime\":null},{\"id\":30,\"field\":\"src_database\",\"type\":\"select\",\"title\":\"$t(src_database)\",\"data\":null,\"options\":null,\"placeholder\":\"Please select source database\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":true,\"valuesMap\":null,\"index\":2,\"createTime\":null,\"updateTime\":null},{\"id\":3,\"field\":\"src_table\",\"type\":\"select\",\"title\":\"$t(src_table)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter source table name\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":true,\"valuesMap\":null,\"index\":3,\"createTime\":null,\"updateTime\":null},{\"id\":4,\"field\":\"src_filter\",\"type\":\"input\",\"title\":\"$t(src_filter)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter filter expression\",\"optionSourceType\":0,\"dataType\":3,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":4,\"createTime\":null,\"updateTime\":null},{\"id\":5,\"field\":\"src_field\",\"type\":\"select\",\"title\":\"$t(src_field)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter column, only single column is supported\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":null,\"index\":5,\"createTime\":null,\"updateTime\":null},{\"id\":6,\"field\":\"statistics_name\",\"type\":\"input\",\"title\":\"$t(statistics_name)\",\"data\":\"duplicate_count.duplicates\",\"options\":null,\"placeholder\":\"Please enter statistics name, the alias in statistics execute sql\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":1,\"isShow\":false,\"canEdit\":false,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":\"{\\\"statistics_name\\\":\\\"duplicate_count.duplicates\\\"}\",\"index\":6,\"createTime\":null,\"updateTime\":null},{\"id\":7,\"field\":\"check_type\",\"type\":\"select\",\"title\":\"$t(check_type)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"Expected - Actual\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"Actual - Expected\\\",\\\"value\\\":\\\"1\\\"},{\\\"label\\\":\\\"Actual / Expected\\\",\\\"value\\\":\\\"2\\\"},{\\\"label\\\":\\\"(Expected - Actual) / Expected\\\",\\\"value\\\":\\\"3\\\"}]\",\"placeholder\":\"please select check type\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":7,\"createTime\":null,\"updateTime\":null},{\"id\":8,\"field\":\"operator\",\"type\":\"select\",\"title\":\"$t(operator)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"=\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"<\\\",\\\"value\\\":\\\"1\\\"},{\\\"label\\\":\\\"<=\\\",\\\"value\\\":\\\"2\\\"},{\\\"label\\\":\\\">\\\",\\\"value\\\":\\\"3\\\"},{\\\"label\\\":\\\">=\\\",\\\"value\\\":\\\"4\\\"},{\\\"label\\\":\\\"!=\\\",\\\"value\\\":\\\"5\\\"}]\",\"placeholder\":\"please select operator\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":8,\"createTime\":null,\"updateTime\":null},{\"id\":9,\"field\":\"threshold\",\"type\":\"input\",\"title\":\"$t(threshold)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter threshold, number is needed\",\"optionSourceType\":0,\"dataType\":2,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":null,\"index\":9,\"createTime\":null,\"updateTime\":null},{\"id\":10,\"field\":\"failure_strategy\",\"type\":\"select\",\"title\":\"$t(failure_strategy)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"Alert\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"Block\\\",\\\"value\\\":\\\"1\\\"}]\",\"placeholder\":\"please select failure strategy\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":10,\"createTime\":null,\"updateTime\":null},{\"id\":17,\"field\":\"comparison_name\",\"type\":\"input\",\"title\":\"$t(comparison_name)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter comparison name, the alias in comparison execute sql\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":false,\"canEdit\":false,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":\"\",\"index\":11,\"createTime\":null,\"updateTime\":null},{\"id\":19,\"field\":\"comparison_type\",\"type\":\"select\",\"title\":\"$t(comparison_type)\",\"data\":\"\",\"options\":null,\"placeholder\":\"Please enter comparison title\",\"optionSourceType\":3,\"dataType\":0,\"inputType\":2,\"isShow\":true,\"canEdit\":false,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":12,\"createTime\":null,\"updateTime\":null}]",
    "executeSqlList" : "[{\"id\":6,\"index\":1,\"sql\":\"SELECT ${src_field} FROM ${src_table} group by ${src_field} having count(*) > 1\",\"tableAlias\":\"duplicate_items\",\"type\":0,\"createTime\":\"2021-03-03 11:31:24\",\"updateTime\":\"2021-03-03 11:31:24\",\"errorOutputSql\":true},{\"id\":7,\"index\":1,\"sql\":\"SELECT COUNT(*) AS duplicates FROM duplicate_items\",\"tableAlias\":\"duplicate_count\",\"type\":1,\"createTime\":\"2021-03-03 11:31:24\",\"updateTime\":\"2021-03-03 11:31:24\",\"errorOutputSql\":false}]",
    "comparisonNeedStatisticsValueTable" : false,
    "compareWithFixedValue" : true,
    "hdfsPath" : "hdfs://mycluster:8020/user/default/data_quality_error_data",
    "sourceConnectorType" : "JDBC",
    "sourceType" : 1,
    "sourceConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"persistence\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence\",\"driverClassName\":\"org.postgresql.Driver\",\"validationQuery\":\"select version()\",\"other\":{\"password\":\"root\"}}",
    "targetConnectorType" : null,
    "targetType" : 0,
    "targetConnectionParams" : null,
    "writerConnectorType" : "JDBC",
    "writerType" : 1,
    "writerTable" : "t_ds_dq_execute_result",
    "writerConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"dolphinscheduler\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\"}",
    "statisticsValueConnectorType" : "JDBC",
    "statisticsValueType" : 1,
    "statisticsValueTable" : "t_ds_dq_task_statistics_value",
    "statisticsValueWriterConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"dolphinscheduler\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\"}"
  },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.914 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.914 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.914 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.919 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.920 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.921 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1055/3948 check successfully
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.921 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.dq.DataQualityTaskChannel successfully
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.922 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.922 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.922 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: DATA_QUALITY create successfully
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.923 +0800 o.a.d.p.t.d.DataQualityTask:[89] - Initialize data quality task params {
  "localParams" : [ ],
  "varPool" : null,
  "ruleId" : 6,
  "ruleInputParameter" : {
    "check_type" : "0",
    "comparison_type" : "1",
    "comparison_name" : "0",
    "failure_strategy" : "0",
    "operator" : "0",
    "src_connector_type" : "1",
    "src_datasource_id" : "6",
    "src_database" : "persistence",
    "src_field" : "content",
    "src_table" : "filtered_data_persistence",
    "threshold" : "0"
  },
  "sparkParameters" : {
    "localParams" : null,
    "varPool" : null,
    "mainJar" : null,
    "mainClass" : null,
    "deployMode" : "local",
    "mainArgs" : null,
    "driverCores" : 1,
    "driverMemory" : "512M",
    "numExecutors" : 1,
    "executorCores" : 1,
    "executorMemory" : "1G",
    "appName" : null,
    "yarnQueue" : "",
    "others" : "--conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n--packages org.postgresql:postgresql:42.7.3",
    "programType" : null,
    "resourceList" : [ ]
  }
}
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.927 +0800 o.a.d.p.t.d.DataQualityTask:[119] - data quality configuration: {
  "name" : "$t(uniqueness_check)",
  "env" : {
    "type" : "batch",
    "config" : null
  },
  "readers" : [ {
    "type" : "JDBC",
    "config" : {
      "database" : "persistence",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "output_table" : "persistence_filtered_data_persistence",
      "table" : "filtered_data_persistence",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence?password=root"
    }
  } ],
  "transformers" : [ {
    "type" : "sql",
    "config" : {
      "index" : 1,
      "output_table" : "duplicate_items",
      "sql" : "SELECT content FROM persistence_filtered_data_persistence group by content having count(*) > 1"
    }
  }, {
    "type" : "sql",
    "config" : {
      "index" : 2,
      "output_table" : "duplicate_count",
      "sql" : "SELECT COUNT(*) AS duplicates FROM duplicate_items"
    }
  } ],
  "writers" : [ {
    "type" : "JDBC",
    "config" : {
      "database" : "dolphinscheduler",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "table" : "t_ds_dq_execute_result",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler",
      "sql" : "select 0 as rule_type,'$t(uniqueness_check)' as rule_name,0 as process_definition_id,1055 as process_instance_id,3948 as task_instance_id,duplicate_count.duplicates AS statistics_value,0 AS comparison_value,1 AS comparison_type,0 as check_type,0 as threshold,0 as operator,0 as failure_strategy,'hdfs://mycluster:8020/user/default/data_quality_error_data/0_1055_[null check] filtered data persistence' as error_output_path,'2024-05-07 08:38:56' as create_time,'2024-05-07 08:38:56' as update_time from duplicate_count "
    }
  }, {
    "type" : "JDBC",
    "config" : {
      "database" : "dolphinscheduler",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "table" : "t_ds_dq_task_statistics_value",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler",
      "sql" : "select 0 as process_definition_id,3948 as task_instance_id,6 as rule_id,'KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG=' as unique_code,'duplicate_count.duplicates'AS statistics_name,duplicate_count.duplicates AS statistics_value,'2024-05-07 08:38:56' as data_time,'2024-05-07 08:38:56' as create_time,'2024-05-07 08:38:56' as update_time from duplicate_count"
    }
  }, {
    "type" : "hdfs_file",
    "config" : {
      "path" : "hdfs://mycluster:8020/user/default/data_quality_error_data/0_1055_[null check] filtered data persistence",
      "input_table" : "duplicate_items"
    }
  } ]
}
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.930 +0800 o.a.d.p.d.a.u.CommonUtils:[136] - Trying to get data quality jar in path
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.934 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.934 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.935 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.937 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.938 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.940 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.940 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.940 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${SPARK_HOME}/bin/spark-submit --master local --driver-cores 1 --driver-memory 512M --num-executors 1 --executor-cores 1 --executor-memory 1G --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
--packages org.postgresql:postgresql:42.7.3 /opt/dolphinscheduler/conf/../libs/dolphinscheduler-data-quality-3.2.1.jar "{\"name\":\"$t(uniqueness_check)\",\"env\":{\"type\":\"batch\",\"config\":null},\"readers\":[{\"type\":\"JDBC\",\"config\":{\"database\":\"persistence\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"output_table\":\"persistence_filtered_data_persistence\",\"table\":\"filtered_data_persistence\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence?password=root\"} }],\"transformers\":[{\"type\":\"sql\",\"config\":{\"index\":1,\"output_table\":\"duplicate_items\",\"sql\":\"SELECT content FROM persistence_filtered_data_persistence group by content having count(*) > 1\"} },{\"type\":\"sql\",\"config\":{\"index\":2,\"output_table\":\"duplicate_count\",\"sql\":\"SELECT COUNT(*) AS duplicates FROM duplicate_items\"} }],\"writers\":[{\"type\":\"JDBC\",\"config\":{\"database\":\"dolphinscheduler\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"table\":\"t_ds_dq_execute_result\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\",\"sql\":\"select 0 as rule_type,'$t(uniqueness_check)' as rule_name,0 as process_definition_id,1055 as process_instance_id,3948 as task_instance_id,duplicate_count.duplicates AS statistics_value,0 AS comparison_value,1 AS comparison_type,0 as check_type,0 as threshold,0 as operator,0 as failure_strategy,'hdfs://mycluster:8020/user/default/data_quality_error_data/0_1055_[null check] filtered data persistence' as error_output_path,'2024-05-07 08:38:56' as create_time,'2024-05-07 08:38:56' as update_time from duplicate_count \"} },{\"type\":\"JDBC\",\"config\":{\"database\":\"dolphinscheduler\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"table\":\"t_ds_dq_task_statistics_value\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\",\"sql\":\"select 0 as process_definition_id,3948 as task_instance_id,6 as rule_id,'KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG=' as unique_code,'duplicate_count.duplicates'AS statistics_name,duplicate_count.duplicates AS statistics_value,'2024-05-07 08:38:56' as data_time,'2024-05-07 08:38:56' as create_time,'2024-05-07 08:38:56' as update_time from duplicate_count\"} },{\"type\":\"hdfs_file\",\"config\":{\"path\":\"hdfs://mycluster:8020/user/default/data_quality_error_data/0_1055_[null check] filtered data persistence\",\"input_table\":\"duplicate_items\"} }]}"
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.941 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.941 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1055/3948/1055_3948.sh
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:38:56.945 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 1041
[WI-0][TI-3948] - [INFO] 2024-05-07 08:38:57.799 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3948, processInstanceId=1055, startTime=1715042336910, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1055/3948.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3948] - [INFO] 2024-05-07 08:38:57.812 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3948, processInstanceId=1055, startTime=1715042336910, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1055/3948.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1715042337793)
[WI-0][TI-3948] - [INFO] 2024-05-07 08:38:57.812 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3948, processInstanceId=1055, startTime=1715042336910, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3948] - [INFO] 2024-05-07 08:38:57.816 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3948, success=true)
[WI-0][TI-3948] - [INFO] 2024-05-07 08:38:57.822 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3948, processInstanceId=1055, startTime=1715042336910, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1715042337793)
[WI-0][TI-3948] - [WARN] 2024-05-07 08:38:57.823 +0800 o.a.d.s.w.m.MessageRetryRunner:[139] - Retry send message to master error
java.util.ConcurrentModificationException: null
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:911)
	at java.util.ArrayList$Itr.next(ArrayList.java:861)
	at org.apache.dolphinscheduler.server.worker.message.MessageRetryRunner.run(MessageRetryRunner.java:127)
[WI-0][TI-0] - [INFO] 2024-05-07 08:38:57.827 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8067226890756303 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3948] - [INFO] 2024-05-07 08:38:57.833 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3948)
[WI-0][TI-3948] - [INFO] 2024-05-07 08:38:57.888 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3948, success=true)
[WI-0][TI-3948] - [INFO] 2024-05-07 08:38:57.931 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3948)
[WI-0][TI-0] - [INFO] 2024-05-07 08:38:57.951 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-05-07 08:38:58.907 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8733509234828497 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:38:59.910 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8285714285714285 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:38:59.958 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[WI-0][TI-0] - [INFO] 2024-05-07 08:39:00.913 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8176470588235294 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:39:00.961 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.postgresql#postgresql added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-a9353a0f-2f4b-459d-99df-b7bb6b07b85f;1.0
		confs: [default]
		found org.postgresql#postgresql;42.7.3 in central
		found org.checkerframework#checker-qual;3.42.0 in central
	:: resolution report :: resolve 242ms :: artifacts dl 12ms
		:: modules in use:
		org.checkerframework#checker-qual;3.42.0 from central in [default]
		org.postgresql#postgresql;42.7.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-a9353a0f-2f4b-459d-99df-b7bb6b07b85f
		confs: [default]
		0 artifacts copied, 2 already retrieved (0kB/8ms)
	24/05/07 08:39:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-07 08:39:01.915 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8088235294117647 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:39:01.971 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:39:01 INFO SparkContext: Running Spark version 3.5.1
	24/05/07 08:39:01 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/05/07 08:39:01 INFO SparkContext: Java version 1.8.0_402
	24/05/07 08:39:01 INFO ResourceUtils: ==============================================================
	24/05/07 08:39:01 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/05/07 08:39:01 INFO ResourceUtils: ==============================================================
	24/05/07 08:39:01 INFO SparkContext: Submitted application: (uniqueness_check)
	24/05/07 08:39:01 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/05/07 08:39:01 INFO ResourceProfile: Limiting resource is cpu
	24/05/07 08:39:01 INFO ResourceProfileManager: Added ResourceProfile id: 0
	24/05/07 08:39:01 INFO SecurityManager: Changing view acls to: default
	24/05/07 08:39:01 INFO SecurityManager: Changing modify acls to: default
	24/05/07 08:39:01 INFO SecurityManager: Changing view acls groups to: 
	24/05/07 08:39:01 INFO SecurityManager: Changing modify acls groups to: 
	24/05/07 08:39:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
	24/05/07 08:39:01 INFO Utils: Successfully started service 'sparkDriver' on port 46797.
	24/05/07 08:39:01 INFO SparkEnv: Registering MapOutputTracker
	24/05/07 08:39:01 INFO SparkEnv: Registering BlockManagerMaster
[WI-0][TI-0] - [INFO] 2024-05-07 08:39:02.919 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8537414965986395 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:39:02.973 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:39:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/05/07 08:39:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/05/07 08:39:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/05/07 08:39:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5bb6dad9-c93c-4d38-9115-c3bbf927c987
	24/05/07 08:39:02 INFO MemoryStore: MemoryStore started with capacity 93.3 MiB
	24/05/07 08:39:02 INFO SparkEnv: Registering OutputCommitCoordinator
	24/05/07 08:39:02 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/05/07 08:39:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/05/07 08:39:02 INFO SparkContext: Added JAR file:///tmp/jars/org.postgresql_postgresql-42.7.3.jar at spark://941f01f0fea3:46797/jars/org.postgresql_postgresql-42.7.3.jar with timestamp 1715042341215
	24/05/07 08:39:02 INFO SparkContext: Added JAR file:///tmp/jars/org.checkerframework_checker-qual-3.42.0.jar at spark://941f01f0fea3:46797/jars/org.checkerframework_checker-qual-3.42.0.jar with timestamp 1715042341215
	24/05/07 08:39:02 INFO SparkContext: Added JAR file:/opt/dolphinscheduler/libs/dolphinscheduler-data-quality-3.2.1.jar at spark://941f01f0fea3:46797/jars/dolphinscheduler-data-quality-3.2.1.jar with timestamp 1715042341215
[WI-0][TI-0] - [INFO] 2024-05-07 08:39:03.923 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7883435582822085 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:39:03.975 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:39:02 INFO Executor: Starting executor ID driver on host 941f01f0fea3
	24/05/07 08:39:02 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/05/07 08:39:02 INFO Executor: Java version 1.8.0_402
	24/05/07 08:39:03 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/05/07 08:39:03 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@102c577f for default.
	24/05/07 08:39:03 INFO Executor: Fetching spark://941f01f0fea3:46797/jars/org.checkerframework_checker-qual-3.42.0.jar with timestamp 1715042341215
	24/05/07 08:39:03 INFO TransportClientFactory: Successfully created connection to 941f01f0fea3/172.18.1.1:46797 after 193 ms (0 ms spent in bootstraps)
	24/05/07 08:39:03 INFO Utils: Fetching spark://941f01f0fea3:46797/jars/org.checkerframework_checker-qual-3.42.0.jar to /tmp/spark-8f0c1510-8b89-4520-9ff8-536d7b86ada9/userFiles-c8ca195d-5174-40e3-b24a-5004c457ef64/fetchFileTemp3294054767539355962.tmp
	24/05/07 08:39:03 INFO Executor: Adding file:/tmp/spark-8f0c1510-8b89-4520-9ff8-536d7b86ada9/userFiles-c8ca195d-5174-40e3-b24a-5004c457ef64/org.checkerframework_checker-qual-3.42.0.jar to class loader default
	24/05/07 08:39:03 INFO Executor: Fetching spark://941f01f0fea3:46797/jars/org.postgresql_postgresql-42.7.3.jar with timestamp 1715042341215
	24/05/07 08:39:03 INFO Utils: Fetching spark://941f01f0fea3:46797/jars/org.postgresql_postgresql-42.7.3.jar to /tmp/spark-8f0c1510-8b89-4520-9ff8-536d7b86ada9/userFiles-c8ca195d-5174-40e3-b24a-5004c457ef64/fetchFileTemp826631604402283270.tmp
	24/05/07 08:39:03 INFO Executor: Adding file:/tmp/spark-8f0c1510-8b89-4520-9ff8-536d7b86ada9/userFiles-c8ca195d-5174-40e3-b24a-5004c457ef64/org.postgresql_postgresql-42.7.3.jar to class loader default
	24/05/07 08:39:03 INFO Executor: Fetching spark://941f01f0fea3:46797/jars/dolphinscheduler-data-quality-3.2.1.jar with timestamp 1715042341215
	24/05/07 08:39:03 INFO Utils: Fetching spark://941f01f0fea3:46797/jars/dolphinscheduler-data-quality-3.2.1.jar to /tmp/spark-8f0c1510-8b89-4520-9ff8-536d7b86ada9/userFiles-c8ca195d-5174-40e3-b24a-5004c457ef64/fetchFileTemp8533798723711089660.tmp
	24/05/07 08:39:03 INFO Executor: Adding file:/tmp/spark-8f0c1510-8b89-4520-9ff8-536d7b86ada9/userFiles-c8ca195d-5174-40e3-b24a-5004c457ef64/dolphinscheduler-data-quality-3.2.1.jar to class loader default
	24/05/07 08:39:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35987.
	24/05/07 08:39:03 INFO NettyBlockTransferService: Server created on 941f01f0fea3:35987
	24/05/07 08:39:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/05/07 08:39:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 941f01f0fea3, 35987, None)
	24/05/07 08:39:03 INFO BlockManagerMasterEndpoint: Registering block manager 941f01f0fea3:35987 with 93.3 MiB RAM, BlockManagerId(driver, 941f01f0fea3, 35987, None)
	24/05/07 08:39:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 941f01f0fea3, 35987, None)
	24/05/07 08:39:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 941f01f0fea3, 35987, None)
[WI-0][TI-0] - [INFO] 2024-05-07 08:39:04.939 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7737003058103976 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:39:04.981 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:39:04 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/05/07 08:39:04 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1055/3948/spark-warehouse'.
[WI-0][TI-3944] - [INFO] 2024-05-07 08:39:07.948 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3944, processInstanceId=1051, status=6, startTime=1715041077174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1051/3944.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1051/3944, endTime=1715041109126, processId=316, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042047497)
[WI-0][TI-3944] - [INFO] 2024-05-07 08:39:07.955 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3944, processInstanceId=1051, status=6, startTime=1715041077174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1051/3944.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1051/3944, endTime=1715041109126, processId=316, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042347948)
[WI-0][TI-0] - [INFO] 2024-05-07 08:39:08.965 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8037037037037037 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:39:10.000 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:39:09 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[WI-0][TI-0] - [INFO] 2024-05-07 08:39:10.001 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9356016989145823 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:39:11.022 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8757225433526011 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:39:11.021 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:39:10 INFO CodeGenerator: Code generated in 434.404925 ms
[WI-0][TI-0] - [INFO] 2024-05-07 08:39:12.058 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:39:11 INFO DAGScheduler: Registering RDD 2 (save at JdbcWriter.java:87) as input to shuffle 0
	24/05/07 08:39:11 INFO DAGScheduler: Got map stage job 0 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:39:11 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (save at JdbcWriter.java:87)
	24/05/07 08:39:11 INFO DAGScheduler: Parents of final stage: List()
	24/05/07 08:39:11 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:39:11 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:39:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 34.8 KiB, free 93.3 MiB)
	24/05/07 08:39:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 93.2 MiB)
	24/05/07 08:39:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 941f01f0fea3:35987 (size: 16.4 KiB, free: 93.3 MiB)
	24/05/07 08:39:11 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:39:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:39:11 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
	24/05/07 08:39:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (941f01f0fea3, executor driver, partition 0, PROCESS_LOCAL, 7878 bytes) 
	24/05/07 08:39:11 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[WI-0][TI-0] - [INFO] 2024-05-07 08:39:12.072 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8804664723032071 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:39:13.072 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:39:12 INFO CodeGenerator: Code generated in 119.682645 ms
	24/05/07 08:39:12 INFO CodeGenerator: Code generated in 11.74792 ms
	24/05/07 08:39:12 INFO CodeGenerator: Code generated in 6.580207 ms
	24/05/07 08:39:12 INFO CodeGenerator: Code generated in 11.534401 ms
	24/05/07 08:39:12 INFO CodeGenerator: Code generated in 8.426884 ms
	24/05/07 08:39:12 INFO JDBCRDD: closed connection
	24/05/07 08:39:12 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2534 bytes result sent to driver
	24/05/07 08:39:12 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1187 ms on 941f01f0fea3 (executor driver) (1/1)
	24/05/07 08:39:12 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
	24/05/07 08:39:12 INFO DAGScheduler: ShuffleMapStage 0 (save at JdbcWriter.java:87) finished in 1.598 s
	24/05/07 08:39:12 INFO DAGScheduler: looking for newly runnable stages
	24/05/07 08:39:12 INFO DAGScheduler: running: Set()
	24/05/07 08:39:12 INFO DAGScheduler: waiting: Set()
	24/05/07 08:39:12 INFO DAGScheduler: failed: Set()
	24/05/07 08:39:12 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
	24/05/07 08:39:12 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[WI-0][TI-0] - [INFO] 2024-05-07 08:39:13.079 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8540372670807452 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:39:14.082 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7720364741641337 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:39:14.081 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:39:13 INFO CodeGenerator: Code generated in 51.851666 ms
	24/05/07 08:39:13 INFO DAGScheduler: Registering RDD 5 (save at JdbcWriter.java:87) as input to shuffle 1
	24/05/07 08:39:13 INFO DAGScheduler: Got map stage job 1 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:39:13 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (save at JdbcWriter.java:87)
	24/05/07 08:39:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
	24/05/07 08:39:13 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:39:13 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[5] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:39:13 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 40.5 KiB, free 93.2 MiB)
	24/05/07 08:39:13 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 93.2 MiB)
	24/05/07 08:39:13 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 941f01f0fea3:35987 (size: 19.1 KiB, free: 93.3 MiB)
	24/05/07 08:39:13 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:39:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[5] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:39:13 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
	24/05/07 08:39:13 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (941f01f0fea3, executor driver, partition 0, NODE_LOCAL, 8032 bytes) 
	24/05/07 08:39:13 INFO Executor: Running task 0.0 in stage 2.0 (TID 1)
	24/05/07 08:39:13 INFO ShuffleBlockFetcherIterator: Getting 1 (2.9 KiB) non-empty blocks including 1 (2.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
	24/05/07 08:39:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms
	24/05/07 08:39:13 INFO CodeGenerator: Code generated in 21.195006 ms
	24/05/07 08:39:13 INFO Executor: Finished task 0.0 in stage 2.0 (TID 1). 5420 bytes result sent to driver
	24/05/07 08:39:13 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 151 ms on 941f01f0fea3 (executor driver) (1/1)
	24/05/07 08:39:13 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
	24/05/07 08:39:13 INFO DAGScheduler: ShuffleMapStage 2 (save at JdbcWriter.java:87) finished in 0.219 s
	24/05/07 08:39:13 INFO DAGScheduler: looking for newly runnable stages
	24/05/07 08:39:13 INFO DAGScheduler: running: Set()
	24/05/07 08:39:13 INFO DAGScheduler: waiting: Set()
	24/05/07 08:39:13 INFO DAGScheduler: failed: Set()
	24/05/07 08:39:13 INFO CodeGenerator: Code generated in 43.955862 ms
	24/05/07 08:39:13 INFO SparkContext: Starting job: save at JdbcWriter.java:87
	24/05/07 08:39:13 INFO DAGScheduler: Got job 2 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:39:13 INFO DAGScheduler: Final stage: ResultStage 5 (save at JdbcWriter.java:87)
	24/05/07 08:39:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
	24/05/07 08:39:13 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:39:13 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[10] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:39:13 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 51.4 KiB, free 93.1 MiB)
	24/05/07 08:39:13 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.0 KiB, free 93.1 MiB)
	24/05/07 08:39:13 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 941f01f0fea3:35987 (size: 23.0 KiB, free: 93.2 MiB)
	24/05/07 08:39:13 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:39:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[10] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:39:13 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
	24/05/07 08:39:13 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 2) (941f01f0fea3, executor driver, partition 0, NODE_LOCAL, 8043 bytes) 
	24/05/07 08:39:13 INFO Executor: Running task 0.0 in stage 5.0 (TID 2)
	24/05/07 08:39:13 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 941f01f0fea3:35987 in memory (size: 16.4 KiB, free: 93.3 MiB)
	24/05/07 08:39:13 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 941f01f0fea3:35987 in memory (size: 19.1 KiB, free: 93.3 MiB)
[WI-0][TI-0] - [INFO] 2024-05-07 08:39:15.086 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:39:14 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
	24/05/07 08:39:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
	24/05/07 08:39:14 INFO CodeGenerator: Code generated in 13.620246 ms
[WI-0][TI-0] - [INFO] 2024-05-07 08:39:16.090 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:39:15 INFO CodeGenerator: Code generated in 26.683229 ms
	24/05/07 08:39:15 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 2)
	java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1055'::int4),('3948'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1055_[null check] filtered data persistence'),('2024-05-07 08:38:56'),('2024-05-07 08:38:56')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	24/05/07 08:39:15 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 2) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1055'::int4),('3948'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1055_[null check] filtered data persistence'),('2024-05-07 08:38:56'),('2024-05-07 08:38:56')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	24/05/07 08:39:16 ERROR TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job
	24/05/07 08:39:16 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
	24/05/07 08:39:16 INFO TaskSchedulerImpl: Cancelling stage 5
	24/05/07 08:39:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 2) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1055'::int4),('3948'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1055_[null check] filtered data persistence'),('2024-05-07 08:38:56'),('2024-05-07 08:38:56')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	Driver stacktrace:
	24/05/07 08:39:16 INFO DAGScheduler: ResultStage 5 (save at JdbcWriter.java:87) failed in 2.349 s due to Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 2) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1055'::int4),('3948'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1055_[null check] filtered data persistence'),('2024-05-07 08:38:56'),('2024-05-07 08:38:56')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	Driver stacktrace:
	24/05/07 08:39:16 INFO DAGScheduler: Job 2 failed: save at JdbcWriter.java:87, took 2.411336 s
[WI-0][TI-0] - [WARN] 2024-05-07 08:39:16.752 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[147] - [id: 0x9b5d4ccc, L:/172.18.1.1:1234 - R:/172.18.0.13:48102] is not writable, over high water level : 65536
[WI-0][TI-0] - [WARN] 2024-05-07 08:39:16.753 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[154] - [id: 0x9b5d4ccc, L:/172.18.1.1:1234 - R:/172.18.0.13:48102] is writable, to low water : 32768
[WI-0][TI-0] - [INFO] 2024-05-07 08:39:17.094 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 2) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1055'::int4),('3948'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1055_[null check] filtered data persistence'),('2024-05-07 08:38:56'),('2024-05-07 08:38:56')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	Driver stacktrace:
		at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
		at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
		at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
		at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
		at scala.Option.foreach(Option.scala:407)
		at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
		at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)
		at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
		at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
		at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
		at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)
		at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3514)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4309)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
		at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4307)
		at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3514)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
		at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
		at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
		at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
		at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
		at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
		at org.apache.dolphinscheduler.data.quality.flow.batch.writer.JdbcWriter.write(JdbcWriter.java:87)
		at org.apache.dolphinscheduler.data.quality.execution.SparkBatchExecution.executeWriter(SparkBatchExecution.java:132)
		at org.apache.dolphinscheduler.data.quality.execution.SparkBatchExecution.execute(SparkBatchExecution.java:58)
		at org.apache.dolphinscheduler.data.quality.context.DataQualityContext.execute(DataQualityContext.java:62)
		at org.apache.dolphinscheduler.data.quality.DataQualityApplication.main(DataQualityApplication.java:78)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
		at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)
		at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
		at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
		at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
		at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
		at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
		at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
	Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1055'::int4),('3948'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1055_[null check] filtered data persistence'),('2024-05-07 08:38:56'),('2024-05-07 08:38:56')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 354
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	24/05/07 08:39:16 INFO SparkContext: Invoking stop() from shutdown hook
	24/05/07 08:39:16 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/05/07 08:39:16 INFO SparkUI: Stopped Spark web UI at http://941f01f0fea3:4040
	24/05/07 08:39:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/05/07 08:39:16 INFO MemoryStore: MemoryStore cleared
	24/05/07 08:39:16 INFO BlockManager: BlockManager stopped
	24/05/07 08:39:16 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/05/07 08:39:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/05/07 08:39:16 INFO SparkContext: Successfully stopped SparkContext
	24/05/07 08:39:16 INFO ShutdownHookManager: Shutdown hook called
	24/05/07 08:39:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-b991c574-633b-4fde-9c78-ab17c5c608ed
	24/05/07 08:39:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-8f0c1510-8b89-4520-9ff8-536d7b86ada9
[WI-0][TI-0] - [INFO] 2024-05-07 08:39:17.097 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7632311977715877 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:39:17.103 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1055/3948, processId:1041 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:39:17.105 +0800 o.a.d.p.t.a.u.LogUtils:[79] - Start finding appId in /opt/dolphinscheduler/logs/20240507/13505298546272/21/1055/3948.log, fetch way: log 
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:39:17.106 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:39:17.107 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:39:17.115 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:39:17.116 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:39:17.139 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:39:17.140 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:39:17.140 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1055/3948
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:39:17.141 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1055/3948
[WI-1055][TI-3948] - [INFO] 2024-05-07 08:39:17.141 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3948] - [INFO] 2024-05-07 08:39:17.990 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3948, processInstanceId=1055, status=6, startTime=1715042336910, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1055/3948.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1055/3948, endTime=1715042357115, processId=1041, appIds=, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3948] - [INFO] 2024-05-07 08:39:18.000 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3948, processInstanceId=1055, status=6, startTime=1715042336910, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1055/3948.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1055/3948, endTime=1715042357115, processId=1041, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042357990)
[WI-0][TI-0] - [INFO] 2024-05-07 08:39:26.209 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8236914600550964 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [WARN] 2024-05-07 08:40:46.054 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[147] - [id: 0x9b5d4ccc, L:/172.18.1.1:1234 - R:/172.18.0.13:48102] is not writable, over high water level : 65536
[WI-0][TI-0] - [WARN] 2024-05-07 08:40:46.055 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[154] - [id: 0x9b5d4ccc, L:/172.18.1.1:1234 - R:/172.18.0.13:48102] is writable, to low water : 32768
[WI-0][TI-0] - [INFO] 2024-05-07 08:40:54.650 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7288135593220338 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:40:58.024 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[55] - Receive TaskInstanceKillRequest: TaskInstanceKillRequest(taskInstanceId=3948)
[WI-0][TI-3948] - [ERROR] 2024-05-07 08:40:58.024 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[62] - Cannot find WorkerTaskExecutor for taskInstance: 3948
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:01.562 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3949, taskName=[null check] filtered data persistence, firstSubmitTime=1715042461532, startTime=0, taskType=DATA_QUALITY, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13505298546272, processDefineVersion=21, appIds=null, processInstanceId=1056, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13001194483488, taskParams={"localParams":[],"resourceList":[],"ruleId":6,"ruleInputParameter":{"check_type":"0","comparison_type":1,"comparison_name":"0","failure_strategy":"0","operator":"0","src_connector_type":1,"src_datasource_id":6,"src_database":"persistence","src_field":"content","src_table":"filtered_data_persistence","threshold":"0"},"sparkParameters":{"deployMode":"local","driverCores":1,"driverMemory":"512M","executorCores":1,"executorMemory":"1G","numExecutors":1,"others":"--conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n--packages org.postgresql:postgresql:42.7.3","yarnQueue":""}}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='[null check] filtered data persistence'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13001194483488'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='1056'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240507'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240506'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3949'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='qualti_test'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13505264408800'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13505298546272'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240507084101'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=org.apache.dolphinscheduler.plugin.task.api.DataQualityTaskExecutionContext@4b087a5f, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.569 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: [null check] filtered data persistence to wait queue success
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.570 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.573 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.573 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.574 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.574 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1715042461574
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.574 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 1056_3949
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.593 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3949,
  "taskName" : "[null check] filtered data persistence",
  "firstSubmitTime" : 1715042461532,
  "startTime" : 1715042461574,
  "taskType" : "DATA_QUALITY",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240507/13505298546272/21/1056/3949.log",
  "processId" : 0,
  "processDefineCode" : 13505298546272,
  "processDefineVersion" : 21,
  "processInstanceId" : 1056,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13001194483488,
  "taskParams" : "{\"localParams\":[],\"resourceList\":[],\"ruleId\":6,\"ruleInputParameter\":{\"check_type\":\"0\",\"comparison_type\":1,\"comparison_name\":\"0\",\"failure_strategy\":\"0\",\"operator\":\"0\",\"src_connector_type\":1,\"src_datasource_id\":6,\"src_database\":\"persistence\",\"src_field\":\"content\",\"src_table\":\"filtered_data_persistence\",\"threshold\":\"0\"},\"sparkParameters\":{\"deployMode\":\"local\",\"driverCores\":1,\"driverMemory\":\"512M\",\"executorCores\":1,\"executorMemory\":\"1G\",\"numExecutors\":1,\"others\":\"--conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n--packages org.postgresql:postgresql:42.7.3\",\"yarnQueue\":\"\"}}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "[null check] filtered data persistence"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13001194483488"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1056"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240507"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240506"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3949"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "qualti_test"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13505264408800"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13505298546272"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240507084101"
    }
  },
  "taskAppId" : "1056_3949",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "dataQualityTaskExecutionContext" : {
    "ruleId" : 6,
    "ruleName" : "$t(uniqueness_check)",
    "ruleType" : 0,
    "ruleInputEntryList" : "[{\"id\":1,\"field\":\"src_connector_type\",\"type\":\"select\",\"title\":\"$t(src_connector_type)\",\"data\":\"\",\"options\":\"[{\\\"label\\\":\\\"HIVE\\\",\\\"value\\\":\\\"HIVE\\\"},{\\\"label\\\":\\\"JDBC\\\",\\\"value\\\":\\\"JDBC\\\"}]\",\"placeholder\":\"please select source connector type\",\"optionSourceType\":2,\"dataType\":2,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":1,\"createTime\":null,\"updateTime\":null},{\"id\":2,\"field\":\"src_datasource_id\",\"type\":\"select\",\"title\":\"$t(src_datasource_id)\",\"data\":\"\",\"options\":null,\"placeholder\":\"please select source datasource id\",\"optionSourceType\":1,\"dataType\":2,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":2,\"createTime\":null,\"updateTime\":null},{\"id\":30,\"field\":\"src_database\",\"type\":\"select\",\"title\":\"$t(src_database)\",\"data\":null,\"options\":null,\"placeholder\":\"Please select source database\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":true,\"valuesMap\":null,\"index\":2,\"createTime\":null,\"updateTime\":null},{\"id\":3,\"field\":\"src_table\",\"type\":\"select\",\"title\":\"$t(src_table)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter source table name\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":true,\"valuesMap\":null,\"index\":3,\"createTime\":null,\"updateTime\":null},{\"id\":4,\"field\":\"src_filter\",\"type\":\"input\",\"title\":\"$t(src_filter)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter filter expression\",\"optionSourceType\":0,\"dataType\":3,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":4,\"createTime\":null,\"updateTime\":null},{\"id\":5,\"field\":\"src_field\",\"type\":\"select\",\"title\":\"$t(src_field)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter column, only single column is supported\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":null,\"index\":5,\"createTime\":null,\"updateTime\":null},{\"id\":6,\"field\":\"statistics_name\",\"type\":\"input\",\"title\":\"$t(statistics_name)\",\"data\":\"duplicate_count.duplicates\",\"options\":null,\"placeholder\":\"Please enter statistics name, the alias in statistics execute sql\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":1,\"isShow\":false,\"canEdit\":false,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":\"{\\\"statistics_name\\\":\\\"duplicate_count.duplicates\\\"}\",\"index\":6,\"createTime\":null,\"updateTime\":null},{\"id\":7,\"field\":\"check_type\",\"type\":\"select\",\"title\":\"$t(check_type)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"Expected - Actual\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"Actual - Expected\\\",\\\"value\\\":\\\"1\\\"},{\\\"label\\\":\\\"Actual / Expected\\\",\\\"value\\\":\\\"2\\\"},{\\\"label\\\":\\\"(Expected - Actual) / Expected\\\",\\\"value\\\":\\\"3\\\"}]\",\"placeholder\":\"please select check type\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":7,\"createTime\":null,\"updateTime\":null},{\"id\":8,\"field\":\"operator\",\"type\":\"select\",\"title\":\"$t(operator)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"=\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"<\\\",\\\"value\\\":\\\"1\\\"},{\\\"label\\\":\\\"<=\\\",\\\"value\\\":\\\"2\\\"},{\\\"label\\\":\\\">\\\",\\\"value\\\":\\\"3\\\"},{\\\"label\\\":\\\">=\\\",\\\"value\\\":\\\"4\\\"},{\\\"label\\\":\\\"!=\\\",\\\"value\\\":\\\"5\\\"}]\",\"placeholder\":\"please select operator\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":8,\"createTime\":null,\"updateTime\":null},{\"id\":9,\"field\":\"threshold\",\"type\":\"input\",\"title\":\"$t(threshold)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter threshold, number is needed\",\"optionSourceType\":0,\"dataType\":2,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":null,\"index\":9,\"createTime\":null,\"updateTime\":null},{\"id\":10,\"field\":\"failure_strategy\",\"type\":\"select\",\"title\":\"$t(failure_strategy)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"Alert\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"Block\\\",\\\"value\\\":\\\"1\\\"}]\",\"placeholder\":\"please select failure strategy\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":10,\"createTime\":null,\"updateTime\":null},{\"id\":17,\"field\":\"comparison_name\",\"type\":\"input\",\"title\":\"$t(comparison_name)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter comparison name, the alias in comparison execute sql\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":false,\"canEdit\":false,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":\"\",\"index\":11,\"createTime\":null,\"updateTime\":null},{\"id\":19,\"field\":\"comparison_type\",\"type\":\"select\",\"title\":\"$t(comparison_type)\",\"data\":\"\",\"options\":null,\"placeholder\":\"Please enter comparison title\",\"optionSourceType\":3,\"dataType\":0,\"inputType\":2,\"isShow\":true,\"canEdit\":false,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":12,\"createTime\":null,\"updateTime\":null}]",
    "executeSqlList" : "[{\"id\":6,\"index\":1,\"sql\":\"SELECT ${src_field} FROM ${src_table} group by ${src_field} having count(*) > 1\",\"tableAlias\":\"duplicate_items\",\"type\":0,\"createTime\":\"2021-03-03 11:31:24\",\"updateTime\":\"2021-03-03 11:31:24\",\"errorOutputSql\":true},{\"id\":7,\"index\":1,\"sql\":\"SELECT COUNT(*) AS duplicates FROM duplicate_items\",\"tableAlias\":\"duplicate_count\",\"type\":1,\"createTime\":\"2021-03-03 11:31:24\",\"updateTime\":\"2021-03-03 11:31:24\",\"errorOutputSql\":false}]",
    "comparisonNeedStatisticsValueTable" : false,
    "compareWithFixedValue" : true,
    "hdfsPath" : "hdfs://mycluster:8020/user/default/data_quality_error_data",
    "sourceConnectorType" : "JDBC",
    "sourceType" : 1,
    "sourceConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"persistence\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence\",\"driverClassName\":\"org.postgresql.Driver\",\"validationQuery\":\"select version()\",\"other\":{\"password\":\"root\"}}",
    "targetConnectorType" : null,
    "targetType" : 0,
    "targetConnectionParams" : null,
    "writerConnectorType" : "JDBC",
    "writerType" : 1,
    "writerTable" : "t_ds_dq_execute_result",
    "writerConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"dolphinscheduler\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\"}",
    "statisticsValueConnectorType" : "JDBC",
    "statisticsValueType" : 1,
    "statisticsValueTable" : "t_ds_dq_task_statistics_value",
    "statisticsValueWriterConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"dolphinscheduler\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\"}"
  },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.595 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.595 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.596 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.627 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.628 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.629 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1056/3949 check successfully
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.630 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.dq.DataQualityTaskChannel successfully
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.630 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.631 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.631 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: DATA_QUALITY create successfully
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.631 +0800 o.a.d.p.t.d.DataQualityTask:[89] - Initialize data quality task params {
  "localParams" : [ ],
  "varPool" : null,
  "ruleId" : 6,
  "ruleInputParameter" : {
    "check_type" : "0",
    "comparison_type" : "1",
    "comparison_name" : "0",
    "failure_strategy" : "0",
    "operator" : "0",
    "src_connector_type" : "1",
    "src_datasource_id" : "6",
    "src_database" : "persistence",
    "src_field" : "content",
    "src_table" : "filtered_data_persistence",
    "threshold" : "0"
  },
  "sparkParameters" : {
    "localParams" : null,
    "varPool" : null,
    "mainJar" : null,
    "mainClass" : null,
    "deployMode" : "local",
    "mainArgs" : null,
    "driverCores" : 1,
    "driverMemory" : "512M",
    "numExecutors" : 1,
    "executorCores" : 1,
    "executorMemory" : "1G",
    "appName" : null,
    "yarnQueue" : "",
    "others" : "--conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n--packages org.postgresql:postgresql:42.7.3",
    "programType" : null,
    "resourceList" : [ ]
  }
}
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.635 +0800 o.a.d.p.t.d.DataQualityTask:[119] - data quality configuration: {
  "name" : "$t(uniqueness_check)",
  "env" : {
    "type" : "batch",
    "config" : null
  },
  "readers" : [ {
    "type" : "JDBC",
    "config" : {
      "database" : "persistence",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "output_table" : "persistence_filtered_data_persistence",
      "table" : "filtered_data_persistence",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence?password=root"
    }
  } ],
  "transformers" : [ {
    "type" : "sql",
    "config" : {
      "index" : 1,
      "output_table" : "duplicate_items",
      "sql" : "SELECT content FROM persistence_filtered_data_persistence group by content having count(*) > 1"
    }
  }, {
    "type" : "sql",
    "config" : {
      "index" : 2,
      "output_table" : "duplicate_count",
      "sql" : "SELECT COUNT(*) AS duplicates FROM duplicate_items"
    }
  } ],
  "writers" : [ {
    "type" : "JDBC",
    "config" : {
      "database" : "dolphinscheduler",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "table" : "t_ds_dq_execute_result",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler",
      "sql" : "select 0 as rule_type,'$t(uniqueness_check)' as rule_name,0 as process_definition_id,1056 as process_instance_id,3949 as task_instance_id,duplicate_count.duplicates AS statistics_value,0 AS comparison_value,1 AS comparison_type,0 as check_type,0 as threshold,0 as operator,0 as failure_strategy,'hdfs://mycluster:8020/user/default/data_quality_error_data/0_1056_[null check] filtered data persistence' as error_output_path,'2024-05-07 08:41:01' as create_time,'2024-05-07 08:41:01' as update_time from duplicate_count "
    }
  }, {
    "type" : "JDBC",
    "config" : {
      "database" : "dolphinscheduler",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "table" : "t_ds_dq_task_statistics_value",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler",
      "sql" : "select 0 as process_definition_id,3949 as task_instance_id,6 as rule_id,'KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG=' as unique_code,'duplicate_count.duplicates'AS statistics_name,duplicate_count.duplicates AS statistics_value,'2024-05-07 08:41:01' as data_time,'2024-05-07 08:41:01' as create_time,'2024-05-07 08:41:01' as update_time from duplicate_count"
    }
  }, {
    "type" : "hdfs_file",
    "config" : {
      "path" : "hdfs://mycluster:8020/user/default/data_quality_error_data/0_1056_[null check] filtered data persistence",
      "input_table" : "duplicate_items"
    }
  } ]
}
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.638 +0800 o.a.d.p.d.a.u.CommonUtils:[136] - Trying to get data quality jar in path
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.638 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.638 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.638 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.638 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.638 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.639 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.639 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.639 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${SPARK_HOME}/bin/spark-submit --master local --driver-cores 1 --driver-memory 512M --num-executors 1 --executor-cores 1 --executor-memory 1G --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
--packages org.postgresql:postgresql:42.7.3 /opt/dolphinscheduler/conf/../libs/dolphinscheduler-data-quality-3.2.1.jar "{\"name\":\"$t(uniqueness_check)\",\"env\":{\"type\":\"batch\",\"config\":null},\"readers\":[{\"type\":\"JDBC\",\"config\":{\"database\":\"persistence\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"output_table\":\"persistence_filtered_data_persistence\",\"table\":\"filtered_data_persistence\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence?password=root\"} }],\"transformers\":[{\"type\":\"sql\",\"config\":{\"index\":1,\"output_table\":\"duplicate_items\",\"sql\":\"SELECT content FROM persistence_filtered_data_persistence group by content having count(*) > 1\"} },{\"type\":\"sql\",\"config\":{\"index\":2,\"output_table\":\"duplicate_count\",\"sql\":\"SELECT COUNT(*) AS duplicates FROM duplicate_items\"} }],\"writers\":[{\"type\":\"JDBC\",\"config\":{\"database\":\"dolphinscheduler\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"table\":\"t_ds_dq_execute_result\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\",\"sql\":\"select 0 as rule_type,'$t(uniqueness_check)' as rule_name,0 as process_definition_id,1056 as process_instance_id,3949 as task_instance_id,duplicate_count.duplicates AS statistics_value,0 AS comparison_value,1 AS comparison_type,0 as check_type,0 as threshold,0 as operator,0 as failure_strategy,'hdfs://mycluster:8020/user/default/data_quality_error_data/0_1056_[null check] filtered data persistence' as error_output_path,'2024-05-07 08:41:01' as create_time,'2024-05-07 08:41:01' as update_time from duplicate_count \"} },{\"type\":\"JDBC\",\"config\":{\"database\":\"dolphinscheduler\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"table\":\"t_ds_dq_task_statistics_value\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\",\"sql\":\"select 0 as process_definition_id,3949 as task_instance_id,6 as rule_id,'KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG=' as unique_code,'duplicate_count.duplicates'AS statistics_name,duplicate_count.duplicates AS statistics_value,'2024-05-07 08:41:01' as data_time,'2024-05-07 08:41:01' as create_time,'2024-05-07 08:41:01' as update_time from duplicate_count\"} },{\"type\":\"hdfs_file\",\"config\":{\"path\":\"hdfs://mycluster:8020/user/default/data_quality_error_data/0_1056_[null check] filtered data persistence\",\"input_table\":\"duplicate_items\"} }]}"
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.640 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.640 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1056/3949/1056_3949.sh
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:01.673 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 1190
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:01.674 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-3949] - [INFO] 2024-05-07 08:41:02.326 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3949, processInstanceId=1056, startTime=1715042461574, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1056/3949.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3949] - [INFO] 2024-05-07 08:41:02.341 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3949, processInstanceId=1056, startTime=1715042461574, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1056/3949.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1715042462309)
[WI-0][TI-3949] - [INFO] 2024-05-07 08:41:02.342 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3949, processInstanceId=1056, startTime=1715042461574, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3949] - [INFO] 2024-05-07 08:41:02.349 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3949, success=true)
[WI-0][TI-3949] - [INFO] 2024-05-07 08:41:02.351 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3949, processInstanceId=1056, startTime=1715042461574, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1715042462309)
[WI-0][TI-3949] - [WARN] 2024-05-07 08:41:02.363 +0800 o.a.d.s.w.m.MessageRetryRunner:[139] - Retry send message to master error
java.util.ConcurrentModificationException: null
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:911)
	at java.util.ArrayList$Itr.next(ArrayList.java:861)
	at org.apache.dolphinscheduler.server.worker.message.MessageRetryRunner.run(MessageRetryRunner.java:127)
[WI-0][TI-3949] - [INFO] 2024-05-07 08:41:02.376 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3949)
[WI-0][TI-3949] - [INFO] 2024-05-07 08:41:02.395 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3949, success=true)
[WI-0][TI-3949] - [INFO] 2024-05-07 08:41:02.408 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3949)
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:02.696 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8780209800578249 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3946] - [INFO] 2024-05-07 08:41:03.364 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3946, processInstanceId=1053, status=9, startTime=1715041547086, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1053/3946.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1053/3946, endTime=1715041560344, processId=628, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042162761)
[WI-0][TI-3946] - [INFO] 2024-05-07 08:41:03.367 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3946, processInstanceId=1053, status=9, startTime=1715041547086, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1053/3946.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1053/3946, endTime=1715041560344, processId=628, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042463364)
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:04.747 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.postgresql#postgresql added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-1b5e237d-9f84-4adc-9c92-358526f32245;1.0
		confs: [default]
		found org.postgresql#postgresql;42.7.3 in central
		found org.checkerframework#checker-qual;3.42.0 in central
	:: resolution report :: resolve 183ms :: artifacts dl 6ms
		:: modules in use:
		org.checkerframework#checker-qual;3.42.0 from central in [default]
		org.postgresql#postgresql;42.7.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-1b5e237d-9f84-4adc-9c92-358526f32245
		confs: [default]
		0 artifacts copied, 2 already retrieved (0kB/4ms)
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:04.753 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8237410071942446 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:05.751 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:41:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	24/05/07 08:41:05 INFO SparkContext: Running Spark version 3.5.1
	24/05/07 08:41:05 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/05/07 08:41:05 INFO SparkContext: Java version 1.8.0_402
	24/05/07 08:41:05 INFO ResourceUtils: ==============================================================
	24/05/07 08:41:05 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/05/07 08:41:05 INFO ResourceUtils: ==============================================================
	24/05/07 08:41:05 INFO SparkContext: Submitted application: (uniqueness_check)
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:05.772 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9056603773584906 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3945] - [INFO] 2024-05-07 08:41:06.407 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3945, processInstanceId=1052, status=6, startTime=1715041538811, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1052/3945.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1052/3945, endTime=1715041563133, processId=534, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042165773)
[WI-0][TI-3945] - [INFO] 2024-05-07 08:41:06.433 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3945, processInstanceId=1052, status=6, startTime=1715041538811, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1052/3945.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1052/3945, endTime=1715041563133, processId=534, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042466407)
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:06.752 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:41:05 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/05/07 08:41:05 INFO ResourceProfile: Limiting resource is cpu
	24/05/07 08:41:05 INFO ResourceProfileManager: Added ResourceProfile id: 0
	24/05/07 08:41:05 INFO SecurityManager: Changing view acls to: default
	24/05/07 08:41:05 INFO SecurityManager: Changing modify acls to: default
	24/05/07 08:41:05 INFO SecurityManager: Changing view acls groups to: 
	24/05/07 08:41:05 INFO SecurityManager: Changing modify acls groups to: 
	24/05/07 08:41:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
	24/05/07 08:41:06 INFO Utils: Successfully started service 'sparkDriver' on port 32807.
	24/05/07 08:41:06 INFO SparkEnv: Registering MapOutputTracker
	24/05/07 08:41:06 INFO SparkEnv: Registering BlockManagerMaster
	24/05/07 08:41:06 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/05/07 08:41:06 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/05/07 08:41:06 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/05/07 08:41:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-42c18daa-b37d-480a-bfad-78a4a6c25ebb
	24/05/07 08:41:06 INFO MemoryStore: MemoryStore started with capacity 93.3 MiB
	24/05/07 08:41:06 INFO SparkEnv: Registering OutputCommitCoordinator
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:06.779 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8406779661016949 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:07.754 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:41:06 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/05/07 08:41:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/05/07 08:41:06 INFO SparkContext: Added JAR file:///tmp/jars/org.postgresql_postgresql-42.7.3.jar at spark://941f01f0fea3:32807/jars/org.postgresql_postgresql-42.7.3.jar with timestamp 1715042465679
	24/05/07 08:41:06 INFO SparkContext: Added JAR file:///tmp/jars/org.checkerframework_checker-qual-3.42.0.jar at spark://941f01f0fea3:32807/jars/org.checkerframework_checker-qual-3.42.0.jar with timestamp 1715042465679
	24/05/07 08:41:06 INFO SparkContext: Added JAR file:/opt/dolphinscheduler/libs/dolphinscheduler-data-quality-3.2.1.jar at spark://941f01f0fea3:32807/jars/dolphinscheduler-data-quality-3.2.1.jar with timestamp 1715042465679
	24/05/07 08:41:07 INFO Executor: Starting executor ID driver on host 941f01f0fea3
	24/05/07 08:41:07 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/05/07 08:41:07 INFO Executor: Java version 1.8.0_402
	24/05/07 08:41:07 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/05/07 08:41:07 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@7d44a19 for default.
	24/05/07 08:41:07 INFO Executor: Fetching spark://941f01f0fea3:32807/jars/dolphinscheduler-data-quality-3.2.1.jar with timestamp 1715042465679
	24/05/07 08:41:07 INFO TransportClientFactory: Successfully created connection to 941f01f0fea3/172.18.1.1:32807 after 72 ms (0 ms spent in bootstraps)
	24/05/07 08:41:07 INFO Utils: Fetching spark://941f01f0fea3:32807/jars/dolphinscheduler-data-quality-3.2.1.jar to /tmp/spark-6346c186-5f61-43fa-b184-b3502f914871/userFiles-06f70a58-227f-48f2-a1b2-e253d4fbcab3/fetchFileTemp3262383260570353957.tmp
	24/05/07 08:41:07 INFO Executor: Adding file:/tmp/spark-6346c186-5f61-43fa-b184-b3502f914871/userFiles-06f70a58-227f-48f2-a1b2-e253d4fbcab3/dolphinscheduler-data-quality-3.2.1.jar to class loader default
	24/05/07 08:41:07 INFO Executor: Fetching spark://941f01f0fea3:32807/jars/org.postgresql_postgresql-42.7.3.jar with timestamp 1715042465679
	24/05/07 08:41:07 INFO Utils: Fetching spark://941f01f0fea3:32807/jars/org.postgresql_postgresql-42.7.3.jar to /tmp/spark-6346c186-5f61-43fa-b184-b3502f914871/userFiles-06f70a58-227f-48f2-a1b2-e253d4fbcab3/fetchFileTemp1647592143006865167.tmp
	24/05/07 08:41:07 INFO Executor: Adding file:/tmp/spark-6346c186-5f61-43fa-b184-b3502f914871/userFiles-06f70a58-227f-48f2-a1b2-e253d4fbcab3/org.postgresql_postgresql-42.7.3.jar to class loader default
	24/05/07 08:41:07 INFO Executor: Fetching spark://941f01f0fea3:32807/jars/org.checkerframework_checker-qual-3.42.0.jar with timestamp 1715042465679
	24/05/07 08:41:07 INFO Utils: Fetching spark://941f01f0fea3:32807/jars/org.checkerframework_checker-qual-3.42.0.jar to /tmp/spark-6346c186-5f61-43fa-b184-b3502f914871/userFiles-06f70a58-227f-48f2-a1b2-e253d4fbcab3/fetchFileTemp6861301787874679859.tmp
	24/05/07 08:41:07 INFO Executor: Adding file:/tmp/spark-6346c186-5f61-43fa-b184-b3502f914871/userFiles-06f70a58-227f-48f2-a1b2-e253d4fbcab3/org.checkerframework_checker-qual-3.42.0.jar to class loader default
	24/05/07 08:41:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36343.
	24/05/07 08:41:07 INFO NettyBlockTransferService: Server created on 941f01f0fea3:36343
	24/05/07 08:41:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/05/07 08:41:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 941f01f0fea3, 36343, None)
	24/05/07 08:41:07 INFO BlockManagerMasterEndpoint: Registering block manager 941f01f0fea3:36343 with 93.3 MiB RAM, BlockManagerId(driver, 941f01f0fea3, 36343, None)
	24/05/07 08:41:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 941f01f0fea3, 36343, None)
	24/05/07 08:41:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 941f01f0fea3, 36343, None)
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:07.786 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8309037900874636 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:08.755 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:41:08 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/05/07 08:41:08 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1056/3949/spark-warehouse'.
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:08.789 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8135593220338982 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:09.801 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8653846153846154 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:10.807 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.882716049382716 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:11.810 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8048780487804879 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:12.920 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7177700348432056 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:13.921 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8935574229691876 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:13.923 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:41:13 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:14.923 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7749077490774908 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:14.926 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:41:14 INFO CodeGenerator: Code generated in 499.972876 ms
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:15.930 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:41:15 INFO DAGScheduler: Registering RDD 2 (save at JdbcWriter.java:87) as input to shuffle 0
	24/05/07 08:41:15 INFO DAGScheduler: Got map stage job 0 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:41:15 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (save at JdbcWriter.java:87)
	24/05/07 08:41:15 INFO DAGScheduler: Parents of final stage: List()
	24/05/07 08:41:15 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:41:15 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:41:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 34.8 KiB, free 93.3 MiB)
	24/05/07 08:41:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 93.3 MiB)
	24/05/07 08:41:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 941f01f0fea3:36343 (size: 16.4 KiB, free: 93.3 MiB)
	24/05/07 08:41:15 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:41:15 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:41:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
	24/05/07 08:41:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (941f01f0fea3, executor driver, partition 0, PROCESS_LOCAL, 7878 bytes) 
	24/05/07 08:41:15 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:15.953 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7484472049689441 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:16.985 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:41:16 INFO CodeGenerator: Code generated in 56.670962 ms
	24/05/07 08:41:16 INFO CodeGenerator: Code generated in 23.568853 ms
	24/05/07 08:41:16 INFO CodeGenerator: Code generated in 15.207626 ms
	24/05/07 08:41:16 INFO CodeGenerator: Code generated in 20.047277 ms
	24/05/07 08:41:16 INFO CodeGenerator: Code generated in 10.491649 ms
	24/05/07 08:41:16 INFO JDBCRDD: closed connection
	24/05/07 08:41:16 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2534 bytes result sent to driver
	24/05/07 08:41:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 830 ms on 941f01f0fea3 (executor driver) (1/1)
	24/05/07 08:41:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
	24/05/07 08:41:16 INFO DAGScheduler: ShuffleMapStage 0 (save at JdbcWriter.java:87) finished in 1.044 s
	24/05/07 08:41:16 INFO DAGScheduler: looking for newly runnable stages
	24/05/07 08:41:16 INFO DAGScheduler: running: Set()
	24/05/07 08:41:16 INFO DAGScheduler: waiting: Set()
	24/05/07 08:41:16 INFO DAGScheduler: failed: Set()
	24/05/07 08:41:16 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
	24/05/07 08:41:16 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
	24/05/07 08:41:16 INFO CodeGenerator: Code generated in 26.38128 ms
	24/05/07 08:41:16 INFO DAGScheduler: Registering RDD 5 (save at JdbcWriter.java:87) as input to shuffle 1
	24/05/07 08:41:16 INFO DAGScheduler: Got map stage job 1 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:41:16 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (save at JdbcWriter.java:87)
	24/05/07 08:41:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
	24/05/07 08:41:16 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:41:16 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[5] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:41:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 40.5 KiB, free 93.2 MiB)
	24/05/07 08:41:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 93.2 MiB)
	24/05/07 08:41:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 941f01f0fea3:36343 (size: 19.1 KiB, free: 93.3 MiB)
	24/05/07 08:41:16 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:41:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[5] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:41:16 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
	24/05/07 08:41:16 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (941f01f0fea3, executor driver, partition 0, NODE_LOCAL, 8032 bytes) 
	24/05/07 08:41:16 INFO Executor: Running task 0.0 in stage 2.0 (TID 1)
	24/05/07 08:41:16 INFO ShuffleBlockFetcherIterator: Getting 1 (2.9 KiB) non-empty blocks including 1 (2.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:16.986 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7769784172661871 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:17.994 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:41:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 28 ms
	24/05/07 08:41:17 INFO CodeGenerator: Code generated in 30.862067 ms
	24/05/07 08:41:17 INFO Executor: Finished task 0.0 in stage 2.0 (TID 1). 5420 bytes result sent to driver
	24/05/07 08:41:17 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 185 ms on 941f01f0fea3 (executor driver) (1/1)
	24/05/07 08:41:17 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
	24/05/07 08:41:17 INFO DAGScheduler: ShuffleMapStage 2 (save at JdbcWriter.java:87) finished in 0.211 s
	24/05/07 08:41:17 INFO DAGScheduler: looking for newly runnable stages
	24/05/07 08:41:17 INFO DAGScheduler: running: Set()
	24/05/07 08:41:17 INFO DAGScheduler: waiting: Set()
	24/05/07 08:41:17 INFO DAGScheduler: failed: Set()
	24/05/07 08:41:17 INFO CodeGenerator: Code generated in 18.532987 ms
	24/05/07 08:41:17 INFO SparkContext: Starting job: save at JdbcWriter.java:87
	24/05/07 08:41:17 INFO DAGScheduler: Got job 2 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:41:17 INFO DAGScheduler: Final stage: ResultStage 5 (save at JdbcWriter.java:87)
	24/05/07 08:41:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
	24/05/07 08:41:17 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:41:17 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[10] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:41:17 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 51.4 KiB, free 93.1 MiB)
	24/05/07 08:41:17 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.0 KiB, free 93.1 MiB)
	24/05/07 08:41:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 941f01f0fea3:36343 (size: 23.0 KiB, free: 93.2 MiB)
	24/05/07 08:41:17 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:41:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[10] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:41:17 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
	24/05/07 08:41:17 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 2) (941f01f0fea3, executor driver, partition 0, NODE_LOCAL, 8043 bytes) 
	24/05/07 08:41:17 INFO Executor: Running task 0.0 in stage 5.0 (TID 2)
	24/05/07 08:41:17 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 941f01f0fea3:36343 in memory (size: 16.4 KiB, free: 93.3 MiB)
	24/05/07 08:41:17 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 941f01f0fea3:36343 in memory (size: 19.1 KiB, free: 93.3 MiB)
	24/05/07 08:41:17 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
	24/05/07 08:41:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
	24/05/07 08:41:17 INFO CodeGenerator: Code generated in 14.400754 ms
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:20.002 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:41:19 INFO CodeGenerator: Code generated in 16.971368 ms
	24/05/07 08:41:19 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 2)
	java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1056'::int4),('3949'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1056_[null check] filtered data persistence'),('2024-05-07 08:41:01'),('2024-05-07 08:41:01')) was aborted: ERROR: value too long for type character varying(15)  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: value too long for type character varying(15)
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	24/05/07 08:41:19 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 2) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1056'::int4),('3949'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1056_[null check] filtered data persistence'),('2024-05-07 08:41:01'),('2024-05-07 08:41:01')) was aborted: ERROR: value too long for type character varying(15)  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: value too long for type character varying(15)
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	24/05/07 08:41:19 ERROR TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job
	24/05/07 08:41:19 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
	24/05/07 08:41:19 INFO TaskSchedulerImpl: Cancelling stage 5
	24/05/07 08:41:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 2) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1056'::int4),('3949'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1056_[null check] filtered data persistence'),('2024-05-07 08:41:01'),('2024-05-07 08:41:01')) was aborted: ERROR: value too long for type character varying(15)  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: value too long for type character varying(15)
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	Driver stacktrace:
	24/05/07 08:41:19 INFO DAGScheduler: ResultStage 5 (save at JdbcWriter.java:87) failed in 1.873 s due to Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 2) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1056'::int4),('3949'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1056_[null check] filtered data persistence'),('2024-05-07 08:41:01'),('2024-05-07 08:41:01')) was aborted: ERROR: value too long for type character varying(15)  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: value too long for type character varying(15)
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	Driver stacktrace:
	24/05/07 08:41:19 INFO DAGScheduler: Job 2 failed: save at JdbcWriter.java:87, took 1.899026 s
	Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 2) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1056'::int4),('3949'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1056_[null check] filtered data persistence'),('2024-05-07 08:41:01'),('2024-05-07 08:41:01')) was aborted: ERROR: value too long for type character varying(15)  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: value too long for type character varying(15)
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	Driver stacktrace:
		at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
		at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
		at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
		at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
		at scala.Option.foreach(Option.scala:407)
		at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
		at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)
		at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
		at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
		at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
		at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)
		at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3514)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4309)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
		at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4307)
		at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3514)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
		at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
		at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
		at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
		at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
		at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
		at org.apache.dolphinscheduler.data.quality.flow.batch.writer.JdbcWriter.write(JdbcWriter.java:87)
		at org.apache.dolphinscheduler.data.quality.execution.SparkBatchExecution.executeWriter(SparkBatchExecution.java:132)
		at org.apache.dolphinscheduler.data.quality.execution.SparkBatchExecution.execute(SparkBatchExecution.java:58)
		at org.apache.dolphinscheduler.data.quality.context.DataQualityContext.execute(DataQualityContext.java:62)
		at org.apache.dolphinscheduler.data.quality.DataQualityApplication.main(DataQualityApplication.java:78)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
		at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)
		at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
		at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
		at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
		at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
		at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
		at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
	Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_execute_result ("rule_type","rule_name","process_definition_id","process_instance_id","task_instance_id","statistics_value","comparison_value","comparison_type","check_type","threshold","operator","failure_strategy","error_output_path","create_time","update_time") VALUES (('0'::int4),('(uniqueness_check)'),('0'::int4),('1056'::int4),('3949'::int4),('1'::int8),('0'::int4),('1'::int4),('0'::int4),('0'::int4),('0'::int4),('0'::int4),('hdfs://mycluster:8020/user/default/data_quality_error_data/0_1056_[null check] filtered data persistence'),('2024-05-07 08:41:01'),('2024-05-07 08:41:01')) was aborted: ERROR: value too long for type character varying(15)  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: value too long for type character varying(15)
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	24/05/07 08:41:19 INFO SparkContext: Invoking stop() from shutdown hook
	24/05/07 08:41:19 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/05/07 08:41:19 INFO SparkUI: Stopped Spark web UI at http://941f01f0fea3:4040
	24/05/07 08:41:19 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/05/07 08:41:19 INFO MemoryStore: MemoryStore cleared
	24/05/07 08:41:19 INFO BlockManager: BlockManager stopped
	24/05/07 08:41:19 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/05/07 08:41:19 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/05/07 08:41:19 INFO SparkContext: Successfully stopped SparkContext
	24/05/07 08:41:19 INFO ShutdownHookManager: Shutdown hook called
	24/05/07 08:41:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-6346c186-5f61-43fa-b184-b3502f914871
	24/05/07 08:41:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-954e1bbd-ef60-4212-8606-ca5e14606bc1
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:20.005 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1056/3949, processId:1190 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:20.007 +0800 o.a.d.p.t.a.u.LogUtils:[79] - Start finding appId in /opt/dolphinscheduler/logs/20240507/13505298546272/21/1056/3949.log, fetch way: log 
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:20.009 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:20.010 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:20.010 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:20.011 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:20.020 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:20.020 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:20.021 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1056/3949
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:20.022 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1056/3949
[WI-1056][TI-3949] - [INFO] 2024-05-07 08:41:20.023 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3949] - [INFO] 2024-05-07 08:41:20.464 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3949, processInstanceId=1056, status=6, startTime=1715042461574, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1056/3949.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1056/3949, endTime=1715042480010, processId=1190, appIds=, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3949] - [INFO] 2024-05-07 08:41:20.467 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3949, processInstanceId=1056, status=6, startTime=1715042461574, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1056/3949.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1056/3949, endTime=1715042480010, processId=1190, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042480463)
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:23.038 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7303370786516854 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [WARN] 2024-05-07 08:41:24.978 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[147] - [id: 0x9b5d4ccc, L:/172.18.1.1:1234 - R:/172.18.0.13:48102] is not writable, over high water level : 65536
[WI-0][TI-0] - [WARN] 2024-05-07 08:41:24.994 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[154] - [id: 0x9b5d4ccc, L:/172.18.1.1:1234 - R:/172.18.0.13:48102] is writable, to low water : 32768
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:52.993 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[55] - Receive TaskInstanceKillRequest: TaskInstanceKillRequest(taskInstanceId=3949)
[WI-0][TI-3949] - [ERROR] 2024-05-07 08:41:52.993 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[62] - Cannot find WorkerTaskExecutor for taskInstance: 3949
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:57.871 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3950, taskName=[null check] filtered data persistence, firstSubmitTime=1715042517840, startTime=0, taskType=DATA_QUALITY, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13505298546272, processDefineVersion=21, appIds=null, processInstanceId=1057, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13001194483488, taskParams={"localParams":[],"resourceList":[],"ruleId":6,"ruleInputParameter":{"check_type":"0","comparison_type":1,"comparison_name":"0","failure_strategy":"0","operator":"0","src_connector_type":1,"src_datasource_id":6,"src_database":"persistence","src_field":"content","src_table":"filtered_data_persistence","threshold":"0"},"sparkParameters":{"deployMode":"local","driverCores":1,"driverMemory":"512M","executorCores":1,"executorMemory":"1G","numExecutors":1,"others":"--conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n--packages org.postgresql:postgresql:42.7.3","yarnQueue":""}}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='[null check] filtered data persistence'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13001194483488'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='1057'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240507'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240506'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3950'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='qualti_test'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13505264408800'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13505298546272'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240507084157'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=org.apache.dolphinscheduler.plugin.task.api.DataQualityTaskExecutionContext@3a09873e, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.872 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: [null check] filtered data persistence to wait queue success
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.873 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.875 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.875 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.875 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.875 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1715042517875
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.875 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 1057_3950
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.875 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3950,
  "taskName" : "[null check] filtered data persistence",
  "firstSubmitTime" : 1715042517840,
  "startTime" : 1715042517875,
  "taskType" : "DATA_QUALITY",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240507/13505298546272/21/1057/3950.log",
  "processId" : 0,
  "processDefineCode" : 13505298546272,
  "processDefineVersion" : 21,
  "processInstanceId" : 1057,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13001194483488,
  "taskParams" : "{\"localParams\":[],\"resourceList\":[],\"ruleId\":6,\"ruleInputParameter\":{\"check_type\":\"0\",\"comparison_type\":1,\"comparison_name\":\"0\",\"failure_strategy\":\"0\",\"operator\":\"0\",\"src_connector_type\":1,\"src_datasource_id\":6,\"src_database\":\"persistence\",\"src_field\":\"content\",\"src_table\":\"filtered_data_persistence\",\"threshold\":\"0\"},\"sparkParameters\":{\"deployMode\":\"local\",\"driverCores\":1,\"driverMemory\":\"512M\",\"executorCores\":1,\"executorMemory\":\"1G\",\"numExecutors\":1,\"others\":\"--conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n--packages org.postgresql:postgresql:42.7.3\",\"yarnQueue\":\"\"}}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "[null check] filtered data persistence"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13001194483488"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1057"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240507"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240506"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3950"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "qualti_test"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13505264408800"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13505298546272"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240507084157"
    }
  },
  "taskAppId" : "1057_3950",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "dataQualityTaskExecutionContext" : {
    "ruleId" : 6,
    "ruleName" : "$t(uniqueness_check)",
    "ruleType" : 0,
    "ruleInputEntryList" : "[{\"id\":1,\"field\":\"src_connector_type\",\"type\":\"select\",\"title\":\"$t(src_connector_type)\",\"data\":\"\",\"options\":\"[{\\\"label\\\":\\\"HIVE\\\",\\\"value\\\":\\\"HIVE\\\"},{\\\"label\\\":\\\"JDBC\\\",\\\"value\\\":\\\"JDBC\\\"}]\",\"placeholder\":\"please select source connector type\",\"optionSourceType\":2,\"dataType\":2,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":1,\"createTime\":null,\"updateTime\":null},{\"id\":2,\"field\":\"src_datasource_id\",\"type\":\"select\",\"title\":\"$t(src_datasource_id)\",\"data\":\"\",\"options\":null,\"placeholder\":\"please select source datasource id\",\"optionSourceType\":1,\"dataType\":2,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":2,\"createTime\":null,\"updateTime\":null},{\"id\":30,\"field\":\"src_database\",\"type\":\"select\",\"title\":\"$t(src_database)\",\"data\":null,\"options\":null,\"placeholder\":\"Please select source database\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":true,\"valuesMap\":null,\"index\":2,\"createTime\":null,\"updateTime\":null},{\"id\":3,\"field\":\"src_table\",\"type\":\"select\",\"title\":\"$t(src_table)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter source table name\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":true,\"valuesMap\":null,\"index\":3,\"createTime\":null,\"updateTime\":null},{\"id\":4,\"field\":\"src_filter\",\"type\":\"input\",\"title\":\"$t(src_filter)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter filter expression\",\"optionSourceType\":0,\"dataType\":3,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":4,\"createTime\":null,\"updateTime\":null},{\"id\":5,\"field\":\"src_field\",\"type\":\"select\",\"title\":\"$t(src_field)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter column, only single column is supported\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":null,\"index\":5,\"createTime\":null,\"updateTime\":null},{\"id\":6,\"field\":\"statistics_name\",\"type\":\"input\",\"title\":\"$t(statistics_name)\",\"data\":\"duplicate_count.duplicates\",\"options\":null,\"placeholder\":\"Please enter statistics name, the alias in statistics execute sql\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":1,\"isShow\":false,\"canEdit\":false,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":\"{\\\"statistics_name\\\":\\\"duplicate_count.duplicates\\\"}\",\"index\":6,\"createTime\":null,\"updateTime\":null},{\"id\":7,\"field\":\"check_type\",\"type\":\"select\",\"title\":\"$t(check_type)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"Expected - Actual\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"Actual - Expected\\\",\\\"value\\\":\\\"1\\\"},{\\\"label\\\":\\\"Actual / Expected\\\",\\\"value\\\":\\\"2\\\"},{\\\"label\\\":\\\"(Expected - Actual) / Expected\\\",\\\"value\\\":\\\"3\\\"}]\",\"placeholder\":\"please select check type\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":7,\"createTime\":null,\"updateTime\":null},{\"id\":8,\"field\":\"operator\",\"type\":\"select\",\"title\":\"$t(operator)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"=\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"<\\\",\\\"value\\\":\\\"1\\\"},{\\\"label\\\":\\\"<=\\\",\\\"value\\\":\\\"2\\\"},{\\\"label\\\":\\\">\\\",\\\"value\\\":\\\"3\\\"},{\\\"label\\\":\\\">=\\\",\\\"value\\\":\\\"4\\\"},{\\\"label\\\":\\\"!=\\\",\\\"value\\\":\\\"5\\\"}]\",\"placeholder\":\"please select operator\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":8,\"createTime\":null,\"updateTime\":null},{\"id\":9,\"field\":\"threshold\",\"type\":\"input\",\"title\":\"$t(threshold)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter threshold, number is needed\",\"optionSourceType\":0,\"dataType\":2,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":null,\"index\":9,\"createTime\":null,\"updateTime\":null},{\"id\":10,\"field\":\"failure_strategy\",\"type\":\"select\",\"title\":\"$t(failure_strategy)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"Alert\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"Block\\\",\\\"value\\\":\\\"1\\\"}]\",\"placeholder\":\"please select failure strategy\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":10,\"createTime\":null,\"updateTime\":null},{\"id\":17,\"field\":\"comparison_name\",\"type\":\"input\",\"title\":\"$t(comparison_name)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter comparison name, the alias in comparison execute sql\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":false,\"canEdit\":false,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":\"\",\"index\":11,\"createTime\":null,\"updateTime\":null},{\"id\":19,\"field\":\"comparison_type\",\"type\":\"select\",\"title\":\"$t(comparison_type)\",\"data\":\"\",\"options\":null,\"placeholder\":\"Please enter comparison title\",\"optionSourceType\":3,\"dataType\":0,\"inputType\":2,\"isShow\":true,\"canEdit\":false,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":12,\"createTime\":null,\"updateTime\":null}]",
    "executeSqlList" : "[{\"id\":6,\"index\":1,\"sql\":\"SELECT ${src_field} FROM ${src_table} group by ${src_field} having count(*) > 1\",\"tableAlias\":\"duplicate_items\",\"type\":0,\"createTime\":\"2021-03-03 11:31:24\",\"updateTime\":\"2021-03-03 11:31:24\",\"errorOutputSql\":true},{\"id\":7,\"index\":1,\"sql\":\"SELECT COUNT(*) AS duplicates FROM duplicate_items\",\"tableAlias\":\"duplicate_count\",\"type\":1,\"createTime\":\"2021-03-03 11:31:24\",\"updateTime\":\"2021-03-03 11:31:24\",\"errorOutputSql\":false}]",
    "comparisonNeedStatisticsValueTable" : false,
    "compareWithFixedValue" : true,
    "hdfsPath" : "hdfs://mycluster:8020/user/default/data_quality_error_data",
    "sourceConnectorType" : "JDBC",
    "sourceType" : 1,
    "sourceConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"persistence\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence\",\"driverClassName\":\"org.postgresql.Driver\",\"validationQuery\":\"select version()\",\"other\":{\"password\":\"root\"}}",
    "targetConnectorType" : null,
    "targetType" : 0,
    "targetConnectionParams" : null,
    "writerConnectorType" : "JDBC",
    "writerType" : 1,
    "writerTable" : "t_ds_dq_execute_result",
    "writerConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"dolphinscheduler\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\"}",
    "statisticsValueConnectorType" : "JDBC",
    "statisticsValueType" : 1,
    "statisticsValueTable" : "t_ds_dq_task_statistics_value",
    "statisticsValueWriterConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"dolphinscheduler\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\"}"
  },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.877 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.877 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.877 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.887 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.888 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.889 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1057/3950 check successfully
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.889 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.dq.DataQualityTaskChannel successfully
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.890 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.890 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.890 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: DATA_QUALITY create successfully
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.891 +0800 o.a.d.p.t.d.DataQualityTask:[89] - Initialize data quality task params {
  "localParams" : [ ],
  "varPool" : null,
  "ruleId" : 6,
  "ruleInputParameter" : {
    "check_type" : "0",
    "comparison_type" : "1",
    "comparison_name" : "0",
    "failure_strategy" : "0",
    "operator" : "0",
    "src_connector_type" : "1",
    "src_datasource_id" : "6",
    "src_database" : "persistence",
    "src_field" : "content",
    "src_table" : "filtered_data_persistence",
    "threshold" : "0"
  },
  "sparkParameters" : {
    "localParams" : null,
    "varPool" : null,
    "mainJar" : null,
    "mainClass" : null,
    "deployMode" : "local",
    "mainArgs" : null,
    "driverCores" : 1,
    "driverMemory" : "512M",
    "numExecutors" : 1,
    "executorCores" : 1,
    "executorMemory" : "1G",
    "appName" : null,
    "yarnQueue" : "",
    "others" : "--conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n--packages org.postgresql:postgresql:42.7.3",
    "programType" : null,
    "resourceList" : [ ]
  }
}
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.900 +0800 o.a.d.p.t.d.DataQualityTask:[119] - data quality configuration: {
  "name" : "$t(uniqueness_check)",
  "env" : {
    "type" : "batch",
    "config" : null
  },
  "readers" : [ {
    "type" : "JDBC",
    "config" : {
      "database" : "persistence",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "output_table" : "persistence_filtered_data_persistence",
      "table" : "filtered_data_persistence",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence?password=root"
    }
  } ],
  "transformers" : [ {
    "type" : "sql",
    "config" : {
      "index" : 1,
      "output_table" : "duplicate_items",
      "sql" : "SELECT content FROM persistence_filtered_data_persistence group by content having count(*) > 1"
    }
  }, {
    "type" : "sql",
    "config" : {
      "index" : 2,
      "output_table" : "duplicate_count",
      "sql" : "SELECT COUNT(*) AS duplicates FROM duplicate_items"
    }
  } ],
  "writers" : [ {
    "type" : "JDBC",
    "config" : {
      "database" : "dolphinscheduler",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "table" : "t_ds_dq_execute_result",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler",
      "sql" : "select 0 as rule_type,'$t(uniqueness_check)' as rule_name,0 as process_definition_id,1057 as process_instance_id,3950 as task_instance_id,duplicate_count.duplicates AS statistics_value,0 AS comparison_value,1 AS comparison_type,0 as check_type,0 as threshold,0 as operator,0 as failure_strategy,'hdfs://mycluster:8020/user/default/data_quality_error_data/0_1057_[null check] filtered data persistence' as error_output_path,'2024-05-07 08:41:57' as create_time,'2024-05-07 08:41:57' as update_time from duplicate_count "
    }
  }, {
    "type" : "JDBC",
    "config" : {
      "database" : "dolphinscheduler",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "table" : "t_ds_dq_task_statistics_value",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler",
      "sql" : "select 0 as process_definition_id,3950 as task_instance_id,6 as rule_id,'KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG=' as unique_code,'duplicate_count.duplicates'AS statistics_name,duplicate_count.duplicates AS statistics_value,'2024-05-07 08:41:57' as data_time,'2024-05-07 08:41:57' as create_time,'2024-05-07 08:41:57' as update_time from duplicate_count"
    }
  }, {
    "type" : "hdfs_file",
    "config" : {
      "path" : "hdfs://mycluster:8020/user/default/data_quality_error_data/0_1057_[null check] filtered data persistence",
      "input_table" : "duplicate_items"
    }
  } ]
}
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.904 +0800 o.a.d.p.d.a.u.CommonUtils:[136] - Trying to get data quality jar in path
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.905 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.905 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.905 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.905 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.905 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.907 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.907 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.907 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${SPARK_HOME}/bin/spark-submit --master local --driver-cores 1 --driver-memory 512M --num-executors 1 --executor-cores 1 --executor-memory 1G --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
--packages org.postgresql:postgresql:42.7.3 /opt/dolphinscheduler/conf/../libs/dolphinscheduler-data-quality-3.2.1.jar "{\"name\":\"$t(uniqueness_check)\",\"env\":{\"type\":\"batch\",\"config\":null},\"readers\":[{\"type\":\"JDBC\",\"config\":{\"database\":\"persistence\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"output_table\":\"persistence_filtered_data_persistence\",\"table\":\"filtered_data_persistence\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence?password=root\"} }],\"transformers\":[{\"type\":\"sql\",\"config\":{\"index\":1,\"output_table\":\"duplicate_items\",\"sql\":\"SELECT content FROM persistence_filtered_data_persistence group by content having count(*) > 1\"} },{\"type\":\"sql\",\"config\":{\"index\":2,\"output_table\":\"duplicate_count\",\"sql\":\"SELECT COUNT(*) AS duplicates FROM duplicate_items\"} }],\"writers\":[{\"type\":\"JDBC\",\"config\":{\"database\":\"dolphinscheduler\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"table\":\"t_ds_dq_execute_result\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\",\"sql\":\"select 0 as rule_type,'$t(uniqueness_check)' as rule_name,0 as process_definition_id,1057 as process_instance_id,3950 as task_instance_id,duplicate_count.duplicates AS statistics_value,0 AS comparison_value,1 AS comparison_type,0 as check_type,0 as threshold,0 as operator,0 as failure_strategy,'hdfs://mycluster:8020/user/default/data_quality_error_data/0_1057_[null check] filtered data persistence' as error_output_path,'2024-05-07 08:41:57' as create_time,'2024-05-07 08:41:57' as update_time from duplicate_count \"} },{\"type\":\"JDBC\",\"config\":{\"database\":\"dolphinscheduler\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"table\":\"t_ds_dq_task_statistics_value\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\",\"sql\":\"select 0 as process_definition_id,3950 as task_instance_id,6 as rule_id,'KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG=' as unique_code,'duplicate_count.duplicates'AS statistics_name,duplicate_count.duplicates AS statistics_value,'2024-05-07 08:41:57' as data_time,'2024-05-07 08:41:57' as create_time,'2024-05-07 08:41:57' as update_time from duplicate_count\"} },{\"type\":\"hdfs_file\",\"config\":{\"path\":\"hdfs://mycluster:8020/user/default/data_quality_error_data/0_1057_[null check] filtered data persistence\",\"input_table\":\"duplicate_items\"} }]}"
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.908 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.908 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1057/3950/1057_3950.sh
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:57.925 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:41:57.925 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 1323
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:58.183 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7302052785923754 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3950] - [INFO] 2024-05-07 08:41:58.512 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3950, processInstanceId=1057, startTime=1715042517875, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1057/3950.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3950] - [INFO] 2024-05-07 08:41:58.516 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3950, success=true)
[WI-0][TI-3950] - [INFO] 2024-05-07 08:41:58.523 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3950, processInstanceId=1057, startTime=1715042517875, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1057/3950.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1715042518512)
[WI-0][TI-3950] - [INFO] 2024-05-07 08:41:58.538 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3950)
[WI-0][TI-3950] - [INFO] 2024-05-07 08:41:58.571 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3950, success=true)
[WI-0][TI-0] - [INFO] 2024-05-07 08:41:59.201 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8308605341246291 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:00.205 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8522727272727273 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:00.941 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:01.210 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8709677419354839 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:01.943 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.postgresql#postgresql added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-662aef23-2965-44bb-bfd2-45ffd67589e1;1.0
		confs: [default]
		found org.postgresql#postgresql;42.7.3 in central
		found org.checkerframework#checker-qual;3.42.0 in central
	:: resolution report :: resolve 268ms :: artifacts dl 7ms
		:: modules in use:
		org.checkerframework#checker-qual;3.42.0 from central in [default]
		org.postgresql#postgresql;42.7.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-662aef23-2965-44bb-bfd2-45ffd67589e1
		confs: [default]
		0 artifacts copied, 2 already retrieved (0kB/11ms)
	24/05/07 08:42:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:02.216 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8908554572271387 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:03.015 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:42:02 INFO SparkContext: Running Spark version 3.5.1
	24/05/07 08:42:02 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/05/07 08:42:02 INFO SparkContext: Java version 1.8.0_402
	24/05/07 08:42:02 INFO ResourceUtils: ==============================================================
	24/05/07 08:42:02 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/05/07 08:42:02 INFO ResourceUtils: ==============================================================
	24/05/07 08:42:02 INFO SparkContext: Submitted application: (uniqueness_check)
	24/05/07 08:42:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/05/07 08:42:02 INFO ResourceProfile: Limiting resource is cpu
	24/05/07 08:42:02 INFO ResourceProfileManager: Added ResourceProfile id: 0
	24/05/07 08:42:02 INFO SecurityManager: Changing view acls to: default
	24/05/07 08:42:02 INFO SecurityManager: Changing modify acls to: default
	24/05/07 08:42:02 INFO SecurityManager: Changing view acls groups to: 
	24/05/07 08:42:02 INFO SecurityManager: Changing modify acls groups to: 
	24/05/07 08:42:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
	24/05/07 08:42:02 INFO Utils: Successfully started service 'sparkDriver' on port 40979.
	24/05/07 08:42:02 INFO SparkEnv: Registering MapOutputTracker
	24/05/07 08:42:02 INFO SparkEnv: Registering BlockManagerMaster
	24/05/07 08:42:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/05/07 08:42:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/05/07 08:42:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/05/07 08:42:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-73b59a92-89c6-402b-b6c6-224f3bf8b88d
	24/05/07 08:42:02 INFO MemoryStore: MemoryStore started with capacity 93.3 MiB
	24/05/07 08:42:02 INFO SparkEnv: Registering OutputCommitCoordinator
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:03.219 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7743190661478598 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:04.022 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:42:03 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/05/07 08:42:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/05/07 08:42:03 INFO SparkContext: Added JAR file:///tmp/jars/org.postgresql_postgresql-42.7.3.jar at spark://941f01f0fea3:40979/jars/org.postgresql_postgresql-42.7.3.jar with timestamp 1715042522066
	24/05/07 08:42:03 INFO SparkContext: Added JAR file:///tmp/jars/org.checkerframework_checker-qual-3.42.0.jar at spark://941f01f0fea3:40979/jars/org.checkerframework_checker-qual-3.42.0.jar with timestamp 1715042522066
	24/05/07 08:42:03 INFO SparkContext: Added JAR file:/opt/dolphinscheduler/libs/dolphinscheduler-data-quality-3.2.1.jar at spark://941f01f0fea3:40979/jars/dolphinscheduler-data-quality-3.2.1.jar with timestamp 1715042522066
	24/05/07 08:42:03 INFO Executor: Starting executor ID driver on host 941f01f0fea3
	24/05/07 08:42:03 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/05/07 08:42:03 INFO Executor: Java version 1.8.0_402
	24/05/07 08:42:03 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/05/07 08:42:03 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@698fee9a for default.
	24/05/07 08:42:03 INFO Executor: Fetching spark://941f01f0fea3:40979/jars/dolphinscheduler-data-quality-3.2.1.jar with timestamp 1715042522066
	24/05/07 08:42:03 INFO TransportClientFactory: Successfully created connection to 941f01f0fea3/172.18.1.1:40979 after 90 ms (0 ms spent in bootstraps)
	24/05/07 08:42:03 INFO Utils: Fetching spark://941f01f0fea3:40979/jars/dolphinscheduler-data-quality-3.2.1.jar to /tmp/spark-7ee05913-317b-423d-87a5-6e57bf444b82/userFiles-907b83c6-53da-4216-8066-fc02f9b99cbb/fetchFileTemp3381381140596436442.tmp
	24/05/07 08:42:03 INFO Executor: Adding file:/tmp/spark-7ee05913-317b-423d-87a5-6e57bf444b82/userFiles-907b83c6-53da-4216-8066-fc02f9b99cbb/dolphinscheduler-data-quality-3.2.1.jar to class loader default
	24/05/07 08:42:03 INFO Executor: Fetching spark://941f01f0fea3:40979/jars/org.postgresql_postgresql-42.7.3.jar with timestamp 1715042522066
	24/05/07 08:42:03 INFO Utils: Fetching spark://941f01f0fea3:40979/jars/org.postgresql_postgresql-42.7.3.jar to /tmp/spark-7ee05913-317b-423d-87a5-6e57bf444b82/userFiles-907b83c6-53da-4216-8066-fc02f9b99cbb/fetchFileTemp1571774360084200869.tmp
	24/05/07 08:42:03 INFO Executor: Adding file:/tmp/spark-7ee05913-317b-423d-87a5-6e57bf444b82/userFiles-907b83c6-53da-4216-8066-fc02f9b99cbb/org.postgresql_postgresql-42.7.3.jar to class loader default
	24/05/07 08:42:03 INFO Executor: Fetching spark://941f01f0fea3:40979/jars/org.checkerframework_checker-qual-3.42.0.jar with timestamp 1715042522066
	24/05/07 08:42:03 INFO Utils: Fetching spark://941f01f0fea3:40979/jars/org.checkerframework_checker-qual-3.42.0.jar to /tmp/spark-7ee05913-317b-423d-87a5-6e57bf444b82/userFiles-907b83c6-53da-4216-8066-fc02f9b99cbb/fetchFileTemp7446497307812527903.tmp
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:04.221 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9019607843137255 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:05.024 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:42:04 INFO Executor: Adding file:/tmp/spark-7ee05913-317b-423d-87a5-6e57bf444b82/userFiles-907b83c6-53da-4216-8066-fc02f9b99cbb/org.checkerframework_checker-qual-3.42.0.jar to class loader default
	24/05/07 08:42:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41637.
	24/05/07 08:42:04 INFO NettyBlockTransferService: Server created on 941f01f0fea3:41637
	24/05/07 08:42:04 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/05/07 08:42:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 941f01f0fea3, 41637, None)
	24/05/07 08:42:04 INFO BlockManagerMasterEndpoint: Registering block manager 941f01f0fea3:41637 with 93.3 MiB RAM, BlockManagerId(driver, 941f01f0fea3, 41637, None)
	24/05/07 08:42:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 941f01f0fea3, 41637, None)
	24/05/07 08:42:04 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 941f01f0fea3, 41637, None)
	24/05/07 08:42:04 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/05/07 08:42:04 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1057/3950/spark-warehouse'.
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:05.222 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7911392405063291 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:06.223 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7411003236245954 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:07.228 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8071625344352616 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:08.297 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7204968944099378 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:09.309 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8333333333333334 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:10.313 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9278688524590164 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:11.074 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:42:10 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:11.319 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7885906040268457 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:12.075 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:42:11 INFO CodeGenerator: Code generated in 437.805513 ms
	24/05/07 08:42:11 INFO DAGScheduler: Registering RDD 2 (save at JdbcWriter.java:87) as input to shuffle 0
	24/05/07 08:42:11 INFO DAGScheduler: Got map stage job 0 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:42:11 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (save at JdbcWriter.java:87)
	24/05/07 08:42:11 INFO DAGScheduler: Parents of final stage: List()
	24/05/07 08:42:11 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:42:11 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:42:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 34.8 KiB, free 93.3 MiB)
	24/05/07 08:42:12 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 93.3 MiB)
	24/05/07 08:42:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 941f01f0fea3:41637 (size: 16.4 KiB, free: 93.3 MiB)
	24/05/07 08:42:12 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:42:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:42:12 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:12.326 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8012820512820513 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:13.080 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:42:12 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (941f01f0fea3, executor driver, partition 0, PROCESS_LOCAL, 7878 bytes) 
	24/05/07 08:42:12 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
	24/05/07 08:42:13 INFO CodeGenerator: Code generated in 90.973082 ms
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:13.332 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.828125 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3947] - [INFO] 2024-05-07 08:42:13.703 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3947, processInstanceId=1054, status=6, startTime=1715041914072, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1054/3947.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1054/3947, endTime=1715041931317, processId=835, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042232975)
[WI-0][TI-3947] - [INFO] 2024-05-07 08:42:13.706 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3947, processInstanceId=1054, status=6, startTime=1715041914072, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1054/3947.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1054/3947, endTime=1715041931317, processId=835, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042533703)
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:14.088 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:42:13 INFO CodeGenerator: Code generated in 26.135721 ms
	24/05/07 08:42:13 INFO CodeGenerator: Code generated in 10.932318 ms
	24/05/07 08:42:13 INFO CodeGenerator: Code generated in 10.63605 ms
	24/05/07 08:42:13 INFO CodeGenerator: Code generated in 12.003991 ms
	24/05/07 08:42:13 INFO JDBCRDD: closed connection
	24/05/07 08:42:13 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2534 bytes result sent to driver
	24/05/07 08:42:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1260 ms on 941f01f0fea3 (executor driver) (1/1)
	24/05/07 08:42:13 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
	24/05/07 08:42:13 INFO DAGScheduler: ShuffleMapStage 0 (save at JdbcWriter.java:87) finished in 1.560 s
	24/05/07 08:42:13 INFO DAGScheduler: looking for newly runnable stages
	24/05/07 08:42:13 INFO DAGScheduler: running: Set()
	24/05/07 08:42:13 INFO DAGScheduler: waiting: Set()
	24/05/07 08:42:13 INFO DAGScheduler: failed: Set()
	24/05/07 08:42:13 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
	24/05/07 08:42:13 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
	24/05/07 08:42:13 INFO CodeGenerator: Code generated in 17.694142 ms
	24/05/07 08:42:13 INFO DAGScheduler: Registering RDD 5 (save at JdbcWriter.java:87) as input to shuffle 1
	24/05/07 08:42:13 INFO DAGScheduler: Got map stage job 1 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:42:13 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (save at JdbcWriter.java:87)
	24/05/07 08:42:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
	24/05/07 08:42:13 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:42:13 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[5] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:42:13 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 40.5 KiB, free 93.2 MiB)
	24/05/07 08:42:13 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 93.2 MiB)
	24/05/07 08:42:13 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 941f01f0fea3:41637 (size: 19.1 KiB, free: 93.3 MiB)
	24/05/07 08:42:13 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:42:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[5] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:42:13 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
	24/05/07 08:42:13 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (941f01f0fea3, executor driver, partition 0, NODE_LOCAL, 8032 bytes) 
	24/05/07 08:42:13 INFO Executor: Running task 0.0 in stage 2.0 (TID 1)
	24/05/07 08:42:13 INFO ShuffleBlockFetcherIterator: Getting 1 (2.9 KiB) non-empty blocks including 1 (2.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
	24/05/07 08:42:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 36 ms
	24/05/07 08:42:13 INFO CodeGenerator: Code generated in 31.850006 ms
	24/05/07 08:42:13 INFO Executor: Finished task 0.0 in stage 2.0 (TID 1). 5420 bytes result sent to driver
	24/05/07 08:42:13 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 239 ms on 941f01f0fea3 (executor driver) (1/1)
	24/05/07 08:42:13 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
	24/05/07 08:42:13 INFO DAGScheduler: ShuffleMapStage 2 (save at JdbcWriter.java:87) finished in 0.275 s
	24/05/07 08:42:13 INFO DAGScheduler: looking for newly runnable stages
	24/05/07 08:42:13 INFO DAGScheduler: running: Set()
	24/05/07 08:42:13 INFO DAGScheduler: waiting: Set()
	24/05/07 08:42:13 INFO DAGScheduler: failed: Set()
	24/05/07 08:42:14 INFO CodeGenerator: Code generated in 38.647086 ms
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:14.337 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8664596273291926 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:15.267 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:42:14 INFO SparkContext: Starting job: save at JdbcWriter.java:87
	24/05/07 08:42:14 INFO DAGScheduler: Got job 2 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:42:14 INFO DAGScheduler: Final stage: ResultStage 5 (save at JdbcWriter.java:87)
	24/05/07 08:42:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
	24/05/07 08:42:14 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:42:14 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[10] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:42:14 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 51.4 KiB, free 93.1 MiB)
	24/05/07 08:42:14 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.0 KiB, free 93.1 MiB)
	24/05/07 08:42:14 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 941f01f0fea3:41637 (size: 23.0 KiB, free: 93.2 MiB)
	24/05/07 08:42:14 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:42:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[10] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:42:14 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
	24/05/07 08:42:14 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 2) (941f01f0fea3, executor driver, partition 0, NODE_LOCAL, 8043 bytes) 
	24/05/07 08:42:14 INFO Executor: Running task 0.0 in stage 5.0 (TID 2)
	24/05/07 08:42:14 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 941f01f0fea3:41637 in memory (size: 16.4 KiB, free: 93.3 MiB)
	24/05/07 08:42:14 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 941f01f0fea3:41637 in memory (size: 19.1 KiB, free: 93.3 MiB)
	24/05/07 08:42:14 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
	24/05/07 08:42:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
	24/05/07 08:42:14 INFO CodeGenerator: Code generated in 42.371768 ms
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:15.355 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7848605577689243 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:17.288 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:42:16 INFO CodeGenerator: Code generated in 17.757652 ms
	24/05/07 08:42:16 INFO Executor: Finished task 0.0 in stage 5.0 (TID 2). 6711 bytes result sent to driver
	24/05/07 08:42:16 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 2) in 2091 ms on 941f01f0fea3 (executor driver) (1/1)
	24/05/07 08:42:16 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
	24/05/07 08:42:16 INFO DAGScheduler: ResultStage 5 (save at JdbcWriter.java:87) finished in 2.388 s
	24/05/07 08:42:16 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
	24/05/07 08:42:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
	24/05/07 08:42:16 INFO DAGScheduler: Job 2 finished: save at JdbcWriter.java:87, took 2.440402 s
	24/05/07 08:42:16 INFO DAGScheduler: Registering RDD 13 (save at JdbcWriter.java:87) as input to shuffle 2
	24/05/07 08:42:16 INFO DAGScheduler: Got map stage job 3 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:42:16 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (save at JdbcWriter.java:87)
	24/05/07 08:42:16 INFO DAGScheduler: Parents of final stage: List()
	24/05/07 08:42:16 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:42:16 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[13] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:42:16 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 34.8 KiB, free 93.2 MiB)
	24/05/07 08:42:16 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 16.5 KiB, free 93.2 MiB)
	24/05/07 08:42:16 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 941f01f0fea3:41637 (size: 16.5 KiB, free: 93.3 MiB)
	24/05/07 08:42:16 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:42:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[13] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:42:16 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
	24/05/07 08:42:16 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 3) (941f01f0fea3, executor driver, partition 0, PROCESS_LOCAL, 7878 bytes) 
	24/05/07 08:42:16 INFO Executor: Running task 0.0 in stage 6.0 (TID 3)
	24/05/07 08:42:16 INFO JDBCRDD: closed connection
	24/05/07 08:42:16 INFO Executor: Finished task 0.0 in stage 6.0 (TID 3). 2448 bytes result sent to driver
	24/05/07 08:42:16 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 3) in 81 ms on 941f01f0fea3 (executor driver) (1/1)
	24/05/07 08:42:16 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
	24/05/07 08:42:16 INFO DAGScheduler: ShuffleMapStage 6 (save at JdbcWriter.java:87) finished in 0.096 s
	24/05/07 08:42:16 INFO DAGScheduler: looking for newly runnable stages
	24/05/07 08:42:16 INFO DAGScheduler: running: Set()
	24/05/07 08:42:16 INFO DAGScheduler: waiting: Set()
	24/05/07 08:42:16 INFO DAGScheduler: failed: Set()
	24/05/07 08:42:16 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
	24/05/07 08:42:17 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
	24/05/07 08:42:17 INFO DAGScheduler: Registering RDD 16 (save at JdbcWriter.java:87) as input to shuffle 3
	24/05/07 08:42:17 INFO DAGScheduler: Got map stage job 4 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:42:17 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (save at JdbcWriter.java:87)
	24/05/07 08:42:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
	24/05/07 08:42:17 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:42:17 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[16] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:42:17 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 40.6 KiB, free 93.1 MiB)
	24/05/07 08:42:17 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 93.1 MiB)
	24/05/07 08:42:17 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 941f01f0fea3:41637 (size: 19.1 KiB, free: 93.2 MiB)
	24/05/07 08:42:17 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:42:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[16] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:42:17 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
	24/05/07 08:42:17 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 4) (941f01f0fea3, executor driver, partition 0, NODE_LOCAL, 8032 bytes) 
	24/05/07 08:42:17 INFO Executor: Running task 0.0 in stage 8.0 (TID 4)
	24/05/07 08:42:17 INFO ShuffleBlockFetcherIterator: Getting 1 (2.9 KiB) non-empty blocks including 1 (2.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
	24/05/07 08:42:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
	24/05/07 08:42:17 INFO Executor: Finished task 0.0 in stage 8.0 (TID 4). 5420 bytes result sent to driver
	24/05/07 08:42:17 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 4) in 28 ms on 941f01f0fea3 (executor driver) (1/1)
	24/05/07 08:42:17 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
	24/05/07 08:42:17 INFO DAGScheduler: ShuffleMapStage 8 (save at JdbcWriter.java:87) finished in 0.048 s
	24/05/07 08:42:17 INFO DAGScheduler: looking for newly runnable stages
	24/05/07 08:42:17 INFO DAGScheduler: running: Set()
	24/05/07 08:42:17 INFO DAGScheduler: waiting: Set()
	24/05/07 08:42:17 INFO DAGScheduler: failed: Set()
	24/05/07 08:42:17 INFO CodeGenerator: Code generated in 10.524849 ms
	24/05/07 08:42:17 INFO SparkContext: Starting job: save at JdbcWriter.java:87
	24/05/07 08:42:17 INFO DAGScheduler: Got job 5 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:42:17 INFO DAGScheduler: Final stage: ResultStage 11 (save at JdbcWriter.java:87)
	24/05/07 08:42:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
	24/05/07 08:42:17 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:42:17 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[21] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:42:17 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 49.2 KiB, free 93.1 MiB)
	24/05/07 08:42:17 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 22.7 KiB, free 93.0 MiB)
	24/05/07 08:42:17 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 941f01f0fea3:41637 (size: 22.7 KiB, free: 93.2 MiB)
	24/05/07 08:42:17 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:42:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[21] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:42:17 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
	24/05/07 08:42:17 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 5) (941f01f0fea3, executor driver, partition 0, NODE_LOCAL, 8043 bytes) 
	24/05/07 08:42:17 INFO Executor: Running task 0.0 in stage 11.0 (TID 5)
	24/05/07 08:42:17 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
	24/05/07 08:42:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
	24/05/07 08:42:17 INFO CodeGenerator: Code generated in 14.738551 ms
	24/05/07 08:42:17 INFO CodeGenerator: Code generated in 22.386966 ms
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:18.290 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:42:17 ERROR Executor: Exception in task 0.0 in stage 11.0 (TID 5)
	java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_task_statistics_value ("process_definition_id","task_instance_id","rule_id","unique_code","statistics_name","statistics_value","data_time","create_time","update_time") VALUES (('0'::int4),('3950'::int4),('6'::int4),('KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG='),('duplicate_count.duplicates'),('1'::int8),('2024-05-07 08:41:57'),('2024-05-07 08:41:57'),('2024-05-07 08:41:57')) was aborted: ERROR: column "data_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 232  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "data_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 232
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	24/05/07 08:42:17 WARN TaskSetManager: Lost task 0.0 in stage 11.0 (TID 5) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_task_statistics_value ("process_definition_id","task_instance_id","rule_id","unique_code","statistics_name","statistics_value","data_time","create_time","update_time") VALUES (('0'::int4),('3950'::int4),('6'::int4),('KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG='),('duplicate_count.duplicates'),('1'::int8),('2024-05-07 08:41:57'),('2024-05-07 08:41:57'),('2024-05-07 08:41:57')) was aborted: ERROR: column "data_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 232  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "data_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 232
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	24/05/07 08:42:17 ERROR TaskSetManager: Task 0 in stage 11.0 failed 1 times; aborting job
	24/05/07 08:42:17 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
	24/05/07 08:42:17 INFO TaskSchedulerImpl: Cancelling stage 11
	24/05/07 08:42:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 5) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_task_statistics_value ("process_definition_id","task_instance_id","rule_id","unique_code","statistics_name","statistics_value","data_time","create_time","update_time") VALUES (('0'::int4),('3950'::int4),('6'::int4),('KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG='),('duplicate_count.duplicates'),('1'::int8),('2024-05-07 08:41:57'),('2024-05-07 08:41:57'),('2024-05-07 08:41:57')) was aborted: ERROR: column "data_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 232  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "data_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 232
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	Driver stacktrace:
	24/05/07 08:42:17 INFO DAGScheduler: ResultStage 11 (save at JdbcWriter.java:87) failed in 0.280 s due to Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 5) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_task_statistics_value ("process_definition_id","task_instance_id","rule_id","unique_code","statistics_name","statistics_value","data_time","create_time","update_time") VALUES (('0'::int4),('3950'::int4),('6'::int4),('KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG='),('duplicate_count.duplicates'),('1'::int8),('2024-05-07 08:41:57'),('2024-05-07 08:41:57'),('2024-05-07 08:41:57')) was aborted: ERROR: column "data_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 232  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "data_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 232
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	Driver stacktrace:
	24/05/07 08:42:17 INFO DAGScheduler: Job 5 failed: save at JdbcWriter.java:87, took 0.302354 s
	Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 5) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_task_statistics_value ("process_definition_id","task_instance_id","rule_id","unique_code","statistics_name","statistics_value","data_time","create_time","update_time") VALUES (('0'::int4),('3950'::int4),('6'::int4),('KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG='),('duplicate_count.duplicates'),('1'::int8),('2024-05-07 08:41:57'),('2024-05-07 08:41:57'),('2024-05-07 08:41:57')) was aborted: ERROR: column "data_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 232  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "data_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 232
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	Driver stacktrace:
		at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
		at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
		at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
		at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
		at scala.Option.foreach(Option.scala:407)
		at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
		at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)
		at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
		at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
		at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
		at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)
		at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3514)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4309)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
		at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4307)
		at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3514)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
		at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
		at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
		at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
		at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
		at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
		at org.apache.dolphinscheduler.data.quality.flow.batch.writer.JdbcWriter.write(JdbcWriter.java:87)
		at org.apache.dolphinscheduler.data.quality.execution.SparkBatchExecution.executeWriter(SparkBatchExecution.java:132)
		at org.apache.dolphinscheduler.data.quality.execution.SparkBatchExecution.execute(SparkBatchExecution.java:58)
		at org.apache.dolphinscheduler.data.quality.context.DataQualityContext.execute(DataQualityContext.java:62)
		at org.apache.dolphinscheduler.data.quality.DataQualityApplication.main(DataQualityApplication.java:78)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
		at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)
		at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
		at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
		at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
		at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
		at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
		at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
	Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_task_statistics_value ("process_definition_id","task_instance_id","rule_id","unique_code","statistics_name","statistics_value","data_time","create_time","update_time") VALUES (('0'::int4),('3950'::int4),('6'::int4),('KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG='),('duplicate_count.duplicates'),('1'::int8),('2024-05-07 08:41:57'),('2024-05-07 08:41:57'),('2024-05-07 08:41:57')) was aborted: ERROR: column "data_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 232  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "data_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 232
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	24/05/07 08:42:17 INFO SparkContext: Invoking stop() from shutdown hook
	24/05/07 08:42:17 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/05/07 08:42:17 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 941f01f0fea3:41637 in memory (size: 16.5 KiB, free: 93.2 MiB)
	24/05/07 08:42:17 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 941f01f0fea3:41637 in memory (size: 22.7 KiB, free: 93.3 MiB)
	24/05/07 08:42:17 INFO SparkUI: Stopped Spark web UI at http://941f01f0fea3:4040
	24/05/07 08:42:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/05/07 08:42:17 INFO MemoryStore: MemoryStore cleared
	24/05/07 08:42:17 INFO BlockManager: BlockManager stopped
	24/05/07 08:42:17 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/05/07 08:42:17 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/05/07 08:42:17 INFO SparkContext: Successfully stopped SparkContext
	24/05/07 08:42:17 INFO ShutdownHookManager: Shutdown hook called
	24/05/07 08:42:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-7ee05913-317b-423d-87a5-6e57bf444b82
	24/05/07 08:42:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-df77a142-22fa-4ec4-a95f-544b0343074f
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:42:18.294 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1057/3950, processId:1323 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:42:18.294 +0800 o.a.d.p.t.a.u.LogUtils:[79] - Start finding appId in /opt/dolphinscheduler/logs/20240507/13505298546272/21/1057/3950.log, fetch way: log 
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:42:18.296 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:42:18.296 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:42:18.297 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:42:18.297 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:42:18.304 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:42:18.304 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:42:18.305 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1057/3950
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:42:18.305 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1057/3950
[WI-1057][TI-3950] - [INFO] 2024-05-07 08:42:18.305 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3950] - [INFO] 2024-05-07 08:42:18.727 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3950, processInstanceId=1057, status=6, startTime=1715042517875, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1057/3950.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1057/3950, endTime=1715042538297, processId=1323, appIds=, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3950] - [INFO] 2024-05-07 08:42:18.731 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3950, processInstanceId=1057, status=6, startTime=1715042517875, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1057/3950.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1057/3950, endTime=1715042538297, processId=1323, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042538727)
[WI-0][TI-0] - [INFO] 2024-05-07 08:42:23.388 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7784090909090909 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [WARN] 2024-05-07 08:42:25.617 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[147] - [id: 0x9b5d4ccc, L:/172.18.1.1:1234 - R:/172.18.0.13:48102] is not writable, over high water level : 65536
[WI-0][TI-0] - [WARN] 2024-05-07 08:42:25.618 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[154] - [id: 0x9b5d4ccc, L:/172.18.1.1:1234 - R:/172.18.0.13:48102] is writable, to low water : 32768
[WI-0][TI-3944] - [INFO] 2024-05-07 08:44:07.968 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3944, processInstanceId=1051, status=6, startTime=1715041077174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1051/3944.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1051/3944, endTime=1715041109126, processId=316, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042347948)
[WI-0][TI-3944] - [INFO] 2024-05-07 08:44:07.974 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3944, processInstanceId=1051, status=6, startTime=1715041077174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1051/3944.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1051/3944, endTime=1715041109126, processId=316, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042647968)
[WI-0][TI-3948] - [INFO] 2024-05-07 08:44:18.985 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3948, processInstanceId=1055, status=6, startTime=1715042336910, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1055/3948.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1055/3948, endTime=1715042357115, processId=1041, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042357990)
[WI-0][TI-3948] - [INFO] 2024-05-07 08:44:19.004 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3948, processInstanceId=1055, status=6, startTime=1715042336910, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1055/3948.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1055/3948, endTime=1715042357115, processId=1041, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042658985)
[WI-0][TI-3946] - [INFO] 2024-05-07 08:46:03.450 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3946, processInstanceId=1053, status=9, startTime=1715041547086, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1053/3946.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1053/3946, endTime=1715041560344, processId=628, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042463364)
[WI-0][TI-3946] - [INFO] 2024-05-07 08:46:03.456 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3946, processInstanceId=1053, status=9, startTime=1715041547086, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1053/3946.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1053/3946, endTime=1715041560344, processId=628, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042763450)
[WI-0][TI-3945] - [INFO] 2024-05-07 08:46:06.463 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3945, processInstanceId=1052, status=6, startTime=1715041538811, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1052/3945.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1052/3945, endTime=1715041563133, processId=534, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042466407)
[WI-0][TI-3945] - [INFO] 2024-05-07 08:46:06.466 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3945, processInstanceId=1052, status=6, startTime=1715041538811, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1052/3945.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1052/3945, endTime=1715041563133, processId=534, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042766463)
[WI-0][TI-3949] - [INFO] 2024-05-07 08:46:20.491 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3949, processInstanceId=1056, status=6, startTime=1715042461574, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1056/3949.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1056/3949, endTime=1715042480010, processId=1190, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042480463)
[WI-0][TI-3949] - [INFO] 2024-05-07 08:46:20.494 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3949, processInstanceId=1056, status=6, startTime=1715042461574, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1056/3949.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1056/3949, endTime=1715042480010, processId=1190, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042780491)
[WI-0][TI-3947] - [INFO] 2024-05-07 08:47:14.687 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3947, processInstanceId=1054, status=6, startTime=1715041914072, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1054/3947.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1054/3947, endTime=1715041931317, processId=835, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042533703)
[WI-0][TI-3947] - [INFO] 2024-05-07 08:47:14.695 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3947, processInstanceId=1054, status=6, startTime=1715041914072, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1054/3947.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1054/3947, endTime=1715041931317, processId=835, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042834687)
[WI-0][TI-3950] - [INFO] 2024-05-07 08:47:19.702 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3950, processInstanceId=1057, status=6, startTime=1715042517875, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1057/3950.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1057/3950, endTime=1715042538297, processId=1323, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042538727)
[WI-0][TI-3950] - [INFO] 2024-05-07 08:47:19.712 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3950, processInstanceId=1057, status=6, startTime=1715042517875, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1057/3950.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1057/3950, endTime=1715042538297, processId=1323, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042839702)
[WI-0][TI-3944] - [INFO] 2024-05-07 08:49:08.948 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3944, processInstanceId=1051, status=6, startTime=1715041077174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1051/3944.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1051/3944, endTime=1715041109126, processId=316, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042647968)
[WI-0][TI-3944] - [INFO] 2024-05-07 08:49:08.959 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3944, processInstanceId=1051, status=6, startTime=1715041077174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1051/3944.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1051/3944, endTime=1715041109126, processId=316, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042948948)
[WI-0][TI-3948] - [INFO] 2024-05-07 08:49:19.981 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3948, processInstanceId=1055, status=6, startTime=1715042336910, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1055/3948.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1055/3948, endTime=1715042357115, processId=1041, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042658985)
[WI-0][TI-3948] - [INFO] 2024-05-07 08:49:19.987 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3948, processInstanceId=1055, status=6, startTime=1715042336910, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1055/3948.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1055/3948, endTime=1715042357115, processId=1041, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042959981)
[WI-0][TI-0] - [INFO] 2024-05-07 08:50:46.340 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[55] - Receive TaskInstanceKillRequest: TaskInstanceKillRequest(taskInstanceId=3950)
[WI-0][TI-3950] - [ERROR] 2024-05-07 08:50:46.340 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[62] - Cannot find WorkerTaskExecutor for taskInstance: 3950
[WI-0][TI-0] - [INFO] 2024-05-07 08:50:48.009 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7585227272727273 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:50:50.050 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3951, taskName=[null check] filtered data persistence, firstSubmitTime=1715043050029, startTime=0, taskType=DATA_QUALITY, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13505298546272, processDefineVersion=21, appIds=null, processInstanceId=1058, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13001194483488, taskParams={"localParams":[],"resourceList":[],"ruleId":6,"ruleInputParameter":{"check_type":"0","comparison_type":1,"comparison_name":"0","failure_strategy":"0","operator":"0","src_connector_type":1,"src_datasource_id":6,"src_database":"persistence","src_field":"content","src_table":"filtered_data_persistence","threshold":"0"},"sparkParameters":{"deployMode":"local","driverCores":1,"driverMemory":"512M","executorCores":1,"executorMemory":"1G","numExecutors":1,"others":"--conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n--packages org.postgresql:postgresql:42.7.3","yarnQueue":""}}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='[null check] filtered data persistence'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13001194483488'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='1058'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240507'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240506'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3951'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='qualti_test'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13505264408800'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13505298546272'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240507085050'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=org.apache.dolphinscheduler.plugin.task.api.DataQualityTaskExecutionContext@57e0464d, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.051 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: [null check] filtered data persistence to wait queue success
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.051 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.065 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.066 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.066 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.066 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1715043050066
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.066 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 1058_3951
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.066 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3951,
  "taskName" : "[null check] filtered data persistence",
  "firstSubmitTime" : 1715043050029,
  "startTime" : 1715043050066,
  "taskType" : "DATA_QUALITY",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240507/13505298546272/21/1058/3951.log",
  "processId" : 0,
  "processDefineCode" : 13505298546272,
  "processDefineVersion" : 21,
  "processInstanceId" : 1058,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13001194483488,
  "taskParams" : "{\"localParams\":[],\"resourceList\":[],\"ruleId\":6,\"ruleInputParameter\":{\"check_type\":\"0\",\"comparison_type\":1,\"comparison_name\":\"0\",\"failure_strategy\":\"0\",\"operator\":\"0\",\"src_connector_type\":1,\"src_datasource_id\":6,\"src_database\":\"persistence\",\"src_field\":\"content\",\"src_table\":\"filtered_data_persistence\",\"threshold\":\"0\"},\"sparkParameters\":{\"deployMode\":\"local\",\"driverCores\":1,\"driverMemory\":\"512M\",\"executorCores\":1,\"executorMemory\":\"1G\",\"numExecutors\":1,\"others\":\"--conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n--packages org.postgresql:postgresql:42.7.3\",\"yarnQueue\":\"\"}}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "[null check] filtered data persistence"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13001194483488"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1058"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240507"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240506"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3951"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "qualti_test"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13505264408800"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13505298546272"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240507085050"
    }
  },
  "taskAppId" : "1058_3951",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "dataQualityTaskExecutionContext" : {
    "ruleId" : 6,
    "ruleName" : "$t(uniqueness_check)",
    "ruleType" : 0,
    "ruleInputEntryList" : "[{\"id\":1,\"field\":\"src_connector_type\",\"type\":\"select\",\"title\":\"$t(src_connector_type)\",\"data\":\"\",\"options\":\"[{\\\"label\\\":\\\"HIVE\\\",\\\"value\\\":\\\"HIVE\\\"},{\\\"label\\\":\\\"JDBC\\\",\\\"value\\\":\\\"JDBC\\\"}]\",\"placeholder\":\"please select source connector type\",\"optionSourceType\":2,\"dataType\":2,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":1,\"createTime\":null,\"updateTime\":null},{\"id\":2,\"field\":\"src_datasource_id\",\"type\":\"select\",\"title\":\"$t(src_datasource_id)\",\"data\":\"\",\"options\":null,\"placeholder\":\"please select source datasource id\",\"optionSourceType\":1,\"dataType\":2,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":2,\"createTime\":null,\"updateTime\":null},{\"id\":30,\"field\":\"src_database\",\"type\":\"select\",\"title\":\"$t(src_database)\",\"data\":null,\"options\":null,\"placeholder\":\"Please select source database\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":true,\"valuesMap\":null,\"index\":2,\"createTime\":null,\"updateTime\":null},{\"id\":3,\"field\":\"src_table\",\"type\":\"select\",\"title\":\"$t(src_table)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter source table name\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":true,\"valuesMap\":null,\"index\":3,\"createTime\":null,\"updateTime\":null},{\"id\":4,\"field\":\"src_filter\",\"type\":\"input\",\"title\":\"$t(src_filter)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter filter expression\",\"optionSourceType\":0,\"dataType\":3,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":4,\"createTime\":null,\"updateTime\":null},{\"id\":5,\"field\":\"src_field\",\"type\":\"select\",\"title\":\"$t(src_field)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter column, only single column is supported\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":null,\"index\":5,\"createTime\":null,\"updateTime\":null},{\"id\":6,\"field\":\"statistics_name\",\"type\":\"input\",\"title\":\"$t(statistics_name)\",\"data\":\"duplicate_count.duplicates\",\"options\":null,\"placeholder\":\"Please enter statistics name, the alias in statistics execute sql\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":1,\"isShow\":false,\"canEdit\":false,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":\"{\\\"statistics_name\\\":\\\"duplicate_count.duplicates\\\"}\",\"index\":6,\"createTime\":null,\"updateTime\":null},{\"id\":7,\"field\":\"check_type\",\"type\":\"select\",\"title\":\"$t(check_type)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"Expected - Actual\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"Actual - Expected\\\",\\\"value\\\":\\\"1\\\"},{\\\"label\\\":\\\"Actual / Expected\\\",\\\"value\\\":\\\"2\\\"},{\\\"label\\\":\\\"(Expected - Actual) / Expected\\\",\\\"value\\\":\\\"3\\\"}]\",\"placeholder\":\"please select check type\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":7,\"createTime\":null,\"updateTime\":null},{\"id\":8,\"field\":\"operator\",\"type\":\"select\",\"title\":\"$t(operator)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"=\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"<\\\",\\\"value\\\":\\\"1\\\"},{\\\"label\\\":\\\"<=\\\",\\\"value\\\":\\\"2\\\"},{\\\"label\\\":\\\">\\\",\\\"value\\\":\\\"3\\\"},{\\\"label\\\":\\\">=\\\",\\\"value\\\":\\\"4\\\"},{\\\"label\\\":\\\"!=\\\",\\\"value\\\":\\\"5\\\"}]\",\"placeholder\":\"please select operator\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":8,\"createTime\":null,\"updateTime\":null},{\"id\":9,\"field\":\"threshold\",\"type\":\"input\",\"title\":\"$t(threshold)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter threshold, number is needed\",\"optionSourceType\":0,\"dataType\":2,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":null,\"index\":9,\"createTime\":null,\"updateTime\":null},{\"id\":10,\"field\":\"failure_strategy\",\"type\":\"select\",\"title\":\"$t(failure_strategy)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"Alert\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"Block\\\",\\\"value\\\":\\\"1\\\"}]\",\"placeholder\":\"please select failure strategy\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":10,\"createTime\":null,\"updateTime\":null},{\"id\":17,\"field\":\"comparison_name\",\"type\":\"input\",\"title\":\"$t(comparison_name)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter comparison name, the alias in comparison execute sql\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":false,\"canEdit\":false,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":\"\",\"index\":11,\"createTime\":null,\"updateTime\":null},{\"id\":19,\"field\":\"comparison_type\",\"type\":\"select\",\"title\":\"$t(comparison_type)\",\"data\":\"\",\"options\":null,\"placeholder\":\"Please enter comparison title\",\"optionSourceType\":3,\"dataType\":0,\"inputType\":2,\"isShow\":true,\"canEdit\":false,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":12,\"createTime\":null,\"updateTime\":null}]",
    "executeSqlList" : "[{\"id\":6,\"index\":1,\"sql\":\"SELECT ${src_field} FROM ${src_table} group by ${src_field} having count(*) > 1\",\"tableAlias\":\"duplicate_items\",\"type\":0,\"createTime\":\"2021-03-03 11:31:24\",\"updateTime\":\"2021-03-03 11:31:24\",\"errorOutputSql\":true},{\"id\":7,\"index\":1,\"sql\":\"SELECT COUNT(*) AS duplicates FROM duplicate_items\",\"tableAlias\":\"duplicate_count\",\"type\":1,\"createTime\":\"2021-03-03 11:31:24\",\"updateTime\":\"2021-03-03 11:31:24\",\"errorOutputSql\":false}]",
    "comparisonNeedStatisticsValueTable" : false,
    "compareWithFixedValue" : true,
    "hdfsPath" : "hdfs://mycluster:8020/user/default/data_quality_error_data",
    "sourceConnectorType" : "JDBC",
    "sourceType" : 1,
    "sourceConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"persistence\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence\",\"driverClassName\":\"org.postgresql.Driver\",\"validationQuery\":\"select version()\",\"other\":{\"password\":\"root\"}}",
    "targetConnectorType" : null,
    "targetType" : 0,
    "targetConnectionParams" : null,
    "writerConnectorType" : "JDBC",
    "writerType" : 1,
    "writerTable" : "t_ds_dq_execute_result",
    "writerConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"dolphinscheduler\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\"}",
    "statisticsValueConnectorType" : "JDBC",
    "statisticsValueType" : 1,
    "statisticsValueTable" : "t_ds_dq_task_statistics_value",
    "statisticsValueWriterConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"dolphinscheduler\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\"}"
  },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.068 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.068 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.068 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.087 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.088 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.089 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1058/3951 check successfully
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.089 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.dq.DataQualityTaskChannel successfully
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.090 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.090 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.090 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: DATA_QUALITY create successfully
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.094 +0800 o.a.d.p.t.d.DataQualityTask:[89] - Initialize data quality task params {
  "localParams" : [ ],
  "varPool" : null,
  "ruleId" : 6,
  "ruleInputParameter" : {
    "check_type" : "0",
    "comparison_type" : "1",
    "comparison_name" : "0",
    "failure_strategy" : "0",
    "operator" : "0",
    "src_connector_type" : "1",
    "src_datasource_id" : "6",
    "src_database" : "persistence",
    "src_field" : "content",
    "src_table" : "filtered_data_persistence",
    "threshold" : "0"
  },
  "sparkParameters" : {
    "localParams" : null,
    "varPool" : null,
    "mainJar" : null,
    "mainClass" : null,
    "deployMode" : "local",
    "mainArgs" : null,
    "driverCores" : 1,
    "driverMemory" : "512M",
    "numExecutors" : 1,
    "executorCores" : 1,
    "executorMemory" : "1G",
    "appName" : null,
    "yarnQueue" : "",
    "others" : "--conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n--packages org.postgresql:postgresql:42.7.3",
    "programType" : null,
    "resourceList" : [ ]
  }
}
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.097 +0800 o.a.d.p.t.d.DataQualityTask:[119] - data quality configuration: {
  "name" : "$t(uniqueness_check)",
  "env" : {
    "type" : "batch",
    "config" : null
  },
  "readers" : [ {
    "type" : "JDBC",
    "config" : {
      "database" : "persistence",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "output_table" : "persistence_filtered_data_persistence",
      "table" : "filtered_data_persistence",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence?password=root"
    }
  } ],
  "transformers" : [ {
    "type" : "sql",
    "config" : {
      "index" : 1,
      "output_table" : "duplicate_items",
      "sql" : "SELECT content FROM persistence_filtered_data_persistence group by content having count(*) > 1"
    }
  }, {
    "type" : "sql",
    "config" : {
      "index" : 2,
      "output_table" : "duplicate_count",
      "sql" : "SELECT COUNT(*) AS duplicates FROM duplicate_items"
    }
  } ],
  "writers" : [ {
    "type" : "JDBC",
    "config" : {
      "database" : "dolphinscheduler",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "table" : "t_ds_dq_execute_result",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler",
      "sql" : "select 0 as rule_type,'$t(uniqueness_check)' as rule_name,0 as process_definition_id,1058 as process_instance_id,3951 as task_instance_id,duplicate_count.duplicates AS statistics_value,0 AS comparison_value,1 AS comparison_type,0 as check_type,0 as threshold,0 as operator,0 as failure_strategy,'hdfs://mycluster:8020/user/default/data_quality_error_data/0_1058_[null check] filtered data persistence' as error_output_path,'2024-05-07 08:50:50' as create_time,'2024-05-07 08:50:50' as update_time from duplicate_count "
    }
  }, {
    "type" : "JDBC",
    "config" : {
      "database" : "dolphinscheduler",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "table" : "t_ds_dq_task_statistics_value",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler",
      "sql" : "select 0 as process_definition_id,3951 as task_instance_id,6 as rule_id,'KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG=' as unique_code,'duplicate_count.duplicates'AS statistics_name,duplicate_count.duplicates AS statistics_value,'2024-05-07 08:50:50' as data_time,'2024-05-07 08:50:50' as create_time,'2024-05-07 08:50:50' as update_time from duplicate_count"
    }
  }, {
    "type" : "hdfs_file",
    "config" : {
      "path" : "hdfs://mycluster:8020/user/default/data_quality_error_data/0_1058_[null check] filtered data persistence",
      "input_table" : "duplicate_items"
    }
  } ]
}
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.103 +0800 o.a.d.p.d.a.u.CommonUtils:[136] - Trying to get data quality jar in path
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.104 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.104 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.104 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.104 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.104 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.105 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.105 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.106 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${SPARK_HOME}/bin/spark-submit --master local --driver-cores 1 --driver-memory 512M --num-executors 1 --executor-cores 1 --executor-memory 1G --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
--packages org.postgresql:postgresql:42.7.3 /opt/dolphinscheduler/conf/../libs/dolphinscheduler-data-quality-3.2.1.jar "{\"name\":\"$t(uniqueness_check)\",\"env\":{\"type\":\"batch\",\"config\":null},\"readers\":[{\"type\":\"JDBC\",\"config\":{\"database\":\"persistence\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"output_table\":\"persistence_filtered_data_persistence\",\"table\":\"filtered_data_persistence\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence?password=root\"} }],\"transformers\":[{\"type\":\"sql\",\"config\":{\"index\":1,\"output_table\":\"duplicate_items\",\"sql\":\"SELECT content FROM persistence_filtered_data_persistence group by content having count(*) > 1\"} },{\"type\":\"sql\",\"config\":{\"index\":2,\"output_table\":\"duplicate_count\",\"sql\":\"SELECT COUNT(*) AS duplicates FROM duplicate_items\"} }],\"writers\":[{\"type\":\"JDBC\",\"config\":{\"database\":\"dolphinscheduler\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"table\":\"t_ds_dq_execute_result\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\",\"sql\":\"select 0 as rule_type,'$t(uniqueness_check)' as rule_name,0 as process_definition_id,1058 as process_instance_id,3951 as task_instance_id,duplicate_count.duplicates AS statistics_value,0 AS comparison_value,1 AS comparison_type,0 as check_type,0 as threshold,0 as operator,0 as failure_strategy,'hdfs://mycluster:8020/user/default/data_quality_error_data/0_1058_[null check] filtered data persistence' as error_output_path,'2024-05-07 08:50:50' as create_time,'2024-05-07 08:50:50' as update_time from duplicate_count \"} },{\"type\":\"JDBC\",\"config\":{\"database\":\"dolphinscheduler\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"table\":\"t_ds_dq_task_statistics_value\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\",\"sql\":\"select 0 as process_definition_id,3951 as task_instance_id,6 as rule_id,'KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG=' as unique_code,'duplicate_count.duplicates'AS statistics_name,duplicate_count.duplicates AS statistics_value,'2024-05-07 08:50:50' as data_time,'2024-05-07 08:50:50' as create_time,'2024-05-07 08:50:50' as update_time from duplicate_count\"} },{\"type\":\"hdfs_file\",\"config\":{\"path\":\"hdfs://mycluster:8020/user/default/data_quality_error_data/0_1058_[null check] filtered data persistence\",\"input_table\":\"duplicate_items\"} }]}"
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.107 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.107 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1058/3951/1058_3951.sh
[WI-0][TI-0] - [INFO] 2024-05-07 08:50:50.127 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:50:50.127 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 1558
[WI-0][TI-3951] - [INFO] 2024-05-07 08:50:50.900 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3951, processInstanceId=1058, startTime=1715043050066, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1058/3951.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3951] - [INFO] 2024-05-07 08:50:50.911 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3951, processInstanceId=1058, startTime=1715043050066, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1058/3951.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1715043050896)
[WI-0][TI-3951] - [INFO] 2024-05-07 08:50:50.911 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3951, processInstanceId=1058, startTime=1715043050066, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3951] - [INFO] 2024-05-07 08:50:50.937 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3951, success=true)
[WI-0][TI-3951] - [INFO] 2024-05-07 08:50:50.939 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3951, processInstanceId=1058, startTime=1715043050066, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1715043050896)
[WI-0][TI-3951] - [WARN] 2024-05-07 08:50:50.941 +0800 o.a.d.s.w.m.MessageRetryRunner:[139] - Retry send message to master error
java.util.ConcurrentModificationException: null
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:911)
	at java.util.ArrayList$Itr.next(ArrayList.java:861)
	at org.apache.dolphinscheduler.server.worker.message.MessageRetryRunner.run(MessageRetryRunner.java:127)
[WI-0][TI-3951] - [INFO] 2024-05-07 08:50:50.957 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3951)
[WI-0][TI-3951] - [INFO] 2024-05-07 08:50:50.966 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3951, success=true)
[WI-0][TI-3951] - [INFO] 2024-05-07 08:50:50.978 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3951)
[WI-0][TI-0] - [INFO] 2024-05-07 08:50:51.042 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8100890207715133 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:50:52.052 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7058823529411764 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:50:53.056 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.856338028169014 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:50:53.134 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.postgresql#postgresql added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-22144f81-5b87-45e0-937e-82530205a855;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-07 08:50:54.066 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7103064066852367 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:50:54.140 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.postgresql#postgresql;42.7.3 in central
		found org.checkerframework#checker-qual;3.42.0 in central
	:: resolution report :: resolve 292ms :: artifacts dl 7ms
		:: modules in use:
		org.checkerframework#checker-qual;3.42.0 from central in [default]
		org.postgresql#postgresql;42.7.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-22144f81-5b87-45e0-937e-82530205a855
		confs: [default]
		0 artifacts copied, 2 already retrieved (0kB/14ms)
	24/05/07 08:50:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-07 08:50:55.069 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9129213483146067 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:50:55.141 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:50:54 INFO SparkContext: Running Spark version 3.5.1
	24/05/07 08:50:54 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/05/07 08:50:54 INFO SparkContext: Java version 1.8.0_402
	24/05/07 08:50:54 INFO ResourceUtils: ==============================================================
	24/05/07 08:50:54 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/05/07 08:50:54 INFO ResourceUtils: ==============================================================
	24/05/07 08:50:54 INFO SparkContext: Submitted application: (uniqueness_check)
	24/05/07 08:50:54 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/05/07 08:50:54 INFO ResourceProfile: Limiting resource is cpu
	24/05/07 08:50:54 INFO ResourceProfileManager: Added ResourceProfile id: 0
	24/05/07 08:50:54 INFO SecurityManager: Changing view acls to: default
	24/05/07 08:50:54 INFO SecurityManager: Changing modify acls to: default
	24/05/07 08:50:54 INFO SecurityManager: Changing view acls groups to: 
	24/05/07 08:50:54 INFO SecurityManager: Changing modify acls groups to: 
	24/05/07 08:50:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
[WI-0][TI-0] - [INFO] 2024-05-07 08:50:56.071 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8743169398907104 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:50:56.149 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:50:55 INFO Utils: Successfully started service 'sparkDriver' on port 39907.
	24/05/07 08:50:55 INFO SparkEnv: Registering MapOutputTracker
	24/05/07 08:50:55 INFO SparkEnv: Registering BlockManagerMaster
	24/05/07 08:50:55 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/05/07 08:50:55 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/05/07 08:50:55 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/05/07 08:50:55 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-37348a0f-1515-4f8a-b634-9be029c5a77c
	24/05/07 08:50:55 INFO MemoryStore: MemoryStore started with capacity 93.3 MiB
	24/05/07 08:50:55 INFO SparkEnv: Registering OutputCommitCoordinator
	24/05/07 08:50:55 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/05/07 08:50:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/05/07 08:50:55 INFO SparkContext: Added JAR file:///tmp/jars/org.postgresql_postgresql-42.7.3.jar at spark://941f01f0fea3:39907/jars/org.postgresql_postgresql-42.7.3.jar with timestamp 1715043054394
	24/05/07 08:50:55 INFO SparkContext: Added JAR file:///tmp/jars/org.checkerframework_checker-qual-3.42.0.jar at spark://941f01f0fea3:39907/jars/org.checkerframework_checker-qual-3.42.0.jar with timestamp 1715043054394
	24/05/07 08:50:55 INFO SparkContext: Added JAR file:/opt/dolphinscheduler/libs/dolphinscheduler-data-quality-3.2.1.jar at spark://941f01f0fea3:39907/jars/dolphinscheduler-data-quality-3.2.1.jar with timestamp 1715043054394
	24/05/07 08:50:56 INFO Executor: Starting executor ID driver on host 941f01f0fea3
	24/05/07 08:50:56 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/05/07 08:50:56 INFO Executor: Java version 1.8.0_402
	24/05/07 08:50:56 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/05/07 08:50:56 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@698fee9a for default.
	24/05/07 08:50:56 INFO Executor: Fetching spark://941f01f0fea3:39907/jars/org.checkerframework_checker-qual-3.42.0.jar with timestamp 1715043054394
[WI-0][TI-0] - [INFO] 2024-05-07 08:50:57.079 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7824675324675325 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:50:57.152 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:50:56 INFO TransportClientFactory: Successfully created connection to 941f01f0fea3/172.18.1.1:39907 after 50 ms (0 ms spent in bootstraps)
	24/05/07 08:50:56 INFO Utils: Fetching spark://941f01f0fea3:39907/jars/org.checkerframework_checker-qual-3.42.0.jar to /tmp/spark-1e10e5e9-9719-4538-ad1c-3b2fb4861e6b/userFiles-08098c52-307a-4685-89a4-aa078ad9ef34/fetchFileTemp8290794802176892232.tmp
	24/05/07 08:50:56 INFO Executor: Adding file:/tmp/spark-1e10e5e9-9719-4538-ad1c-3b2fb4861e6b/userFiles-08098c52-307a-4685-89a4-aa078ad9ef34/org.checkerframework_checker-qual-3.42.0.jar to class loader default
	24/05/07 08:50:56 INFO Executor: Fetching spark://941f01f0fea3:39907/jars/org.postgresql_postgresql-42.7.3.jar with timestamp 1715043054394
	24/05/07 08:50:56 INFO Utils: Fetching spark://941f01f0fea3:39907/jars/org.postgresql_postgresql-42.7.3.jar to /tmp/spark-1e10e5e9-9719-4538-ad1c-3b2fb4861e6b/userFiles-08098c52-307a-4685-89a4-aa078ad9ef34/fetchFileTemp4629013584302807264.tmp
	24/05/07 08:50:56 INFO Executor: Adding file:/tmp/spark-1e10e5e9-9719-4538-ad1c-3b2fb4861e6b/userFiles-08098c52-307a-4685-89a4-aa078ad9ef34/org.postgresql_postgresql-42.7.3.jar to class loader default
	24/05/07 08:50:56 INFO Executor: Fetching spark://941f01f0fea3:39907/jars/dolphinscheduler-data-quality-3.2.1.jar with timestamp 1715043054394
	24/05/07 08:50:56 INFO Utils: Fetching spark://941f01f0fea3:39907/jars/dolphinscheduler-data-quality-3.2.1.jar to /tmp/spark-1e10e5e9-9719-4538-ad1c-3b2fb4861e6b/userFiles-08098c52-307a-4685-89a4-aa078ad9ef34/fetchFileTemp2133556996078880584.tmp
	24/05/07 08:50:56 INFO Executor: Adding file:/tmp/spark-1e10e5e9-9719-4538-ad1c-3b2fb4861e6b/userFiles-08098c52-307a-4685-89a4-aa078ad9ef34/dolphinscheduler-data-quality-3.2.1.jar to class loader default
	24/05/07 08:50:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37725.
	24/05/07 08:50:56 INFO NettyBlockTransferService: Server created on 941f01f0fea3:37725
	24/05/07 08:50:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/05/07 08:50:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 941f01f0fea3, 37725, None)
	24/05/07 08:50:56 INFO BlockManagerMasterEndpoint: Registering block manager 941f01f0fea3:37725 with 93.3 MiB RAM, BlockManagerId(driver, 941f01f0fea3, 37725, None)
	24/05/07 08:50:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 941f01f0fea3, 37725, None)
	24/05/07 08:50:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 941f01f0fea3, 37725, None)
	24/05/07 08:50:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/05/07 08:50:56 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1058/3951/spark-warehouse'.
[WI-0][TI-0] - [INFO] 2024-05-07 08:51:02.198 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:51:01 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[WI-0][TI-0] - [INFO] 2024-05-07 08:51:03.110 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7678018575851393 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:51:03.225 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:51:02 INFO CodeGenerator: Code generated in 400.459004 ms
	24/05/07 08:51:02 INFO DAGScheduler: Registering RDD 2 (save at JdbcWriter.java:87) as input to shuffle 0
	24/05/07 08:51:02 INFO DAGScheduler: Got map stage job 0 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:51:02 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (save at JdbcWriter.java:87)
	24/05/07 08:51:02 INFO DAGScheduler: Parents of final stage: List()
	24/05/07 08:51:02 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:51:02 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:51:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 34.8 KiB, free 93.3 MiB)
	24/05/07 08:51:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 93.3 MiB)
	24/05/07 08:51:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 941f01f0fea3:37725 (size: 16.4 KiB, free: 93.3 MiB)
	24/05/07 08:51:02 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:51:02 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:51:02 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
	24/05/07 08:51:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (941f01f0fea3, executor driver, partition 0, PROCESS_LOCAL, 7878 bytes) 
	24/05/07 08:51:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[WI-0][TI-3946] - [INFO] 2024-05-07 08:51:04.015 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3946, processInstanceId=1053, status=9, startTime=1715041547086, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1053/3946.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1053/3946, endTime=1715041560344, processId=628, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042763450)
[WI-0][TI-3946] - [INFO] 2024-05-07 08:51:04.019 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3946, processInstanceId=1053, status=9, startTime=1715041547086, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1053/3946.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1053/3946, endTime=1715041560344, processId=628, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043064015)
[WI-0][TI-0] - [INFO] 2024-05-07 08:51:04.226 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:51:03 INFO CodeGenerator: Code generated in 52.380073 ms
	24/05/07 08:51:03 INFO CodeGenerator: Code generated in 18.804653 ms
	24/05/07 08:51:03 INFO CodeGenerator: Code generated in 17.774067 ms
	24/05/07 08:51:03 INFO CodeGenerator: Code generated in 6.760978 ms
	24/05/07 08:51:03 INFO CodeGenerator: Code generated in 30.317995 ms
	24/05/07 08:51:03 INFO JDBCRDD: closed connection
	24/05/07 08:51:03 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2534 bytes result sent to driver
	24/05/07 08:51:03 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 872 ms on 941f01f0fea3 (executor driver) (1/1)
	24/05/07 08:51:03 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
	24/05/07 08:51:03 INFO DAGScheduler: ShuffleMapStage 0 (save at JdbcWriter.java:87) finished in 1.103 s
	24/05/07 08:51:03 INFO DAGScheduler: looking for newly runnable stages
	24/05/07 08:51:03 INFO DAGScheduler: running: Set()
	24/05/07 08:51:03 INFO DAGScheduler: waiting: Set()
	24/05/07 08:51:03 INFO DAGScheduler: failed: Set()
	24/05/07 08:51:04 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
	24/05/07 08:51:04 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
	24/05/07 08:51:04 INFO CodeGenerator: Code generated in 22.654753 ms
	24/05/07 08:51:04 INFO DAGScheduler: Registering RDD 5 (save at JdbcWriter.java:87) as input to shuffle 1
	24/05/07 08:51:04 INFO DAGScheduler: Got map stage job 1 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:51:04 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (save at JdbcWriter.java:87)
	24/05/07 08:51:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
	24/05/07 08:51:04 INFO DAGScheduler: Missing parents: List()
[WI-0][TI-0] - [INFO] 2024-05-07 08:51:05.237 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:51:04 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[5] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:51:04 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 40.5 KiB, free 93.2 MiB)
	24/05/07 08:51:04 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 93.2 MiB)
	24/05/07 08:51:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 941f01f0fea3:37725 (size: 19.1 KiB, free: 93.3 MiB)
	24/05/07 08:51:04 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:51:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[5] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:51:04 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
	24/05/07 08:51:04 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (941f01f0fea3, executor driver, partition 0, NODE_LOCAL, 8032 bytes) 
	24/05/07 08:51:04 INFO Executor: Running task 0.0 in stage 2.0 (TID 1)
	24/05/07 08:51:04 INFO ShuffleBlockFetcherIterator: Getting 1 (2.9 KiB) non-empty blocks including 1 (2.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
	24/05/07 08:51:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 22 ms
	24/05/07 08:51:04 INFO CodeGenerator: Code generated in 21.929179 ms
	24/05/07 08:51:04 INFO Executor: Finished task 0.0 in stage 2.0 (TID 1). 5420 bytes result sent to driver
	24/05/07 08:51:04 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 136 ms on 941f01f0fea3 (executor driver) (1/1)
	24/05/07 08:51:04 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
	24/05/07 08:51:04 INFO DAGScheduler: ShuffleMapStage 2 (save at JdbcWriter.java:87) finished in 0.162 s
	24/05/07 08:51:04 INFO DAGScheduler: looking for newly runnable stages
	24/05/07 08:51:04 INFO DAGScheduler: running: Set()
	24/05/07 08:51:04 INFO DAGScheduler: waiting: Set()
	24/05/07 08:51:04 INFO DAGScheduler: failed: Set()
	24/05/07 08:51:04 INFO CodeGenerator: Code generated in 47.97067 ms
	24/05/07 08:51:04 INFO SparkContext: Starting job: save at JdbcWriter.java:87
	24/05/07 08:51:04 INFO DAGScheduler: Got job 2 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:51:04 INFO DAGScheduler: Final stage: ResultStage 5 (save at JdbcWriter.java:87)
	24/05/07 08:51:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
	24/05/07 08:51:04 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:51:04 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[10] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:51:04 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 51.4 KiB, free 93.1 MiB)
	24/05/07 08:51:04 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.0 KiB, free 93.1 MiB)
	24/05/07 08:51:04 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 941f01f0fea3:37725 (size: 23.0 KiB, free: 93.2 MiB)
	24/05/07 08:51:04 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:51:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[10] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:51:04 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
	24/05/07 08:51:04 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 2) (941f01f0fea3, executor driver, partition 0, NODE_LOCAL, 8043 bytes) 
	24/05/07 08:51:04 INFO Executor: Running task 0.0 in stage 5.0 (TID 2)
	24/05/07 08:51:04 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 941f01f0fea3:37725 in memory (size: 16.4 KiB, free: 93.3 MiB)
	24/05/07 08:51:04 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 941f01f0fea3:37725 in memory (size: 19.1 KiB, free: 93.3 MiB)
	24/05/07 08:51:05 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
	24/05/07 08:51:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
	24/05/07 08:51:05 INFO CodeGenerator: Code generated in 11.074192 ms
[WI-0][TI-3945] - [INFO] 2024-05-07 08:51:07.022 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3945, processInstanceId=1052, status=6, startTime=1715041538811, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1052/3945.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1052/3945, endTime=1715041563133, processId=534, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042766463)
[WI-0][TI-3945] - [INFO] 2024-05-07 08:51:07.025 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3945, processInstanceId=1052, status=6, startTime=1715041538811, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1052/3945.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1052/3945, endTime=1715041563133, processId=534, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043067022)
[WI-0][TI-0] - [INFO] 2024-05-07 08:51:07.239 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:51:06 INFO CodeGenerator: Code generated in 15.318769 ms
	24/05/07 08:51:06 INFO Executor: Finished task 0.0 in stage 5.0 (TID 2). 6711 bytes result sent to driver
	24/05/07 08:51:06 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 2) in 1610 ms on 941f01f0fea3 (executor driver) (1/1)
	24/05/07 08:51:06 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
	24/05/07 08:51:06 INFO DAGScheduler: ResultStage 5 (save at JdbcWriter.java:87) finished in 1.773 s
	24/05/07 08:51:06 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
	24/05/07 08:51:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
	24/05/07 08:51:06 INFO DAGScheduler: Job 2 finished: save at JdbcWriter.java:87, took 1.814253 s
	24/05/07 08:51:06 INFO DAGScheduler: Registering RDD 13 (save at JdbcWriter.java:87) as input to shuffle 2
	24/05/07 08:51:06 INFO DAGScheduler: Got map stage job 3 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:51:06 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (save at JdbcWriter.java:87)
	24/05/07 08:51:06 INFO DAGScheduler: Parents of final stage: List()
	24/05/07 08:51:06 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:51:06 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[13] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:51:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 34.8 KiB, free 93.2 MiB)
	24/05/07 08:51:06 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 16.5 KiB, free 93.2 MiB)
	24/05/07 08:51:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 941f01f0fea3:37725 (size: 16.5 KiB, free: 93.3 MiB)
	24/05/07 08:51:06 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:51:06 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[13] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:51:06 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
	24/05/07 08:51:06 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 3) (941f01f0fea3, executor driver, partition 0, PROCESS_LOCAL, 7878 bytes) 
	24/05/07 08:51:06 INFO Executor: Running task 0.0 in stage 6.0 (TID 3)
	24/05/07 08:51:06 INFO JDBCRDD: closed connection
	24/05/07 08:51:06 INFO Executor: Finished task 0.0 in stage 6.0 (TID 3). 2448 bytes result sent to driver
	24/05/07 08:51:06 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 3) in 54 ms on 941f01f0fea3 (executor driver) (1/1)
	24/05/07 08:51:06 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
	24/05/07 08:51:06 INFO DAGScheduler: ShuffleMapStage 6 (save at JdbcWriter.java:87) finished in 0.064 s
	24/05/07 08:51:06 INFO DAGScheduler: looking for newly runnable stages
	24/05/07 08:51:06 INFO DAGScheduler: running: Set()
	24/05/07 08:51:06 INFO DAGScheduler: waiting: Set()
	24/05/07 08:51:06 INFO DAGScheduler: failed: Set()
	24/05/07 08:51:06 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
	24/05/07 08:51:06 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
	24/05/07 08:51:06 INFO DAGScheduler: Registering RDD 16 (save at JdbcWriter.java:87) as input to shuffle 3
	24/05/07 08:51:06 INFO DAGScheduler: Got map stage job 4 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:51:06 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (save at JdbcWriter.java:87)
	24/05/07 08:51:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
	24/05/07 08:51:07 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:51:07 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[16] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:51:07 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 40.6 KiB, free 93.1 MiB)
	24/05/07 08:51:07 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 93.1 MiB)
	24/05/07 08:51:07 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 941f01f0fea3:37725 (size: 19.1 KiB, free: 93.2 MiB)
	24/05/07 08:51:07 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:51:07 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[16] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:51:07 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
	24/05/07 08:51:07 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 4) (941f01f0fea3, executor driver, partition 0, NODE_LOCAL, 8032 bytes) 
	24/05/07 08:51:07 INFO Executor: Running task 0.0 in stage 8.0 (TID 4)
	24/05/07 08:51:07 INFO ShuffleBlockFetcherIterator: Getting 1 (2.9 KiB) non-empty blocks including 1 (2.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
	24/05/07 08:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
	24/05/07 08:51:07 INFO Executor: Finished task 0.0 in stage 8.0 (TID 4). 5420 bytes result sent to driver
	24/05/07 08:51:07 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 4) in 28 ms on 941f01f0fea3 (executor driver) (1/1)
	24/05/07 08:51:07 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
	24/05/07 08:51:07 INFO DAGScheduler: ShuffleMapStage 8 (save at JdbcWriter.java:87) finished in 0.045 s
	24/05/07 08:51:07 INFO DAGScheduler: looking for newly runnable stages
	24/05/07 08:51:07 INFO DAGScheduler: running: Set()
	24/05/07 08:51:07 INFO DAGScheduler: waiting: Set()
	24/05/07 08:51:07 INFO DAGScheduler: failed: Set()
	24/05/07 08:51:07 INFO CodeGenerator: Code generated in 12.699599 ms
	24/05/07 08:51:07 INFO SparkContext: Starting job: save at JdbcWriter.java:87
	24/05/07 08:51:07 INFO DAGScheduler: Got job 5 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:51:07 INFO DAGScheduler: Final stage: ResultStage 11 (save at JdbcWriter.java:87)
	24/05/07 08:51:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
	24/05/07 08:51:07 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:51:07 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[21] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:51:07 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 49.2 KiB, free 93.1 MiB)
	24/05/07 08:51:07 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 22.7 KiB, free 93.0 MiB)
	24/05/07 08:51:07 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 941f01f0fea3:37725 (size: 22.7 KiB, free: 93.2 MiB)
	24/05/07 08:51:07 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:51:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[21] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:51:07 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
	24/05/07 08:51:07 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 5) (941f01f0fea3, executor driver, partition 0, NODE_LOCAL, 8043 bytes) 
	24/05/07 08:51:07 INFO Executor: Running task 0.0 in stage 11.0 (TID 5)
	24/05/07 08:51:07 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
	24/05/07 08:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
	24/05/07 08:51:07 INFO CodeGenerator: Code generated in 10.288328 ms
	24/05/07 08:51:07 INFO CodeGenerator: Code generated in 17.610943 ms
[WI-0][TI-0] - [INFO] 2024-05-07 08:51:08.285 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:51:07 ERROR Executor: Exception in task 0.0 in stage 11.0 (TID 5)
	java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_task_statistics_value ("process_definition_id","task_instance_id","rule_id","unique_code","statistics_name","statistics_value","data_time","create_time","update_time") VALUES (('0'::int4),('3951'::int4),('6'::int4),('KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG='),('duplicate_count.duplicates'),('1'::int8),('2024-05-07 08:50:50'),('2024-05-07 08:50:50'),('2024-05-07 08:50:50')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 235  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 235
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	24/05/07 08:51:07 WARN TaskSetManager: Lost task 0.0 in stage 11.0 (TID 5) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_task_statistics_value ("process_definition_id","task_instance_id","rule_id","unique_code","statistics_name","statistics_value","data_time","create_time","update_time") VALUES (('0'::int4),('3951'::int4),('6'::int4),('KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG='),('duplicate_count.duplicates'),('1'::int8),('2024-05-07 08:50:50'),('2024-05-07 08:50:50'),('2024-05-07 08:50:50')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 235  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 235
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	24/05/07 08:51:07 ERROR TaskSetManager: Task 0 in stage 11.0 failed 1 times; aborting job
	24/05/07 08:51:07 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
	24/05/07 08:51:07 INFO TaskSchedulerImpl: Cancelling stage 11
	24/05/07 08:51:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 5) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_task_statistics_value ("process_definition_id","task_instance_id","rule_id","unique_code","statistics_name","statistics_value","data_time","create_time","update_time") VALUES (('0'::int4),('3951'::int4),('6'::int4),('KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG='),('duplicate_count.duplicates'),('1'::int8),('2024-05-07 08:50:50'),('2024-05-07 08:50:50'),('2024-05-07 08:50:50')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 235  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 235
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	Driver stacktrace:
	24/05/07 08:51:07 INFO DAGScheduler: ResultStage 11 (save at JdbcWriter.java:87) failed in 0.181 s due to Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 5) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_task_statistics_value ("process_definition_id","task_instance_id","rule_id","unique_code","statistics_name","statistics_value","data_time","create_time","update_time") VALUES (('0'::int4),('3951'::int4),('6'::int4),('KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG='),('duplicate_count.duplicates'),('1'::int8),('2024-05-07 08:50:50'),('2024-05-07 08:50:50'),('2024-05-07 08:50:50')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 235  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 235
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	Driver stacktrace:
	24/05/07 08:51:07 INFO DAGScheduler: Job 5 failed: save at JdbcWriter.java:87, took 0.195996 s
	Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 5) (941f01f0fea3 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_task_statistics_value ("process_definition_id","task_instance_id","rule_id","unique_code","statistics_name","statistics_value","data_time","create_time","update_time") VALUES (('0'::int4),('3951'::int4),('6'::int4),('KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG='),('duplicate_count.duplicates'),('1'::int8),('2024-05-07 08:50:50'),('2024-05-07 08:50:50'),('2024-05-07 08:50:50')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 235  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 235
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	
	Driver stacktrace:
		at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
		at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
		at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
		at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
		at scala.Option.foreach(Option.scala:407)
		at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
		at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)
		at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
		at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
		at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
		at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)
		at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3514)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4309)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
		at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4307)
		at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3514)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
		at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
		at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
		at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
		at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
		at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
		at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
		at org.apache.dolphinscheduler.data.quality.flow.batch.writer.JdbcWriter.write(JdbcWriter.java:87)
		at org.apache.dolphinscheduler.data.quality.execution.SparkBatchExecution.executeWriter(SparkBatchExecution.java:132)
		at org.apache.dolphinscheduler.data.quality.execution.SparkBatchExecution.execute(SparkBatchExecution.java:58)
		at org.apache.dolphinscheduler.data.quality.context.DataQualityContext.execute(DataQualityContext.java:62)
		at org.apache.dolphinscheduler.data.quality.DataQualityApplication.main(DataQualityApplication.java:78)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
		at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)
		at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
		at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
		at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
		at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
		at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
		at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
	Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO dolphinscheduler.t_ds_dq_task_statistics_value ("process_definition_id","task_instance_id","rule_id","unique_code","statistics_name","statistics_value","data_time","create_time","update_time") VALUES (('0'::int4),('3951'::int4),('6'::int4),('KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG='),('duplicate_count.duplicates'),('1'::int8),('2024-05-07 08:50:50'),('2024-05-07 08:50:50'),('2024-05-07 08:50:50')) was aborted: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 235  Call getNextException to see other errors in the batch.
		at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
		at org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2413)
		at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:579)
		at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:912)
		at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:936)
		at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1733)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
		at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
		at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
		at org.apache.spark.scheduler.Task.run(Task.scala:141)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: org.postgresql.util.PSQLException: ERROR: column "create_time" is of type timestamp without time zone but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 235
		at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
		at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
		... 21 more
	24/05/07 08:51:07 INFO SparkContext: Invoking stop() from shutdown hook
	24/05/07 08:51:07 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/05/07 08:51:07 INFO SparkUI: Stopped Spark web UI at http://941f01f0fea3:4040
	24/05/07 08:51:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/05/07 08:51:07 INFO MemoryStore: MemoryStore cleared
	24/05/07 08:51:07 INFO BlockManager: BlockManager stopped
	24/05/07 08:51:07 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/05/07 08:51:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/05/07 08:51:07 INFO SparkContext: Successfully stopped SparkContext
	24/05/07 08:51:07 INFO ShutdownHookManager: Shutdown hook called
	24/05/07 08:51:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-c3addbff-1ab8-405e-899a-c44cbf746b67
	24/05/07 08:51:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-1e10e5e9-9719-4538-ad1c-3b2fb4861e6b
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:51:08.288 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1058/3951, processId:1558 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:51:08.288 +0800 o.a.d.p.t.a.u.LogUtils:[79] - Start finding appId in /opt/dolphinscheduler/logs/20240507/13505298546272/21/1058/3951.log, fetch way: log 
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:51:08.290 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:51:08.290 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:51:08.290 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:51:08.291 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:51:08.303 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:51:08.304 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:51:08.304 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1058/3951
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:51:08.304 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1058/3951
[WI-1058][TI-3951] - [INFO] 2024-05-07 08:51:08.305 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3951] - [INFO] 2024-05-07 08:51:09.028 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3951, processInstanceId=1058, status=6, startTime=1715043050066, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1058/3951.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1058/3951, endTime=1715043068290, processId=1558, appIds=, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3951] - [INFO] 2024-05-07 08:51:09.031 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3951, processInstanceId=1058, status=6, startTime=1715043050066, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1058/3951.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1058/3951, endTime=1715043068290, processId=1558, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043069027)
[WI-0][TI-0] - [INFO] 2024-05-07 08:51:13.292 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7528089887640449 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:51:14.335 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8222811671087533 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:51:15.338 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8205128205128206 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [WARN] 2024-05-07 08:51:18.144 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[147] - [id: 0x9b5d4ccc, L:/172.18.1.1:1234 - R:/172.18.0.13:48102] is not writable, over high water level : 65536
[WI-0][TI-0] - [WARN] 2024-05-07 08:51:18.145 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[154] - [id: 0x9b5d4ccc, L:/172.18.1.1:1234 - R:/172.18.0.13:48102] is writable, to low water : 32768
[WI-0][TI-3949] - [INFO] 2024-05-07 08:51:21.051 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3949, processInstanceId=1056, status=6, startTime=1715042461574, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1056/3949.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1056/3949, endTime=1715042480010, processId=1190, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042780491)
[WI-0][TI-3949] - [INFO] 2024-05-07 08:51:21.055 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3949, processInstanceId=1056, status=6, startTime=1715042461574, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1056/3949.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1056/3949, endTime=1715042480010, processId=1190, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043081051)
[WI-0][TI-3947] - [INFO] 2024-05-07 08:52:15.131 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3947, processInstanceId=1054, status=6, startTime=1715041914072, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1054/3947.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1054/3947, endTime=1715041931317, processId=835, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042834687)
[WI-0][TI-3947] - [INFO] 2024-05-07 08:52:15.135 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3947, processInstanceId=1054, status=6, startTime=1715041914072, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1054/3947.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1054/3947, endTime=1715041931317, processId=835, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043135131)
[WI-0][TI-3950] - [INFO] 2024-05-07 08:52:20.140 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3950, processInstanceId=1057, status=6, startTime=1715042517875, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1057/3950.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1057/3950, endTime=1715042538297, processId=1323, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042839702)
[WI-0][TI-3950] - [INFO] 2024-05-07 08:52:20.144 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3950, processInstanceId=1057, status=6, startTime=1715042517875, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1057/3950.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1057/3950, endTime=1715042538297, processId=1323, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043140140)
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:05.378 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[55] - Receive TaskInstanceKillRequest: TaskInstanceKillRequest(taskInstanceId=3951)
[WI-0][TI-3951] - [ERROR] 2024-05-07 08:53:05.379 +0800 o.a.d.s.w.r.o.TaskInstanceKillOperationFunction:[62] - Cannot find WorkerTaskExecutor for taskInstance: 3951
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:11.706 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7226890756302521 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:12.728 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3952, taskName=[null check] filtered data persistence, firstSubmitTime=1715043192711, startTime=0, taskType=DATA_QUALITY, workflowInstanceHost=172.18.0.11:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13505298546272, processDefineVersion=21, appIds=null, processInstanceId=1059, scheduleTime=0, globalParams=null, executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13001194483488, taskParams={"localParams":[],"resourceList":[],"ruleId":6,"ruleInputParameter":{"check_type":"0","comparison_type":1,"comparison_name":"0","failure_strategy":"0","operator":"0","src_connector_type":1,"src_datasource_id":6,"src_database":"persistence","src_field":"content","src_table":"filtered_data_persistence","threshold":"0"},"sparkParameters":{"deployMode":"local","driverCores":1,"driverMemory":"512M","executorCores":1,"executorMemory":"1G","numExecutors":1,"others":"--conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n--packages org.postgresql:postgresql:42.7.3","yarnQueue":""}}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='[null check] filtered data persistence'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13001194483488'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='1059'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240507'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240506'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3952'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='qualti_test'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13505264408800'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13505298546272'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240507085312'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=org.apache.dolphinscheduler.plugin.task.api.DataQualityTaskExecutionContext@3fe06938, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.730 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: [null check] filtered data persistence to wait queue success
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.731 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.735 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.736 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.736 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.736 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1715043192736
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.737 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 1059_3952
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.737 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3952,
  "taskName" : "[null check] filtered data persistence",
  "firstSubmitTime" : 1715043192711,
  "startTime" : 1715043192736,
  "taskType" : "DATA_QUALITY",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240507/13505298546272/21/1059/3952.log",
  "processId" : 0,
  "processDefineCode" : 13505298546272,
  "processDefineVersion" : 21,
  "processInstanceId" : 1059,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13001194483488,
  "taskParams" : "{\"localParams\":[],\"resourceList\":[],\"ruleId\":6,\"ruleInputParameter\":{\"check_type\":\"0\",\"comparison_type\":1,\"comparison_name\":\"0\",\"failure_strategy\":\"0\",\"operator\":\"0\",\"src_connector_type\":1,\"src_datasource_id\":6,\"src_database\":\"persistence\",\"src_field\":\"content\",\"src_table\":\"filtered_data_persistence\",\"threshold\":\"0\"},\"sparkParameters\":{\"deployMode\":\"local\",\"driverCores\":1,\"driverMemory\":\"512M\",\"executorCores\":1,\"executorMemory\":\"1G\",\"numExecutors\":1,\"others\":\"--conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n--packages org.postgresql:postgresql:42.7.3\",\"yarnQueue\":\"\"}}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "[null check] filtered data persistence"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13001194483488"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1059"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240507"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240506"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3952"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "qualti_test"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13505264408800"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13505298546272"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240507085312"
    }
  },
  "taskAppId" : "1059_3952",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "dataQualityTaskExecutionContext" : {
    "ruleId" : 6,
    "ruleName" : "$t(uniqueness_check)",
    "ruleType" : 0,
    "ruleInputEntryList" : "[{\"id\":1,\"field\":\"src_connector_type\",\"type\":\"select\",\"title\":\"$t(src_connector_type)\",\"data\":\"\",\"options\":\"[{\\\"label\\\":\\\"HIVE\\\",\\\"value\\\":\\\"HIVE\\\"},{\\\"label\\\":\\\"JDBC\\\",\\\"value\\\":\\\"JDBC\\\"}]\",\"placeholder\":\"please select source connector type\",\"optionSourceType\":2,\"dataType\":2,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":1,\"createTime\":null,\"updateTime\":null},{\"id\":2,\"field\":\"src_datasource_id\",\"type\":\"select\",\"title\":\"$t(src_datasource_id)\",\"data\":\"\",\"options\":null,\"placeholder\":\"please select source datasource id\",\"optionSourceType\":1,\"dataType\":2,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":2,\"createTime\":null,\"updateTime\":null},{\"id\":30,\"field\":\"src_database\",\"type\":\"select\",\"title\":\"$t(src_database)\",\"data\":null,\"options\":null,\"placeholder\":\"Please select source database\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":true,\"valuesMap\":null,\"index\":2,\"createTime\":null,\"updateTime\":null},{\"id\":3,\"field\":\"src_table\",\"type\":\"select\",\"title\":\"$t(src_table)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter source table name\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":true,\"valuesMap\":null,\"index\":3,\"createTime\":null,\"updateTime\":null},{\"id\":4,\"field\":\"src_filter\",\"type\":\"input\",\"title\":\"$t(src_filter)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter filter expression\",\"optionSourceType\":0,\"dataType\":3,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":4,\"createTime\":null,\"updateTime\":null},{\"id\":5,\"field\":\"src_field\",\"type\":\"select\",\"title\":\"$t(src_field)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter column, only single column is supported\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":null,\"index\":5,\"createTime\":null,\"updateTime\":null},{\"id\":6,\"field\":\"statistics_name\",\"type\":\"input\",\"title\":\"$t(statistics_name)\",\"data\":\"duplicate_count.duplicates\",\"options\":null,\"placeholder\":\"Please enter statistics name, the alias in statistics execute sql\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":1,\"isShow\":false,\"canEdit\":false,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":\"{\\\"statistics_name\\\":\\\"duplicate_count.duplicates\\\"}\",\"index\":6,\"createTime\":null,\"updateTime\":null},{\"id\":7,\"field\":\"check_type\",\"type\":\"select\",\"title\":\"$t(check_type)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"Expected - Actual\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"Actual - Expected\\\",\\\"value\\\":\\\"1\\\"},{\\\"label\\\":\\\"Actual / Expected\\\",\\\"value\\\":\\\"2\\\"},{\\\"label\\\":\\\"(Expected - Actual) / Expected\\\",\\\"value\\\":\\\"3\\\"}]\",\"placeholder\":\"please select check type\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":7,\"createTime\":null,\"updateTime\":null},{\"id\":8,\"field\":\"operator\",\"type\":\"select\",\"title\":\"$t(operator)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"=\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"<\\\",\\\"value\\\":\\\"1\\\"},{\\\"label\\\":\\\"<=\\\",\\\"value\\\":\\\"2\\\"},{\\\"label\\\":\\\">\\\",\\\"value\\\":\\\"3\\\"},{\\\"label\\\":\\\">=\\\",\\\"value\\\":\\\"4\\\"},{\\\"label\\\":\\\"!=\\\",\\\"value\\\":\\\"5\\\"}]\",\"placeholder\":\"please select operator\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":8,\"createTime\":null,\"updateTime\":null},{\"id\":9,\"field\":\"threshold\",\"type\":\"input\",\"title\":\"$t(threshold)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter threshold, number is needed\",\"optionSourceType\":0,\"dataType\":2,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":null,\"index\":9,\"createTime\":null,\"updateTime\":null},{\"id\":10,\"field\":\"failure_strategy\",\"type\":\"select\",\"title\":\"$t(failure_strategy)\",\"data\":\"0\",\"options\":\"[{\\\"label\\\":\\\"Alert\\\",\\\"value\\\":\\\"0\\\"},{\\\"label\\\":\\\"Block\\\",\\\"value\\\":\\\"1\\\"}]\",\"placeholder\":\"please select failure strategy\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":3,\"isShow\":true,\"canEdit\":true,\"isEmit\":false,\"isValidate\":false,\"valuesMap\":null,\"index\":10,\"createTime\":null,\"updateTime\":null},{\"id\":17,\"field\":\"comparison_name\",\"type\":\"input\",\"title\":\"$t(comparison_name)\",\"data\":null,\"options\":null,\"placeholder\":\"Please enter comparison name, the alias in comparison execute sql\",\"optionSourceType\":0,\"dataType\":0,\"inputType\":0,\"isShow\":false,\"canEdit\":false,\"isEmit\":false,\"isValidate\":true,\"valuesMap\":\"\",\"index\":11,\"createTime\":null,\"updateTime\":null},{\"id\":19,\"field\":\"comparison_type\",\"type\":\"select\",\"title\":\"$t(comparison_type)\",\"data\":\"\",\"options\":null,\"placeholder\":\"Please enter comparison title\",\"optionSourceType\":3,\"dataType\":0,\"inputType\":2,\"isShow\":true,\"canEdit\":false,\"isEmit\":true,\"isValidate\":false,\"valuesMap\":null,\"index\":12,\"createTime\":null,\"updateTime\":null}]",
    "executeSqlList" : "[{\"id\":6,\"index\":1,\"sql\":\"SELECT ${src_field} FROM ${src_table} group by ${src_field} having count(*) > 1\",\"tableAlias\":\"duplicate_items\",\"type\":0,\"createTime\":\"2021-03-03 11:31:24\",\"updateTime\":\"2021-03-03 11:31:24\",\"errorOutputSql\":true},{\"id\":7,\"index\":1,\"sql\":\"SELECT COUNT(*) AS duplicates FROM duplicate_items\",\"tableAlias\":\"duplicate_count\",\"type\":1,\"createTime\":\"2021-03-03 11:31:24\",\"updateTime\":\"2021-03-03 11:31:24\",\"errorOutputSql\":false}]",
    "comparisonNeedStatisticsValueTable" : false,
    "compareWithFixedValue" : true,
    "hdfsPath" : "hdfs://mycluster:8020/user/default/data_quality_error_data",
    "sourceConnectorType" : "JDBC",
    "sourceType" : 1,
    "sourceConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"persistence\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence\",\"driverClassName\":\"org.postgresql.Driver\",\"validationQuery\":\"select version()\",\"other\":{\"password\":\"root\"}}",
    "targetConnectorType" : null,
    "targetType" : 0,
    "targetConnectionParams" : null,
    "writerConnectorType" : "JDBC",
    "writerType" : 1,
    "writerTable" : "t_ds_dq_execute_result",
    "writerConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"dolphinscheduler\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\"}",
    "statisticsValueConnectorType" : "JDBC",
    "statisticsValueType" : 1,
    "statisticsValueTable" : "t_ds_dq_task_statistics_value",
    "statisticsValueWriterConnectionParams" : "{\"user\":\"root\",\"password\":\"root\",\"address\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432\",\"database\":\"dolphinscheduler\",\"jdbcUrl\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\"}"
  },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.739 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.739 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.740 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.757 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.758 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.758 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1059/3952 check successfully
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.759 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.dq.DataQualityTaskChannel successfully
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.760 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.760 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.761 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: DATA_QUALITY create successfully
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.764 +0800 o.a.d.p.t.d.DataQualityTask:[89] - Initialize data quality task params {
  "localParams" : [ ],
  "varPool" : null,
  "ruleId" : 6,
  "ruleInputParameter" : {
    "check_type" : "0",
    "comparison_type" : "1",
    "comparison_name" : "0",
    "failure_strategy" : "0",
    "operator" : "0",
    "src_connector_type" : "1",
    "src_datasource_id" : "6",
    "src_database" : "persistence",
    "src_field" : "content",
    "src_table" : "filtered_data_persistence",
    "threshold" : "0"
  },
  "sparkParameters" : {
    "localParams" : null,
    "varPool" : null,
    "mainJar" : null,
    "mainClass" : null,
    "deployMode" : "local",
    "mainArgs" : null,
    "driverCores" : 1,
    "driverMemory" : "512M",
    "numExecutors" : 1,
    "executorCores" : 1,
    "executorMemory" : "1G",
    "appName" : null,
    "yarnQueue" : "",
    "others" : "--conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n--packages org.postgresql:postgresql:42.7.3",
    "programType" : null,
    "resourceList" : [ ]
  }
}
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.768 +0800 o.a.d.p.t.d.DataQualityTask:[119] - data quality configuration: {
  "name" : "$t(uniqueness_check)",
  "env" : {
    "type" : "batch",
    "config" : null
  },
  "readers" : [ {
    "type" : "JDBC",
    "config" : {
      "database" : "persistence",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "output_table" : "persistence_filtered_data_persistence",
      "table" : "filtered_data_persistence",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence?password=root"
    }
  } ],
  "transformers" : [ {
    "type" : "sql",
    "config" : {
      "index" : 1,
      "output_table" : "duplicate_items",
      "sql" : "SELECT content FROM persistence_filtered_data_persistence group by content having count(*) > 1"
    }
  }, {
    "type" : "sql",
    "config" : {
      "index" : 2,
      "output_table" : "duplicate_count",
      "sql" : "SELECT COUNT(*) AS duplicates FROM duplicate_items"
    }
  } ],
  "writers" : [ {
    "type" : "JDBC",
    "config" : {
      "database" : "dolphinscheduler",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "table" : "t_ds_dq_execute_result",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler",
      "sql" : "select 0 as rule_type,'$t(uniqueness_check)' as rule_name,0 as process_definition_id,1059 as process_instance_id,3952 as task_instance_id,duplicate_count.duplicates AS statistics_value,0 AS comparison_value,1 AS comparison_type,0 as check_type,0 as threshold,0 as operator,0 as failure_strategy,'hdfs://mycluster:8020/user/default/data_quality_error_data/0_1059_[null check] filtered data persistence' as error_output_path,'2024-05-07 08:53:12' as create_time,'2024-05-07 08:53:12' as update_time from duplicate_count "
    }
  }, {
    "type" : "JDBC",
    "config" : {
      "database" : "dolphinscheduler",
      "password" : "root",
      "driver" : "org.postgresql.Driver",
      "user" : "root",
      "table" : "t_ds_dq_task_statistics_value",
      "url" : "jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler",
      "sql" : "select 0 as process_definition_id,3952 as task_instance_id,6 as rule_id,'KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG=' as unique_code,'duplicate_count.duplicates'AS statistics_name,duplicate_count.duplicates AS statistics_value,'2024-05-07 08:53:12' as data_time,'2024-05-07 08:53:12' as create_time,'2024-05-07 08:53:12' as update_time from duplicate_count"
    }
  }, {
    "type" : "hdfs_file",
    "config" : {
      "path" : "hdfs://mycluster:8020/user/default/data_quality_error_data/0_1059_[null check] filtered data persistence",
      "input_table" : "duplicate_items"
    }
  } ]
}
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.769 +0800 o.a.d.p.d.a.u.CommonUtils:[136] - Trying to get data quality jar in path
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.769 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.769 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.769 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.769 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.769 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.770 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.770 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.770 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
${SPARK_HOME}/bin/spark-submit --master local --driver-cores 1 --driver-memory 512M --num-executors 1 --executor-cores 1 --executor-memory 1G --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
--packages org.postgresql:postgresql:42.7.3 /opt/dolphinscheduler/conf/../libs/dolphinscheduler-data-quality-3.2.1.jar "{\"name\":\"$t(uniqueness_check)\",\"env\":{\"type\":\"batch\",\"config\":null},\"readers\":[{\"type\":\"JDBC\",\"config\":{\"database\":\"persistence\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"output_table\":\"persistence_filtered_data_persistence\",\"table\":\"filtered_data_persistence\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/persistence?password=root\"} }],\"transformers\":[{\"type\":\"sql\",\"config\":{\"index\":1,\"output_table\":\"duplicate_items\",\"sql\":\"SELECT content FROM persistence_filtered_data_persistence group by content having count(*) > 1\"} },{\"type\":\"sql\",\"config\":{\"index\":2,\"output_table\":\"duplicate_count\",\"sql\":\"SELECT COUNT(*) AS duplicates FROM duplicate_items\"} }],\"writers\":[{\"type\":\"JDBC\",\"config\":{\"database\":\"dolphinscheduler\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"table\":\"t_ds_dq_execute_result\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\",\"sql\":\"select 0 as rule_type,'$t(uniqueness_check)' as rule_name,0 as process_definition_id,1059 as process_instance_id,3952 as task_instance_id,duplicate_count.duplicates AS statistics_value,0 AS comparison_value,1 AS comparison_type,0 as check_type,0 as threshold,0 as operator,0 as failure_strategy,'hdfs://mycluster:8020/user/default/data_quality_error_data/0_1059_[null check] filtered data persistence' as error_output_path,'2024-05-07 08:53:12' as create_time,'2024-05-07 08:53:12' as update_time from duplicate_count \"} },{\"type\":\"JDBC\",\"config\":{\"database\":\"dolphinscheduler\",\"password\":\"root\",\"driver\":\"org.postgresql.Driver\",\"user\":\"root\",\"table\":\"t_ds_dq_task_statistics_value\",\"url\":\"jdbc:postgresql://dolphinscheduler-postgresql:5432/dolphinscheduler\",\"sql\":\"select 0 as process_definition_id,3952 as task_instance_id,6 as rule_id,'KA0AXXWVHWW+GZX4BPZXFGFZ0F0OYMUQ+BVYG4+ZIKG=' as unique_code,'duplicate_count.duplicates'AS statistics_name,duplicate_count.duplicates AS statistics_value,'2024-05-07 08:53:12' as data_time,'2024-05-07 08:53:12' as create_time,'2024-05-07 08:53:12' as update_time from duplicate_count\"} },{\"type\":\"hdfs_file\",\"config\":{\"path\":\"hdfs://mycluster:8020/user/default/data_quality_error_data/0_1059_[null check] filtered data persistence\",\"input_table\":\"duplicate_items\"} }]}"
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.773 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.774 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1059/3952/1059_3952.sh
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:12.782 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 1711
[WI-0][TI-3952] - [INFO] 2024-05-07 08:53:13.339 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3952, processInstanceId=1059, startTime=1715043192736, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1059/3952.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3952] - [INFO] 2024-05-07 08:53:13.347 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3952, processInstanceId=1059, startTime=1715043192736, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1059/3952.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1715043193338)
[WI-0][TI-3952] - [INFO] 2024-05-07 08:53:13.350 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3952, processInstanceId=1059, startTime=1715043192736, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3952] - [INFO] 2024-05-07 08:53:13.372 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3952, processInstanceId=1059, startTime=1715043192736, workflowInstanceHost=172.18.0.11:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1715043193338)
[WI-0][TI-3952] - [INFO] 2024-05-07 08:53:13.390 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3952, success=true)
[WI-0][TI-3952] - [INFO] 2024-05-07 08:53:13.405 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3952)
[WI-0][TI-3952] - [INFO] 2024-05-07 08:53:13.414 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3952, success=true)
[WI-0][TI-3952] - [INFO] 2024-05-07 08:53:13.427 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3952)
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:13.761 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7146814404432132 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:13.785 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:14.806 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8982558139534884 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:15.807 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.postgresql#postgresql added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-74b7620c-7c1e-4d4b-a781-a231b1e55c23;1.0
		confs: [default]
		found org.postgresql#postgresql;42.7.3 in central
		found org.checkerframework#checker-qual;3.42.0 in central
	:: resolution report :: resolve 162ms :: artifacts dl 5ms
		:: modules in use:
		org.checkerframework#checker-qual;3.42.0 from central in [default]
		org.postgresql#postgresql;42.7.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-74b7620c-7c1e-4d4b-a781-a231b1e55c23
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:15.809 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7976653696498055 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:16.808 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		0 artifacts copied, 2 already retrieved (0kB/7ms)
	24/05/07 08:53:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:16.810 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9237804878048781 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:17.814 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7966804979253113 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:17.814 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:53:17 INFO SparkContext: Running Spark version 3.5.1
	24/05/07 08:53:17 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/05/07 08:53:17 INFO SparkContext: Java version 1.8.0_402
	24/05/07 08:53:17 INFO ResourceUtils: ==============================================================
	24/05/07 08:53:17 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/05/07 08:53:17 INFO ResourceUtils: ==============================================================
	24/05/07 08:53:17 INFO SparkContext: Submitted application: (uniqueness_check)
	24/05/07 08:53:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/05/07 08:53:17 INFO ResourceProfile: Limiting resource is cpu
	24/05/07 08:53:17 INFO ResourceProfileManager: Added ResourceProfile id: 0
	24/05/07 08:53:17 INFO SecurityManager: Changing view acls to: default
	24/05/07 08:53:17 INFO SecurityManager: Changing modify acls to: default
	24/05/07 08:53:17 INFO SecurityManager: Changing view acls groups to: 
	24/05/07 08:53:17 INFO SecurityManager: Changing modify acls groups to: 
	24/05/07 08:53:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:18.821 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7272727272727273 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:18.821 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:53:17 INFO Utils: Successfully started service 'sparkDriver' on port 33347.
	24/05/07 08:53:18 INFO SparkEnv: Registering MapOutputTracker
	24/05/07 08:53:18 INFO SparkEnv: Registering BlockManagerMaster
	24/05/07 08:53:18 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/05/07 08:53:18 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/05/07 08:53:18 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/05/07 08:53:18 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-61a63595-3e6c-4dd1-a16f-61638b5ca0ab
	24/05/07 08:53:18 INFO MemoryStore: MemoryStore started with capacity 93.3 MiB
	24/05/07 08:53:18 INFO SparkEnv: Registering OutputCommitCoordinator
	24/05/07 08:53:18 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/05/07 08:53:18 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/05/07 08:53:18 INFO SparkContext: Added JAR file:///tmp/jars/org.postgresql_postgresql-42.7.3.jar at spark://941f01f0fea3:33347/jars/org.postgresql_postgresql-42.7.3.jar with timestamp 1715043197356
	24/05/07 08:53:18 INFO SparkContext: Added JAR file:///tmp/jars/org.checkerframework_checker-qual-3.42.0.jar at spark://941f01f0fea3:33347/jars/org.checkerframework_checker-qual-3.42.0.jar with timestamp 1715043197356
	24/05/07 08:53:18 INFO SparkContext: Added JAR file:/opt/dolphinscheduler/libs/dolphinscheduler-data-quality-3.2.1.jar at spark://941f01f0fea3:33347/jars/dolphinscheduler-data-quality-3.2.1.jar with timestamp 1715043197356
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:19.827 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:53:18 INFO Executor: Starting executor ID driver on host 941f01f0fea3
	24/05/07 08:53:18 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/05/07 08:53:18 INFO Executor: Java version 1.8.0_402
	24/05/07 08:53:18 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/05/07 08:53:18 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@350d3f4d for default.
	24/05/07 08:53:18 INFO Executor: Fetching spark://941f01f0fea3:33347/jars/org.checkerframework_checker-qual-3.42.0.jar with timestamp 1715043197356
	24/05/07 08:53:19 INFO TransportClientFactory: Successfully created connection to 941f01f0fea3/172.18.1.1:33347 after 59 ms (0 ms spent in bootstraps)
	24/05/07 08:53:19 INFO Utils: Fetching spark://941f01f0fea3:33347/jars/org.checkerframework_checker-qual-3.42.0.jar to /tmp/spark-96bb6405-a05c-4c0f-ba6f-323279a7f6ff/userFiles-b75e4710-b4b3-4453-9ebe-a693bccb0c0d/fetchFileTemp7424694496991074420.tmp
	24/05/07 08:53:19 INFO Executor: Adding file:/tmp/spark-96bb6405-a05c-4c0f-ba6f-323279a7f6ff/userFiles-b75e4710-b4b3-4453-9ebe-a693bccb0c0d/org.checkerframework_checker-qual-3.42.0.jar to class loader default
	24/05/07 08:53:19 INFO Executor: Fetching spark://941f01f0fea3:33347/jars/dolphinscheduler-data-quality-3.2.1.jar with timestamp 1715043197356
	24/05/07 08:53:19 INFO Utils: Fetching spark://941f01f0fea3:33347/jars/dolphinscheduler-data-quality-3.2.1.jar to /tmp/spark-96bb6405-a05c-4c0f-ba6f-323279a7f6ff/userFiles-b75e4710-b4b3-4453-9ebe-a693bccb0c0d/fetchFileTemp6868550244943677805.tmp
	24/05/07 08:53:19 INFO Executor: Adding file:/tmp/spark-96bb6405-a05c-4c0f-ba6f-323279a7f6ff/userFiles-b75e4710-b4b3-4453-9ebe-a693bccb0c0d/dolphinscheduler-data-quality-3.2.1.jar to class loader default
	24/05/07 08:53:19 INFO Executor: Fetching spark://941f01f0fea3:33347/jars/org.postgresql_postgresql-42.7.3.jar with timestamp 1715043197356
	24/05/07 08:53:19 INFO Utils: Fetching spark://941f01f0fea3:33347/jars/org.postgresql_postgresql-42.7.3.jar to /tmp/spark-96bb6405-a05c-4c0f-ba6f-323279a7f6ff/userFiles-b75e4710-b4b3-4453-9ebe-a693bccb0c0d/fetchFileTemp906795205829076066.tmp
	24/05/07 08:53:19 INFO Executor: Adding file:/tmp/spark-96bb6405-a05c-4c0f-ba6f-323279a7f6ff/userFiles-b75e4710-b4b3-4453-9ebe-a693bccb0c0d/org.postgresql_postgresql-42.7.3.jar to class loader default
	24/05/07 08:53:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39917.
	24/05/07 08:53:19 INFO NettyBlockTransferService: Server created on 941f01f0fea3:39917
	24/05/07 08:53:19 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/05/07 08:53:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 941f01f0fea3, 39917, None)
	24/05/07 08:53:19 INFO BlockManagerMasterEndpoint: Registering block manager 941f01f0fea3:39917 with 93.3 MiB RAM, BlockManagerId(driver, 941f01f0fea3, 39917, None)
	24/05/07 08:53:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 941f01f0fea3, 39917, None)
	24/05/07 08:53:19 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 941f01f0fea3, 39917, None)
	24/05/07 08:53:19 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/05/07 08:53:19 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1059/3952/spark-warehouse'.
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:20.835 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8575418994413407 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:21.857 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8794520547945206 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:22.993 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7597533233259568 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:25.039 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9571045576407506 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:25.989 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:53:25 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:26.060 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9627659574468085 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:27.067 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.847682119205298 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:27.992 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:53:27 INFO CodeGenerator: Code generated in 448.418398 ms
	24/05/07 08:53:27 INFO DAGScheduler: Registering RDD 2 (save at JdbcWriter.java:87) as input to shuffle 0
	24/05/07 08:53:27 INFO DAGScheduler: Got map stage job 0 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:53:27 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (save at JdbcWriter.java:87)
	24/05/07 08:53:27 INFO DAGScheduler: Parents of final stage: List()
	24/05/07 08:53:27 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:53:27 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:53:27 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 34.8 KiB, free 93.3 MiB)
	24/05/07 08:53:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 93.2 MiB)
	24/05/07 08:53:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 941f01f0fea3:39917 (size: 16.4 KiB, free: 93.3 MiB)
	24/05/07 08:53:27 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:53:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:53:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:28.075 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7882439701544335 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:28.994 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:53:28 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (941f01f0fea3, executor driver, partition 0, PROCESS_LOCAL, 7878 bytes) 
	24/05/07 08:53:28 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
	24/05/07 08:53:28 INFO CodeGenerator: Code generated in 66.784927 ms
	24/05/07 08:53:28 INFO CodeGenerator: Code generated in 20.442927 ms
	24/05/07 08:53:28 INFO CodeGenerator: Code generated in 6.453061 ms
	24/05/07 08:53:28 INFO CodeGenerator: Code generated in 14.505664 ms
	24/05/07 08:53:28 INFO CodeGenerator: Code generated in 9.862024 ms
	24/05/07 08:53:28 INFO JDBCRDD: closed connection
	24/05/07 08:53:28 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2534 bytes result sent to driver
	24/05/07 08:53:28 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 795 ms on 941f01f0fea3 (executor driver) (1/1)
	24/05/07 08:53:28 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
	24/05/07 08:53:28 INFO DAGScheduler: ShuffleMapStage 0 (save at JdbcWriter.java:87) finished in 1.132 s
	24/05/07 08:53:28 INFO DAGScheduler: looking for newly runnable stages
	24/05/07 08:53:28 INFO DAGScheduler: running: Set()
	24/05/07 08:53:28 INFO DAGScheduler: waiting: Set()
	24/05/07 08:53:28 INFO DAGScheduler: failed: Set()
	24/05/07 08:53:28 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:29.076 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8311688311688311 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:29.997 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:53:29 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
	24/05/07 08:53:29 INFO CodeGenerator: Code generated in 20.494941 ms
	24/05/07 08:53:29 INFO DAGScheduler: Registering RDD 5 (save at JdbcWriter.java:87) as input to shuffle 1
	24/05/07 08:53:29 INFO DAGScheduler: Got map stage job 1 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:53:29 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (save at JdbcWriter.java:87)
	24/05/07 08:53:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
	24/05/07 08:53:29 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:53:29 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[5] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:53:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 40.5 KiB, free 93.2 MiB)
	24/05/07 08:53:29 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 93.2 MiB)
	24/05/07 08:53:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 941f01f0fea3:39917 (size: 19.1 KiB, free: 93.3 MiB)
	24/05/07 08:53:29 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:53:29 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[5] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:53:29 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
	24/05/07 08:53:29 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (941f01f0fea3, executor driver, partition 0, NODE_LOCAL, 8032 bytes) 
	24/05/07 08:53:29 INFO Executor: Running task 0.0 in stage 2.0 (TID 1)
	24/05/07 08:53:29 INFO ShuffleBlockFetcherIterator: Getting 1 (2.9 KiB) non-empty blocks including 1 (2.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
	24/05/07 08:53:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 29 ms
	24/05/07 08:53:29 INFO CodeGenerator: Code generated in 33.539994 ms
	24/05/07 08:53:29 INFO Executor: Finished task 0.0 in stage 2.0 (TID 1). 5420 bytes result sent to driver
	24/05/07 08:53:29 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 162 ms on 941f01f0fea3 (executor driver) (1/1)
	24/05/07 08:53:29 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
	24/05/07 08:53:29 INFO DAGScheduler: ShuffleMapStage 2 (save at JdbcWriter.java:87) finished in 0.200 s
	24/05/07 08:53:29 INFO DAGScheduler: looking for newly runnable stages
	24/05/07 08:53:29 INFO DAGScheduler: running: Set()
	24/05/07 08:53:29 INFO DAGScheduler: waiting: Set()
	24/05/07 08:53:29 INFO DAGScheduler: failed: Set()
	24/05/07 08:53:29 INFO CodeGenerator: Code generated in 48.681756 ms
	24/05/07 08:53:29 INFO SparkContext: Starting job: save at JdbcWriter.java:87
	24/05/07 08:53:29 INFO DAGScheduler: Got job 2 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:53:29 INFO DAGScheduler: Final stage: ResultStage 5 (save at JdbcWriter.java:87)
	24/05/07 08:53:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
	24/05/07 08:53:29 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:53:29 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[10] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:53:29 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 941f01f0fea3:39917 in memory (size: 16.4 KiB, free: 93.3 MiB)
	24/05/07 08:53:29 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 51.4 KiB, free 93.2 MiB)
	24/05/07 08:53:29 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.0 KiB, free 93.2 MiB)
	24/05/07 08:53:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 941f01f0fea3:39917 (size: 23.0 KiB, free: 93.3 MiB)
	24/05/07 08:53:29 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:53:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[10] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:53:29 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
	24/05/07 08:53:29 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 2) (941f01f0fea3, executor driver, partition 0, NODE_LOCAL, 8043 bytes) 
	24/05/07 08:53:29 INFO Executor: Running task 0.0 in stage 5.0 (TID 2)
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:30.083 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8017751479289941 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:31.005 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:53:30 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 941f01f0fea3:39917 in memory (size: 19.1 KiB, free: 93.3 MiB)
	24/05/07 08:53:30 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
	24/05/07 08:53:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
	24/05/07 08:53:30 INFO CodeGenerator: Code generated in 12.529914 ms
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:32.018 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:53:31 INFO CodeGenerator: Code generated in 26.23715 ms
	24/05/07 08:53:31 INFO Executor: Finished task 0.0 in stage 5.0 (TID 2). 6711 bytes result sent to driver
	24/05/07 08:53:31 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 2) in 1647 ms on 941f01f0fea3 (executor driver) (1/1)
	24/05/07 08:53:31 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
	24/05/07 08:53:31 INFO DAGScheduler: ResultStage 5 (save at JdbcWriter.java:87) finished in 1.890 s
	24/05/07 08:53:31 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
	24/05/07 08:53:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
	24/05/07 08:53:31 INFO DAGScheduler: Job 2 finished: save at JdbcWriter.java:87, took 1.941663 s
	24/05/07 08:53:31 INFO DAGScheduler: Registering RDD 13 (save at JdbcWriter.java:87) as input to shuffle 2
	24/05/07 08:53:31 INFO DAGScheduler: Got map stage job 3 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:53:31 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (save at JdbcWriter.java:87)
	24/05/07 08:53:31 INFO DAGScheduler: Parents of final stage: List()
	24/05/07 08:53:31 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:53:31 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[13] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:53:31 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 34.8 KiB, free 93.2 MiB)
	24/05/07 08:53:31 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 16.5 KiB, free 93.2 MiB)
	24/05/07 08:53:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 941f01f0fea3:39917 (size: 16.5 KiB, free: 93.3 MiB)
	24/05/07 08:53:31 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:53:31 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[13] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:53:31 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
	24/05/07 08:53:31 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 3) (941f01f0fea3, executor driver, partition 0, PROCESS_LOCAL, 7878 bytes) 
	24/05/07 08:53:31 INFO Executor: Running task 0.0 in stage 6.0 (TID 3)
	24/05/07 08:53:31 INFO JDBCRDD: closed connection
	24/05/07 08:53:31 INFO Executor: Finished task 0.0 in stage 6.0 (TID 3). 2448 bytes result sent to driver
	24/05/07 08:53:31 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 3) in 125 ms on 941f01f0fea3 (executor driver) (1/1)
	24/05/07 08:53:31 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
	24/05/07 08:53:31 INFO DAGScheduler: ShuffleMapStage 6 (save at JdbcWriter.java:87) finished in 0.152 s
	24/05/07 08:53:31 INFO DAGScheduler: looking for newly runnable stages
	24/05/07 08:53:31 INFO DAGScheduler: running: Set()
	24/05/07 08:53:31 INFO DAGScheduler: waiting: Set()
	24/05/07 08:53:31 INFO DAGScheduler: failed: Set()
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:32.101 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7548746518105849 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:33.021 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/07 08:53:32 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
	24/05/07 08:53:32 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
	24/05/07 08:53:32 INFO DAGScheduler: Registering RDD 16 (save at JdbcWriter.java:87) as input to shuffle 3
	24/05/07 08:53:32 INFO DAGScheduler: Got map stage job 4 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:53:32 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (save at JdbcWriter.java:87)
	24/05/07 08:53:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
	24/05/07 08:53:32 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:53:32 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[16] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:53:32 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 40.6 KiB, free 93.1 MiB)
	24/05/07 08:53:32 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 93.1 MiB)
	24/05/07 08:53:32 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 941f01f0fea3:39917 (size: 19.1 KiB, free: 93.2 MiB)
	24/05/07 08:53:32 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:53:32 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[16] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:53:32 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
	24/05/07 08:53:32 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 4) (941f01f0fea3, executor driver, partition 0, NODE_LOCAL, 8032 bytes) 
	24/05/07 08:53:32 INFO Executor: Running task 0.0 in stage 8.0 (TID 4)
	24/05/07 08:53:32 INFO ShuffleBlockFetcherIterator: Getting 1 (2.9 KiB) non-empty blocks including 1 (2.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
	24/05/07 08:53:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
	24/05/07 08:53:32 INFO Executor: Finished task 0.0 in stage 8.0 (TID 4). 5420 bytes result sent to driver
	24/05/07 08:53:32 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 4) in 28 ms on 941f01f0fea3 (executor driver) (1/1)
	24/05/07 08:53:32 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
	24/05/07 08:53:32 INFO DAGScheduler: ShuffleMapStage 8 (save at JdbcWriter.java:87) finished in 0.044 s
	24/05/07 08:53:32 INFO DAGScheduler: looking for newly runnable stages
	24/05/07 08:53:32 INFO DAGScheduler: running: Set()
	24/05/07 08:53:32 INFO DAGScheduler: waiting: Set()
	24/05/07 08:53:32 INFO DAGScheduler: failed: Set()
	24/05/07 08:53:32 INFO CodeGenerator: Code generated in 7.423017 ms
	24/05/07 08:53:32 INFO SparkContext: Starting job: save at JdbcWriter.java:87
	24/05/07 08:53:32 INFO DAGScheduler: Got job 5 (save at JdbcWriter.java:87) with 1 output partitions
	24/05/07 08:53:32 INFO DAGScheduler: Final stage: ResultStage 11 (save at JdbcWriter.java:87)
	24/05/07 08:53:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
	24/05/07 08:53:32 INFO DAGScheduler: Missing parents: List()
	24/05/07 08:53:32 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[21] at save at JdbcWriter.java:87), which has no missing parents
	24/05/07 08:53:32 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 49.2 KiB, free 93.1 MiB)
	24/05/07 08:53:32 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 22.7 KiB, free 93.0 MiB)
	24/05/07 08:53:32 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 941f01f0fea3:39917 (size: 22.7 KiB, free: 93.2 MiB)
	24/05/07 08:53:32 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
	24/05/07 08:53:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[21] at save at JdbcWriter.java:87) (first 15 tasks are for partitions Vector(0))
	24/05/07 08:53:32 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
	24/05/07 08:53:32 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 5) (941f01f0fea3, executor driver, partition 0, NODE_LOCAL, 8043 bytes) 
	24/05/07 08:53:32 INFO Executor: Running task 0.0 in stage 11.0 (TID 5)
	24/05/07 08:53:32 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
	24/05/07 08:53:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
	24/05/07 08:53:32 INFO CodeGenerator: Code generated in 15.985662 ms
	24/05/07 08:53:32 INFO CodeGenerator: Code generated in 16.709131 ms
	24/05/07 08:53:32 INFO Executor: Finished task 0.0 in stage 11.0 (TID 5). 6625 bytes result sent to driver
	24/05/07 08:53:32 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 5) in 94 ms on 941f01f0fea3 (executor driver) (1/1)
	24/05/07 08:53:32 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
	24/05/07 08:53:32 INFO DAGScheduler: ResultStage 11 (save at JdbcWriter.java:87) finished in 0.156 s
	24/05/07 08:53:32 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
	24/05/07 08:53:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
	24/05/07 08:53:32 INFO DAGScheduler: Job 5 finished: save at JdbcWriter.java:87, took 0.176140 s
	24/05/07 08:53:32 WARN FileSystem: Failed to initialize fileystem hdfs://mycluster:8020/user/default/data_quality_error_data/0_1059_%5Bnull%20check%5D%20filtered%20data%20persistence: java.lang.IllegalArgumentException: java.net.UnknownHostException: mycluster
	Exception in thread "main" java.lang.IllegalArgumentException: java.net.UnknownHostException: mycluster
		at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:466)
		at org.apache.hadoop.hdfs.NameNodeProxiesClient.createProxyWithClientProtocol(NameNodeProxiesClient.java:134)
		at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:374)
		at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308)
		at org.apache.hadoop.hdfs.DistributedFileSystem.initDFSClient(DistributedFileSystem.java:202)
		at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:187)
		at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)
		at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
		at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
		at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
		at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
		at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
		at org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:454)
		at org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:530)
		at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
		at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
		at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)
		at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)
		at org.apache.dolphinscheduler.data.quality.flow.batch.writer.file.BaseFileWriter.outputImpl(BaseFileWriter.java:114)
		at org.apache.dolphinscheduler.data.quality.flow.batch.writer.file.HdfsFileWriter.write(HdfsFileWriter.java:40)
		at org.apache.dolphinscheduler.data.quality.execution.SparkBatchExecution.executeWriter(SparkBatchExecution.java:132)
		at org.apache.dolphinscheduler.data.quality.execution.SparkBatchExecution.execute(SparkBatchExecution.java:58)
		at org.apache.dolphinscheduler.data.quality.context.DataQualityContext.execute(DataQualityContext.java:62)
		at org.apache.dolphinscheduler.data.quality.DataQualityApplication.main(DataQualityApplication.java:78)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
		at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)
		at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
		at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
		at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
		at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
		at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
		at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
	Caused by: java.net.UnknownHostException: mycluster
		... 36 more
	24/05/07 08:53:32 INFO SparkContext: Invoking stop() from shutdown hook
	24/05/07 08:53:32 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/05/07 08:53:32 INFO SparkUI: Stopped Spark web UI at http://941f01f0fea3:4040
	24/05/07 08:53:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/05/07 08:53:32 INFO MemoryStore: MemoryStore cleared
	24/05/07 08:53:32 INFO BlockManager: BlockManager stopped
	24/05/07 08:53:32 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/05/07 08:53:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/05/07 08:53:32 INFO SparkContext: Successfully stopped SparkContext
	24/05/07 08:53:32 INFO ShutdownHookManager: Shutdown hook called
	24/05/07 08:53:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-96bb6405-a05c-4c0f-ba6f-323279a7f6ff
	24/05/07 08:53:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-23854e52-7e6d-4aab-b123-c7249eff0d2a
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:33.026 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1059/3952, processId:1711 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:33.026 +0800 o.a.d.p.t.a.u.LogUtils:[79] - Start finding appId in /opt/dolphinscheduler/logs/20240507/13505298546272/21/1059/3952.log, fetch way: log 
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:33.030 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:33.031 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:33.031 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:33.032 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:33.036 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:33.036 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:33.036 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1059/3952
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:33.037 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1059/3952
[WI-1059][TI-3952] - [INFO] 2024-05-07 08:53:33.037 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3952] - [INFO] 2024-05-07 08:53:33.458 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3952, processInstanceId=1059, status=6, startTime=1715043192736, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1059/3952.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1059/3952, endTime=1715043213032, processId=1711, appIds=, varPool=[], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3952] - [INFO] 2024-05-07 08:53:33.463 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3952, processInstanceId=1059, status=6, startTime=1715043192736, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1059/3952.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1059/3952, endTime=1715043213032, processId=1711, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043213457)
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:40.152 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7550432276657061 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-07 08:53:41.174 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7883008356545962 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [WARN] 2024-05-07 08:53:42.989 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[147] - [id: 0x9b5d4ccc, L:/172.18.1.1:1234 - R:/172.18.0.13:48102] is not writable, over high water level : 65536
[WI-0][TI-0] - [WARN] 2024-05-07 08:53:42.990 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[154] - [id: 0x9b5d4ccc, L:/172.18.1.1:1234 - R:/172.18.0.13:48102] is writable, to low water : 32768
[WI-0][TI-3944] - [INFO] 2024-05-07 08:54:09.523 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3944, processInstanceId=1051, status=6, startTime=1715041077174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1051/3944.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1051/3944, endTime=1715041109126, processId=316, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042948948)
[WI-0][TI-3944] - [INFO] 2024-05-07 08:54:09.527 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3944, processInstanceId=1051, status=6, startTime=1715041077174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1051/3944.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1051/3944, endTime=1715041109126, processId=316, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043249523)
[WI-0][TI-3948] - [INFO] 2024-05-07 08:54:20.590 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3948, processInstanceId=1055, status=6, startTime=1715042336910, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1055/3948.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1055/3948, endTime=1715042357115, processId=1041, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715042959981)
[WI-0][TI-3948] - [INFO] 2024-05-07 08:54:20.601 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3948, processInstanceId=1055, status=6, startTime=1715042336910, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1055/3948.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1055/3948, endTime=1715042357115, processId=1041, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043260590)
[WI-0][TI-3946] - [INFO] 2024-05-07 08:56:04.703 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3946, processInstanceId=1053, status=9, startTime=1715041547086, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1053/3946.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1053/3946, endTime=1715041560344, processId=628, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043064015)
[WI-0][TI-3946] - [INFO] 2024-05-07 08:56:04.713 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3946, processInstanceId=1053, status=9, startTime=1715041547086, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1053/3946.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1053/3946, endTime=1715041560344, processId=628, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043364703)
[WI-0][TI-3945] - [INFO] 2024-05-07 08:56:07.718 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3945, processInstanceId=1052, status=6, startTime=1715041538811, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1052/3945.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1052/3945, endTime=1715041563133, processId=534, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043067022)
[WI-0][TI-3945] - [INFO] 2024-05-07 08:56:07.727 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3945, processInstanceId=1052, status=6, startTime=1715041538811, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1052/3945.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1052/3945, endTime=1715041563133, processId=534, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043367718)
[WI-0][TI-3951] - [INFO] 2024-05-07 08:56:09.731 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3951, processInstanceId=1058, status=6, startTime=1715043050066, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1058/3951.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1058/3951, endTime=1715043068290, processId=1558, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043069027)
[WI-0][TI-3951] - [INFO] 2024-05-07 08:56:09.737 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3951, processInstanceId=1058, status=6, startTime=1715043050066, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1058/3951.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1058/3951, endTime=1715043068290, processId=1558, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043369731)
[WI-0][TI-3949] - [INFO] 2024-05-07 08:56:21.750 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3949, processInstanceId=1056, status=6, startTime=1715042461574, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1056/3949.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1056/3949, endTime=1715042480010, processId=1190, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043081051)
[WI-0][TI-3949] - [INFO] 2024-05-07 08:56:21.769 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3949, processInstanceId=1056, status=6, startTime=1715042461574, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1056/3949.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1056/3949, endTime=1715042480010, processId=1190, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043381750)
[WI-0][TI-0] - [WARN] 2024-05-07 08:56:38.870 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[147] - [id: 0x9b5d4ccc, L:/172.18.1.1:1234 - R:/172.18.0.13:48102] is not writable, over high water level : 65536
[WI-0][TI-0] - [WARN] 2024-05-07 08:56:38.870 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[154] - [id: 0x9b5d4ccc, L:/172.18.1.1:1234 - R:/172.18.0.13:48102] is writable, to low water : 32768
[WI-0][TI-3947] - [INFO] 2024-05-07 08:57:15.841 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3947, processInstanceId=1054, status=6, startTime=1715041914072, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1054/3947.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1054/3947, endTime=1715041931317, processId=835, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043135131)
[WI-0][TI-3947] - [INFO] 2024-05-07 08:57:15.844 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3947, processInstanceId=1054, status=6, startTime=1715041914072, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1054/3947.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1054/3947, endTime=1715041931317, processId=835, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043435841)
[WI-0][TI-3950] - [INFO] 2024-05-07 08:57:20.848 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3950, processInstanceId=1057, status=6, startTime=1715042517875, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1057/3950.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1057/3950, endTime=1715042538297, processId=1323, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043140140)
[WI-0][TI-3950] - [INFO] 2024-05-07 08:57:20.855 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3950, processInstanceId=1057, status=6, startTime=1715042517875, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1057/3950.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1057/3950, endTime=1715042538297, processId=1323, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043440847)
[WI-0][TI-0] - [WARN] 2024-05-07 08:57:53.386 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[147] - [id: 0x9b5d4ccc, L:/172.18.1.1:1234 - R:/172.18.0.13:48102] is not writable, over high water level : 65536
[WI-0][TI-0] - [WARN] 2024-05-07 08:57:53.387 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[154] - [id: 0x9b5d4ccc, L:/172.18.1.1:1234 - R:/172.18.0.13:48102] is writable, to low water : 32768
[WI-0][TI-0] - [WARN] 2024-05-07 08:57:54.847 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[147] - [id: 0x9b5d4ccc, L:/172.18.1.1:1234 - R:/172.18.0.13:48102] is not writable, over high water level : 65536
[WI-0][TI-0] - [WARN] 2024-05-07 08:57:54.849 +0800 o.a.d.e.b.s.JdkDynamicServerHandler:[154] - [id: 0x9b5d4ccc, L:/172.18.1.1:1234 - R:/172.18.0.13:48102] is writable, to low water : 32768
[WI-0][TI-3952] - [INFO] 2024-05-07 08:58:33.993 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3952, processInstanceId=1059, status=6, startTime=1715043192736, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1059/3952.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1059/3952, endTime=1715043213032, processId=1711, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043213457)
[WI-0][TI-3952] - [INFO] 2024-05-07 08:58:33.998 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3952, processInstanceId=1059, status=6, startTime=1715043192736, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1059/3952.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1059/3952, endTime=1715043213032, processId=1711, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043513993)
[WI-0][TI-3944] - [INFO] 2024-05-07 08:59:10.055 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3944, processInstanceId=1051, status=6, startTime=1715041077174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1051/3944.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1051/3944, endTime=1715041109126, processId=316, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043249523)
[WI-0][TI-3944] - [INFO] 2024-05-07 08:59:10.059 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3944, processInstanceId=1051, status=6, startTime=1715041077174, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1051/3944.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1051/3944, endTime=1715041109126, processId=316, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043550055)
[WI-0][TI-3948] - [INFO] 2024-05-07 08:59:21.085 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3948, processInstanceId=1055, status=6, startTime=1715042336910, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1055/3948.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1055/3948, endTime=1715042357115, processId=1041, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043260590)
[WI-0][TI-3948] - [INFO] 2024-05-07 08:59:21.092 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=3948, processInstanceId=1055, status=6, startTime=1715042336910, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.11:5678, logPath=/opt/dolphinscheduler/logs/20240507/13505298546272/21/1055/3948.log, executePath=/tmp/dolphinscheduler/exec/process/default/13001194483488/13505298546272_21/1055/3948, endTime=1715042357115, processId=1041, appIds=, varPool=[], eventCreateTime=0, eventSendTime=1715043561085)
